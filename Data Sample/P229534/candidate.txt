as with any application of machine learning, web search ranking requires labeled data. the labels usually come in the form of relevance assessments made by editors. click logs can also provide an important source of implicit feedback and can be used as a cheap proxy for editorial labels. the main difficulty however comes from the so called position bias urls appearing in lower positions are less likely to be clicked even if they are relevant. in this paper, we propose a dynamic bayesian network which aims at providing us with unbiased estimation of the relevance from the click logs. experiments show that the proposed click model outperforms other existing click models in predicting both click through rate and relevance. web page ranking has been traditionally based on hand designed ranking functions such as bm. with the inclusion of thousands of features for ranking, hand tuning of ranking function becomes intractable. several machine learning algorithms have been applied to automatically optimize ranking functions. machine learned ranking requires a large number of training examples, with relevance labels indicating the degree of relevance for each querydocument pair. the cost of the editorial labeling is usually quite expensive. moreover, the relevance labels of the training examples could change over time. for example, if the query is time sensitive or recurrent, a search engine is expected to return the copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. most up to date documents sites to the users. however, it would be prohibitive to keep all the relevance labels up to date. click logs embed important information about user satisfaction with a search engine and can provide a highly valuable source of relevance information. compare to editorial labels, clicks are much cheaper to obtain and always re ect current relevance. clicks have been used in multiple ways by a search engine: to tune search parameters, to evaluate di erent ranking functions, or as signals to directly in uence ranking. however, clicks are known to be biased, by the presentation order, the appearance of the documents, and the reputation of individual sites. many studies have attempted to account the position bias of click. carterette and jones proposed to model the relationship between clicks and relevance so that clicks can be used to unbiasedly evaluate search engine when lack of editorial relevance judgment. other research attempted to model user click behavior during search so that future clicks may be accurately predicted based on observations of past clicks. two di erent types of the click models are position models and the cascade model. a position model assumes that a click depends on both relevance and examination. each rank has a certain probability of being examined, which decays by rank and depends only on rank. a click on a url indicates that the url is examined and considered relevant by the user. however this model treats the individual urls in a search result page independently and fails to capture the interaction among urls in the examination probability. take for example two equally relevant urls for a query: a user may only click on the top one, feel satis ed, and then leave the search result page. in this case, the positional bias cannot fully explain the lack of clicks for the second url. the cascade model assumes that users examine the results sequentially and stop as soon as a relevant document is clicked. here, the probability of examination is indirectly determined by two factors: the rank of the url and the relevance of all previous urls. the cascade model makes a strong assumption that there is only one click per search and hence it could not explain the abandoned search or search with more than one clicks. even though the cascade model is quite restrictive, the authors of that paper showed that we refer to url as a shorthand for the entire display block consisting of the title, abstract and url of the corresponding result. it can predict click through rates more accurately than the position models described above. none of the above models distinguish perceived relevance and actual relevance. because users cannot examine the content of a document until they click on the url, the decision to click is made based on perceived relevance. while there is a strong correlation between perceived relevance and actual relevance, there are also many cases where they di er. in this paper, a dynamic bayesian network model is proposed to model the users browsing behavior. as in the position model, we assume that a click occurs if and only if the user has examined the url and deemed it relevant. similar to the cascade model, our model assumes that users make a linear transversal through the results and decide whether to click based on the perceived relevance of the document. the user chooses to examine the next url if he she is unsatis ed with the clicked url. our model di ers from the cascade model in two aspects: because a click does not necessarily mean that the user is satis ed with the clicked document, we attempt to distinguish the perceived relevance and actual relevance. while numerous metrics for information retrieval are available in the case of binary relevance, there is only one commonly used metric for graded relevance, namely the discounted cumulative gain. a drawback of dcg is its additive nature and the underlying independence assumption: a document in a given position has always the same gain and discount independently of the documents shown above it. inspired by the cascade user model, we present a new editorial metric for graded relevance which overcomes this difficulty and implicitly discounts documents which are shown below very relevant documents. more precisely, this new metric is defined as the expected reciprocal length of time that the user will take to find a relevant document. this can be seen as an extension of the classical reciprocal rank to the graded relevance case and we call this metric expected reciprocal rank. we conduct an extensive evaluation on the query logs of a commercial search engine and show that err correlates better with clicks metrics than other editorial metrics. interleaving is an increasingly popular technique for evaluating information retrieval systems based on implicit user feedback. while a number of isolated studies have analyzed how this technique agrees with conventional offline evaluation approaches and other online techniques, a complete picture of its efficiency and effectiveness is still lacking. in particular, we analyze the agreement of interleaving with manual relevance judgments and observational implicit feedback measures, estimate the statistical efficiency of interleaving, and explore the relative performance of different interleaving variants. we also show how to learn improved credit assignment functions for clicks that further increase the sensitivity of interleaving. in this paper we extend and combine the body of empirical evidence regarding interleaving, and provide a comprehensive analysis of interleaving using data from two major commercial search engines and a retrieval system for scientific literature. while the conventional approach of using expert judgments has proven itself effective in many respects, it has at least two limitations. as such, more exible and ef cient evaluation methods are required especially for applications with resource constraints including desktop search, personalized web search, intranet search, helpdesk support, and academic literature search. permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies show this notice on the rst page or initial screen of a display along with the full citation. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior speci. permissions may be requested from publications dept, acm, inc, penn plaza, suite, new york, ny year# usa, fax, or permissions acm org. year# acm year# year# art doi year# http: doi acm org year#. the basic idea behind all variants of the interleaving approach is to perform paired online comparisons of two rankings. the algorithm used to produce the interleaved ranking is designed to be fair, so that users clicks can be interpreted as unbiased judgments about the relative quality of the two rankings. in this way, interleaving interactively modi es, in a controlled experiment, the search results presented to the user so that the observed user behavior is more interpretable. this avoids the problem of post hoc interpretation of observational data common to most other approaches to interpreting implicit feedback. speci cally, this paper reviews, analyzes, and extends the balanced interleaving and team draft interleaving methods. after introducing the two interleaving methods and the systems used for evaluation in the next two sections, we validate and analyze interleaved evaluation by answering a series of speci. finally, we address the limitations of interleaved evaluation as compared to other approaches in depth. in particular, we would like to acknowledge the contributions of madhu kurup, nick craswell, and yue gao and ya zhang. proper evaluation of search quality is essential for developing effective information retrieval systems. we aim to provide a comprehensive body of evidence regarding its effectiveness, accuracy, and limitations. first, expert judgments may not re ect the actual relevance and utility that users experience while using a retrieval system, especially since judges often cannot reliably estimate the users intents. second, its associated cost and turnaround times are substantial and often prohibitive. this research was funded in part through nsf award iis and through a gift from yahoo authoraddress: chapelle; email: chap yahoo inc com. chapelle et al these limitations have motivated research on alternative approaches to retrieval evaluation, especially approaches based on observable user behavior such as clicks, query reformulations, and response times. unlike expert judgments, usage data can be collected at essentially zero cost, is available in real time, and re ects the judgments of the users rather than those of judges far removed from the users context at the time of the information need. the key problem with retrieval evaluation based on usage data lies in its proper interpretation, in particular understanding how certain observable statistics relate to retrieval quality. in this article, we analyze the interleaving approach to solving this key problem. this involves merging the two rankings into a single interleaved ranking, and then presenting the interleaved ranking to the user. the analysis relies on results from a new large scale eld study on the yahoo search engine, tied into published and additional unpublished data from experiments on the bing search engine, and the fulltext search of the arxiv org repository of scienti. we ask whether interleaving agrees with the conventional evaluation approach based on relevance judgments collected from experts; whether it agrees with other online metrics; how the statistical sensitivity and reliability of these different alternatives compares; and how to select among different credit assignment schemes for clicks. to provide a complete picture of interleaving as an evaluation technique, this paper combines new data and experiments with data and results from past collaborations with other authors. a result page of a modern web search engine is often much more complicated than a simple list of ten blue links. in particular, a search engine may combine results from different sources, and display these as grouped results to provide a better user experience. such a system is called an aggregated or federated search system. because search engines evolve over time, their results need to be constantly evaluated. however, one of the most efficient and widely used evaluation methods, interleaving, cannot be directly applied to aggregated search systems, as it ignores the need to group results originating from the same source. we propose an interleaving algorithm that allows comparisons of search engine result pages containing grouped vertical documents. we compare our algorithm to existing interleaving algorithms and other evaluation methods, both on real life click log data and in simulation experiments. we find that our algorithm allows us to perform unbiased and accurate interleaved comparisons that are comparable to conventional evaluation techniques. we also show that our interleaving algorithm produces a ranking that does not substantially alter the user experience, while being sensitive to changes in both the vertical result block and the non vertical document rankings. all this makes our proposed interleaving algorithm an essential tool for comparing ir systems with complex aggregated pages. in a result page returned by a modern search system some results look di erent and may be more visually attractive than others. moreover, results from di erent subcollections are usually grouped to improve the search result browsing experience. if the vertical results are grouped, we call such group a vertical block. in figure # we provide a schematic picture of two document lists containing vertical blocks; the vertical block occupies positions to in ranking a and positions to in ranking. ranking branking ad d figure #: two rankings with a vertical block present. vertical documents are shown as dotted lines and also marked with. there is an ecient way of comparing two rankings called interleaving: it produces an interleaved ranked list out of rankings a and, shows it to the user and then infers user preferences from their clicks. that is, those interleaving methods do not respect the grouping of vertical results. as was found by dumais et al, this can signi cantly alter the user experi ence, which violates one of the core principles of user based evaluations formulated by joachims. the main research questions we address in this paper are: can we perform interleaving measurements in a way that respects grouping of the vertical results and does not change the user experience how does the quality of the interleaved list compare to the quality of the aggregated result pages being interleaved. how does an interleaving algorithm that respects grouping compare to state of the art interleaving algorithms and ab testing to which extent do they agree. does a comparison based on a vertical aware interleaving method represent a fair and unbiased comparison, or does it erroneously infer any preference for a randomly clicking user can it capture quality di erences between rankings as well as conventional interleaving methods, while preserving vertical blocks the main contributions of this paper are answers to these questions, which includes a proposed vertical aware interleaving method and the corresponding experiments. section # is dedi cated to the design of an interleaving method that respects grouping of vertical results. sections and describe our ex periments to compare the proposed and non vertical aware interleaving algorithms. in section # we sketch and discuss additional possibilities for designing vertical aware interleaving algorithms, after which we conclude in section #. however, if we want to interleave ranked lists from figure # using balanced, team draft or probabilistic interleaving, we may end up in a situation where the resulting interleaved rankinghas vertical documents mixed with regular documents. in recent years many models have been proposed that are aimed at predicting clicks of web search users. in addition, some information retrieval evaluation metrics have been built on top of a user model. in this paper we bring these two directions together and propose a common approach to converting any click model into an evaluation metric. we then put the resulting model based metrics as well as traditional metrics into a common evaluation framework and compare them along a number of dimensions. one of the dimensions we are particularly interested in is the agreement between offline and online experimental outcomes. it is widely believed, especially in an industrial setting, that online atesting and interleaving experiments are generally better at capturing system quality than offline measurements. we show that offline metrics that are based on click models are more strongly correlated with online experimental outcomes than traditional offline metrics, especially in situations when we have incomplete relevance judgements. there are currently two orthogonal approaches to evaluating the quality of ranking systems. the rst approach is usually called the cran eld approach and is done. xed set of queries and documents judged by permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than acm or the author must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. ranking systems are then evaluated by comparing how good their ranked lists are among other things, a system is expected to place relevant documents higher than irrelevant ones. another approach described by kohavi et al makes use of real online users by assigning some portion of the users to test groups. the simplest variant, called atesting, randomly assigns some users to the control group and the treatment group. ranking systems are then compared by analysing the clicks of the users in the control against those in the treatment group. in the interleaving method by joachims users are presented with a combined list made out of two rankings. then the system that receives more clicks is assumed to be better. one of the main advantages of online evaluation schemes is that they are user based and, as a result, often assumed to give us more realistic insights into the real system quality. interleaving experiments are now widely being used by large commercial search engines like bing and yahoo as well as studied in academia. ine measurements, whereas in the traditional cran eld approach one can re use the same set of judged documents to evaluate any ranking. ine editor based evaluation methods unavoidable during the early development phase of ranking algorithms. one should take care, however, that the resulting editor based measurements agree with the outcomes of online experiments online comparison is often used as the nal validation step before releasing a new version of a ranking algorithm. in order to bring the two evaluation approaches closer to each other, we propose a method for building an. ine information retrieval metric from a user click model. click models, probabilistic models of the behavior of web search users, have been studied extensively by the ir community during the last ve years. the main purpose of predicting clicks, as seen in previous works, is: modeling user behavior when real users are not available; improving ranking using relevance inferred from clicks. we hypothesize that click models can also be turned into. ine metrics and the resulting click modelbased metrics should be closely tied to the user and hence should better correlate with online measurements than traditional. in addition, there is a growing trend to ground. ine metrics in a user model and that is exactly copyright year# acm year#. what click modeling does trying to propose a better user model. so, the question is why not use better user models, based on click behavior, as the basis for. ine metrics we put our proposal for transforming click models into metrics to the test through a set of thorough comparisons with online measurements. our comparison includes an analysis of correlations with the outcomes of interleaving experiments, an analysis of correlations with absolute online metrics, an analysis of correlations between traditional. ine metrics and our new click model based metrics, as well as an analysis of the discriminative power of the various metrics. one dimension to which we devote special attention in our comparison framework concerns unjudged documents. as was shown by buckley and voorhees, having partially judged result pages in the evaluation pool may result in biased measurements. we also show that in situations when we cannot. ord to use only fully judged data, we can still make good use of the available data by making adjustments, by either a technique called condensation or a new threshold method that we propose. the main research questions that we address in this work are: how do click model based ir metrics compare to the traditional. ine ir metrics agree with online experiments do click model based metrics show higher agreement how well do di erent. ine metrics perform in the pres ence of unjudged documents how can we modify. ine metrics to enhance agreement with online experiments our main contributions in this paper are a method for converting click models into click model based. click model based metrics with online measurements and traditional. the rest of the paper is organized as follows. section # shows how to transform a click model into a model based. in section # we examine click model based and traditional. we nish with a conclusion and discussion in section #. secondly, we present a thorough analysis and comparison of speci. search engine click logs provide an invaluable source of relevance information but this information is biased because we ignore which documents from the result list the users have actually seen before and after they clicked. otherwise, we could estimate document relevance by simple counting. in this paper, we propose a set of assumptions on user browsing behavior that allows the estimation of the probability that a document is seen, thereby providing an unbiased estimate of document relevance. our solution outperforms very significantly all previous models. they also explain why documents situated just after a very relevant document are clicked more often. to train, test and compare our model to the best alternatives described in the literature, we gather a large set of real data and proceed to an extensive cross validation experiment. as a side effect, we gain insight into the browsing behavior of users and we can compare it to the conclusions of an eye tracking experiments by joachims et al. in particular, our findings confirm that a user almost always see the document directly after a clicked document. users permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bearthisnoticeand thefull citationonthe rstpage tocopy otherwise, to republish, topost on servers or to redistribute tolists, requiresprior speci. benjamin piwowarski yahoo research latin america bpiwowar yahoo inc com areincreasingly understood tobethedrivingforceof theinternet and many initiatives are aimed at empowering them. social search, as its name implies, supposes participation from users who tag, bookmark, andcomment their search results. in addition to this information explicitly provided by users, there is a much larger source of implicit data which is collectedby search engines. itis apoll of millions of users over an enormous variety of topics. examples of applicationsincludewebpersonalization, web spam detection, query term recommendation. unlike human tags and bookmarks, implicit feedback is also not biased towards socially active web users. that is, the data is collected from all users, notjust usersthat choosetoedit a wikipage, orjoin a social network such asmyspace orfriendster. click data seems the perfect source of information when deciding whichdocuments to showin answerto a query. this information can be fed back into the engine, to tune search parameters or even used asdirect evidencetoin uence ranking. nevertheless, they cannot be used without further processing: a fundamental problem is the position bias. the probability of a document being clicked depends not only on its relevance, but on other factors as its position in the result page. contributions user activity models within web search can be broadly divided in three categories: analysis models where the aim istogaininsightintotypical userbehavior, modelsthat try topredictthenext useraction, and eventually models that estimate the attractiveness or perceived relevance of a document independently of the layout in uence. this work focusses on thelatter, usingas the only source ofinformation the web search logs produced by the search engines. yet users do not browse the whole list and documents situated earlier in the rankinghave ahigherprobability ofbeing examined. as a consequence, they also have a higher probability of being clicked independently of how relevant they are. if we could estimatetheprobability that adocumentis examined by the user, we could estimate its relevance as the ratio of the number of times a user clicked on the document to the expected number of times the document is examined. the main contribution of this work is a model of user browsing behavior when consulting a page of search results. this model estimates the probability of examination of a documentgiven the rankofthedocument andthedistance to the last clicked document. our model sheds light on user behavior, is in agreement with the user experiments ofgranka et al and extends andquanti esthe user model ofjoachims et al. in section # we review the literature for click models and we present our contributions. in section # we compare the predicting abilities on unseen data of the di erent models. westudy in moredetailstheimplications of theuserbrowsing model and we relate the ndings with the eye tracking experiments of insection. social search is quickly gaining acceptance as a promising way of harnessing the common knowledge of millions of users to help each other and search more. arguably, this is a long term trend that started with kleinberg idea of hubs and authorities, which proposed that a hyperlink from one document to another was a vote in favor of the document linked to, an idea in practice exploited in the pagerank algorithm. thisfeedbackprovidesdetailed and valuable information about users interactions with the system as theissuedquery, thepresentedurls, the selected documents and their ranking. it has been used in many ways to mine user interests and preferences. it can be thought as the result of users voting in favor of the documents they nd interesting. in top results lists, the probability of observing a click decays with rank. eye tracking experiments show that a user is less likely to examine results near the bottom of the list, although click probability decays faster than examination probability so there are probably additional sources of bias. experiments also show that a document is not clicked with the same frequency if situated after a highly relevant or a mediocre document. if the users looked with attention all the documents in the ranking list, the relevance of one of them could be estimated simply by counting thenumberof timesitisselected. click models provide a principled way of understanding user interaction with web search results in a query session and a statistical tool for leveraging search engine click logs to analyze and improve user experience. usually the average user behavior pattern is summarized in a small set of global parameters. we demonstrate that better predicative power could be achieved by fitting two click models for navigational queries and informational queries respectively, as evidenced by the likelihood and perplexity evaluation results on a subset of the msn year# rfp data which consists of, distinct query terms and over million query sessions. we also propose search relevance score as a flexible evaluation metric of search engine performance. this metric can be derived as summary statistics under any click model, and is applicable to a single query session, a particular query term and the search engine overall. an important component in all existing click models is the user behavior assumption how users scan, examine and click web documents listed in the result page. can we fit multiple models with different user behavior parameters on a click data set a previous study showed that the mixture modeling approach did not lead to better performance despite extra computational cost. in this paper, we present how to tailor click models to user goals in web search through query term classification. the cranfield evaluation method has some disadvantages, including its high cost in labor and inadequacy for evaluating interactive retrieval techniques. as a very promising alternative, automatic comparison of retrieval systems based on observed clicking behavior of users has recently been studied. several methods have been proposed, but there has so far been no systematic way to assess which strategy is better, making it difficult to choose a good method for real applications. in this paper, we propose a general way to evaluate these relative comparison methods with two measures: utility to users and effectiveness of differentiation. we evaluate two state of the art methods by systematically simulating different retrieval scenarios. inspired by the weakness of these methods revealed through our evaluation, we further propose a novel method by considering the positions of clicked documents. experiment results show that our new method performs better than the existing methods. evaluation of an information retrieval system is critical for improving search techniques. so far, the dominant method for ir evaluation has been the cran eld evaluation method. however, it has some disadvantages such as high cost in labor and inadequacy for iteractive retrieval evaluation. as a promising alternative, automatic evaluation of retrieval systems based on the implicit feedback of users has recently been studied. there are two main categories of methods based on absolute metric and relative comparison test, respectively. in the methods of the rst category, it infers the absolute relevance of the retrieved documents permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. the methods of the second category attempt to compare two systems by leveraging clickthroughs. previous work showed such a relative comparison strategy is more robust against the bias in clickthroughs. although several methods have been proposed for relative comparison of retrieval systems, there has so far been no systematic way to assess which method is better, making it di cult to choose a good method for any real application. in this paper, we propose a general way to evaluate these relative comparison methods in two dimensions: utility to users, which refers to the perceived utility of the merged results from a userperspective, and. ectiveness of di erentiation, which refers to their. the utility to users of a method can be measured by applying standard retrieval measures, such as map, to the merged result list. the effectiveness of di erentiation of a method can be measured based on the accuracy of the prediction of which system is better. with these measures, we can systematically evaluate any relative comparison method using a large sample of simulated search results of two retrieval systems. simulation also allows us to systematically vary the composition of samples to simulate di erent scenarios. such variations are necessary to help understanding the relative strengths and weaknesses of di erent methods in di erent application scenarios. using the proposed evaluation method, we systematically evaluated two state of the art methods of relative comparison: balanced and team draft. the results show that: the two methods have identical utility to users the balanced method is more. ective in distinguishing retrieval systems than the team draft method in most cases; our evaluation also reveals a common de ciency of both existing methods, which inspired us to further propose a novel extension of the balanced method, called preferencebased balanced, which is shown to outperform both existing methods. information retrieval evaluation most often involves manually assessing the relevance of particular query document pairs. in cases where this is difficult, interleaved comparison methods are becoming increasingly common. these methods compare pairs of ranking functions based on user clicks on search results, thus better reflecting true user preferences. however, by depending on clicks, there is a potential for bias. for example, users have been previously shown to be more likely to click on results with attractive titles and snippets. an interleaving evaluation where one ranker tends to generate results that attract more clicks may thus be biased. we present an approach for detecting and compensating for this type of bias in interleaving evaluations. introducing a new model of caption bias, we propose features that model bias based on per document effects, and the relationships between a document and surrounding documents. we show that our model can effectively capture click behavior, with best results achieved by a model that combines both per document and pairwise features. applying this model to re weight observed user clicks, we find a small overall effect on real interleaving comparisons, but also identify a case where initially detected preferences vanish after caption bias re weighting is applied. our results indicate that our model of caption bias is effective and can successfully identify interleaving experiments affected by caption bias. most information retrieval evaluation methods rely on manual relevance assessments of query document pairs. when used to evaluate web search retrieval functions, such an evaluation assumes that assessors can reliably infer users information most of this research was done while the author was at microsoft. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. accurately making such inferences may be dif cult for ambiguous queries, and even more so in settings where relevance can vary widely by user, such as personalized or localized search. in settings where such relevance assessment is dif cult, interleaved comparison methods are a promising alternative. these methods compare two retrieval functions by presenting users with a list of results that combines documents returned by the two rankers, inferring preferences based on users clicks. because interleaving can be applied transparently to all results shown to users, and web search engines can collect click data at low cost, interleaving offers a scalable way of performing evaluation that re ects true user needs and preferences. a drawback of current interleaving methods is that they do not account for potential click bias introduced by effects of result presentation, rather assuming that this bias affects all rankers equally users click behavior is known to be in uenced by eg, highlighting, as well as url length, the occurrence of the query in result titles, and other aspects of result presentation. thus, rankers that rank results with these characteristics highly may attract more clicks than warranted by the quality of the ranking, thereby biasing interleaving outcomes. this is the problem that we address in this paper: how are interleaving outcomes affected by differences in result presentation in practice on the one hand, as previous work has assumed, caption bias may affect rankers equally. this would increase variance in evaluation but not introduce bias. on the other hand, typical ranker optimization changes may affect captions, thereby creating a systematic bias. when interleaving methods are applied to measure preferences between rankers, it is important to be able to identify both when bias may be occurring, and to be able to avoid this bias. speci cally, our contributions are: we introduce a general probabilistic approach for modeling caption bias in user clicks. online learning to rank for information retrieval holds promise for allowing the development of self learning search engines that can automatically adjust to their users. with the large amount of eg, click data that can be collected in web search settings, such techniques could enable highly scalable ranking optimization. in this paper we investigate whether and how previously collected interaction data can be used to speed up learning in online learning to rank for ir. we devise the first two methods that can utilize historical data to make feedback available during learning more reliable and to preselect candidate ranking functions to be evaluated in interactions with users of the retrieval system. we evaluate both approaches on learning to rank data sets and find that historical data can speed up learning, leading to substantially and significantly higher online performance. our results show that historical data can be used to make online learning to rank for ir much more effective than previously possible, especially when feedback is noisy. however, feedback obtained from user interactions is noisy, and developing approaches that can learn from this feedback quickly and reliably is a major challenge. in particular, our pre selection method proves highly effective at compensating for noise in user feedback. however, most current approaches work of ine, meaning that manually annotated data needs to be collected beforehand, and that, once deployed, the system cannot continue to adjust to user needs, unless it is retrained with additional data. in contrast to of ine learning to rank approaches, online approaches do not require any initial training material, but rather automatically improve rankers while they are being used. learning speed is particularly important in terms of the number of user interactions. however, a recently developed probabilistic method for inferring relative feedback has been shown to allow data re use for ranker evaluation. here, we investigate whether and how this evaluation method can be integrated with online learning to rank approaches, and whether and in what way these additional evaluations can lead to faster learning. we propose the rst two approaches for reusing historical data in online learning to rank for ir: reliable historical comparisons, which uses historical data directly to make feedback more reliable, and candidate preselection, which uses historical data to preselect candidate rankers. in extensive experiments using learning to rank data sets, we in this paper we focus on the effectiveness of the learning algorithm, and assume a system is used by a group of users with similar search behavior and preferences. investigate whether and how historical data reuse can speed up online learning to rank and lead to higher online performance. in addition to investigating our proposed methods, we analyze the effect of noise in user feedback, and the effect of bias and variance in ranker comparisons based on historical data. we nd that historical data can be effectively reused to speed up online learning to rank for ir. our results directly impact the effectiveness of online learning to rank approaches, especially in settings where feedback may be noisy. we analyze our results in § and conclude in §. in recent years, learning to rank methods have become popular in information retrieval as a means of tuning retrieval systems to the requirements of speci. or commercial advantage and that copies bear this notice and the full citation on the rst page. this was necessary because, until recently, it was not clear how feedback from previous user interactions could be reused. search environments, groups of users, or individual users. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. an alternative setting is online learning to rank, where the system learns directly from interactions with its users. these approaches are typically based on reinforcement learning techniques, meaning that the system tries out new ranking functions, and learns from feedback inferred from users interactions with the presented rankings. a main challenge that online learning to rank for ir approaches have to address is to learn as quickly as possible from the limited quality and quantity of feedback that can be inferred from user interactions. the better the systemperformance is after a smaller number of interactions, the more likely users are to be satis ed with the system. also, the more effective an online learning to rank algorithm is, the more feasible it is to adapt to smaller groups of users, or even individual users furthermore, user feedback is limited because the learning algorithm should be invisible to system users, ie, feedback is inferred from natural user interactions. in this paper, we address the following question: can previously observed interaction data be reused to speed up online learning to rank current online learning to rank approaches for ir utilize each observed data sample only once. it was found to be effective for making ranker comparisons more reliable, especially when large amounts of historical data were available. the question of how to form user groups to which to adapt is orthogonal to this work. particularly effective is the cps approach, which reuses historical data to preselect candidate rankers, and can thereby compensate for noise in user feedback. we discuss related work in §, and detail our approaches in §. our experimental setup and results are described in § and § respectively. this paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. while previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. this makes them difficult and expensive to apply. the goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query log of the search engine in connection with the log of links the users clicked on in the presented ranking. such clickthrough data is available in abundance and can be recorded at very low cost. taking a support vector machine approach, this paper presents a method for learning retrieval functions. from a theoretical perspective, this method is shown to be well founded in a risk minimization framework. furthermore, it is shown to be feasible even for large sets of queries and features. the theoretical results are verified in a controlled experiment. it shows that the method can effectively adapt the retrieval function of a meta search engine to a particular group of users, outperforming google in terms of retrieval quality after only a couple of hundred training examples. which www page does a user actually want to re trieve when he types some keywords into a search engine there are typically thousands of pages that contain these words, but the user is interested in a much smaller subset. if we knew the set of pages actually relevant to the user query, we could use this as training data for optimizing the retrieval function. unfortunately, experience shows that users are only rarely willing to give explicit feedback. however, this paper argues that sufficient information is already hidden in the logfiles of www search engines. ceive millions of queries per day, such data is available in abundance. compared to explicit feedback data, which is typically elicited in laborious user studies, any information that can be extracted from logfiles is virtually free and sub stantially more timely. this leads to a problem of learning with preference examples like for query, document, should be ranked higher than document db. more generally, will formulate the problem of learning a ranking function over a finite domain in terms of empirical risk minimization. for this formulation, will present a support vector machine algorithm that leads to a convex program and that can be extended to non linear ranking functions. it starts with a defi nition of what clickthrough data is, how it can be recorded, and how it can be used to generate training examples in the form of preferences. section # then introduces a gen eral framework for learning retrieval functions, leading to an svm algorithm for learning parameterized orderings in section #. section # evaluates the method based on experi mental results. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific pemaission and or a fee. one could simply ask the user for feedback. since major search engines re permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. this paper presents an approach to learning retrieval func tions by analyzing which links the users click on in the pre sented ranking. experi ments show that the method can successfully learn a highly effective retrieval function for a meta search engine. information retrieval effectiveness is usually evaluated using measures such as normalized discounted cumulative gain, mean average precision and precision at some cutoff on a set of judged queries. recent research has suggested an alternative, evaluating information retrieval systems based on user behavior. particularly promising are experiments that interleave two rankings and track user clicks. according to a recent study, interleaving experiments can identify large differences in retrieval effectiveness with much better reliability than other click based methods. we study interleaving in more detail, comparing it with traditional measures in terms of reliability, sensitivity and agreement. to detect very small differences in retrieval effectiveness, a reliable outcome with standard metrics requires about, judged queries, and this is about as reliable as interleaving with, user impressions. amongst the traditional measures, ndcg has the strongest correlation with interleaving. finally, we present some new forms of analysis, including an approach to enhance interleaving sensitivity. a tremendous amount of research has improved information retrieval systems over the last few decades. ective approaches mature and relative improvements become smaller, the sensitivity of evaluation metrics and their delity to actual user experience becomes increasingly critical. without sensitive measurement we might reject a small but signi cant improvement. this becomes a problem if we reject a large number of independent small improvements, because we have forgone an overall large improvement. without delity in measurement, a small change in a retrieval permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. nick craswell microsoft redmond, wa, usa nickcr microsoft com model might be taking into account some bias of relevance judges, rather than the preferences of real users. the predominant form of evaluation in information retrieval is based on test collections comprising query topics, a document corpus and human relevance judgments of topic document pairs. this allows the application of standard metrics such as ndcg, map and precision. fidelity depends on whether the test collection re ects realworld search behavior. for example, the trec web track found that changing from informational to navigational assumptions when judging can change the outcome of an evaluation. an alternate evaluation approach is based on user behavior, estimating user success by measuring click, re querying and general browsing patterns on search results. on delity, judges are usually far removed from the search process, so may generate unrealistic query topics from observed queries, and have a hard time assessing documents in a way that re ects a useractual information need. additionally, traditional measures combine document judgments to obtain a score per query, for example based on discount and gain, but these may not match real user experience. finally, judgments are slow and expensive to collect. for a system with real users, usage based evaluation is far cheaper, despite the fact that the click data collected may not be reusable in the way that most test collections are. this paper considers the reliability, sensitivity and agreement of these competing evaluation approaches. on the cran eld trec side, we consider relevance judgments for up to, queries. on the user metric side, we perform click based tests involving the interleaving of two retrieval functions over, user impressions, which we de ne as events where a user runs a query and clicks a result. using a large commercial dataset, we establish results that we believe would also hold true in an academic setting. we test sensitivity by measuring outcomes with varying numbers of queries impressions. this is done on pairs of retrieval functions with varying degrees of di erence, including one pair with a very small di erence in. we test agreement in overall outcomes between traditional measures and interleaving. we tend to nd agreement, which is an indication of the delity of the judgmentbased metric, since it is agreeing with an experiment involving real users. we then study various new ways of aggregating and analyzing the interleaving data, showing how to improve agreement with traditional metrics and also attain reliability with fewer impressions. we also show that, in contrast to judgment based metrics, interleaving can measure the fraction of users for whom a ranking change was meaningful. this allows assessment to move beyond an assumption that relevance for all users is identical, and that relevance of individual documents should be aggregated identically for all queries. sensitivity depends on the number of topics and judgments. this can be motivated on grounds of delity and cost. our results show that both approaches can be very sensitive, but judged evaluation may require thousands of judged queries to obtain the required sensitivity. in particular, we investigate two paired comparison tests that analyze clickthrough data from an interleaved presentation of ranking pairs, and we find that both give accurate and consistent results. automatically judging the quality of retrieval functions based on observable user behavior holds promise for making retrieval evaluation faster, cheaper, and more user centered. however, the relationship between observable user behavior and retrieval quality is not yet fully understood. we present a sequence of studies investigating this relationship for an operational search engine on the arxiv orgprint archive. we find that none of the eight absolute usage metrics we explore reliably reflect retrieval quality for the sample sizes we consider. however, we find that paired experiment designs adapted from sensory analysis produce accurate and reliable statements about the relative quality of two retrieval functions. we conclude that both paired comparison tests give substantially more accurate and sensitive evaluation results than absolute usage metrics in our domain. evaluation methods for information retrieval systems come in three types: offline evaluation, using static data sets annotated for relevance by human judges; user studies, usually conducted in a lab based setting; and online evaluation, using implicit signals such as clicks from actual users. we propose a new approach to online evaluation called multileaved comparisons that is useful in the prevalent case where designers are interested in the relative performance of more than two rankers. for the latter, preferences between rankers are typically inferred from implicit signals via interleaved comparison methods, which combine a pair of rankings and display the result to the user. rather than combining only a pair of rankings, multileaved comparisons combine an arbitrary number of rankings. the resulting user clicks then give feedback about how all these rankings compare to each other. the first, called team draft multileave, is an extension of team draft interleave. the second, called optimized multileave, is an extension of optimized interleave and is designed to handle cases where a large number of rankers must be multileaved. we present experimental results that demonstrate that both team draft multileave and optimized multileave can accurately determine all pairwise preferences among a set of rankers using far less data than the interleaving methods that they extend. to determine whether the candidate rankers they develop are indeed improvements, such teams need experimental feedback about their performance relative to the production ranker. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. however, in order to develop and re ne those candidate rankers in the rst place, they also need more detailed feedback about how the candidate rankers compare to each other. several existing approaches could be used to generate this feedback. firstly, assessors could produce relevance assessments from which of ine metrics could be com puted. however, of ine metrics do not tell the whole story since relevance assessments come from assessors, not users. secondly, online experiments could generate user feedback such as clicks from which rankers could be evaluated. in particular, interleaved comparison methods enable such evaluations with greater data ef ciency than atesting. constraint, new de nitions of unbiasedness and sensitivity, a new credit function upon which these de nitions depend, and a new sampling scheme to make optimization tractable. we present experimental results on several datasets that aim to answer the following research questions. deployed search engines often have several teams of engineers tasked with developing potential improvements to the current production ranker. copyrights for components of this work owned by others than the author must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. for example, to explore a parameter space of interest, they may be interested in the relative performance of multiple rankers in that space. but teams of engineers can easily produce enough candidate rankers that comparing all of them to each other using interleaving methods quickly becomes infeasible. to address this dif culty, we propose a new evaluation paradigm, which we call multileaved comparison, that makes it possible to compare more than two rankers at once. multileaved comparisons can provide detailed feedback about how multiple candidate rankers compare to each other using much less interaction data than would be required using interleaved comparisons. the rst, which we call team draft multileave, builds off of team draft, an interleaving method that assigns documents in the interleaved list to a team per ranker. surpisingly, only a minor extension to td is necessary to enable it to perform multileaved comparisons, yielding tdm. however, despite its appealing simplicity, tdm has the important drawback that it requires multileavings, ie, the result lists shown to the user, to be long enough to represent teams for each ranker. therefore, we propose a second method that we call optimized multileave, which builds off of optimized interleave, an interleaved comparison method that uses a pre. constraint to restrict the allowed interleavings to those that are in between the two rankers and then solves an optimization problem to ensure unbiasedness and maximize sensitivity of the interleavings shown to users. because it avoids the limitations of tdm, om is better suited to handle larger numbers of rankers. the main contributions of this work are: a novel ranker evaluation paradigm in which more than two rankers can be compared at once, two implementations of this new paradigm, tdm and om, and a thorough experimental comparison of tdm and om against each other and against td and oi that shows that multileaved comparison methods can nd preferences between rankers much faster than interleaved comparison methods. our experiments also show that tdm outperforms om unless the number of rankers becomes too large to handle for tdm, at which point om performs better. finally, our experiments show that, when the differences between evaluated rankers are varied, the sensitivity of tdm and om is affected in the same way as for td and oi. or commercial advantage and that copies bear this notice and the full citation on the rst page. in this paper we address the issue of learning to rank for document retrieval. in the task, a model is automatically created with some training data and then is utilized for ranking of documents. the goodness of a model is usually evaluated with performance measures such as map and ndcg. ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data. existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures. for example, ranking svm and rankboost train ranking models by minimizing classification errors on instance pairs. to deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures. our algorithm, referred to as adarank, repeatedly constructs weak rankers on the basis of reweighted training data and finally linearly combines the weak rankers for making ranking predictions. we prove that the training process of adarank is exactly that of enhancing the performance measure used. experimental results on four benchmark datasets show that adarank significantly outperforms the baseline methods of bm, ranking svm, and rankboost. recently learning to rank has gained increasing attention in both the elds of information retrieval and machine learning. when permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. zhichun road, haidian distinct beijing, china year# hangli microsoft com applied to document retrieval, learning to rank becomes a task as follows. in ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model. ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized. several methods for learning to rank have been developed and applied to document retrieval. for example, herbrich et al propose a learning algorithm for ranking on the basis of support vector machines, called ranking svm. freund et al take a similar approach and perform the learning by using boosting, referred to as rankboost. all the existing methods used for document retrieval are designed to optimize loss functions loosely related to the ir performance measures, not loss functions directly based on the measures. in this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval. inspired by the work of adaboost for classi cation, we propose to develop a boosting algorithm for information retrieval, referred to as adarank. adarank utilizes a linear combination of weak rankers as its model. in learning, it repeats the process of re weighting the training sample, creating a weak ranker, and calculating a weight for the ranker. we show that adarank algorithm can iteratively optimize an exponential loss function based on any of ir performance measures. a lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process. ers several advantages: ease in implementation, theoretical soundness,ciency in training, and high accuracy in ranking. tuning ranking models using certain training data and a performance measure is a common practice in ir. as the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder. from the viewpoint of ir, adarank can be viewed as a machine learning method for ranking model tuning. several methods for classi cation and ranking have been proposed. adarank can be viewed as a machine learning method for direct optimization of performance measures, based on a di erent approach. the rest of the paper is organized as follows. after a summary of related work in section #, we describe the proposed adarank algorithm in details in section #. experimental results and discussions are given in section #. section # concludes this paper and gives future work. in training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans. in document retrieval, usually ranking results are evaluated in terms of performance measures such as map and ndcg. for example, ranking svm and rankboost train ranking models by minimizing classi cation errors on instance pairs. experimental results indicate that adarank can outperform the baseline methods of bm, ranking svm, and rankboost, on four benchmark datasets including ohsumed, wsj, ap, and gov. recently, direct optimization of performance measures in learning has become a hot research topic. most information retrieval evaluation metrics are designed to measure the satisfaction of the user given the results returned by a search engine. this paper proposes ebu, a new evaluation metric that uses a sophisticated user model tuned by observations over many thousands of real search sessions. in order to evaluate user satisfaction, most of these metrics have underlying user models, which aim at modeling how users interact with search engine results. hence, the quality of an evaluation metric is a direct function of the quality of its underlying user model. we compare ebu with a number of state of the art evaluation metrics and show that it is more correlated with real user behavior captured by clicks. in the context of web search, users typically interact with the search engine by clicking and browsing a tiny fraction of the top ranked documents returned by the engine. that is, although several relevant documents might be returned by the search engine, the user is often only in uenced by the quality of scanned snippets and clicked documents. that is, a relevant document at rankis assumed to have the same likelihood of being visited by the user compared to a nonrelevant document at the same rank. furthermore, the relevance of the document at rank is often ignored when computing the importance of the document at rank. we propose a new user model that can be trained by using previous click logs. inspired by the recent work on click prediction, our user model assigns the importance of each document according to its probability of examination by the user and the probability that it satis es the information need. information retrieval evaluation metrics are designed to measure the satisfaction of users with respect to the results they receive for their queries. therefore, to estimate the satisfaction rate of the user for a given result list, in addition to the knowledge about the relevance of documents, it is important permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. to predict which documents are more likely to be visited by the user. most common evaluation metrics can be interpreted as simple user models in which the discount functions are used to assign more weights to documents that are more likely to be visited by the user. however, the user models of existing metrics are often primitive and ad hoc. furthermore, the discount function of current metrics are typically static and relevance independent. click logs can be used to design user models that are highly correlated with real user behavior. based on our suggested user model, we then design expected browsing utility, a new metric for evaluating information retrieval systems. ine metric; even though we use past click data to learn parameters of our model, once these parameters are learned, our model does not need any further click information. it uses the labels of the documents retrieved in a ranked list to predict how users are expected to behave if this ranking were presented to them, and to assign the importance of documents. information retrieval relevance judges are trained to search for evidence of relevance when assessing documents. relevance judgments sit at the core of test collection construction, and are assumed to model the utility of documents to real users. our results demonstrate that the amount of effort required to find the relevant information in a document plays an important role in the utility of that document to a real user. in this paper, we study one important source of the mis match between user data and relevance judgments, those due to the high degree of effort required by users to identify and consume the information in a document. for complex documents, this can lead to judges spending substantial time considering each document. however, in practice, search users are often much more impatient: if they do not see evidence of relevance quickly, they tend to give up. however, comparisons of judgments with signals of relevance obtained from real users, such as click counts and dwell time, have demonstrated a systematic mismatch. this effort is ignored in the way relevance judgments are currently obtained, despite the expectation that judges inform us about real users. we propose that if the goal is to evaluate the likelihood of utility to the user, effort as well as relevance should be taken into consideration, and possibly characterized independently, when judgments are obtained. there are two broad approaches to information retrieval. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than acm must be honored. these are used to compute evaluation measures such as precision and recall, mean average precision, and many others. each of these approaches have strengths and weaknesses. collection based evaluations are fast, repeatable, and relatively inexpensive as the data collected can be reused many times. in contrast, user based approaches can take into account endto end user satisfaction. however, they tend to be expensive to perform due to the need to obtain users for every system. they are also di cult to analyze due to the need to control for variance across tasks, population and time. ideally, the outcome of collection based evaluation should be predictive of the satisfaction of real users of a search system. one of the main reasons behind this mismatch are the simplifying assumptions made in collection based evaluation about relevance and how users behave when they use a search system. the main goal of our work is to identify the reasons behind these disagreements. ort needed to nd and consume relevant information in a document. relevance judges are explicitly asked to identify the relevance of documents they assess. therefore, they must evaluate each document thoroughly before marking it as relevant or non relevant. in performing these judgments, judges often spend a signi cant amount of. ort on documents that may not have signi cant relevant content or that may be hard to read. if they do not see evidence of su cient relevance quickly or if they think relevant information is di cult to consume, they tend to give up and move on to another document. based on this observation, we propose that the judgment process be modi ed to incorporate how real users interact with documents. ort to nd relevant information in a document as well as document relevance, or by logging the amount of time it takes a judge to assess the relevance of a document, and using this information in evaluation. we also show through a regression model that features related to. ort play a signi cant role in a relevant document being considered as low utility by an actual user. ort required to nd relevant information together with the relevance of the document. we further describe a post processing heuristic that can be used to infer the utility of a document with respect to an actual user. ectiveness evaluation: the collection based approach, and permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. nick craswell, filip radlinski, peter bailey microsoft microsoft com the user based approach. ectiveness of retrieval systems using test collections comprising canned information needs and static relevance judgments. on the other hand, the user based approach involves actual users being observed and measured in terms of their interactions with a retrieval system. however, collection based evaluations make many simplifying assumptions about real user information needs, what constitutes relevance, and many other aspects of retrieval. therefore, collection based evaluation is commonly used in evaluating the quality of retrieval systems, especially when reusability is a prime concern for enabling rapid experimental iteration among a number of alternatives. yet research has shown that these two forms of evaluation often do not completely agree with each other, or agree with each other only when there is a signi cant gap in terms of the quality of the systems compared. therefore, there is increasing interest in better modeling user needs and user interaction with an engine in collectionbased. we claim that a key source for disagreement between collection based evaluation and user based online experiments is due to the disagreements between what judges consider as relevant versus the utility of a document with respect to an actual user. to address this goal, we rely on implicit feedback provided by users of a retrieval system. we compare indicators of utility inferred from implicit feedback to judgments obtained from relevance judges, and identify sources of disagreement. we further focus on the reasons of mismatch between relevance judgments and implicit signals obtained from the clicked documents and show that one of the main reasons for such mismatch is due to the. on the other hand, users simply wish to ful ll an information need, and are often much less patient when determining if a particular document is relevant. therefore, even if a document is relevant to a query, it provides only minimal utility to an actual user if nding and understanding the relevant portions of the document is di cult. this could be done by either explicitly asking the judges to provide information regarding the. ort plays an important role in user satisfaction and that further thought should be given to the concept of relevance versus utility. our results also show that features related to the. ort to nd consume relevant information could be used as ranking features when retrieval systems are designed as they have a signi cant impact on the utility of a document to an actual user. in what follows, we start by describing related work in the area. we then follow with a user model that considers the di erent stages of how users interact with a search engine and compare this with the behavior of judges. through the user model, we show how the utility of a document with respect to an actual user could be di erent than the relevance of such document. therefore, we argue that if we would like to measure the utility of a search system to an actual user, current judging mechanisms should be changed and the assessors should be asked to provide judgments in terms of the. we present an on line learning framework tailored towards real time learning from observed user behavior in search engines and other information retrieval systems. in particular, we only require pairwise comparisons which were shown to be reliably inferred from implicit feedback. we will present an algorithm with theoretical guarantees as well as simulation results. when responding to queries, the goal of an information retrieval system ranging from web search, to desktop search, to call center support is to return the results that maximize user utility. so, how can a retrieval system learn to provide results that maximize utility the conventional approach is to optimize a proxymeasure that is hoped to correlate with utility. a wide range of measures has been proposed to this. most obviously, they require expensive manual relevance judgments that ignore the identity of the user and the usercontext. this makes it unclear whether maximization of a proxy measure truly optimizes the search experience for the user. we therefore take a di erent approach based on implicit feedback gathered directly from users. but how can a learning algorithm access the utility a user sees in a set of results while it is unclear how to reliably derive cardinal utility values for a set of results, it was shown that interactive experiments can reliably provide ordinal judgments between two sets of results owner. for example, to elicit whether a user prefers ranking over, radlinski et al showed how to present an interleaved ranking of and so that clicks indicate which of the two has higher utility. this leads to the following on line learning problem addressed in this paper. given a space of retrieval functions and a pairwise test for comparing any two retrieval functions, we wish to nd a sequence of comparisons that has low regret. we call this the dueling bandits problem, since only ordinal feedback is observable, not cardinal feedback as required by conventional bandit algorithms. in this paper, we formalize the dueling bandits problem and an appropriate notion of regret. furthermore, we propose a gradient descent method which builds on methods for on line convex optimization. the method is compatible with many existing classes of retrieval functions, and we provide theoretical regret bounds and an experimental evaluation.