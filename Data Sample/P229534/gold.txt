ranker evaluation has long been a central topic in information retrieval. the classical approach is to perform of ine evaluation based on the cran eld paradigm, where a collection of doc uments is manually annotated by human experts. a representative set of queries together with associated user intents are crafted by a group of experts. the documents and their relevance labels can then be used in metrics such as ndcg. but it is expensive and assessors relevance judgements may not adequately re ect real users opinions. in atesting, the user population is split into two groups, and pairs of rankers are compared by presenting one group with one ranker and the other group with another ranker. the ranker with the best performance on a selected metric is typically considered to be the winner. another approach is interleaved comparison: search engine result pages presented to users are obtained by interleaving serps of two competing rankers under consideration. the user feedback in the form of clicks is then interpreted as user preference for one ranker over the other. the key problem of online methods is that they require user feedback to evaluate each pair of rankers, which often requires exposing suboptimal serps. interleaving methods that exploit importance sampling pro vide a way to compare rankers using only historical click data. importance sampling guarantees only that the resulting estimator is unbiased, not that it has low variance: the addition of more historical data is not guaranteed to improve estimates, as new data can actually increase the variance. ad ditionally, importance sampling has poor generalization properties compared to click based approaches that infer relevance labels of documents. in particular, the latter can generalize across serps that differ in the order of documents, while the former cannot. in recent years, a number of probabilistic models have been developed to describe, understand, and predict user behavior while interacting with an ir system. in particular, click models such as dbn, dcm and ubm infer the relevance of documents for queries and model user clicks on documents in the serp by analysing search engine log les. these inferred relevance labels can be further used for learning to rank and for ranker compar isons using click model based metrics. however, to our knowl edge, none of these approaches provide a way to quantify the uncertainty in the resulting comparison, which is critical for making informed decisions about when to switch from a production ranker to a candidate ranker. for metrics such as ebu, err and the utility and effort based metrics in, the effect on the compari son of a document seen just once and that of one seen a hundred of times is the same, given that the inferred relevance labels have the same value. furthermore, these metrics do not take into account the number of previously unseen documents that appear in the serps produced by the candidate rankers. hence, these approaches cannot properly control the risks associated with switching to a candidate ranker, as the uncertainty in the comparison is not measured. we present a new approach to ranker evaluation that is designed to overcome these limitations. it compares rankers using only the log les collected through natural user interactions with the production ranker. in contrast to importance sampling based interleaving methods, our method does not require the data to be obtained stochastically. in contrast to the metrics in, our method takes into account the uncertainty associated with the inferred relevance labels and the presence of unseen documents and measures the con dence of the resulting comparison.