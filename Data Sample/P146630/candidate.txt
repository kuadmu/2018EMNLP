this paper introduces architectural and interaction patterns for integrating crowdsourced human contributions directly into user interfaces. we focus on writing and editing, complex endeavors that span many levels of conceptual and pragmatic activity. authoring tools offer help with pragmatics, but for higher level help, writers commonly turn to other people. we thus present soylent, a word processing interface that enables writers to call on mechanical turk workers to shorten, proofread, and otherwise edit parts of their documents on demand. to improve worker quality, we introduce the find fix verify crowd programming pattern, which splits tasks into a series of generation and review stages. evaluation studies demonstrate the feasibility of crowdsourced editing and investigate questions of reliability, cost, wait time, and work time for edits. word processing is a complex task that touches on many goals of human computer interaction. it supports a deep cognitive activity writing and requires complicated manipulations. writing is difficult: even experts routinely make style, grammar, and spelling mistakes. then, when a writer makes highlevel decisions like changing a passage from past to present tense or fleshing out citation sketches into a true references section, she is faced with executing daunting numbers of nontrivial tasks across the entire document. finally, when the document is a half page over length, interactive software provides little support to help us trim those last few paragraphs. good user interfaces aid these tasks; good artificial intelligence helps as well, but it is clear that we have far to go. in our everyday life, when we need help with complex cognition and manipulation tasks, we often turn to other people. writing is no exception: we commonly recruit friends and colleagues to help us shape and polish our writing. but we cannot always rely on them: colleagues do not want to proofread every sentence we write, cut a few lines from every paragraph in a page paper, or help us format acm style references. soylent is a word processing interface that utilizes crowd contributions to aid complex writing tasks ranging from error prevention and paragraph shortening to automation of tasks such as citation searches and tense changes. using soylent is like having an entire editorial staff available as you write. we hypothesize that crowd workers with a basic knowledge of written english can support both novice and expert writers. these workers perform tasks that the writer might not, such as scrupulously scanning for text to cut or updating a list of addresses to include a zip code. they can also solve problems that artificial intelligence cannot yet, for example flagging writing errors that the word processor does not catch. soylent aids the writing process by integrating paid crowd workers from amazonmechanical turk platform into microsoft word. soylent is peoplea: its core algorithms involve calls to mechanical turk workers. soylent is comprised of three main components: shortn, a text shortening service that cuts selected text down to of its original length on average without changing the meaning of the text or introducing writing errors. the vast majority of advances in sensor network research over the last five years have focused on the development of a series of small scale testbeds and specialized applications that are built on low powered sensor devices that self organize to form application specific multihop wireless networks. we believe that sensor networks have reached an important crossroads in their development. the question we address in this paper is how to propel sensor networks from their small scale application specific network origins, into the commercial mainstream of people every day lives; the challenge being: how do we develop large scale general purpose sensor networks for the general public capable of supporting a wide variety of applications in urban settings. we propose metrosense, a new people centric paradigm for urban sensing at the edge of the internet, at very large scale. we discuss a number of challenges, interactions and characteristics in urban sensing applications, and then present the metrosense architecture which is based fundamentally on three design principles: network symbiosis, asymmetric design, and localized interaction. the ability of metrosense to scale to very large areas is based on the use of an opportunistic sensor networking approach. opportunistic sensor networking leverages mobility enabled interactions and provides coordination between people centric mobile sensors, static sensors and edge wireless access nodes in support of opportunistic sensing, opportunistic tasking, and opportunistic data collection. we discuss architectural challenges including providing sensing coverage with sparse mobile sensors, how to hand off roles and responsibilities between sensors, improving network performance and connectivity using adaptive multihop, and importantly, providing security and privacy for people centric sensors and data. to date, the bulk of work in the wireless sensor network space has focused on environmental, agricultural or industrial monitoring. networks of static sensing elements are either physically placed or randomly distributed across a target area of interest, with a focus on application speci. a substantial body of literature exists addressing a wide spectrum of issues in such sensor networks. er research challenges to scientists and engineers, but do not currently directly bene. furthermore, humans are disengaged bystanders in the sensing and communication processes, passively waiting on the fringe of the network for data to appear. in this paper, we move away from the traditional focus of wireless sensor networks and propose a new people centric sensing paradigm for urban sensing at very large scale. as our focus is on enabling human centric applications, requirements on the architectural solution include the ability to sense people and characteristics of their immediate surroundings, and the ability to sense data related to interactions between people and interactions between people and their surroundings. these requirements are made more challenging by human and vehicle mobility. furthermore, the people centric nature of the data collected implies a requirement for privacy beyond what is present in traditional wireless sensor network application targets such as forest microclimates. in addition, the urban environment presents a host of challenges absent, or only partially present in other sensing domains. the architectural solution must scale at least across a large metropolitan area, must be able to handle a diversity of hardware platforms, application heterogeneity, interactions between a multitude of administrative domains, and a highly dynamic environment. to meet these requirements and challenges a number of architectural alternatives are possible, including the ubisense approach, tiered sensor networks, and sensor meshes. however, by considering the cost and delity trends of each as the network scales to metropolitan dimensions, we see that these approaches are not feasible. cost includes the monetary expense of sensors and network access and infrastructure to deploy the sensor network. fidelity refers generically to the amount of data collected from the sensor eld and delivered to an interested user within a given time period. for example, for event based applications delity can mean receiving enough packets to detect features of interest in a given event. for periodic monitoring applications, delity can be characterized in terms of data stream continuity. figure # illustrates qualitatively how existing networks scale in terms of cost and delivered delity if deployed at increasing scale. however, the cost of doing this is prohibitively high when considering complete coverage across an urban area. in addition, the ubisense platform is not programmable and only supports one sensing modality. tiered static mesh networks such as exscal, tenet, and siphon all add hierarchies of powerful nodes with secondary radio overlays. er better delity in comparison to non tiered multihop networks. however, the coverage and communication cost of deploying these static sensor and wifi overlay networks is also considerable for large areas. non tiered mesh networks present a lower investment cost but the delity of these network does not scale because of link unreliability, congestion, and the funneling. ect implicit in many to one multihop sensor networks. we propose metrosense, a network architecture for urbanscale people centric sensing with a design goal of broad application and sensor heterogeneity support. metrosense provides the network architecture that is lacking in current urban scale pervasive systems. metrosense has a symbiotic relationship between itself and the human community it serves, leveraging existing infrastructure and human mobility to opportunistically sense and collect data about people for people. real time delity for improved cost and coverage, enabling sparse sensing across large areas using potentially very large communities of rechargeable people centric mobile sensors. in so doing metrosense hits the architectural sweet spot that provides reasonable delity at low cost at urban deployment scale. the network architecture is designed with the speci. requirements of urban sensing in mind; it is not intended to be a general purpose communications infrastructure, but rather an extension from it a new sensing edge network for the internet. scalability design space metrosense assumes the ability of sensors to be recharged regularly like cell phones or pdas of today. thus, in comparison to embedded static sensor meshes and manets, metrosense is less energy constrained and therefore targets a network lifetime of years rather than weeks or months. furthermore, unlike static sensor meshes that leverage deep multi hop, metrosense adaptively limits multi hop radio interactions in an. metrosense enables the general purpose programming of the infrastructure with the goal of supporting the execution of multiple applications in parallel across the network. the ability of metrosense to scale to very large areas is based on the use of an opportunistic sensor networking paradigm. opportunistic sensor networking leverages mobility enabled interactions and provides coordination between people centric mobile sensors, static sensors, and edge wireless access nodes in support of opportunistic sensing, opportunistic tasking, and opportunistic data collection. in the next section, we outline our vision of the present and future of urban sensing, including descriptions of some practical leading edge applications whose requirements drive the design of metrosense. in section #, we present an overview of the hardware and software architectural components of metrosense, and discuss a number of design principles that underpin the metrosense architecture. section # provides focus on fundamental issues in the opportunistic sensor networking paradigm, namely, sensing coverage via mobility of sparse mobile sensors, selective responsibility transfer between sensors, sensor tasking and data collection. section # addresses the important issue of security in a peoplecentric urban scale sensor network. while traditional sensor networks target remote and unattended deployments we target the urban setting, an environment possessing a rich diversity of lifestyles, activities, and thus potential applications. the exact notion of delity is application speci c. ubisense represents the most costly deployment scenario but might continue to provide high delity as the network scales up. this allows the network designers to better provision the network to. er considerably more bandwidth than sensor radios they still lead to similar scaling problems albeit at higher throughput. ort to reduce complexity and wireless packet loss. section # discusses the gap between the work done by the sensor network and ubiquitous pervasive computing communities that is bridged by metrosense, before we conclude in section #. the network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. we develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of context on the world wide web. the central issue we address within our framework is the distillation of broad search topics, through the discovery of authorative information sources on such topics. we propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of hub pages that join them together in the link structure. our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristrics for link based analysis. to copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and or a fee. of course, there are a number of potential pitfalls in the application of links for such a purpose. the network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. in this work, we develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of contexts on preliminary versions of this paper appeared in proceedings of the annual acm siam symposium on discrete algorithms. and as ibm research report rj year#, may year#. this work was performed in large part while. kleinberg was on leave at the ibm almaden research center, san jose, ca. sloan research fellowship, an onr young investigator award, and by nsf faculty early career development award clr. authoraddress: department of computer science, cornell university, ithaca, ny year#, mail: kleinber cs cornell edu. permission to make digital hard copy of part or all of this work for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication, and its date appear, and notice is given that copying is by permission of the association for computing machinery, inc. in particular, we focus on the use of links for analyzing the collection of pages relevant to a broad search topic, and for discovering the most authoritative pages on such topics. while our techniques are not specific to the www, we find the problems of search and structural analysis particularly compelling in the context of this domain. the www is a hypertext corpus of enormous complexity, and it continues to expand at a phenomenal rate. moreover, it can be viewed as an intricate form of populist hypermedia, in which millions of on line participants, with diverse and often conflicting goals, are continuously creating hyperlinked content. thus, while individuals can impose order at an extremely local level, its global organization is utterly unplanned high level structure can emerge only through a posteriori analysis. our work originates in the problem of searching on the www, which we could define roughly as the process of discovering pages that are relevant to a given query. the quality of a search method necessarily requires human evaluation, due to the subjectivity inherent in notions such as relevance. we begin from the observation that improving the quality of search methods on the www is, at the present time, a rich and interesting problem that is in many ways orthogonal to concerns of algorithmic efficiency and storage. in particular, consider that current search engines typically index a sizable portion of the www and respond on the order of seconds. although there would be considerable utility in a search tool with a longer response time, provided that the results were of significantly greater value to a user, it has typically been very hard to say what such a search tool should be computing with this extra time. clearly, we are lacking objective functions that are both concretely defined and correspond to human notions of quality. we view searching as beginning from a user supplied query. it seems best not to take too unified a view of the notion of a query; there is more than one type of query, and the handling of each may require different techniques. consider, for example, the following types of queries: specific queries. for example, does netscape support the jdk code signing api broad topic queries. for example, find information about the java program ming language. for example, find pages similar to java sun com. concentrating on just the first two types of queries for now, we see that they present very different sorts of obstacles. the difficulty in handling specific queries is centered, roughly, around what could be called the scarcity problem: there are very few pages that contain the required information, and it is often difficult to determine the identity of these pages. for broad topic queries, on the other hand, one expects to find many thousand relevant pages on the www; such a set of pages might be generated by variants of term matching do not use the term on their pages. this is a fundamental and recurring phenomenon as another example, there is no reason to expect the home pages of honda or toyota to contain the term automobile manufacturers. analyzing the hyperlink structure among www pages gives us a way to address many of the difficulties discussed above. hyperlinks encode a considerable amount of latent human judgment, and we claim that this type of judgment is precisely what is needed to formulate a notion of authority. specifically, the creation of a link on the www represents a concrete indication of the following type of judgment: the creator of page, by including a link to page, has in some measure conferred authority on. moreover, links afford us the opportunity to find potential authorities purely through the pages that point to them; this offers a way to circumvent the problem, discussed above, that many prominent pages are not sufficiently self descriptive. first of all, links are created for a wide variety of reasons, many of which have nothing to do with the conferral of authority. for example, a large number of links are created primarily for navigational purposes; others represent paid advertisements. another issue is the difficulty in finding an appropriate balance between the criteria of relevance and popularity, each of which contributes to our intuitive notion of authority. it is instructive to consider the serious problems inherent in the following simple heuristic for locating authoritative pages: of all pages containing the query string, return those with the greatest number of in links. we have already argued that for a great many queries, a number of the most authoritative pages do not contain the associated query string. conversely, this heuristic would consider a universally popular page such as www yahoo com or www. netscape com to be highly authoritative with respect to any query string that it contained. in this work, we propose a link based model for the conferral of authority, and show how it leads to a method that consistently identifies relevant, authoritative www pages for broad search topics. our model is based on the relationship that exists between the authorities for a topic and those pages that link to many related authorities we refer to pages of this latter type as hubs. we observe that a certain natural type of equilibrium exists between hubs and authorities in the graph defined by the link structure, and we exploit this to develop an algorithm that identifies both types of pages simultaneously. the algorithm operates on focused subgraphs of the www that we construct from the output of a text based www search engine; our technique for constructing such subgraphs is designed to produce small collections of pages likely to contain the most authoritative pages for a given topic. our approach to discovering authoritative www sources is meant to have a global nature: we wish to identify the most central pages for broad search topics in the context of the www as a whole. global approaches involve basic problems of representing and filtering large volumes of information, since the entire set of pages relevant to a broad topic query can have a size in the millions. this is in contrast to local approaches that seek to understand the interconnections among the set of www pages belonging to a single logical site or intranet; in such cases the amount of data is much smaller, and often a different set of considerations dominates. it is also important to note the sense in which our main concerns are fundamentally different from problems of clustering. clustering addresses the issue of dissecting a heterogeneous population into subpopulations that are in some way more cohesive; in the context of the www, this may involve distinguishing pages related to different meanings or senses of a query term. thus, clustering is intrinsically different from the issue of distilling broad topics via the discovery of authorities, although a subsequent section will indicate some connections. for even if we were able perfectly to dissect the multiple senses of an ambiguous query term, we would still be left with the same underlying problem of representing and filtering the vast number of pages that are relevant to each of the main senses of the query term. section # discusses the method by which we construct a focused subgraph of the www with respect to a broad search topic, producing a set of relevant pages rich in candidate authorities. sections and discuss our main algorithm for identifying hubs and authorities in such a subgraph, and some of the applications of this algorithm. section # discusses the connections with related work in the areas of www search, bibliometrics, and the study of social networks. section # describes how an extension of our basic algorithm produces multiple collections of hubs and authorities within a common link structure. finally, section # investigates the question of how broad a topic must be in order for our techniques to be effective, and section # surveys some work that has been done on the evaluation of the method presented here. the development of sensing systems for urban deployments is still in its infancy. this issue has significant implications as to where the complexity and the main challenges in building urban sensing systems will reside. we contrast two end points of the spectrum of conscious human involvement, namely participatory sensing, and opportunistic sensing. we develop an evaluation model and argue that opportunistic sensing more easily supports larger scale applications and broader diversity within such applications. in this paper, we provide preliminary analysis which supports this conjecture, and outline techniques we are developing in support of opportunistic sensing systems. an interesting unresolved issue is the precise role assumed by people within such systems. this issue will also impact the scale and diversity of applications that are able to be supported. we raise the issue of the need for evaluation of sensing methodologies in a one page extended abstract included as an unpublished work in progress at the sensing on everyday mobile phones in support of participatory research workshop, nov year#. over the past decade, the focus of wireless sensor networking research has evolved from static networks of specialized devices deployed to sense the environment, to networks making use of robotic or other controlled mobility to adapt to the sensing conditions, to a people centric approach relying on the mobility of people. an expanding research community is developing techniques to bring about very large scale urban sensing by leveraging the increasing sensing capabilities found in consumer devices such as cell phones. data collected from these mobile sensors provide the foundation for excitingpeople centric applications. in the application query submission stage, a query is submitted to the system specifying at least one required sensor type and a set of conditions under which sampling of the required sensor should take place. in the device selection phase, a choice is made amongavailable devices as towhich willbe tasked to meet a particular submitted application query. the results of the analysis, and possibly the raw samples, are then returned to the querying application and may also be shared with others, depending on issues such as connectivity and privacy. including consumer devices as a fundamental building block of the sensing system implies that thehumanownersof these devices play an important role in the resulting system architecture. in this paper, we consider the question of what roles people, as sensing device custodians, are willing to play in large scale urban sensing systems, particularly to what extent they should be conscious active participants in meeting application requirements. we examine the two end points on the spectrum of custodian awareness and involvement in the architecture, referring to one as participatory and the other as opportunistic. with participatorysensing the custodian consciously opts to meet an application request out of personal or nancial interest. a participatory approach incorporates people into signi cant decision stages of the sensing system, such as deciding what data is shared and to what extent privacy mechanisms should be allowed to impact data delity. consequently, aparticipatorysystem designfocusesontoolsand mechanisms that assistpeople to share, publish, search, interpret and verify information collected using custodian devices, as well as social technical techniques to encourage the involvement of the public. with opportunistic sensing, the custodian may notbeaware of active applications. this state is automatically detected; the custodian does not knowingly change the device state for the purpose of meeting the applicationrequest. to support symbiosisbetween the custodian and the system, sensor samplingoccurs only if the privacy and transparency needs of the custodian are met. to maintain transparency, opportunistic use of a device should not noticeably impact the normal user experience of the custodian as he uses it for his own needs. thus, the primarychallenges in opportunistic sensing are determining when the state of the sensing device matches the requirements of applications, and sampling when the device state and custodian requirements are met. participatory sensing places demands on involved device custodians that restrict the pool of willing participants. the tolerance of people to endure interruptions on behalf of applications limits the number of concurrent applications that can likely be supported. further, under the participatory approach, an application needs to have a critical mass of community appeal and engagement. these factors combine, we conjecture, to limit both an applicationscale and the diversity of applications that are likely to be supportedby a purely participatorypeople centric network. applicationsarebest suitedtotheparticipatory modelwhen they have a collection of interested custodians whose size is at least as large as number of sensors required to carry out the application. thus, a strong motivation for opportunistic sensing is to increase the scale and scope diversity of applications that may otherwise not be supported. opportunistic sensing shifts the burden of supporting an application from the custodian to the sensing system, automatically determining when devices can be used to meet application requests. in this way, applications can leverage the sensing capabilitiesof all system userswithoutrequiringhuman intervention to actively and consciously participate in the application, lowering the bar for applications to run in people centric networks. in section #, we propose a simple analytical model to quantitatively evaluate the characteristics of each of these extremes in terms of the probability a particular application query is successfully met. ageneral purposepeople centric sensing system canbe thought of in terms of the following stages: application querysubmission; device selection; sensor sampling; and data analysis, sharing and presentation. in the sensor sampling stage, the selected device decides when to sample the sensor requiredbythe application querybycomparing itsown sampling context with that speci ed in the query. in the data analysis, sharing, and presentation stage, depending on the application requirements, the sensor samples are analyzed perhaps in combination with results from other queries. insteada custodiandevice is utilized whenever its state matches the requirements of an application. the main privacy concern is the potential leak of personally sensitive information indirectly when providing sensor data. the characteristics of opportunistic and participatory sensing impact the applications that can be practically supported. the model parameters capture at an abstract level the application query submission, device selection, and sensor sampling stages mentioned above, as they apply to a particular application scenario sampling in support of google streetview. we consider the problem of monitoring road and traffic conditions in a city. prior work in this area has required the deployment of dedicated sensors on vehicles and or on the roadside, or the tracking of mobile phones by service providers. furthermore, prior work has largely focused on the developed world, with its relatively simple traffic flow patterns. in fact, traffic flow in cities of the developing regions, which comprise much of the world, tends to be much more complex owing to varied road conditions, chaotic traffic, and a heterogeneous mix of vehicles. to monitor road and traffic conditions in such a setting, we present nericell, a system that performs rich sensing by piggybacking on smartphones that users carry with them in normal course. in this paper, we focus specifically on the sensing component, which uses the accelerometer, microphone, gsm radio, and or gps sensors in these phones to detect potholes, bumps, braking, and honking. nericell addresses several challenges including virtually reorienting the accelerometer on a phone that is at an arbitrary orientation, and performing honk detection and localization in an energy efficient manner. we also touch upon the idea of triggered sensing, where dissimilar sensors are used in tandem to conserve energy. we evaluate the effectiveness of the sensing functions in nericell based on experiments conducted on the roads of bangalore, with promising results. are a key part of the day today lives of people. therefore, monitoring their conditions has received a signi cant amount of attention. prior work in this area has primarily focused on the developed world, where good roads and orderly tra. conditions on a stretch of road can largely be characterized by the volume and speed of traowing through it. to monitor this information, intelligent transportation systems havebeendeveloped. manyoftheseinvolvedeployingdedicated sensorsin vehicles and or on roads, which can be an expensive proposition and so is typically restricted to the busiest stretches of road. surveillance technologies and section # for related work. conditions in the developing world tend to be more varied because of various socioeconomic reasons. road quality tends to be variable, with bumpy roads and potholes being commonplace even in the heart of cities. can be chaotic, with little or no adherence to right of way protocols at some intersections andliberal use ofhonking. vehicles types are also very heterogeneous, ranging from wheelers and wheelers to wheelers andlarger vehicles. conditionsischallenging, butitholdsthepromise of enabling new and useful functionality. for instance, information gathered via rich sensing could be used to annotate a map, thereby allowing a user to search for driving directions that would minimize stress by avoiding chaotic roads and intersections. to address this challenge, we present nericell, a system for rich monitoring of road and tra. nericell orchestrates the smartphones to perform sensing and report data back to a server for aggregation. indeed, smartphones include a range of sensing and communication capabilities, in addition to computing. a phone might include any or all of a microphone, camera, gps, andaccelerometer, each of which could nericell is a play on the word nerisal, which means congestion in tamil. in addition, the phone would include a cellular radio, possibly with data communication capabilities. monitoring is a goodmatchfordeveloping regionsbecauseit avoidstheneed for expensive and specialized tra. it also avoidsdependenceonadvanced vehiclefeatures suchthecontrollerareanetwork busthat are absent in the many low cost vehicles that are commonplace in developing regions. moreover, it takes advantage of the booming growth of mobile telephony in such regions. for example, as of mid year#, indiahad million mobiletelephony subscribers, growing atanestimated millioneach month. althoughthe majority of users have basic mobile phones today, a large number of them, in fact more than the number of pc internetusersinindia, accesstheinternetontheirphones, suggesting thattheprospectsofgreaterpenetrationof more capable phones are good. there are similar growth trends in many other parts of the world, with the total number of mobile subscriptionsworldwide estimated at billion. note that despite being mobile phone based, our approach totra. oursensing andinferencesgoesbeyondjust monitoring location and speed information, hence requiring a presence on the phones themselves. our focus in this paper is on the sensing component of nericell; we defer a discussion of the larger system, including the aggregation server, to future work. several technical challenges arisefrom ourdesign choicetoperform rich sensing and base the system on mobile smartphones. nericell leverages sensors besides gps accelerometer and microphone, in particular to glean rich information, eg, the quality of the road or the noisiness of tra c. the use of an accelerometer introduces the challenge of virtually reorienting it to compensate for the arbitrary orientation of the phone that it is embedded in. cient and robust bump, brake and honk detectors in order to infer road and tra. moreover, since a smartphone is battery powered and is primarily someonephone, energy. to this end, we employ the concept of triggered sensing, wherein a sensor that is relatively inexpensivefrom an energy viewpoint is used to trigger the operation of a more expensivesensor. fore ciencyin communication and energy usage, each node processes the sensed data locally before shipping the processed data back to the server. the main contributions ofthis work are: algorithms to virtually reorient adisoriented accelerometeralong acanonical set of axes andthen use simple threshold basedheuristics todetectbumps andpotholes, andbraking; heuristicstoidentify honkingby using audiosamples sensed viathe microphone; evaluationoftheuseof cellular tower information in dense deployments in developing countries to perform energy. cient localization; and triggered sensing techniques, wherein alowenergy sensor is used to trigger the operation of a highenergy sensor. finally, we have implemented most of these techniques on smartphones running windows mobile. there are two important issues that we do not address in this paper. one is the question of privacy of the users whose phones participate in nericell. it may be possible to achievegood enoughprivacy simplyby suppressingtheidentity of aparticipatingphone when reporting and aggregating the sensed data. a more sophisticated privacy aware community sensing approach that incorporatesformal modelsof sharingpreferencesispresented in. the second issue is of providing incentives for participation in nericell. providing incentives for participation in such decentralized systems is an active area of research and we maybeabletoleveragethisfor nericell. this paper presents isee, a crowdsourced approach to detecting and localizing events in outdoor environments. upon spotting an event, an isee user only needs to swipe on her smartphone touchscreen in the direction of the event. these swiping directions are often inaccurate and so are the compass measurements. moreover, the swipes do not encode any notion of how far the event is located from the user, neither is the gps location of the user accurate. furthermore, multiple events may occur simultaneously and users do not explicitly indicate which events they are swiping towards. nonetheless, as more users start contributing data, we show that our proposed system is able to quickly detect and estimate the locations of the events. we have implemented isee on android phones and have experimented in real world settings by planting virtual events in our campus and asking volunteers to swipe on seeing one. results show that isee performs appreciably better than established triangulation and clustering based approaches, in terms of localization accuracy, detection coverage, and robustness to sensor noise. consequently, a major challenge in quality control is to discover true events from diverse and noisy participants reports. the ubiquity of smartphones has led to the emergence of mobile crowdsourcing tasks such as the detection of spatial events when smartphone users move around in their daily lives. however, the credibility of those detected events can be negatively impacted by unreliable participants with low quality data. this truth discovery problem is uniquely distinct from its online counterpart in that it involves uncertainties in both participants mobility and reliability. decoupling these two types of uncertainties through location tracking will raise severe privacy and energy issues, whereas simply ignoring missing reports or treating them as negative reports will significantly degrade the accuracy of the discovered truth. in this paper, we propose a new method to tackle this truth discovery problem through principled probabilistic modeling. in particular, we integrate the modeling of location popularity, location visit indicators, truth of events and three way participant reliability in a unified framework. the proposed model is thus capable of efficiently handling various types of uncertainties and automatically discovering truth without any supervision or the need of location tracking. experimental results demonstrate that our proposed method outperforms existing state of the art truth discovery approaches in the mobile crowdsourcing environment. crowdsourced detection of spatial events is one such application where participants detect events while moving around in their daily lives. since participants only sporadically reveal their locations when reporting events for the geotagging purpose, we cannot obtain their detailed trajectories. each missing report will then become a negative report and imply a lack of an event. the growing smartphone user base has enabled mobile crowdsourcing applications on a large scale. several commercial markets such as field agent, gigwalk and taskrabbit permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. norman university of aberdeen aberdeen, uk abdn ac uk have emerged, which represent the mobile equivalent of online crowdsourcing markets such as the amazon mechanical turk. these events are arbitrary phenomena that the task requester is interested in, eg, potholes on streets, graf ti on walls and bike racks in public places. consider the task of detecting the locations of potholes as an example, where figure #a shows a user interface for task instruction. since the number of possible event locations is huge and most locations normally do not have an event, a participant uses her smartphone to make a report only when she detects an event. in other words, a participant either reports a detection or does not report at all, but never reports a lack of an event. as participants may erroneously report events due to misunderstanding, confusion, carelessness, incompetence or even intent to deceive, there is a demand for ef cient algorithms to handle these diverse and noisy participants reports and automatically discover the truth. this truth discovery problem is uniquely distinct from its online counterpart in that it involves uncertainties in both participants mobility and reliability. this imposes a significant challenge in interpreting missing reports at candidate event locations, which consequently impacts the quality of truth discovery. a missing report is ambiguous since it can be due to either the mobility issue that a participant did not visit a location and thus could not assess the event there, or a negative event assessment when she visited that location. it is important to distinguish these two cases, as the former does not carry any information about the truth of the event and the participant reliability while the latter does. a possible solution to this problem is to continuously track participants locations such that the missing reports corresponding to unvisited locations are ignored and those corresponding to visited locations are treated as negative. we illustrate this strategy in figure #a, where reports that are taken into consideration are marked with gray backgrounds and the inferred negative reports are annotated as. after eliminating the uncertainty in mobility, we can apply existing truth discovery methods for online crowdsourcing. however, location tracking is impractical as it raises severe privacy and energy issues. alternatively, one can try to reconstruct a participantmobility path. nevertheless, machine learning based path reconstruction methods require historical location traces figure #: example user interface for task instruction. example user interface for reporting a spatial event. illustration of the space of all true events and participant reported events. input into the truth discovery algorithm and the expected output. participants and reported events shown in a matrix form, where a and a blank space represent a positive and a missing report respectively. figure #: different strategies of handling missing reports. a entry with a gray background means that the corresponding report is taken into consideration for truth discovery. a indicates a positive report, a indicates that the missing report is treated as negative and a blank space indicates that the missing report is ignored. which can only be obtained through tracking and such methods will easily fail when a participant deviates from her usual paths. mapmatching based path reconstruction methods require road network information and they will easily fail if the time interval between consecutively revealed locations is larger than minutes. strategies used for tackling missing data in related domains may be useful. for example, raykar et al simply ignore the missing data for online crowdsourced binary image classi cation. this is because online crowd workers are required to provide either a positive or a negative response, and the missing data simply imply that workers did not choose the images to work on. by applying this strategy to crowdsourced event detection, however, we will end up with only positive reports without any con ict. this will lead to a trivial conclusion that every reported event is true, which is obviously erroneous. in tackling con icting web information for data integration, zhao et al treat missing reports as negative reports if a source did not make claims on some of the facts but on others about an entity. by applying this strategy to crowdsourced event detection, we can regard the spatial area of interest as an entity and events inside it as multiple facts, each can be either true or false. if none of the events receives positive reports from more than half of the participants due to mobility issues, we will then conclude that all the events are false by majority voting, which is again erroneous. a work on social sensing similarly treats missing reports as negative reports. in this paper, we propose a new method to tackle the truth discovery problem in crowdsourced event detection through principled probabilistic modeling. we observe that a participantlikelihood of reporting an event depends on three factors: whether the participant visited the event location, whether the event at that location is true or false, and how reliable the participant is. based on these observations, we model that each event location has certain popularity, which in uences the possibility of a randomly selected participant to visit that location. this is motivated by the fact that some locations naturally attract more people while others attract fewer. moreover, we treat the truth of events as latent variables and model three way participant reliability, including true positive rate and false positive rate while present at a location and reporting rate while absent from a location. by doing so, positive and missing reports become random variables generated by conditioning on all these factors. our approach thus directly incorporates the mobility issues in the model, can ef ciently handle missing reports and can automatically infer the truth of events and different aspects of participant reliability. moreover, it is unsupervised and avoids location tracking. in summary, this paper makes the following contributions: we propose to address the truth discovery problem in crowdsourced detection of spatial events. our algorithms facilitate rapid coverage while maintaining positioning accuracy comparable to that achievable with survey driven indoor deployments. some recent systems have incorporated surveying by users. most current methods for based indoor localization depend on surveys conducted by experts or skilled technicians. structuring localization systems organically, however, introduces its own set of challenges: conveying uncertainty, determining when user input is actually required, and discounting erroneous and stale data. through deployment of an organic location system in our nine story building, which contains nearly, distinct spaces, we evaluate new algorithms for addressing these challenges. we describe the use of voronoi regions for conveying uncertainty and reasoning about gaps in coverage, and a clustering method for identifying potentially erroneous user data. while these methods can localize indoors to within a few meters in regions with high infrastructure coverage, they have a high deployment burden. surveyors must methodically walk from room to room, gaining access to all areas of a building to create the required ngerprint database. incorporation of information about a userlocation can enhance a variety of applications, including calendars, reminders, navigation assistants, and communication tools. however, most current location aware applications are restricted to permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copyotherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. these methods share underlying elements: rst, createa database that associates ambient wireless or cellular signals, or ngerprints, with physical locations; next, to localize, nd the most similar ngerprint in the database to what onedevice currently observes, and return the associated location as the result. ce building, this process can take several days and cost tens of thousands of dollars, and must be repeated when the wireless infrastructure changes. the next section provides the necessary background required to understand the algorithmic and systems contributions of the paper. section # describes our voronoi based methodforcharacterizinglocalization uncertainty. section # discusses our system implementation and user interface. for example, the locale application automatically adjusts mobile phone behavior based on location. outdoor operation; they depend upon gps, which requires clear sky visibility and may take minutes to provide a location estimate. much of the research into alternatives to gps has converged on methods that rely on existing wireless and cellular infrastructure. because this deployment cost is prohibitive for all but the most managed environments, researchers have developed systems in which usersperform the required surveying activity. while these organic location systems reduce deployment and management burden significantly, they also introduce a new set of challenges. for example, if the ngerprint database is initially empty and grows in a piecemeal fashion, thus providing location estimates of spatially varying quality, how can the system meaningfully convey to users both the need for more data, and the relative accuracy of its current location estimate how can the system determine when to prompt user surveyors for input insu cient prompting will not produce enough ngerprint data for a useful system, while too much prompting will annoy users. additionally, user surveyors will provide data of varying quality; how can the system sift through users contributions to retain accurate contributions while discarding stale, erroneous or even malicious data this paper addresses these questions through the following contributions: a voronoi diagram based method for conveying localizer uncertainty and increasing coverage; aclustering based methodthat automatically discards erroneous user input through outlier detection in the signal space; and an evaluation of these methods during a nine day trial with nineteen users. section describes how rf scan data can be clustered and how these signal space clusters can be used to detect outliers arising from erroneous user input. section # details our evaluation of these new algorithms and our system as a whole through simulation and a live nine day deployment. the particle filter works better than other comparable methods for estimating the centers of earthquakes and the trajectories of typhoons. twitter, a popular microblogging service, has received much attention recently. an important characteristic of twitter is its real time nature. for example, when an earthquake occurs, people make many twitter posts related to the earthquake, which enables detection of earthquake occurrence promptly, simply by observing the tweets. as described in this paper, we investigate the real time interaction of events such as earthquakes in twitter and propose an algorithm to monitor tweets and to detect a target event. to detect a target event, we devise a classifier of tweets based on features such as the keywords in a tweet, the number of words, and their context. subsequently, we produce a probabilistic spatiotemporal model for the target event that can find the center and the trajectory of the event location. we consider each twitter user as a sensor and apply kalman filtering and particle filtering, which are widely used for location estimation in ubiquitous pervasive computing. as an application, we construct an earthquake reporting system in japan. because of the numerous earthquakes and the large number of twitter users throughout the country, we can detect an earthquake with high probability seismic intensity scale or more are detected merely by monitoring tweets. our system detects earthquakes promptly and sendsmails to registered users. notification is delivered much faster than the announcements that are broadcast by the jma. consequently, social sensors are very noisy compared to ordinal physical sensors. as an application, we develop an earthquake reporting system usingjapanese tweets. in section #, we describe the experiments and evaluation of event detection. twitter, a popular microblogging service, has received much attention recently. it is an online social network used by millions of people around the world to remain socially connected to their friends, family members and co workers through their computers and mobile phones. twitter asks one question, whathappening answers must be fewer than characters. astatus update message, calleda tweet, is often used as a message to friends and colleagues. a user can follow other users; her followers can read her tweets. a user who is being followed by another user need not necessarily reciprocate by following them back, which renders the links of the network as directed. after its launch on july year#, twitter users have increased rapidly. monthly growth of users has been year# year on year, which makes twitter one of the fastest growing sites in the world. some researchers have examined twitter: java et al analyzed twitter as early as year#. they described the social network of twitter users and investigated the motivation of twitter users. they discovered that the relation between friends is the key to understanding interaction in twitter. recently, boyd et al investigated retweet activity, which is the twitter equivalent ofmail forwarding, by which users post messages originally posted by others. microblogging is a form of blogging that allows users to send brief text updates or micromedia such as photographs or audio clips. microblogging services other than twitter include tumblr, plurk, emote in, squeelr, jaiku, identi ca, and others. some examples are the following: squeelr adds geolocation and pictures to microblogging; and plurk has a timeline view integrating video and picture sharing. although our study, which is based on their real time nature, is applicable to other microblogging services, we speci cally examine twitter in this study becauseof itspopularity anddatavolume. an important common characteristic among microblogging services is their real time nature. although blog users http: www techcrunch com year# twitter reaches million people worldwide in june comscore according to a report from http: blog nielsen com nielsenwire online mobile twitterstweet smell of success www tumblr com, www plurk com, www emote in, www squeelr com, www jaiku com, identi ca figure #: earthquake map. typically update their blogs once every several days, twitter users write tweets several times in a single day. users can know how other users are doing and often what they are thinking about now, users repeatedly return to the site and check to see what other people are doing. the large number of updates results in numerous reports related to events. they include social events such as parties, baseball games, and presidential campaigns. they also include disastrous events such as storms, res, tra. actually, twitter is used for various real time noti cation such as that necessary for help during a large scale re emergency and live tra. adam ostrow, an editor in chief at mashable, a social media news blog, wrote in his blog about the interesting phenomenon of real time media as follows: japan earthquake shakes twitter users. and beyonce: earthquakes are one thing you canbet onbeingcovered on twitter rst, because, quite frankly, if the ground is shaking, you re going to tweet about it before it even registers with the usgs and long before it gets reported by the media. that seems to be the case again today, as the third earthquake in a week has hit japan and its surrounding islands, about an hour ago. the rst user we can nd that tweeted about it was ricardo duran of scottsdale, az, who, judging from his twitter feed, has been traveling the world, arriving in japan yesterday. this post well represents the motivation of our study. the research question of our study is, can we detect such event occurrence in real time by monitoring tweets this paper presents an investigation of the real time nature of twitter and proposes an event noti cation system that monitors tweets and delivers noti cation promptly. to obtain tweets on the target event precisely, we apply semantic analysis of a tweet: for example, users might make tweets http: mashable com year# japan earthquake such as earthquake or now it is shaking, for which earthquake or shaking could be keywords, but users might also make tweets such asam attending an earthquake conference, oram shaking hands with his boss. we prepare the training data and devise a classi er using a support vector machine based on features such as keywords in a tweet, the number of words, and the context of target event words. after doing so, we can produce a probabilistic spatiotemporal model of an event. we make a crucial assumption: each twitter user is regarded as a sensor and each tweet as sensory information. these virtual sensors, which we call social sensors, areofahugevariety andhavevariouscharacteristics: some sensors are very active; others are not. regarding each twitter user as a sensor, the event detection problem can be reduced into one of object detection and location estimation in a ubiquitous pervasive computing environment in which we have numerous location sensors: a user has a mobile device or an active badge in an environment where sensors are placed. through infrared communication or a wifi signal, the user location is estimated as providing location based services such as navigation and museum guides. we apply kalman lters and particle lters, which are widely used for location estimation in ubiquitous pervasive computing. because japan has numerous earthquakes and because twitter users are similarly numerous and geographically dispersed throughout the country, it is sometimes possible to detect an earthquake by monitoring tweets. in other words, many earthquake events occur in japan. figure #portraysa map of twitter usersworldwide; figure # depicts a map of earthquake occurrences worldwide. it is apparent that the only intersection of the two maps, which means regions with many earthquakes and large twitter users, is japan. other regions such as indonesia, turkey, iran, italy, and paci. coastal us cities such as los angeles and san francisco also roughly intersect, although their respective densities are much lower than in japan. our system detects an earthquake occurrence and sends anmail, possibly before an earthquake actually arrives at a certain location: an earthquake propagates at about km. for that reason, aperson who is km distant from an earthquake has about before the arrival of an earthquake wave. we present a brief overview of twitter in japan: the japanese version of twitter was launched on april year#. at the time of this writing, japan has the th largest number of users in the world. although event detection is currentlypossiblebecause of the high density of twitter users and earthquakes in japan, our study is useful to detect events of various types throughout the world. the contributions of this paper are summarized as follows: the paper provides an example of integration of semantic analysis and real time nature of twitter, and presents potential uses for twitter data. http: blog twitter com year# twitter web tra. aroundworld html for earthquake prediction and early warning, many studies have been made in the seismology eld. this paper presents an innovative social approach that has not been reported before in the literature. this paper is organized as follows: in the next section, we explain semantic analysis and sensory information, followed by the spatiotemporal model in section #. the earthquake reporting system is introduced into section #. section # is devoted to an explanation of related works and discussion. the work is motivated by the emergence of social sensing as a data collection paradigm of growing interest, where humans perform sensory data collection tasks. a challenge in social sensing applications lies in the noisy nature of data. unlike the case with well calibrated and well tested infrastructure sensors, humans are less reliable, and the likelihood that participants measurements are correct is often unknown a priori. given a set of human participants of unknown reliability together with their sensory measurements, this paper poses the question of whether one can use this information alone to determine, in an analytically founded manner, the probability that a given measurement is true. while some previous work approached the answer in a heuristic manner, we offer the first optimal solution to the above truth discovery problem. this paper addresses the challenge of truth discovery from noisy social sensing data. optimality, in the sense of maximum likelihood estimation, is attained by solving an expectation maximization problem that returns the best guess regarding the correctness of each measurement. the approach is shown to outperform the state of the art fact finding heuristics, as well as simple baselines such as majority voting. social sensing has emerged as a new paradigm for collecting sensory measurements by means of crowd sourcing sensory data collectiontasksto ahumanpopulation. theparadigm is made possible by the proliferation of a variety of sensors in the possession of common individuals, together with networking capabilities that enable data sharing. examples includes cell phone accelerometers, cameras, gps devices, smart power meters, and interactive game consoles. individuals who own such sensors can thus engage in data collection for some purpose of mutual interest. a classical example is geotagging campaigns, where participants report locations of conditions in their environment that need attention. data collection is often open to a large population. hence, the participants and their reliability are typically not known a priori. reliability may be impaired because of poor used sensor quality, lack of sensor calibration, lack of attentiontothetask, or evenintenttodeceive. the question posed in this paper is whether or not we can determine, given only the measurements sent and without knowing the reliability of sources, which of the reported observations are true and which are not. in this paper, we concern ourselves with binary measurements only. we develop a maximum likelihood estimator that assigns truth values to measurements withoutpriorknowledge of source reliability. it is shown to be very accurate in assessing measurement correctness as long as sources, on average, make multiple observations, and as long as some sources make the same observation. note that, a trivial way of accomplishing the truth discovery task is by believing only those observations that are reported by a su cient number of sources. the problem with voting schemes is that they do not attempt to infer source reliability and do not take that estimate into account. we solve the problem using the expectation maximization algorithm. expectationmaximization is ageneral optimization technique for nding the maximum likelihood estimation of parameters in a statistic model where the data are incomplete. the paper shows that social sensing applicationslend themselves nicely to an em formulation. the optimal solution, in the sense of maximum likelihood estimation, directly leads to an accurate quanti cation of measurement correctness as well as participant reliability. prior literature attempted to solve a similar trust analysis problem in information networks using heuristics whose inspiration can be traced back to googlepagerank. pagerank iteratively ranks the credibility of sources on the web, by iteratively considering the credibility of sources who link to them. extensions of pagerank, known as fact nders, iteratively compute the credibility of sources and claims. speci cally, they estimate the credibility of claims from the credibility of sources that make them, then estimate the credibility of sources based on the credibility of their claims. several algorithms exist that feature modi cations ofthe abovebasicheuristic scheme. in contrast, ours is the rst attempt to optimally solve the truthdiscoveryproblemin social sensingby castingit as one of expectation maximization. we evaluate our algorithmin simulation, an emulatedgeotagging scenario as well as a real world social sensing application. the rest of this paper is organized as follows: we review related workinsection. insection, wepresentthetruth discovery model for social sensing applications. we discuss the limitations of current model and future work in section #. finally, we conclude the paper in section #. thispaperpresents a maximumlikelihood estimation approach to truth discovery from social sensing data. asigni cant challengein social sensing applicationsliesin ascertaining the correctness of collected data. theterm, participant reliability is used in this papertodenotetheprobability thattheparticipant reports correct observations. thealgorithm makes inferences regarding both source reliability and measurement correctness by observing which observations coincide and which don. instead, wecast thetruthdiscoveryproblem as one ofjoint maximumlikelihood estimation of both source reliability and observation correctness. evaluationresultsshowthattheproposedmaximum likelihood scheme outperforms the state of art heuristics as well assimplebaselines inquantifying theprobability of measurement correctness and participant reliability. the proposed maximum likelihood estimation approach is discussed insection. hence, observations made by several unreliable sources may be believed overthosemadeby afew reliableones. the explosive growth in social network content suggests that the largest sensor network yet might be human. extending the participatory sensing model, this paper explores the prospect of utilizing social networks as sensor networks, which gives rise to an interesting reliable sensing problem. in this problem, individuals are represented by sensors who occasionally make observations about the physical world. these observations may be true or false, and hence are viewed as binary claims. the reliable sensing problem is to determine the correctness of reported observations. from a networked sensing standpoint, what makes this sensing problem formulation different is that, in the case of human participants, not only is the reliability of sources usually unknown but also the original data provenance may be uncertain. individuals may report observations made by others as their own. the contribution of this paper lies in developing a model that considers the impact of such information sharing on the analytical foundations of reliable sensing, and embed it into a tool called apollo that uses twitter as a sensor network for observing events in the physical world. evaluation, using twitter based case studies, shows good correspondence between observations deemed correct by apollo and ground truth. we propose unloc, an unsupervised indoor localization scheme that bypasses the need for war driving. our key observation is that certain locations in an indoor environment present identifiable signatures on one or more sensing dimensions. we hypothesize that these kind of signatures naturally exist in the environment, and can be envisioned as internal landmarks of a building. mobile devices that sense these landmarks can recalibrate their locations, while dead reckoning schemes can track them between landmarks. results from different indoor settings, including a shopping mall, demonstrate median location errors of. we believe this is an unconventional approach to indoor localization, holding promise for real world deployment. an elevator, for instance, imposes a distinct pattern on a smartphone accelerometer; a corridor corner may overhear a unique set of wifi access points; a specific spot may experience an unusual magnetic fluctuation. war driving is not necessary, neither are floorplans the system simultaneously computes the locations of users and landmarks, in a manner that they converge reasonably quickly. in trying to trace the reasons, we distilled two main messages: indoor spaces require fairly permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. for instance, location error outdoors may still indicate the same street, but indoors may mean two different aisles in a grocery store. an inventory management application may very well require aisle level precision. this zero sum game between accuracy and calibration overhead has been an important hurdle to deploying indoor localization systems. therefore, outdoor localization schemes like compacc have triggered periodic gps measurements to recalibrate the userlocation. unfortunately, gps is unreliable indoors, rendering dead reckoning based approaches useless. for example, these signatures can be treated as landmarks, useful to recalibrate indoor dead reckoning. past work has mostly relied on signal calibration to develop wifi based localization. thus, a landmark signature need not be unique in the entire building so long as it is unique within a wifi sub space, it can be recognized without ambiguity. unloc combines these ideas into a framework for unsupervised localization. by assimilating data from these devices, unloc detects sensory signatures that are unique within their respective wifi sub spaces. since dead reckoning provides a rough location to the phone, it is also possible to roughly localize the signatures based on when the phone senses them. these landmarks can then be used to improve dead reckoning of subsequent phones, which in turn can re ne the landmark locations. this recursive process continues to improve localization accuracy over time. observe that, the system does not need calibration the rst few users may experience inferior location accuracy, but a little more data brings the system to convergence. extracting the accurate location of the landmarks from multiple erroneous locations is problematic, since the errors may not be all equal. performance results show median localization accuracy of when unloc runs online, and when the locations are computed of ine. we believe this could be a promising direction, and with rigorous testing and tuning, a potential candidate for the real world. our main contributions may be summarized as follows: we identify an opportunity to simultaneously harness sensor based dead reckoning and environment sensing for localization. buildings such that dead reckoning is practical and reason ably accurate. we develop unloc on the android os, and evaluate across different indoor spaces, including the north gate shopping mall in durham. despite innovative research, indoor localization is still not in the mainstream. or commercial advantage and that copies bear this notice and the full citation on the rst page. mobisys, june, year#, low wood bay, lake district, uk. high location accuracy because the contexts vary at ner spatial granularity. while such precision is attainable with pervasive wifi systems, they come at a prohibitively high cost, mostly in the form of meticulous calibration. such calibration is not necessarily a one time cost since rf ngerprints could change, perhaps due to changes in layout and objects in the environment. attempts to simplify the calibration process have been successful, but at the expense of reduced location accuracy. this paper is tasked to break away from this tradeoff, and achieve meter level accuracy with zero calibration. although a high bar, we believe this is feasible and make an attempt to do so. our scheme cuts across isolated ideas in mobile computing. we introduce these ideas rst, and then describe the value in making them compatible. a few recent schemes have demonstrated the ability to compute the motion trajectory of a mobile phone, using its accelerometer and compasses. due to noise in the mobile sensors, the dead reckoned trajectories are accurate in the beginning, but diverge from the truth over time. nonetheless, if one can identify other means of recalibration, dead reckoning could be applicable even indoors. urban sensing and activity recognition literature have demonstrated the ability to recognize ambiences and user behavior. for instance, inertial sensors can detect when a user is walking, turning into a corridor, or climbing up the stairs; microphones and magnetometers can detect ambient sounds and magnetic uctuations. while these signatures have been primarily used for various forms of contextawareness, they lend themselves to localization as well. we observe that wifi can be valuable even without calibration. for instance, one can use overheard wifi aps to partition an indoor space into smaller sub spaces. brie, mobile users move naturally in the building collecting accelerometer, compass, gyroscope, and wifi readings. now using the same collected data, unloc dead reckons the devices starting from a known reference location, say the entrance of the building. now, the locations of these signatures also called landmarks can be made more accurate by combining the rough estimates from multiple phones. of course, translating this idea sketch into a functional system entails a variety of challenges: dead reckoning is non trivial in indoor environments where metals and electrical equipment signi cantly affect the compass bearing. some notion of con dence on the errors need to be built, so that the estimates can be suitably weighted before combination. identifying landmark signatures from the sensed data warrants unsupervised learning on sensor features. finally, the system needs to be optimized for energy, to avoid a signi cant battery drain during localization. this paper addresses these challenges one step at a time, prototypes on a testbed of android nexus series phones, and evaluates with volunteers naturally walking in university buildings and shopping mall. the system quickly reaches convergence in less than man hours, and remains robust to dynamic changes in the environment. our approach does not require calibration or installation of additional infrastructure. we show that adequate landmarks exist inside prior work demonstrated dead reckoning mostly for outdoor environments. we achieved less than error without any pre deployment effort; in fact, we used the map of the building only to compute ground truth for evaluation. the subsequent sections expand on each of these contributions, beginning with architectural overview and intuition, followed by measurement, design, and evaluation. the core idea is recursive but not complicated. we design a practical scheme that employs unsupervised learning to extract unique sensor signatures called landmarks. local search users today decide what business to visit solely based on distance information, and business ratings that can be sparse or stale. we believe that when users search for local businesses, such as bars or restaurants, they need to know more about the ambience of each business, such as how crowded it is, how loud and of what type the music it plays is, as well as how loud the human chatter in the business is. in this paper, we propose to automatically crowdsource such rich, local business ambience metadata through real user check in events. every time a user checks into a business, the phone is in user hands, and the phone sensors can sense the business environment. we leverage the phone microphone during this time to infer the occupancy and human chatter levels, the music type, as well as the music and noise levels in the business. as people check in to businesses throughout the day, business metadata can be automatically updated over time, enabling a new generation of local search experience. using approximately audio traces collected from real businesses of various types over a period of months, we show that by properly extracting the temporal and frequency signatures of the audio signal, it is feasible to train models that can simultaneously infer occupancy, human chatter, music, and noise levels in a business, with higher than accuracy. users increasingly rely on their devices to search for local entities, typically businesses, while either on the go, looking for the next business to immediately visit, or at the comfort of their home planning their future activity. despite the rapid growth and wide adoption of local search services, the current user experience is copyright is held by the international world wide web conference committee. as figure # shows, a typical local search result page consists of a list of businesses along with usually static business metadata, such as ratings, number of reviews, pricing and phone information. this type of business ambience metadata has the potential to transform the local search experience by changing the way users select businesses to visit, and by changing the way local search algorithms rank businesses. a father that looks for a restaurant to enjoy dinner with his family can prioritize the quiet italian restaurant. at the same time, search engines can index this metadata to enable users to query local entities based on physical attributes. the key to enabling this new generation of local search experience is the extraction of accurate business ambience metadata. surprisingly, this is not a data mining or ranking problem, but rather a systems and data inferencing problem. first, user reviews are sparse covering only a small subset of the existing businesses. third, this type of business metadata can change during the day or across days, making crawling this information even more dif cult. given a local search query, the most recent business metadata available can be surfaced in the search results, enabling users to get a near real time peek into the ambience of the business. information to navigate the road network, people can now use this rich business ambience metadata to navigate the local search results more ef ciently and effectively. iw reserves the right to provide a hyperlink to the authorsite if the material is used in electronic media. dimitrios lymberopoulos, jie liu microsoft research redmond, wa, usa microsoft com mainly inherited from traditional web search, and fails to meet users needs. no information is provided about the ambience of the business, such as how crowded the business is, what type of music it plays, how loud the music is, or if outdoor seating is available at the business. for instance, consider the search results shown in figure # where rich ambience information is provided for each business. different users can now evaluate the same results much more effectively according to their current context. a young professional that wants to go out for a happy hour with his colleagues can prioritize the crowded places with loud pop music. for instance, users could submit queries such as crowded bar playing loud pop music, or quiet italian restaurants with outdoor seating. none of the commercially available search engines today have the intelligence to understand what a crowded bar or a bar playing loud pop music actually is. current search engines collect and index data that is available on the web through powerful web crawlers and data mining tools designed to leverage usergenerated content, such as reviews. this approach fails when it comes to rich business metadata for three reasons. second, important information such as how crowded a business is, or how loud the music or the human chatter in the business is, might not be available in online user reviews. in this paper, we present the design, implementation, and evaluation of a scalable system for crowdsourcing business ambience metadata. in particular, we propose to leverage the mobile phones of users visiting businesses to extract the necessary ambience metadata in real time. as shown in figure #, every time a customer checks in to a business on his mobile device, the device is placed in his hands for several tens of seconds. this is enough time for the phone to leverage its on board http: dx doi org year#. figure #: bing local search results local search results enhanced with rich business metadata. sensors to sense the environment, and infer information about the current ambience of the business. the inferred information can be leveraged in two ways. when recent information about a business is not available, historical metadata can be accumulated and analyzed over time to enable forecasting of the business ambience at any given day and time. in essence, this rich business information can become for local search what traf. to enable this experience, we leverage the phonemicrophone to record the audio signature of the business. the on board microphone is able to simultaneously capture music, noise and human chatter information, providing a rich base for ambience metadata extraction. we carefully analyze the magnitude and smoothness of audio signals at the temporal and frequency domain to extract features that capture the unique characteristics of the different audio sources, and then use these features to properly train individual models for classifying occupancy, background chatter, music, and noise levels across businesses. intuitively, occupancy, even though not directly measured by audio, can be inferred by properly exploiting the audio characteristics of background human chatter to estimate its power, and then use this as a proxy to infer occupancy in the business. in general, the higher the background chatter, the higher the occupancy of the business. furthermore, we train a model to accurately detect when people are talking near the phone, and use it to improve the resilience of the business metadata extraction process under this type of noise. using more than real business audio traces, collected over a period of months through a variety of windows phone, android, and ios devices, we demonstrate that the proposed system can classify occupancy, background chattter, music, and noise levels with higher than accuracy. as people use near real time or forecasted traf. we investigate using smartphone wifi signals to track human queues, which are common in many business areas such as retail stores, airports, and theme parks. real time monitoring of such queues would enable a wealth of new applications, such as bottleneck analysis, shift assignments, and dynamic workflow scheduling. we take a minimum infrastructure approach and thus utilize a single monitor placed close to the service area along with transmitting phones. our strategy extracts unique features embedded in signal traces to infer the critical time points when a person reaches the head of the queue and finishes service, and from these inferences we derive a person waiting and service times. we develop two approaches in our system, one is directly feature driven and the second uses a simple bayesian network. extensive experiments conducted both in the laboratory as well as in two public facilities demonstrate that our system is robust to real world environments. we show that in spite of noisy signal readings, our methods can measure service and waiting times to within a second resolution. the popular, almost addictive, usage of smartphones and their data intensive apps creates novel opportunities to exploit their network traf. research has shown, for example, how cellular call data records permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. mobisys, june, year#, bretton woods, new hampshire, usa. can be used to infer large scale transportation patterns or how cellular signal traces allow inferring the level of congestion on roadways. in this work, we ask whether signal power readings from cell phone traf. are also suf cient to monitor a much ner scale, yet common, process: human queues. such queues are a familiar and often frustrating occurrence, for example in retail stores, banks, theme parks, hospitals and transportation stations. figure # shows the abstraction we map onto these environments. as people arrive, the waiting period is the time spent waiting for service. during the service period people receive service, such as paying for items or checking in travel bags. a person exits the service area during the leaving period. note that we interpret the concept of a queue loosely, people do not need to stand in line but could sit in a waiting room and do not always need to be served in a strict rst in, rst out order. real time quanti cation of the waiting and service times in such queues allows optimizing service processes, ranging from retail, to heath care, to transportation and entertainment. for example, many hospital emergency departments surveys have average waiting times of several hours. more complete waiting and service time statistics allow customers, travelers, managers and service providers make changes to their behavior and processes. for example, an airport checkpoint might be experiencing abnormal delays and require interventions by diverting screeners from queues with shorter waiting times. customers also can bene, for example, knowing at what times retail store checkout lines can be expected to be shorter, and a customer can decide whether to stay in the queue or go to do more urgent tasks. managers can use such information to make staf ng decisions based on the service length. for example, during particular hours in a day, service times may grow at a coffee shop due to increased demands for espresso drinks compared to other items. in such a case, it might be more effective to change the staf ng to use experienced baristas as opposed to simply adding staff. a hospital emergency department may shift nursing staff to assist with triage when waiting times become too long. in the transportation eld, bus and train schedules or boarding and payment processes could be adjusted. existing solutions to the queue monitoring problem rely on cameras or special sensors and usually require sensors at multiple locations. the approaches using cameras face occlusion and increased privacy issues. bluetooth signals emanating from phones have also been used to measure travel times between two sensing points, both at airports and for vehicle traf. these prior techniques using wireless networks were too coarse grained to differentiate between the waiting and service time. moreover, all these solutions require multiple sensors to fully monitor a single longer queue, which increases installation and system cost. our approach uses only a single sensor, a wifi monitor near the head of the queue that measures the received signal strength of packets emitted from phones. intuitively, the received signal power should follow a known pattern. as the person moves towards the service point, the phone moves closer to the monitor and the received signal power should increase. when the person is receiving service, the signal power should be very strong and relatively constant. finally, when the person exits the service point, there should be a rapid decline in signal strength. our approach seeks to exploit this pattern to track service and waiting times of persons in a queue. this does require, of course, that at least some people in the queue carry phones that generate traf c. in our existing system and experiments, we have used a special app to generate periodical beacons. in the future, we expect, however, that it is possible to reuse existing traf, since it is increasingly the case as people tend to ddle with their phones while waiting in queues and many venues now offer loyalty apps that customers may use while paying, for example, the starbucks mobile app has more than million active users. while this approach may also be applicable to other wireless technologies, we have chosen wifi in our realization because its range is suf ciently large to cover the entire queue in most cases, especially for large size queues, which usually requires the installation of multiple cameras or sensors in order to obtain the complete view of the queue; it is particularly easy to monitor wifi traf. is poised to increase due to the wifi of oading trend. accurately discerning the points where the person begins and ends service is challenging, however, in this single point monitoring system, because the multi path, shadowing, and fading components of a signal are quite dynamic due to the movement of many people. we therefore investigate two approaches for extracting the waiting and service times from a signal trace: a direct featuredriven one and a bayesian network one. for both approaches, we use similar ltering techniques to reduce the impact of the dynamic environment. we experimentally evaluate both approaches in three settings: a laboratory, a coffee shop and an airport. we nd a simple bayesian classi er gives the best results, and is able to measure the waiting time to with average errors less than and maximum errors below s. our contributions are: proposing a single point wifi monitoring approach for human queue measurement systems that requires less infrastructure and can provide real time service information for both business and customers. designing queue measurement techniques that can distinguish between important time periods in human queues including waiting, service, and leaving times using unique wifi signal patterns based on feature extraction and bayesian networks. leveraging the availability of multiple antennas and multiple smartphone users in the queue to accurately perform queue parameter estimation. demonstrating through extensive experiments with participants randomly placed in a variety of queue patterns in real environments that it is figure #: important time periods and corresponding positions in a human queue. feasible to track human queues using single point wifi monitoring with high accuracy. the rest of the paper is organized as follows. in section, we investigate whether a suf cient amount of wifi users are present in human queues to facilitate queue measurements. we describe the challenges, system overview, and two core schemes to measure parameters of human queues in section #. we present our system implementation by leveraging multiple antennas and smartphone user traces in section #. in section #, we perform extensive evaluation of our system in real environments, ie, a coffee shop and an airport. we discuss limitations in section # and put our work into the context of related research in section #. the world wide web has become the most important information source for most of us. unfortunately, there is no guarantee for the correctness of information on the web. moreover, different web sites often provide conflicting information on a subject, such as different specifications for the same product. in this paper we propose a new problem called veracity, ie, conformity to truth, which studies how to find true facts from a large amount of conflicting information on many subjects that is provided by various web sites. we design a general framework for the veracity problem, and invent an algorithm calledruthinder, which utilizes the relationships between web sites and their information, ie, a web site is trustworthy if it provides many pieces of true information, and a piece of information is likely to be true if it is provided by many trustworthy web sites. our experiments show thatruthinder successfully finds true facts among conflicting information, and identifies trustworthy web sites better than the popular search engines. the world wide web has become a necessary part of our lives, and might have become the most important information source for most people. when looking for inter the work was supported in part by the. any opinions, ndings, and conclusions or recommendations expressed here are those of the authors and do not necessarily re ect the views of the funding agencies. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. is the world wide web always trustable unfortunately, the answer is no. there is no guarantee for the correctness of information on the web. we tried to nd out who wrote the book rapid contextual design. we found many di erent sets of authors from di erent online bookstores, and we show several of them in table #. there are often con icting facts on the web, such as di erent sets of authors for a book. a fact is likely to be true if it is provided by trustworthy web sites. at each iteration, the probabilities of facts being true and the trustworthiness of web sites are inferred from each other. the rst di erence is in the de nitions. second and more importantly, di erent facts in uence each other. second, we propose a framework to solve this problem, by de ning the trustworthiness of web sites, con dence of facts, and in uences between facts. everyday people retrieve all kinds of information from the web. for example, when shopping online, people nd product speci cations from web sites like amazon com or shopzilla com. national science foundation nsf iis year# and nsf bdi year#. esting dvds, they get information and read movie reviews on web sites such as netflix com or imdb com. even worse, di erent web sites often provide con icting information, as shown below. from the image of the book cover we found that a books provides the most accurate information. in comparison, the information from powellbooks is incomplete, and that from lakeside books is incorrect. web site authors a books karen holtzblatt, jessamyn burns wendell, shelley wood powellbooks holtzblatt, karen cornwall books holtzblatt karen, wendell jessamyn burns, wood mellonbooks wendell, jessamyn lakeside books wendell, jessamynholtzblatt, karenwood, shelley blackwell online wendell, jessamyn, holtzblatt, karen, wood, shelley barnes noble karen holtzblatt, jessamyn wendell, shelley wood table #: con icting information about book authors the trustworthiness problem of the web has been realized by todayinternet users. according to a survey on credibility of web sites, of internet users trust news web sites at least most of time, while this ratio is only for web sites that sell products, and is merely for blogs. there have been many studies on ranking web pages according to authority based on hyperlinks, such as authority hub analysis, pagerank, and more general link based analysis. but does authority or popularity of web sites lead to accuracy of information the answer is unfortunately no. for example, according to our experiments the bookstores ranked on top by google contain many errors on book author information, and some small bookstores provide more accurate information. in this paper we propose a new problem called veracity problem, which is formulated as follows: given a large amount of con icting information about many objects, which is provided by multiple web sites, how to discover the true fact about each object. we use the word fact to represent something that web sites facts objects figure #: input of truthfinder is claimed as a fact by some web site, and such a fact can be either true or false. there are also many web sites, some of which are more trustworthy than some others. a web site is trustworthy if most facts it provides are true. because of this inter dependency between facts and web sites, we choose an iterative computational method. this iterative procedure is rather di erent from authority hub analysis. the trustworthiness of a web site does not depend on how many facts it provides, but on the accuracy of those facts. nor can we compute the probability of a fact being true by adding up the trustworthiness of web sites providing it. for example, if a web site says a book is written by jessamyn wendell, and another says jessamyn burns wendell, then these two web sites actually support each other although they provide slightly di erent facts. in summary, we make three major contributions in this paper. first, we formulate the veracity problem about how to discover true facts from con icting information. finally, we propose an algorithm called truthfinder for identifying true facts using iterative methods. the rest of the paper is organized as follows. we describe the problem in section #, and propose the computational model in section #. experimental results are presented in section #, and we conclude this study in section #.