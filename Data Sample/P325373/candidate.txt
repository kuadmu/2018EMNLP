as improvements in per transistor speed and energy efficiency diminish, radical departures from conventional approaches are becoming critical to improving the performance and energy efficiency of general purpose processors. we propose a solution from circuit to compiler that enables general purpose use of limited precision, analog hardwareto accelerate approximable code code that can tolerate imprecise execution. we utilize an algorithmic transformation that automatically converts approximable regions of code from a von neumann model to an analog neural model. we outline the challenges of taking an analog approach, including restricted range value encoding, limited precision in computation, circuit inaccuracies, noise, and constraints on supported topologies. we address these limitations with a combination of circuit techniques, a hardware software interface, neuralnetwork training techniques, and compiler support. analog neural acceleration provides whole application speedup of and energy savings of with quality loss less than for all except one benchmark. these results show that using limited precision analog circuits for code acceleration, through a neural approach, is both feasible and beneficial over a range of approximation tolerant, emerging applications including financial analysis, signal processing, robotics, gaming, compression, and image processing. energy ef ciency now fundamentally limits microprocessor performance gains. cmos scaling no longer provides gains in ef ciency commensurate with transistor density increases. as a result, both the semiconductor industry and the research community are increasingly focused on specialized accelerators, which provide large gains in ef ciency and performance by restricting the workloads that bene t. the community is facing an iron triangle; we can choose any two of performance, ef ciency, and generality at the expense of the third. before the effective end of dennard scaling, we improved all three consistently for decades. solutions that improve performance and ef ciency, while retaining as much generality as possible, are highly desirable, hence the growing interest in gpgpus and fpgas. a growing body of recent work has focused on approximation as a strategy for the iron triangle. many classes of applications can tolerate small errors in their outputs with no discernible loss in qor. jongse park§ bradley thwaites§ luis ceze doug burger §georgia institute of technology microsoft research jspark gatech edu bthwaites gatech edu luisceze cs washington edu dburger microsoft com many conventional techniques in energy ef cient computing navigate a design space de ned by the two dimensions of performance and energy, and traditionally trade one for the other. general purpose approximate computing explores a third dimension that of error. many design alternatives become possible once precision is relaxed. an obvious candidate is the use of analog circuits for computation. however, computation in the analog domain has several major challenges, even when small errors are permissible. first, analog circuits tend to be special purpose, good for only speci. second, the bit widths they can accommodate are smaller than current oating point standards, since the ranges must be represented by physical voltage or current levels. another consideration is determining where the boundaries between digital and analog computation lie. using individual analog operations will not be effective due to the overhead of aanda conversions. finally, effective storage of temporary analog results is challenging in current cmos technologies. these limitations has made it ineffective to design analog von neumann processors that can be programmed with conventional languages. despite these challenges, the potential performance and energy gains from analog execution are highly attractive. an important challenge is thus to architect designs where a significant portion of the computation can be run in the analog domain, while also addressing the issues of value range, domain conversions, and relative error. recent work on neural processing units may provide a possible approach. npu enabled systems rely on an algorithmic transformation that converts regions of approximable general purpose code into a neural representation at compile time. at run time, the processor invokes the npu instead of running the original code. npus have shown large performance and ef ciency gains, since they subsume an entire code region. they have an added advantage in that they convert many distinct code patterns into a common representation that can be run on a single physical accelerator, improving generality. npus may be a good match for mixed signal implementations for a number of reasons. first, prior research has shown that neural networks can be implemented in analog domain to solve classes of domain speci. second, the process of invoking a neural network and returning a result de nes a clean, year# ieee coarse grained interface fora and aconversion. third, the compile time training of the network permits any analogspeci. the programmer simply speci es which region of the code can be approximated, without adding any neural network speci. thus, no additional changes to the programming model are necessary. in this paper we evaluate an npu design with mixed signal components and develop a compilation work ow for utilizing the mixed signal npu for code acceleration. the goal of this study is to investigate challenges and de ne potential solutions to enable effective mixed signal npu execution. the objective is to both bound application error to suf ciently low levels and achieve worthwhile performance or ef ciency gains for general purpose approximable code. this study makes the following four ndings: due to range limitations, it is necessary to limit the scope of the analog execution to a single neuron; inter neuron communication should be in the digital domain. approximating ideal program outputs is a common technique for solving computationally difficult problems, for adhering to processing or timing constraints, and for performance optimization in situations where perfect precision is not necessary. to this end, programmers often use approximation algorithms, iterative methods, data resampling, and other heuristics. however, programming such variable accuracy algorithms presents difficult challenges since the optimal algorithms and parameters may change with different accuracy requirements and usage environments. this problem is further compounded when multiple variable accuracy algorithms are nested together due to the complex way that accuracy requirements can propagate across algorithms and because of the size of the set of allowable compositions. as a result, programmers often deal with this issue in an ad hoc manner that can sometimes violate sound programming practices such as maintaining library abstractions. in this paper, we propose language extensions that expose trade offs between time and accuracy to the compiler. the compiler performs fully automatic compile time and installtime autotuning and analyses in order to construct optimized algorithms to achieve any given target accuracy. we present novel compiler techniques and a structured genetic tuning algorithm to search the space of candidate algorithms and accuracies in the presence of recursion and sub calls to other variable accuracy code. these techniques benefit both the library writer, by providing an easy way to describe and search the parameter and algorithmic choice space, and the library user, by allowing high level specification of accuracy requirements which are then met automatically without the need for the user to understand any algorithm specific parameters. additionally, we present a new suite of benchmarks, written in our language, to examine the efficacy of our techniques. our experimental results show that by relaxing accuracy requirements, we can easily obtain performance improvements ranging from to orders of magnitude of speedup. energy efficient computing is important in several systems ranging from embedded devices to large scale data centers. several application domains offer the opportunity to tradeoff quality of service solution for improvements in performance and reduction in energy consumption. programmers sometimes take advantage of such opportunities, albeit in an ad hoc manner and often without providing any qos guarantees. we propose a system called green that provides a simple and flexible framework that allows programmers to take advantage of such approximation opportunities in a systematic manner while providing statistical qos guarantees. green enables programmers to approximate expensive functions and loops and operates in two phases. in the calibration phase, it builds a model of the qos loss produced by the approximation. this model is used in the operational phase to make approximation decisions based on the qos constraints specified by the programmer. the operational phase also includes an adaptation function that occasionally monitors the runtime behavior and changes the approximation decisions and qos model to provide strong statistical qos guarantees. to evaluate the effectiveness of green, we implemented our system and language extensions using the phoenix compiler framework. our experiments using benchmarks from domains such as graphics, machine learning, signal processing, and finance, and an in production, real world web search engine, indicate that green can produce significant improvements in performance and energy consumption with small and controlled qos degradation. emerging applications increasingly use estimates such as sensor data, probabilistic models, machine learning, big data, and human data. unfortunately, representing this uncertain data with discrete types encourages developers to pretend it is not probabilistic, which causes three types of uncertainty bugs. using estimates as facts ignores random error in estimates. boolean questions on probabilistic data induce false positives and negatives. this paper introduces uncertain whereas previous probabilistic programming languages focus on experts, uncertain. applications that sense and reason about the complexity of the world use estimates. mobile phone applications estimate location with gps sensors, search estimates information needs from search terms, machine learning estimates hidden parameters from data, and approximate hardware estimates precise hardware to improve energy ef ciency. the difference between an estimate and its true value is uncertainty. every estimate has uncertainty due to random or systematic error. random variables model uncertainty with probability distributions, which assign a probability to each possible value. for example, each ip of a biased coin may have a chance of heads and chance of tails. the outcome of one ip is only a sample and not a good estimate of the true value. figure # shows a sample from a gaussian distribution which is a poor approximation for the entire distribution. most programming languages force developers to reason about uncertain data with discrete types. motivated application developers reason about uncertainty in ad hoc ways, but because this task is complex, many more simply ignore uncertainty. for instance, we surveyed popular smartphone applications that use gps and nd only one reasons about the error in gps measurements. ignoring uncertainty creates three types of uncertainty bugs which developers need help to avoid: using estimates as facts ignores random noise in data and introduces errors. computation compounds errors since computations on uncertain data often degrade accuracy signi cantly. conditionals ask boolean questions of probabilistic data, leading to false positives and false negatives. solutions address parts of this problem, they demand expertise far beyond what client applications require. for example in current probabilistic programming languages, domain experts create and query distributions through generative models. current apis for estimated data from these programs, sensors, big data, machine learning, and other sources then project the resulting distributions into discrete types. we observe that the probabilistic nature of estimated data does not stop at the api boundary. applications using estimated data are probabilistic programs too existing languages do not consider the needs of applications that consume estimated data, leaving their developers to face this dif cult problem unaided. this paper introduces the uncertain type, uncertain, a programming language abstraction for arbitrary probability distributions. the syntax and semantics emphasize simplicity for non experts. we describe how expert developers derive and expose probability distributions for estimated data. similar to probabilistic programming, the uncertain type de nes an algebra over random variables to propagate uncertainty through calculations. we introduce a bayesian network semantics for computations and conditional expressions. instead of eagerly evaluating probabilistic computations as in prior languages, we lazily evaluate evidence for the conditions. finally, we show how the uncertain type eases the use of prior knowledge to improve estimates. our novel implementation strategy performs lazy evaluation by exploiting the semantics of conditionals. the uncertain runtime creates a bayesian network that represents computations on distributions and then samples it at conditional expressions. a sample executes the computations in the network. the runtime exploits hypothesis tests to take only as many samples as necessary for the particular conditional, rather than eagerly and exhaustively producing unnecessary precision. these hypothesis tests both guarantee accuracy bounds and provide high performance. we demonstrate these claims with three case studies. we show how uncertain improves accuracy and expressiveness of speed computations from gps, a widely used hardware sensor. we show how uncertain exploits prior knowledge to minimize random noise in digital sensors. we show how uncertain encourages developers to explicitly reason about and improve accuracy in machine learning, using a neural network that approximates hardware. in concert, the syntax, semantics, and case studies illustrate that uncertain eases probabilistic reasoning, improves estimates, and helps domain experts and developers work with uncertain data. our contributions are characterizing uncertainty bugs; uncertain, an abstraction and semantics for uncertain data; implementation strategies that make this semantics practical; and case studies that show uncertain potential to improve expressiveness and correctness. approximate program transformations such as skipping tasks, loop perforation, reduction sampling, multiple selectable implementations, dynamic knobs, synchronization elimination, approximate function memoization, and approximate data types produce programs that can execute at a variety of points in an underlying performance versus accuracy tradeoff space. we present language constructs for developing and specifying relaxed programs. these transformed programs have the ability to trade accuracy of their results for increased performance by dynamically and nondeterministically modifying variables that control their execution. we call such transformed programs relaxed programs because they have been extended with additional nondeterminism to relax their semantics and enable greater flexibility in their execution. we also present proof rules for reasoning about properties which the program must satisfy to be acceptable. our proof rules work with two kinds of acceptability properties: acceptability properties, which characterize desired relationships between the values of variables in the original and relaxed programs, and unary acceptability properties, which involve values only from a single program. the proof rules support a staged reasoning approach in which the majority of the reasoning effort works with the original program. exploiting the common structure that the original and relaxed programs share, relational reasoning transfers reasoning effort from the original program to prove properties of the relaxed program. we have formalized the dynamic semantics of our target programming language and the proof rules in coq and verified that the proof rules are sound with respect to the dynamic semantics. our coq implementation enables developers to obtain fully machine checked verifications of their relaxed programs. these relational assertions facilitate the overall veri cation of the relaxed program. the assertstatement states thatmust hold at the point where the statement appears. accuracy properties formalize how accurate the outputs must be to stay within the acceptable range. for example, a perforatable loop may produce a range of acceptable results, with some more accurate than others. these properties assure developers that veri ed relaxation produces a program that satis es the stated acceptability properties. coq veri cation framework we have formalized the dynamic original and relaxed semantics with the coq proof assistant. we have also used coq to formal ize our proof rules and obtain a fully machine checked proof that the rules are sound with respect to the dynamic semantics and provide the stated semantic properties. our coq formalization makes it possible to develop fully machine checked veri cations of relaxed programs. our coq implementation contains approximately lines of code and proof scripts, with year# lines devoted to the original semantics and its soundness proofs and year# additional lines devoted to the relaxed semantics and its soundness proofs. relational reasoning and proof rules: it presents a basic reasoning approach and proof rules for verifying acceptability properties of relaxed programs. coq formalization and soundness results: it presents a formalization of the dynamic semantics and proof rules in coq. acceptability to use these relaxation mechanisms, a developer must ensure that the resulting relaxed program is acceptable. we formalize acceptability as acceptability properties that the program must satisfy to be acceptable. reasoning about relaxed programs we present language constructs for developing relaxed programs and stating acceptability properties. we also present proof rules for verifying the stated acceptability properties. relaxed programming constructs basic relaxed programming constructs include nondeterministic variable assignments, relational assertions that relate the relaxed semantics to the original semantics, unary assertions, and unary assumptions. the relatestatement asserts that the predicatemust hold at the program point where the statement appears. the predicateis a relational predicate it may reference values from both the original and relaxed executions. key properties of acceptable relaxed programs our approach makes it possible to formalize key properties that are critical to the development and deployment of acceptable relaxed programs. the assume statement provides developers with the ability to state assumptions that they believe to be true in the original program. but if an assumption is not valid, the program may fail or exhibit unintended behaviors. axiomatic relaxed semantics: the relational hoare style semantics relates executions in the relaxed semantics to executions in the original semantics. relaxed progress: if the program veri es under both the original and relaxed axiomatic semantics and no execution in the dynamic original semantics violates an assumption, then no execution in the dynamic relaxed semantics violates an assertion or an assumption. relaxed progress modulo original assumptions: if the program veri es under both the original and relaxed axiomatic semantics, then if an execution in the dynamic relaxed semantics violates an assertion or an assumption, then an execution in the dynamic original semantics violates an assumption. the variants often occupy a range of points in an underlying performance versus accuracy trade off space. but to successfully deploy relaxed programs, developers need to have con dence that the relaxation satis es important acceptability properties. in recent years researchers have developed a range of mechanisms for dynamically varying application behavior. typical goals include maximizing performance subject to an accuracy constraint, maximizing accuracy subject to a performance constraint, or dynamically adjusting program behavior to adapt to changes in the characteristics of the underlying hardware platform. mechanisms include skip ping tasks, loop perforation, sampling reduction inputs, multiple selectable implementations of a given component or components, dynamic knobs, synchronization elimination, approximate function memo ization, and approximate data types. all of these mechanisms can produce a relaxed program a program that may adjust its execution by changing one or more variables subject to a speci ed relaxation predicate. for example, a perforated program may dynamically choose to skip loop iterations each time it enters a given loop. a relaxed program is therefore a nondeterministic program, with each execution a variant of the original execution. the different executions typically share a common global structure, with local differences at only those parts of the computation affected by the modi ed variables. acceptability properties include integrity properties that the program must satisfy to successfully produce a result and accuracy properties that characterize how accurate the produced result must be. for example, an integrity property might state that a function must return a value greater than zero, while an accuracy property might state that the relaxed program must produce a result that differs by at most a speci ed percentage from the result that the original program produces. our language and proof rules support a staged approach in which the developer rst develops a standard program and uses standard approaches to reason about this program to determine that it satis es desired acceptability properties. we refer to the dynamic semantics of the program at this stage as the original semantics of the program. either the developer or an automated system then relaxes the program to enable additional nondeterministic executions. we refer to the dynamic semantics of the program at this stage as the relaxed semantics of the program. finally, the developer uses relational reasoning to verify that the relaxation maintains the desired acceptability properties. speci cally, the developer speci es and veri es additional relational assertions that characterize the relationship between the original and relaxed semantics. this approach is designed to reduce the overall reasoning effort by exploiting the common structure that the original and relaxed programs share. with this approach the majority of the reasoning effort works with the original program and is then transferred via relational reasoning to verify the nondeterministic relaxed program. the relax st statement speci es a nondeterministic assignment to the set of variables. speci cally, the relax statement can assign the variables into any set of values that satis es the relaxation predicate. when added to the program, the relax statement affects only the relaxed semantics of the program; in the original semantics it has no effect. so, for example, the statement might require the value of a variablein relaxed executions to be greater than or equal to the value ofin the original execution. in contrast to the relate statement, is a unary predicate it only references values from a single execution as in a standard assertion. in the original semantics, our proof rules generate an obligation to prove that an assert statement holds for all executions. to ensure that the assert statement remains valid in the relaxed semantics, our proof rule in the relaxed semantics generates an obligation to prove that if the assertion is valid in the original semantics, then the current relation between the original and relaxed semantics establishes that the assertion is valid in the relaxed semantics. for example, it may be possible to prove that all the variables referenced in an assertion have the same values in the original and relaxed semanticsie, the relaxation does not interfere with the assertion. in this way, relational reasoning serves as a bridge to transfer properties of the original program over to the relaxed program. the assumestatement states that the unary predicateholds at the point where the statement appears. in the original semantics the assume statement does not generate any proof obligations the proof system simply accepts thatholds. to verify that the relaxation does not interfere with the reasoning behind the assumption, the proof rules for the relaxed semantics generate an obligation to prove that if the assumptionholds in all original executions, then it holds in all relaxed executions. the proofs work in much the same way as for the assert statement except that the proof rules do not generate an obligation to verify thatholds in the original semantics. essentially all programs have basic integrity properties that must hold for all executions of the program. examples include the absence of out of bounds array accesses or null pointer dereferences. developers typically use either assert or assume statements to formalize these integrity properties. because successful relaxations do not typically interfere with the basic integrity of the original program, the reasoning that establishes the validity of the integrity properties typically transfers directly from the original program over to the relaxed program. relational assertions that establish the equality of values of variables in the original and relaxed executions often form the bridge that enables this direct transfer. relaxed programs exploit the freedom of the computation to, within limits, produce a range of different outputs. because it is often convenient to express accuracy requirements by bounding the difference between results from the original and relaxed executions, developers can use relate statements to express accuracy properties. relaxation without veri cation can therefore complicate debugging it may cause the relaxed program to violate assumptions that are valid in the original program. our proof rules, by ensuring that if the assumption is valid in the original program, then it remains valid in the relaxed program, simplify debugging by eliminating this potential source of unintended behaviors. note that our proof rules can work together effectively to prove important acceptability properties. for example, the developer may use a relate statement to establish a relationship between values in variables in the original and relaxed executions, then use this relationship to prove that the property speci ed by an assert or assume statement holds in all relaxed executions. proof rules and formal properties we structure our proof rules as a set of hoare logics: axiomatic original semantics: the original hoare style semantics models the original execution of the program wherein relax statements have no effect. the predicates of the judgment are given in a relational logic that enables us to express properties over the values of variables in both the original and relaxed executions of the program. a proof with the axiomatic relaxed semantics relates the two semantics of the program in lockstep and, therefore, supports the transfer of reasoning about the original semantics to the relaxed semantics by enabling, for example, noninterference proofs. one key aspect of the axiomatic relaxed semantics is that it must also give an appropriate semantics for relaxed programs in which the original and relaxed executions may branch in different directions at a control ow construct. in particular, we do not support relational reasoning for program points at which the executions are no longer in lockstep. our relaxed semantics therefore incorporates a nonrelational axiomatic intermediate semantics that captures the desired behavior of the relaxed execution as it executes without a corresponding original execution. we also appropriately restrict the location of relate statements to program points at which the original and relaxed programs execute in lockstep. our proof rules are sound and establish the following semantic properties of veri ed relaxed programs: original progress modulo assumptions: if the program veri es under the axiomatic original semantics, then no execution of the program in the dynamic original semantics violates an assertion. by design, a program may still terminate in error if a speci ed assume statement is not valid. soundness of relational assertions: if the program veri es under the axiomatic relaxed semantics, then all pairs of executions in the dynamic original and relaxed semantics satisfy the relate statements in the program. relative relaxed progress: if the program veri es under the axiomatic relaxed semantics and no executions in the dynamic original semantics violate an assertion or an assumption, then no execution in the dynamic relaxed semantics violates an assertion or an assumption. our design supports a development process in which developers can use the full range of standard techniques to validate properties that they believe to be true of the original program. they can then use assume statements to formally state these properties and incorporate them into the veri cation of the relaxed program. this veri cation ensures that if the relaxed program fails because of a violated assumption, then the developer can reproduce the violated assumption in the original program. we have used our framework to develop and verify several small relaxed programs. a large portion is devoted to formalizing the semantics of our relational assertion logic and its soundness with respect to operations such as substitution. contributions this paper makes the following contributions: relaxed programming: it identi es the concept of relaxed programming as a way to specify nondeterministic variants of an original program. current techniques that can produce relaxed programs include skipping tasks, loop perforation, reduction sampling, multiple selectable implementations, dynamic knobs, synchronization elimination, approximate function memoization, and approximate data types. with this approach, the majority of the reasoning effort works with the original program and is transferred to the relaxed program through relational reasoning. we have used this formalization to prove that the proof rules are sound with respect to the dynamic original and relaxed semantics. we note that our coq formalization contains a reusable implementation of our relational assertion logic that is, in principle, suitable for other uses such as verifying traditional compiler transformations. veri ed programs: it presents several relaxed programs for which we have used the coq formalization to develop fully machine checked veri cations. relaxed programs can deliver substantial exibility, performance, and resource consumption bene ts. this paper presents a foundational formal reasoning system that leverages the structure and relationships that the original and relaxed executions share to enable the veri cation of these properties. for example, approximate computing applications can often naturally tolerate soft errors. rely allows developers to specify the reliability requirements for each value that a function produces. emerging high performance architectures are anticipated to contain unreliable components that may exhibit soft errors, which silently corrupt the results of computations. full detection and masking of soft errors is challenging, expensive, and, for some applications, unnecessary. we present rely a programming language that enables developers to reason about the quantitative reliability of an application namely, the probability that it produces the correct result when executed on unreliable hardware. we present a static quantitative reliability analysis that verifies quantitative requirements on the reliability of an application, enabling a developer to perform sound and verified reliability engineering. the analysis takes a rely program with a reliability specification and a hardware specification that characterizes the reliability of the underlying hardware components and verifies that the program satisfies its reliability specification when executed on the underlying unreliable hardware platform. we demonstrate the application of quantitative reliability analysis on six computations implemented in rely. many computations, however, can tolerate occasional unmasked errors. in contrast to existing approaches, which support only a binary distinction between critical and approximate regions, quantitative reliability can provide precise static probabilistic acceptability guarantees for computations that execute on unreliable hardware platforms. rely rely is an imperative language that enables developers to specify and verify quantitative reliability speci cations for programs that allocate data in unreliable memory regions and incorporate unreliable arithmetic logical operations. for example, a developer can declare a function signature int, where is the reliability speci cation fors return value. joint reliabilities serve as an abstraction of a functioninput distribution, which enables relyanalysis to be both modular and oblivious to the exact shape of the distributions. we present rely, a language that allows developers to specify reliability requirements for programs that allocate data in unreliable memory regions and use unreliable arithmetic logical operations. true false exp cmp exp bexp lop bexp bexp bexp. skipexpa a exp id id ife bexps; whilee bexp repeate. relylanguage syntax we present a dynamic semantics for rely via a probabilis eiebok eieegt tic small step operational semantics. this semantics is pa define width rameterized by a hardware reliability speci cation that char acterizes the probability that an unreliable arithmetic logical or memory read write operation executes correctly. we formalize the semantics of quantitative reliability as it relates to the probabilistic dynamic semantics of a rely program. we present a program analysis that veri es that the dynamic semantics of a rely program satis es its quantitative reliability speci cations. system reliability is a major challenge in the design of emerging architectures. energy ef ciency and circuit scaling are becoming major goals when designing new devices. however, aggressively pursuing these design goals can often increase the frequency of soft errors in small and large systems alike. researchers have developed numerous techniques for detecting and masking soft errors in both hardware and software. these techniques typically come at the price of increased execution time, increased energy consumption, or both. an approximate computation can often acceptably tolerate occasional errors in its execution and or the data that it manipulates. a checkable computation can be augmented with an ef cient checker that veri es the acceptability of the computationresults. if the checker does detect an error, it can reexecute the computation to obtain an acceptable result. for both approximate and checkable computations, operating without mechanisms that detect and mask soft errors can produce fast and energy ef cient execution that delivers acceptably accurate results often enough to satisfy the needs of their users despite the presence of unmasked soft errors. background researchers have identi ed a range of both approximate computations and checkable computations. their results show that it is possible to exploit these properties for a variety of purposes increased performance, reduced energy consumption, increased adaptability, and increased fault tolerance. one key aspect of such computations is that they typically contain critical regions and approximate regions. to support such computations, researchers have proposed energy ef cient architectures that, because they omit some error detection and correction mechanisms, may expose soft errors to the computation. a key aspect of these architectures is that they contain both reliable and unreliable components for executing the critical and approximate regions of a computation, respectively. the rationale behind this design is that developers can identify and separate the critical regions of the computation from the approximate regions of the computation. existing systems, tools, and type systems have focused on helping developers identify, separate, and reason about the binary distinction between critical and approximate regions. however, in practice, no computation can tolerate an unbounded accumulation of soft errors to execute acceptably, even the approximate regions must execute correctly with some minimum reliability. quantitative reliability we present a new programming language, rely, and an associated program analysis that computes the quantitative reliability of the computationie, the probability with which the computation produces a correct result when its approximate regions execute on unreliable hardware. more speci cally, given a hardware speci cation and a rely program, the analysis computes, for each value that the computation produces, a conservative probability that the value is computed correctly despite the possibility of soft errors. rely supports quantitative reliability speci cations for the results that functions produce. the symbolic expression is the joint reliability ofandnamely, the probability that they both simultaneously have the correct value on entry to the function. this speci cation states that the reliability of the return value ofmust be at least ofands reliability when the function was called. this is important because such exact shapes can be dif cult for developers to identify and specify and known tractable classes of probability distributions are not closed under many operations found in standard programming languages, which can complicate attempts to develop compositional program analyses that work with such exact shapes. rely assumes a simple machine model that consists of a processor and a main memory. the model includes unreliable arithmetic logical operations and unreliable physical memories. rely works with a hardware reliability speci cation that lists the probability with which each operation in the machine model executes correctly. rely is an imperative language with integer, logical, and oating point expressions, arrays, conditionals, while loops, and function calls. in addition to these standard language features, rely also allows a developer to allocate data in unreliable memories and write code that uses unreliable arithmetic logical operations. for example, the declaration intin urel allocates the variablein an unreliable memory named urel where both reads and writes ofmay fail with some probability. which is an unreliable addition of the values a andthat may produce an incorrect result. we have designed the semantics of rely to exploit the full availability of unreliable computation in an application. speci cally, rely only requires reliable computation at points where doing so ensures that programs are memory safe and exhibit control ow integrity. relysemantics models an abstract machine that consists of a heap and a stack. the stack consists of frames that contain references to the locations of each invoked functionvariables. to protect references from corruption, the stack is allocated in a reliable memory region and stack operationsie, pushing and popping frames execute reliably. to prevent out ofbounds memory accesses that may occur as a consequence of an unreliable array index computation, rely requires that each array read and write includes a bounds check; these bounds check computations also execute reliably. underlying mechanism to execute these operations reliably; one can use any applicable software or hardware technique. to prevent the execution from taking control ow edges that are not in the programstatic control ow graph, rely assumes that instructions are stored, fetched, and decoded reliably and control ow branch targets are computed reliably. quantitative reliability analysis given a rely program and a hardware reliability speci cation, relyanalysis uses a precondition generation approach to generate a symbolic reliability precondition for each function. a reliability precondition captures a set of constraints that is suf cient to ensure that a function satis es its reliability speci cation when executed on the underlying unreliable hardware platform. the reliability precondition is a conjunction of predicates of the form aoutr, where aout is a placeholder for a developer provided reliability speci cation for an output named out, is a real number between and, and the term is the joint reliability of a set of parameters. conceptually, each predicate speci es that the reliability given in the speci cation should be less than or equal to the reliability of a path that the program may take to compute the result. the analysis computes the reliability of a path from the probability that all operations along the path execute reliably. the speci cation is valid if the probabilities for all paths to computing a result exceed that of the resultspeci cation. to avoid the inherent intractability of considering all possible paths, rely uses a simpli cation procedure to reduce the precondition to one that characterizes the least reliable path through the function. one of the core challenges in designing rely and its analysis is dealing with unreliable computation within loops. the reliability of variables updated within a loop may depend on the number of iterations that the loop executes. speci cally, if a variable has a loop carried dependence and updates to that variable involve unreliable operations, then the variablereliability is a monotonically decreasing function of the number of iterations of the loop on each loop iteration the reliability of the variable degrades relative to its previous reliability. if a loop does not have a compile time bound on the maximum number of iterations, then the reliability of such a variable can, in principle, degrade arbitrarily, and the only conservative approximation of the reliability of such a variable is zero. to provide speci cation and veri cation exibility, rely provides two types of loop constructs: statically bounded while loops and statically unbounded while loops. statically bounded while loops allow a developer to provide a static bound on the maximum number of iterations of a loop. the dynamic semantics of such a loop is to exit if the number of executed iterations reaches this bound. this bound allows relyanalysis to soundly construct constraints on the reliability of variables modi ed within the loop by unrolling the loop for its maximum bound. statically unbounded while loops have the same dynamic semantics as standard while loops. in the absence of a static bound on the number of executed loop iterations, however, relyanalysis constructs a dependence graph of the loopbody to identify variables that are reliably updated speci cally, all operations that in uence these variables values are reliable. because the execution of the loop does not decrease the reliability of these variables, the analysis identi es that their reliabilities are unchanged. for the remaining, unreliably updated variables, relyanalysis conservatively sets their reliability to zero. in the last step of the analysis of a function, rely veri es that the functionspeci cations are consistent with its reliability precondition. because reliability speci cations are also of the formr, the nal precondition is a conjunction of predicates of the form, where is a reliability speci cation and is a path reliability. if these predicates are valid, then the reliability of each computed output is greater than that given by its speci cation. the validity problem for these predicates has a sound mapping to the conjunction of two simple constraint validity problems: inequalities between real numbers and set inclusion constraints over nite sets. checking the validity of a reliability precondition is therefore decidable and ef ciently checkable. case studies we have used rely to build unreliable versions of six building block computations for media processing, machine learning, and data analytics applications. our case studies illustrate how quantitative reliability enables a developer to use principled reasoning to relax the semantics of both approximate and checkable computations. for approximate computations, quantitative reliability allows a developer to reify and verify the results of the fault injection and accuracy explorations that are typically used to identify the minimum acceptable reliability of a computation. for checkable computations, quantitative reliability allows a developer to use the performance speci cations of both the computation and its checker to determine the computationoverall performance given that with some probability it may produce an incorrect answer and therefore needs to be reexecuted. contributions this paper presents the following contributions: quantitative reliability speci cations. we present quantitative reliability speci cations, which characterize the probability that a program executed on unreliable hardware produces the correct result, as a constructive method for developing applications. quantitative reliability speci cations enable developers who build applications for unreliable hardware architectures to perform sound and veri ed reliability engineering. speci cally, we de ne the quantitative reliability of a variable as the probability that its value in an unreliable execution of the program is the same as that in a fully reliable execution. we also de ne the semantics of a logical predicate language that can characterize the reliability of variables in a program. for each function in the program, the analysis computes a symbolic reliability precondition that characterizes the set of valid speci cations for the function. the analysis then veri es that the developer provided speci cations are valid according to the reliability precondition. we have used our rely implementation to develop unreliable versions of six building block computations for media processing, machine learning, and data analytics applications. these case studies illustrate how to use quantitative reliability to develop and reason about both approximate and checkable computations in a principled way. scalable heterogeneous computing systems, which are composed of a mix of compute devices, such as commodity multicore processors, graphics processors, reconfigurable processors, and others, are gaining attention as one approach to continuing performance improvement while managing the new challenge of energy efficiency. as these systems become more common, it is important to be able to compare and contrast architectural designs and programming systems in a fair and open forum. to this end, we have designed the scalable heterogeneous computing benchmark suite. shoc initial focus is on systems containing graphics processing units and multi core processors, and on the new opencl programming standard. shoc is a spectrum of programs that test the performance and stability of these scalable heterogeneous computing systems. at the lowest level, shoc uses microbenchmarks to assess architectural features of the system. at higher levels, shoc uses application kernels to determine system wide performance including many system features such as intranode and internode communication among devices. shoc includes benchmark implementations in both opencl and cuda in order to provide a comparison of these programming models. rst appeared in computers as specialized devices used to. oad simple rendering operations from the central microprocessor. over the years, gpus have increased in performance, architectural complexity, and programmability, inching closer towards a general purpose computing device. in fact, vendors have facilitated this trend for general purpose computation on gpus by including both programmable stages and higher precision arithmetic in the gpurendering pipelines. modern gpus are becoming a possible platform for accelerating scienti. clusters of heterogeneous architectures based on gpus starting to appear in the top list of the most powerful available computers in the world, and this trend is expected to continue due to their potential for improved energy. more importantly, these scalable heterogeneous systems will place di erent demands on the node architecture than a stand alone graphics system. in particular, the node design will have to. ectively support intranode and internode communication: communication with the gpus, communication with peer nodes through mpi, and io to a disk. aside from the linpack benchmark in top, there are few accepted benchmark suites that allow the community to characterize and compare the architectures and programming environments for these scalable heterogeneous systems. in this paper, we introduce shoc: a suite of benchmarks for heterogeneous computing platforms designed to ful ll these requirements. to demonstrate the applicability of shoc, we present results from an extensive set of commercially available graphic cards from both nvidia and amd, as well as some of the current cpu architectures. we also highlight the di erences across these devices based on the performance achieved in several shoc benchmarks. additionally, we also compare and discuss the performance differences observed between the relatively new opencl and equivalent cuda implementations of some of the benchmarks. programming systems programming systems for contemporary heterogeneous architectures remains a quickly evolving eld. nvidiacuda dramatically improved the programmability of gpus for non graphics applications. opencl was rati ed only recently in order to provide a portable interface to software and applications developers. opencl in december year#, the khronos group introduced opencl, an open standard for parallel computing on heterogeneous platforms. opencl speci es a language, based on, that allows a programmer to write parallel functions called kernels which can execute on any device with a supporting opencl implementation. openclinitial speci cation is oriented towards multi core processors, gpus, fpgas, and ibm cell. the most salient di erence between these devices and traditional cpu architectures is parallelism. opencl is primarily focused on ne grained data parallelism, but also provides support for task parallelism. in opencl, parallel kernels, functions which execute on the device, are divided into thousands of work items, which are organized into local work groups. for example, in matrix multiplication, a single work item might calculate one entry in the solution matrix, and a local work group might calculate a sub matrix of the solution matrix. another key feature of heterogeneous systems is the memory hierarchy, which can be radically di erent than a traditional cache architecture. opencl provides a set of abstractions for device memory hierarchies and an api for controlling memory allocation and data transfer. device memory is divided into several distinct address spaces including global, local, constant, and image memory. in order to achieve high performance, the programmer must meticulously specify the location of each piece of data. opencl also provides a rich api for device interrogation. at run time, programs can query information about how many devices are available and the computational capabilities of those devices including available computational units, memory and cache sizes, clock speeds, and support for extensions to the opencl baseline speci cation such as support for double precision oating point operations. cuda in addition to opencl, shoc contains version of each benchmark in compute uni ed device architecture. cuda is nvidiac like language environment for programming gpus and is currently the most popular environment for gpgpu. cuda is quite similar to opencl in many respects. they both focus on massively data parallel kernels, which use tens of thousands of threads organized into a grid of thread blocks. also, they both abstract device memory hierarchies into several di erent address spaces. however, cuda only supports nvidia gpus, while opencl implementations already support gpus from nvidia and ati, as well as multi core cpus. as technology scales ever further, device unreliability is creating excessive complexity for hardware to maintain the illusion of perfect operation. in this paper, we consider whether exposing hardware fault information to software and allowing software to control fault recovery simplifies hardware design and helps technology scaling. the combination of emerging applications and emerging many core architectures makes software recovery a viable alternative to hardware based fault recovery. emerging applications tend to have few io and memory side effects, which limits the amount of information that needs checkpointing, and they allow discarding individual sub computations with small qualitative impact. software recovery can harness these properties in ways that hardware recovery cannot. we describe relax, an architectural framework for software recovery of hardware faults. relax includes three core components: an isa extension that allows software to mark regions of code for software recovery, a hardware organization that simplifies reliability considerations and provides energy efficiency with hardware recovery support removed, and software support for compilers and programmers to utilize the relax isa. applying relax to counter the effects of process variation, our results show a energy efficiency improvement for parsec applications with only minimal source code changes and simpler hardware. as cmos technology scales, individual transistor components will soon consist of only a handful of atoms. at these permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page to copyotherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. sizes, transistors are extremely di cult to control in terms of their individual power and performance characteristics, their susceptibility to soft errors caused by particle strikes, the rate at which their performance degrades over time, and their manufacturability concerns commonly referred to as variability, soft errors, wear out, and yield, respectively. already, the illusion that hardware is perfect is becoming hard to maintain at the vlsi circuit design, cad, and manufacturing layers. ciency are lost due to the conservative voltage and frequency assumptions necessary to overcome unpredictability. this trend towards increasingly unreliable hardware has led to an abundance of work on hardware fault detection and recovery. additionally, researchers have explored architectural pruning and timing speculation as ways to mitigate chip design and manufacturing constraints. however, in all cases these proposals have focused on conventional applications running on conventional architectures, with a typical separation of hardware and software concerns. in this paper, we observe two complementary trends in emerging applications and architectures that favor a new overall architectural vision: hardware faults recovered in software. below, we explain these trends, articulate the challenges in designing an architecture with software recovery, and nally describe our proposed framework, relax. emerging applications applications that continue to drive increases in chip performance include computer vision, data mining, search, media processing, and data intensive scienti. many of these applications have two distinct characteristics that make them interesting from a reliability perspective. first, and a key observation unique to this work, is that many have few memory side. in particular, state modifying io operations are rare and memory operations are primarily loads, because the compute regions of these applications perform reductions over large amounts of data. second, for many emerging applications, a perfect answer is not attainable due to the inherent computational complexity of the problem and or noisy input data. therefore, they employ approximation techniques to maximize the qualitative usefulness of their output. this suggests that these applications might be error tolerant, which has been observed in prior work as well. in this paper, we speci cally explore the phenomenon that the application can discard computations in the event of an error. the concurrent architecture trend is that massively multicore architectures are emerging to meet the computational copyright year#acm. figure #: the evolution of hardware, architecture, and applications in the context of relax. these architectures often employ simple, in order cores to maximize throughput and energy. ciency with little or no support for speculative execution or bu ering. hence, the paradigm that hardware misspeculation recovery mechanisms can be repurposed for error recovery does not apply for these architectures. the valuable chip real estate that would otherwise be devoted to hardware recovery resources could be better spent elsewhere if software recovery were. ects and error tolerance that exists in large portions of emerging applications renders hardware recovery in exible, unnecessarily conservative, and too expensive for emerging many core architectures. figure # shows the evolutionary path to software recovery considering these trends in hardware, architecture, and applications. historically, traditional applications running on traditional superscalar processor architectures built with perfect cmos devices required no recovery. even with imperfect cmos, these applications still work best utilizing hardware recovery when running on traditional processor architectures. however, with emerging applications running on emerging many core architectures, hardware recovery introduces the ine ciencies we have described. in the future, while hardware substrates will be unreliable, we require mechanisms that provide exibility to software and keep the architecture simple. an architecture that exposes hardware errors to allow software recovery enables synergy between applications and architectures as shown in figure #. the design of a system architecture that allows such software recovery of hardware faults involves many important questions and challenges. the rst and most obvious question is whether changes to the isa are necessary. to answer this question, we refer to prior studies that show application tolerance to arbitrary instruction level errors is very poor. operations relating to control ow and memory accesses are failure prone and constitute a large percentage of application operations. for an architecture to allow reasonably ne grained software recovery without isa changes, it would be necessary for the hardware to somehow distinguish these critical operations from the non critical operations as it executes code. to date, no one has been able to propose an. the next logical question concerns what form isa support should take. software recovery of hardware faults has been proposed before in the context of software detection, using compiler automated triple modular redundancy. tmr makes sense when the overhead of detection is already very high, as is the case with comprehensive software detection. however, it is expensive and does not allow the application to exploit error tolerance. cient solution that allows an application to choose its own form of recovery is closer to ideal. we divide relax into three core components: an isa extension, hardware support to implement the relax isa, and software support for applications to use the relax isa. we discuss each component in a separate section: isa extension: in section #, we describe the relax isa extension, which enables software to register a fault handler for a region of code. the extension allows applications to encode behavior similar to the try catch behavior found in modern programming languages. the isa behavior is intuitive to programmers, and the compiler and hardware combine to make guarantees about the state of the program as the region is executed. we also provide a rigorous de nition of the isasemantics. hardware support: we cover the hardware support for relax in section #. the relax isasemantics allow hardware design simpli cation and provide energy. ciency by relaxing the reliability constraints of the hardware. we describe support for fault detection and discuss hardware organizations that support relax. we show that mechanisms such as aggressive voltage scaling, frequency overclocking, and turning. we also consider statically heterogeneous architectures, where cores are constructed with di erent reliability guarantees at design time. software support: in section #, we develop ac language level recovery construct to expose the relax isa extension to developers. we propose two key ideas: relax blocks to mark regions that may experience a hardware fault, and optional recover blocks to specify recovery code if a fault occurs. our results indicate promise for alternative forms of application support as well, such as automated support through compiler static analysis or pro le guided compilation. to support relax, we develop performance models to guide the development of relaxed applications. the models, discussed in section #, determine the. ciency of relax based on application and architecture characteristics and can be used to compute the achievable. ciency improvements for a given application, recovery behavior, and architecture combination. we evaluate relax in sections and, where we apply our language construct and relax compiler to real applications, and simulate how relax enables energy. ciency gains using process variation as a case study. we discuss directions for future work in section #, related work in section #, and nally we conclude in section #. disciplined approximate programming lets programmers declare which parts of a program can be computed approximately and consequently at a lower energy cost. the compiler proves statically that all approximate computation is properly isolated from precise computation. we describe an isa extension that provides approximate operations and storage, which give the hardware freedom to save energy at the cost of accuracy. the basis of our design is dual voltage operation, with a high voltage for precise operations and a low voltage for approximate operations. the hardware is then free to selectively apply approximate storage and approximate computation with no need to perform dynamic correctness checks. in this paper, we propose an efficient mapping of disciplined approximate programming onto hardware. we then propose truffle, a microarchitecture design that efficiently supports the isa extensions. the key aspect of the microarchitecture is its dependence on the instruction stream to determine when to use the low voltage. we evaluate the power savings potential of in order and out of order truffle configurations and explore the resulting quality of service degradation. we evaluate several applications and demonstrate energy savings up to. as improvements in per transistor speed and energy efficiency diminish, radical departures from conventional approaches are needed to continue improvements in the performance and energy efficiency of general purpose processors. general purpose approximate computing explores a third dimension error and trades the accuracy of computation for gains in both energy and performance. techniques to harvest large savings from small errors have proven elusive. for a set of diverse applications, npu acceleration provides whole application speedup of and energy savings of on average with average quality loss of at most. one such departure is approximate computing, where error in computation is acceptable and the traditional robust digital abstraction of near perfect accuracy is relaxed. conventional techniques in energy efficient computing navigate a design space defined by the two dimensions of performance and energy, and traditionally trade one for the other. this paper describes a new approach that uses machine learning based transformations to accelerate approximation tolerant programs. the core idea is to train a learning model how an approximable region of code code that can produce imprecise but acceptable results behaves and replace the original code region with an efficient computation of the learned model. we use neural networks to learn code behavior and approximate it. we describe the parrot algorithmic transformation, which leverages a simple programmer annotation to transform a code region from a von neumann model to a neural model. after the learning phase, the compiler replaces the original code with an invocation of a low power accelerator called a neural processing unit. the npu is tightly coupled to the processor pipeline to permit profitable acceleration even when small regions of code are transformed. offloading approximable code regions to npus is faster and more energy efficient than executing the original code. npus form a new class of accelerators and show that significant gains in both performance and efficiency are achievable when the traditional abstraction of near perfect accuracy is relaxed in general purpose computing. it is widely understood that energy efficiency now fundamentally limits microprocessor performance gains. in this post dennard scaling era, solutions that improve performance and efficiency while retaining as much generality as possible are highly desirable; hence the exploding interest in gpgpus and fpgas. these applications are common in mobile, embedded, and server systems and fall into four broad categories: applications with analog inputs. cmos scaling is no longer providing gains in efficiency commensurate with transistor density increases, as a result, both the semiconductor industry and the research community are increasingly focusing on specialized accelerators, which can provide large gains in efficiency and performance by restricting the workloads that benefit. recent work has quantified three orders of magnitude of difference in efficiency between general purpose processors and asics the community is facing an iron triangle in this respect; we can choose any two of performance, energy efficiency, and generality at the expense of the third. before the traditional trend of transistor scaling dennard scaling broke down, we were able to improve all three on a consistent basis for decades. such programmable accelerators exploit some characteristic of an application domain to achieve efficiency gains at the cost of generality. fpgas, for example, exploit copious, fine grained, and irregular parallelism while gpus exploit many threads and data level simd style parallelism. whether an application can use an accelerator effectively depends on the degree to which it exhibits the acceleratorrequired characteristics. tolerance to approximation is one such program characteristic. a growing body of recent work, has focused on approximation as a strategy for improving efficiency. large classes of applications can tolerate small errors in their outputs with no discernible loss in their quality of result. this category includes image processing, sensor data processing, voice recognition, etc, that operate on noisy real world data. they are inherently resilient to some noise and can handle an extra noise resulting from approximation. to meet an insatiable consumer demand for greater performance at less power, silicon technology has scaled to unprecedented dimensions. given the rise of processor reliability as a first order design constraint, there has been a growing interest in low cost, non intrusive techniques for transient fault detection. many of these recent proposals have counted on the availability of hardware recovery mechanisms. although common in aggressive out of order cores, hardware support for speculative rollback and recovery is less common in lower end commodity processors. this paper presents encore, a software based fault recovery mechanism tailored for these lower cost systems that lack native hardware support for speculative rollback recovery. encore combines program analysis, profile data, and simple code transformations to create statistically idempotent code regions that can recover from faults at very little cost. using this software only, compiler based approach, encore provides the ability to recover from transient faults without specialized hardware or the costs of traditional, full system checkpointing solutions. experimental results show that encore, with just of runtime overhead, can safely recover, on average from of transient faults when coupled with existing detection schemes. however, the pursuit of faster processors and longer battery life has come at the cost of reliability. although it is impossible to build a completely reliable system, hardware vendors attempt to target failure rates that are imperceptibly small. with the course of aggressive technology scaling that has been followed by industry, many sources of unreliability are permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not madeordistributedforpro. orcommercial advantage and that copies bearthisnoticeand thefull citation onthe rstpage tocopy otherwise, to republish, topost on servers orto redistribute tolists, requiresprior speci. one prominent source, and thefocus ofthispaper, is softerrors. traditionally, architects have designed systems that would take periodic checkpoints of processor and memory state. in the event of a soft error the system could rollback to an existing, fault free snapshot and continue execution. thesehighly robustfault recovery solutionshavehistorically also relied on some form ofmodular redundancy toprovide the necessarydetection capabilities. however, the resultant overheads of these coupled detection and recovery schemes, a large component of which was the cost of creating checkpoints, usually relegated their use to to high end, enterprise systems. these simpleyet eleganttechniques, having served thoseinthe mission critical server arena for decades, are not practical outside this niche domain. although reliability cannot be completely ignored in lower end systems, they are not usually designed to provide the ve nines of fault tolerance capable of sending someone safely to the moon. that said, the overheads associated with these conventional solutions areprohibitively expensiveforbudgetconscious designers with less demanding reliability requirements. infact, thisis the same argument madeby, and to a similar extent, which argues that most commodity systemsdo not require reliability guarantees but will settle for probabilistic, best effort, fault tolerance. this insight has sparked a recent interest in transient fault detection techniques that are able to maintain low runtime overheads by sacri cing a small degree of reliability, focusing primarily on addressing the bulk of faults that are relatively inexpensive to detect. however, thesetechniques assumehardwareprovides rollback recovery, arguingthat suchhardware would already exist to support performance speculation. although this argument may hold for aggressive out of order processors, such hardware support is not present in the majority of low end commodity systems. withthatin mind, wepropose, encore, a software only solution thatseekstoprovideprobabilistic rollback recovery capabilities at minimal costs. encore was developed to complement emerging probabilistic detection techniques, enabling them figure #: percentage of dynamic instruction traces that are inherently idempotent asafunction of size theexecutiontraces wereextractedfrom an assortment of specand mediabench workloads. the idempotence target curve illustrates encoregoal of exposing, and exploiting, greater degrees of idempotence through intelligent compiler analysis and transformations. as an automated, compiler driven technique, encore is able to utilize programmable heuristics that allow the end user todialin thedesireddegree offault tolerance and therefore only incur as much runtime overhead as they are able to budget. encore can achieve this behavior by mimicking the same checkpoint, rollback, and re execute model used by earlier enterprise systems. however, rather than performing, full system, heavyweight checkpoints, encoreis able to exploit the statistically idempotent property of applications to reduce, and in certain situations completely eliminate the overheads required to supply rollback recovery. at ahigh level, an idempotent region of code is simply one that canbe re executed multiple times and stillproduces the same, correct result. in the context of rollback recovery, this means that at leasttothe rstorder, afault occurring withinanidempotentpiece of code can be recovered from without any overhead for checkpointing state. to better understand the extent of idempotent code present in an application, figure # shows the distribution of idempotent execution traces across a set of desktop and media benchmarks. results are shown as afunction of the trace size. the surprisingly large percentage of naturally occurring idempotent regions of execution seen in figure # is what initially encouraged thedevelopmentofencore tothe rst order, thecode regions corresponding to the traces that wereidenti ed asidempotent couldbe easilyinstrumentedfor rollback recovery with almost noimpact on runtimeperformance. itisimportanttopoint outhowever, that althoughthereisplenty of opportunitypresent, only afew of these regions actually span an entire function. most are spread throughout the application, making manual inspection to identify themimpractical. with more instructions comes the greater chance that there exists some sequence of instructions that violatethewar constraints requiredto maintainidempotence. thisintuitionis reinforcedbythedata, which exhibits a sharpdrop in the likelihood of being idempotent when moving from traces withjust ahandful ofinstructions to those with or more. lastly, itis alsointerestingto notethatforthetracesthatdo notexhibitfull idempotence, many tend to be nearly idempotent, ie, containing only a few offending instructions. furthermore, these instructions often only occur along statistically unlikelypaths. encore seeks to expose even greater amounts of statistical idempotence by recognizingtheseproperties of applicationbehavior. to make exploiting program idempotence feasible, this paper proposes techniques to automate the analysis and instrumentation within compiler optimization passes. we present the algorithms and heuristics developed that enable encore to carefully partition application code into ne grained regions with favorable idempotence behavior, and then to instrument them for rollback recovery. byrecognizingthe statisticallyidempotentstructure naturallypresent in manyapplications, encore cantransparentlyprovide rollback recovery on commodity systems at prices that they can afford. the contributions of thispaper are asfollows: a low cost transient fault recovery technique for commodity systems without hardware support for aggressive performance speculation. selectively trading off recoverability with cost by performing code transformations thatleverage application pro lingstatistics. a performance evaluation and analysis of encoreability to recoverfrom transient faults withfull system simulations across adiverse set of representative workloads. additionally, in newlyproposed architecturesthat embracetheprinciples of stochastic and near threshold computing, they can alsobe the result of extremetiming speculation and orfrequency and voltage scaling. tobedeployedin commodity systems without nativehardware supportfor rollback recovery. this typically means that there cannot exist any pathsthroughthe regionthatcan read, modify, andthen writetothe same location,e. new compiler algorithms andheuristicsfor automaticallyidentifyingcandidateidempotent regions ingeneralized code regions with supportfor cycles. available in spatial and temporal variants, modular redundancygenerallyinvolved redundant execution followedbydetailed comparisonsthatwouldidentify thepresence of afault. alsoknown astransientfaults, they canbeinducedby a variety ofphenomenalike electrical noise and high energy particle strikes that result from cosmic radiation and chippackagingimpurities. we present powerdial, a system for dynamically adapting application behavior to execute successfully in the face of load and power fluctuations. powerdial transforms static configuration parameters into dynamic knobs that the powerdial control system can manipulate to dynamically trade off the accuracy of the computation in return for reductions in the computational resources that the application requires to produce its results. these reductions translate directly into performance improvements and power savings. our experimental results show that powerdial can enable our benchmark applications to execute responsively in the face of power caps that would otherwise significantly impair responsiveness. they also show that powerdial can significantly reduce the number of machines required to service intermittent load spikes, enabling reductions in power and capital costs. between the accuracy of the result that they produce and the power and or time that they require to produce that result. because an applicationoptimal operating point can vary depending on characteristics of the environment in which it executes, developers often provide a static interface that makes it possible to choose di erent points in the trade. space for di erent executions of the application. con gured at startup, the application operates at the selected point for its entire execution. but phenomena such as load uctuations or variations in available power can change the optimal operating point of the appli henry ho mann and stelios sidiroglou contributed equally to the research presented in this paper. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. static con guration leaves the application with two unappealing choices: either continue its execution at a suboptimal point in the trade. space or terminate its current execution and restart at a new operating point. dynamic knobs and adaptive response we present a new system, powerdial, for dynamically adapting the behavior of running applications to respond to uctuations in load, power, or any other event that threatens the ability of the underlying computing platform to deliver adequate capacity to satisfy demand: dynamic knob insertion: powerdial uses dynamic in uence tracing to transform static application con guration parameters into dynamic control variables stored in the address space of the running application. these control variables are made available via a set of dynamic knobs that can change the con guration of a running application without interrupting service or otherwise perturbing the execution. dynamic knob calibration: powerdial explores the underlying accuracy versus performance trade. space to characterize the accuracy and performance of each dynamic knob setting. it uses a quality of service metric to quantify the accuracy of each setting. dynamic knob control: powerdial is designed for applications that are deployed to produce results at a target frequency. it uses the application heartbeats framework to dynamically monitor the application. an existing control strategy is combined with a novel actuation mechanism to maintain performance. when the performance either drops below target or rises above target, the powerdial system uses the calibrated dynamic knobs to move the application to a more appropriate point in its trade. the goal is to maximize accuracy while preserving responsiveness in the face of uctuations in the capabilities of the underlying computing platform. summary of experimental results we evaluate powerdialability to control the behavior of four benchmark applications dynamically in environments with uctuating load and power characteristics. space: all of the applications exhibit a large viable trade. space three of the applications can execute from four to six times faster than their baseline with acceptable quality of service losses. swish can execute approximately times faster than its baseline. power capping: systems often respond to power caps by dynamic voltage frequency scaling. the ensuing reduction in the delivered computational capacity of the system can make it di cult or impossible for applications to continue to deliver responsive service. our results show that powerdial enables applications to adapt. ectively as a power cap is rst imposed, then lifted. when the power cap is imposed, powerdial preserves responsiveness by moving the applications to new pareto optimal points with less computational demands and slightly lower quality of service. when the power cap is lifted, powerdial restores the original quality of service by moving the applications back to the baseline. peak load provisioning: systems are often provisioned to service the peak anticipated load. the system therefore usually contains idle machines that consume power but perform no useful work. our results show that powerdial can reduce the number of machines required to successfully service time varying workloads. when a load spike overwhelms the ability of the system to service the load with the baseline application con guration, powerdial preserves responsive performance by dynamically recon guring the application to use less computation to produce lower quality results. speci cally, our results show that powerdial can make it possible to reduce the number of machines required to provide responsive service in the face of intermittent load spikes. the system provides baseline quality of service for the vast majority of tasks; during peak loads, the system provides acceptable quality of service and negligible performance loss. powerdial is not designed for all applications it is instead designed for applications that have viable performance versus qos trade. spaces and have been engineered to operate successfully at multiple points within those spaces and operate in contexts where they must satisfy responsiveness requirements even in the face of uctuations in the capacity of the underlying computing platform. in this paper we focus on uctuations in power and load, but powerdial can enable applications to adapt dynamically to any change that. ects the computational capacity delivered to the application. contributions this paper makes the following contributions: dynamic knobs: it presents the concept of dynamic knobs, which manipulate control variables in the address space of a running application to dynamically change the point in the underlying performance versus quality of service trade. powerdial: it presents powerdial, a system that transforms static con guration parameters into calibrated dynamic knobs and uses the dynamic knobs to enable the application to oper ate successfully in the face of uctuating operating conditions. analysis and instrumentation: it presents the powerdial analysis and instrumentation systems, which dynamically analyze the application to nd and insert the dynamic knobs. control: it presents the powerdial control system, which uses established control techniques combined with novel actuators to automatically maintain the applicationdesired performance while minimizing quality of service loss. resource requirements: it shows how to use dynamic knobs to reduce the number of machines required to successfully service peak loads and to enable applications to tolerate the imposition of power caps. it analyzes the resulting reductions in the amount of resources required to acquire and operate a computational platform that can successfully deliver responsive service in the face of power and load uctuations. experimental results: it presents experimental results characterizing the trade. space that dynamic knobs make available in our benchmark applications. it also presents results demonstrating powerdialability to enable automatic, dynamic adaptation of applications in response to uctuations in system load and power. a growing number of applications from various domains such as multimedia, machine learning and computer vision are inherently fault tolerant. however, for these soft workloads, not all computations are fault tolerant. in this paper, we propose a compiler based approach that takes advantage of soft computations inherent in the aforementioned class of workloads to bring down the cost of software only transient fault detection. the technique works by identifying a small subset of critical variables that are necessary for correct macro operation of the program. traditional duplication and comparison are used to protect these variables. for the remaining variables and temporaries that only affect the micro operation of the program, strategic expected value checks are inserted into the code. intuitively, a computation chain result near the expected value is either correct or close enough to the correct result so that it does not matter for non critical variables. overall, the proposed solution has, on average, only performance overhead and reduces the number of silent data corruptions from down to and user visible silent data corruptions from down to in comparison to an unmodified application. this unacceptable silent data corruption rate is even lower than a traditional full duplication scheme that has, on average, overhead. however, checkpoints introduce overheads, add complexity, and often save more state than necessary. recovery functionality has many applications in computing systems, from speculation recovery in modern microprocessors to fault recovery in high reliability systems. this paper develops a novel compiler technique to recover program state without the overheads of explicit checkpoints. the technique breaks programs into idempotent regions regions that can be freely re executed which allows recovery without checkpointed state. leveraging the property of idempotence, recovery can be obtained by simple re execution. we develop static analysis techniques to construct these regions and demonstrate low overheads and large region sizes for an llvm based implementation. across a set of diverse benchmark suites, we construct idempotent regions close in size to those that could be obtained with perfect runtime information. although the resulting code runs more slowly, typical performance overheads are in the range of just. the paradigm of executing entire programs as a series of idempotent regions we call idempotent processing, and it has many applications in computer systems. as a concrete example, we demonstrate it applied to the problem of compiler automated hardware fault recovery. in comparison to two other state of the art techniques, redundant execution and checkpoint logging, our idempotent processing technique outperforms both by over. checkpoints provide a conceptually simple solution, and have strong commercial precedent. uses of ne grained idempotence in hardware and software design. however, typical overheads are in the range of just. the remainder of this paper is organized as follows. section # gives a complete overview of this paper. recovery capability is a fundamental component of modern computer systems. it is used to recover from branch misprediction and out of order execution, hardware faults, speculative memory reordering in vliw machines, optimistic dynamic binary translation and code optimization, and transactional memory. in each of these cases, recovery is used permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. to repair the state of the program in the rare event that an execution failure occurs. first, software checkpoints often have high performance overhead and hence, to maintain reasonable performance, hardware support is often necessary. this hardware support, however, forces interdependencies between processor structures, occupies space on the chip, and entails recurring energy expenditure regardless of failure occurrence. particularly for emerging massively parallel and mobile processor designs, the per core hardware support comes at a premium, while the recovery support may be desirable only under speci. hardware checkpointing resources are also rarely exposed to software, and are even less often con gurable in terms of their checkpointing granularity, limiting their wider applicability. finally, checkpoints have limited application visibility and are often overly aggressive in saving more state than is required by the application. to combat these dif culties, idempotence the property that reexecution is free of side effects has been previously proposed as an alternative to checkpoints. in contrast to explicit checkpoints, idempotence allows the architecture state at the beginning of a code region to be used as an implicit checkpoint that is never explicitly saved or restored. in the event of an execution failure, idempotence is used to correct the state of the system by simple re execution. over the years, idempotence has been both explicitly and implicitly employed as an alternative to checkpoints. table # classi es prior work in terms of its application domain and the level at which idempotence is used and identi ed. one of the earliest uses is by mahlke et al in using restartable instruction sequences for exception recovery in speculative processors. more recently, hampton and asanoviapply idempotence to support virtual memory on vector machines, tseng and tullsen apply idempotence to support data triggered parallel thread execution, and feng et al leverage idempotence for low cost hardware fault recovery. as the table shows, idempotence has historically been applied only under speci. domains such as exception recovery and multithreading, often only under restricted program scope, and often using only limited or no static analysis. in this paper, we develop an analysis framework to enable each of the above uses, irrespective of their application domain and their underlying purpose, and across entire programs. in particular, we develop static analysis techniques and a compilation strategy to statically partition programs into large idempotent regions. we develop a provably correct region partitioning algorithm, demonstrate a working compiler implementation, and demonstrate application to at least one speci. table # and unify them under a single paradigm we call idempotent processing, which allows the synthesis of multiple such uses in a single system or implementation artifact. first, we note that using a conventional compiler, idempotent regions are typically small. our static analysis eliminates the compilation artifacts responsible for these small idempotent region sizes by identifying regions in a function that are semantically idempotent. these regions are then compiled in such a way that no artifacts are introduced and the idempotence property is preserved throughout code generation. to do this, the compiler limits register and stack memory re use, which reduces locality and thereby introduces runtime overhead. in exchange for these overheads, our analysis partitions a function into regions that are close in size to the largest regions that would be constructed given near perfect runtime information. we show that the problem of nding very large idempotent regions can be cast as a vertex multicut problem, a problem known to be np complete in the general case. we apply an approximation algorithm and a heuristic that incorporates loop information to optimize for dynamic behavior and nd that overall region sizes are in most cases close to ideal. overall, we make the following contributions: we perform a detailed analysis of idempotent regions in conventional programs and quantitatively demonstrate that conventional compilers arti cially inhibit the sizes of the idempotent regions in programs, severely limiting their usefulness. we develop static analysis and compiler techniques to preserve the inherent idempotence in applications and construct large idempotent regions. we formulate the problem of nding idempotent regions as a graph cutting problem and optimize for runtime behavior using a heuristic that incorporates loop information. we present a detailed characterization of our idempotent regions, which can be applied in the context of the various uses previously proposed in the literature. we demonstrate our idempotent processing solution applied to the problem of recovery from transient faults in microprocessors. our idempotence based recovery implementation performs over better than two competing state of the art compiler automated recovery techniques. section # presents a quantitative study of idempotent regions as they exist inherently in application programs. section # presents our idempotent region construction algorithm. section # gives details of our compiler implementation. energy has become a first class design constraint in computer systems. memory is a significant contributor to total system power. this paper introduces flikker, an application level technique to reduce refresh power in dram memories. flikker enables developers to specify critical and non critical data in programs and the runtime system allocates this data in separate parts of memory. the portion of memory containing critical data is refreshed at the regular refresh rate, while the portion containing non critical data is refreshed at substantially lower rates. this partitioning saves energy at the cost of a modest increase in data corruption in the non critical data. flikker thus exposes and leverages an interesting trade off between energy consumption and hardware correctness. we show that many applications are naturally tolerant to errors in the non critical data, and in the vast majority of cases, the errors have little or no impact on the application final outcome. we also find that flikker can save between of the power consumed by the memory sub system in a mobile device, with negligible impact on application performance. flikker is implemented almost entirely in software, and requires only modest changes to the hardware. rst class design constraint in many computer systems, particularly in mobile devices, clusters, and serverfarms. in a mobile phone, saving energy can extend battery life and enhance mobility. recently, mobile phones have morphed into general purpose computing platforms, often called smartphones. smartphones are typically used in short bursts over extended periods of time, ie, they are idle most of the time. nonetheless, they are always on as users expect to resume their applications in the state they were last used. hence, even when the permission to make digital or hard copies of all or part of this work for personal or classroomuseisgrantedwithout feeprovidedthat copies arenot madeordistributed forpro torcommercialadvantage andthatcopiesbearthisnoticeandthefullcitation onthe rstpage to copy otherwise, to republish, topostonserversorto redistribute tolists, requirespriorspeci cpermission and ora fee. phone is not being used, application state is stored in the phonememory to maintain responsiveness. this wastes power because dynamic random access memories leak charge and need to be refreshed periodically, or else theywill lose data. memory is a major part of overall system power in smartphones. measures of dram power as a fraction of overall power range from and depend on the application model, operating system, and underlying hardware. some smartphone application programming models, such as android, emphasize reducing application dram usage in idle mode. further, the memory capacity of smartphones has been steadily increasing and, as a result, memory power consumption will be even more important in the future. memory consumes power both when the device is active and when it is suspended. in standby mode, the refresh operation is the dominant consumer of power, and hence we focus on reducing refresh power in this paper. this paper proposes flikker, a software technique to save energy by reducing refresh power in drams. dram manufacturers typically set the refresh rate to be higher than the leakage rate of the fastest leaking memory cells. however, studies have shown that the leakage distribution of memory cells follows an exponential distribution, with a small fraction of the cells having signi cantly higher leakage rates than other cells. hence, the vast majority of the cells will retain their values even if the refresh rate of the memory chip is signi cantly reduced. flikker leverages this observation to obtain power reduction in dram memories at the cost of knowingly introducing a modest number of errors in application data. typical smartphone applications include games, audio video processing and productivity tasks such as email and web browsing. these applications are insensitiveto errorsinallbuta small portion of their data. we call such data critical data, as it is important for theoverall correctnessofthe application. forexample, in a video processing application, the data structure containing the list of frames is more important than the outputbuffer to which frames are rendered. therefore, this data structure would be considered as critical data. flikker enables the programmer to distinguish between critical and non critical data in applications. at runtime, flikker allocates the critical and non critical data in separate memory pages and reduces the refresh rate for pages containing non critical data at the cost of increasing the number of errors in these pages. pages containing critical data are refreshed at the regular rate and are hence free of errors. this differentiated allocation strategy enables flikker to achieve power savings with only marginal degradation of the applicationreliability. crt monitors occasionally exhibited ickering, ie, loss of resolution, when their refresh rates were lowered hence the name. our approach in flikker fundamentally differs from existing solutions for saving energy in low power systems. in these solutions, energy reduction is achieved by appropriately trading offperformance metrics, such as throughput latency, quality of service, or user response time, eg, in contrast, our approach explores a largeley unexplored trade off in system design, namely trading off energy consumption for data integrity at the application level. by intentionally lowering hardware correctness in an application aware manner, we show that it is possible to achieve signi cant power savings at the cost of a negligible reduction in application reliability. to the best of our knowledge, flikker is the rst software technique to intentionally introduce hardware errors for memory power savings based on the characteristics of the application. while we focus on mobile applications in this paper, we believe that flikker approach can also be applied to data center applications. aspects of flikker make it appealing for use in practice. first, flikker allows programmers to control what errors are exposed to the applications, and hence explicitly specify the trade offbetween power consumption and reliability. programmers can de ne what parts of the application are subject to errors, and take appropriate measures to handle the introduced errors. second, flikker requires only minor changes to the hardware in the form of interfaces to expose refresh rate controls to the software. current mobile drams already allow the software to specify how much of the memory should be refreshed, and we show that it is straightforward to enhance the pasr architecture to refresh different portions of the memory at different rates. finally, legacyapplications can work unmodi ed with flikker, as it can be selectively enabled or disabled on demand. hence, flikker can be incrementally deployed on new applications. we have evaluated flikker both using analytical and experimental methods on ve diverse applications representative of mobile workloads. we nd that flikker can save between to of the total dram power in mobile applications, with negligible degradation in reliability and performance. based on previous study of dram power as a fraction of total power consumption, this corresponds to of total power savings in a smartphone. we also nd that the effort required to deploy flikker is modest for the applications considered in the paper. we present quickstep, a novel system for parallelizing sequential programs. unlike standard parallelizing compilers, quickstep is instead designed to generate parallel programs that produce acceptably accurate results acceptably often. the freedom to generate parallel programs whose output may differ from the output of the sequential program enables a dramatic simplification of the compiler, a dramatic increase in the range of applications that it can parallelize, and a significant expansion in the range of parallel programs that it can legally generate. results from our benchmark set of applications show that quickstep can automatically generate acceptably accurate and efficient parallel programs the automatically generated parallel versions of five of our six benchmark applications run between and times faster on eight cores than the original sequential versions. these applications and parallelizations contain features that place them inherently beyond the reach of standard approaches. the dominant paradigm for reasoning about the behavior of software systems revolves around hard binary correctness properties. a standard approach is to identify a concept of correct behavior, then engineer with the goal of producing a perfect system that delivers correct behavior in all circumstances. one advantage of this approach is that it simpli es reasoning for modularity purposes perfect components that always behave correctly can signi cantly reduce the number of scenarios that a developer must consider when using the components in a larger system. the complexity reduction is so valuable that developers typically operate under this assumption even though the components are almost never perfect virtually all components exhibit incorrect behavior in some scenarios. when attempting to build a perfect system, developers often invest substantially more engineering effort and produce a this research was supported by darpa cooperative agreement fa tion, and nsf awards, and year#. rinard, mit csail, the stata center, vassar st, cambridge, ma; email: misailo csail mit edu. permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies show this notice on the rst page or initial screen of a display along with the full citation. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior speci. permissions may be requested from publications dept, acm, inc, penn plaza, suite, new york, ny year# usa, fax, or permissions acm org. year# acm year# year# art doi: http: dx doi org year#. misailovic et al system that consumes more resources than strictly required to deliver acceptable behavior. an alternate approach overengineering motivates an alternate approach whose goal is to produce systems that deliver acceptably accurate results acceptably often. instead of a guarantee that the system will, on every execution, produce correct behavior, such systems may instead come with a statistical or probabilistic guarantee. this approach can provide both developers and automated systems with signi cantly more engineering freedom than standard approaches. this freedom, in turn, can translate directly into reduced engineering effort, reduced resource consumption, or increased functionality. quickstep this article presents a system, quickstep, for automatically parallelizing sequential programs. freed from the constraint of preserving the precise semantics of the original sequential program, quickstep instead generates a search space of parallel programs, then searches this space to nd a parallel program that will, with high likelihood, produce outputs that are acceptably close to the outputs that the original sequential program would have produced. because quickstep does not aspire to preserve the semantics of the original sequential program, it has no need to analyze problematic constructs, such as pointers, object references, and dynamic method invocations. it instead simply applies a statistical test to determine if test executions are acceptably accurate. as a result, quickstep is able to effectively parallelize programs whose use of modern programming constructs place them inherently beyond the reach of standard techniques. the fact that the eld of traditional parallelizing compilers has been active for decades but still has yet to produce a compiler capable of parallelizing programs that are within quickstepreach demonstrates the advantages of quickstepstatistical acceptability approach in comparison with standard semantics preserving approaches. note that because the programs that quickstep generates only need to satisfy statistical accuracy bounds, quickstep has the freedom to generate ef cient nondeterministic parallel programs. our current quickstep implementation generates parallel programs with two potential sources of nondeterminism, but in general any parallel program, deterministic or nondeterministic, is acceptable as long as it satis es the statistical accuracy guarantee. such ef cient nondeterministic parallelizations, of course, lie inherently beyond the reach of standard approaches. transformations quickstep deploys three kinds of transformations to generate its search space of parallel programs. quickstep currently implements one parallelism introduction transformation: loop parallelization. note that because the iterations of the resulting parallel loop execute without synchronization, anomalies such as data races may cause the parallel program to crash or produce an unacceptably accurate result. transformations that are designed to enhance the accuracy of the parallel program. if a parallelism introduction transformation produces an unacceptably accurate program, accuracy enhancing transformations may restore acceptably accurate execution. quickstep implements two accuracy enhancing transformations: synchronization introduction, which replaces unsynchronized operations with synchronized operations, and privatization, which gives each thread its own copy of otherwise shared local variables. transformations that are designed to enhance the performance of the parallel program. quickstep implements two performance enhancing transformations: replication introduction, which replicates objects accessed by parallel tasks then combines the replicas for sequential access and loop scheduling, which applies different parallel loop scheduling algorithms. searching the parallelization space given an initial sequential program, the parallelization transformations induce a space of corresponding parallel programs. quickstep attempts to parallelize a single loop at a time, prioritizing the attempted parallelization of loops that consume more execution time over the attempted parallelization of loops that consume less execution time. quickstep rst applies the parallelization transformation to the current loop. it then explores the parallelization space for this loop by repeatedly applying accuracy and performance enhancing transformations. if it is unable to obtain an acceptably accurate parallelization, quickstep abandons the current loop and moves on to the next. once it has processed the most timeconsuming loops and obtained the nal parallelization, quickstep produces an interactive parallelization report that a developer can navigate to evaluate the acceptability of this parallelization and obtain insight into how the application responds to different parallelization strategies. accuracy metric in many cases, it may be desirable to produce a parallel program that produces the same result as the original sequential program. but in other cases, the best parallel version may, because of phenomena such as infrequent data races or reordered parallel accumulations, produce a result that differs within acceptable bounds from the result that the sequential program produces. quickstep is therefore designed to work with an accuracy metric that quanti es the difference between an output from the original sequential program and a corresponding output from the automatically generated parallel program run on the same input. the accuracy metric rst uses an output abstraction to obtain a sequence of numbers, om from a sequential execution and a corresponding sequence, m from a parallel execution on the same input. it then uses the following formula to compute the distortion, which measures the accuracy of the parallel execution. misailovic et al the closer the distortionis to zero, the less the parallel execution distorts the output. given an accuracy metric, an accuracy boundis an upper bound on the acceptable distortion. statistical accuracy test quickstepstatistical accuracy test executes the generated parallel program multiple times, treating each execution as a bernoulli trial which succeeds if the execution satis es the accuracy bound and fails if it does not. the test terminates when quickstep has observed enough executions to make a statistically well founded judgement to either accept or reject the parallelization. quickstep uses the wald sequential probability ratio test to make this judgement. during its search of the parallel program space, quickstep sets the parameters of this test to accept a parallelization if the program satis es the accuracy bound at least of the time with a false positive rate of at most. during the nal accuracy test, the program must satisfy the accuracy bound at least of the time with a false positive rate of at most. it is possible for a parallelization to cause the program to fail to produce a wellformed output. parallelization reports we anticipate that some developers may wish to examine the quickstep parallelization or use it as a foundation for further development. quickstep therefore produces an interactive parallelization report that summarizes the applied transformations and the accuracy and performance results. the developer can use this report to evaluate the overall acceptability of the parallelization or to obtain the understanding required to further modify the program. experimental results to evaluate the effectiveness of quickstepapproach, we obtained a set of benchmark sequential programs, then used quickstep to parallelize this set of programs and generate corresponding parallelization reports. our results show that quickstep is able to effectively parallelize ve out of the six programs, with the nal parallel versions running, on our test inputs, between a factor of and faster than the corresponding sequential versions. we used the parallelization reports to evaluate the acceptability of the nal parallelizations. our evaluation shows that, for the programs in our benchmark set, quick step is able to produce parallelizations that are acceptable for all inputs. moreover, each nal parallel program contains at most a handful of parallel loops, each of which requires at most only several synchronization or replication transformations to produce an acceptably accurate program with good parallel performance. the parallelizations are therefore amenable to developer evaluation with reasonable developer effort. it introduces the basic quickstep approach of developing a set of parallelization transformations then searching the resulting induced space of parallel programs to nd a program that maximizes performance while preserving acceptably accurate execution. parallelization transformations that work together to produce ef cient and accurate parallel programs. it presents an algorithm for automatically searching the induced space of parallel programs. this algorithm uses pro ling information, performance measurements, and accuracy results from executions on representative inputs to guide the search. it introduces the use of statistical accuracy tests to determine if the likelihood that a candidate parallel program will produce an acceptably accurate result is acceptable. it introduces interactive parallelization reports that present the performance and accuracy characteristics of the candidate parallel programs, identify the applied transformations for each program, and summarize the overall search process. these reports are designed to facilitate developer evaluation of the automatically generated parallel programs and to help the developer obtain insight into how the original sequential program responds to various parallelization strategies. it presents experimental results obtained by using quickstep to parallelize a set of benchmark sequential applications. these results show that quickstep is able to produce accurate and ef cient parallel versions of ve of the six applications. and our examination of the resulting parallelizations indicates that they are acceptable for a wider set of inputs than just the representative inputs used to drive the parallelization process. a comparison with parallelizations produced by the intel icc compiler shows that the icc compiler, in contrast, was able to parallelize only small inner loops and produced no improvement in the performance of the application. it presents additional evidence that there is a fundamental trade off between accuracy and performance. mechanism for exploiting this trade off in the context of parallelizing compilers. we present a new technique for enabling computations to survive errors and faults while providing a bound on any resulting output distortion. a developer using the technique first partitions the computation into tasks. the execution platform then simply discards any task that encounters an error or a fault and completes the computation by executing any remaining tasks. this technique can substantially improve the robustness of the computation in the face of errors and faults. a potential concern is that discarding tasks may change the result that the computation produces our technique randomly samples executions of the program at varying task failure rates to obtain a quantitative, probabilistic model that characterizes the distortion of the output as a function of the task failure rates. by providing probabilistic bounds on the distortion, the model allows users to confidently accept results produced by executions with failures as long as the distortion falls within acceptable bounds. this approach may prove to be especially useful for enabling computations to successfully survive hardware failures in distributed computing environments our technique also produces a timing model that characterizes the execution time as a function of the task failure rates. the combination of the distortion and timing models quantifies an accuracy execution time tradeoff. it therefore enables the development of techniques that purposefully fail tasks to reduce the execution time while keeping the distortion within acceptable bounds. the user can then examine this bound to determine if the output satis es the accuracy requirements within an acceptable level of certainty. hardware faults the distortion model can provide distortion bounds for techniques that make software more robust against hardware faults by discarding tasks that experience such faults. we anticipate that the availability of distortion bounds will make such approaches more acceptable and therefore more widely used. the timing models assume that failed tasks consume no execution time. evaluation: it presents our experience applying our technique to several scienti. the remainder of the paper is structured as follows. with standard program execution platforms, an error anywhere within the computation may propagate to cause the entire computation to fail. such failures can be problematic because they deny the user access to the output that the computation would otherwise have provided. this research was supported in part by the singapore mit alliance, darpa cooperative agreement fa, nsf grant ccr, nsf grant ccr, nsf grant ccf, and nsf grant ccr. we propose a new approach to handling such errors. the developer starts with a program written in a standard programming language such asor java. he or she then uses a metalanguage to partition the computation into tasks. when a task encounters a software error or hardware fault, the execution platform simply discards the task, then continues on to complete the computation by running the remaining tasks. distortion and timing models a potential complication is that discarding tasks may unacceptably change the output. we address this complication by developing a probabilistic distortion model that characterizes the distortion of the output as a function of the task failure rates in the computation. instead of confronting the user with an output that has unknown distortion, the model provides a probabilistic bound on the distortion. in addition to the distortion model, we also provide a timing model that characterizes the execution time as a function of the task failure rates. in many cases higher task failure rates correspond to lower execution times when a computation discards tasks, it often performs less work. the presence of both a distortion model and a timing model enables the deployment of techniques that purposefully fail tasks to reduce the execution time while keeping the resulting distortion within acceptable bounds. basic approach our basic approach consists of the following steps: task decomposition: the developer uses the metalanguage to identify task blocks in the program. each task block consists of a block of code whose execution corresponds to a task in the computation. note that a given task block may execute many times during the course of a computation and may therefore generate many tasks into the computation. baseline: we obtain several sample inputs for which the program is known to generate correct output. we run the program on those inputs and record the outputs that it generates. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. criticality testing: we con gure the execution platform to randomly fail executions of selected task blocks at target failure rates. we then select each task block in the program in turn, fail executions of that task block at a targeted rate, and observe the resulting output distortion. if the failures produce unacceptable distortion, we mark the task block as critical, otherwise we mark the task block as failable. distortion model: given a set of failable task blocks, we run repeated trials in which we randomly select a target task failure rate for each failable task block, execute the computation, then record both the observed task failure rates and the resulting output distortion. we then use regression to obtain a probabilistic model that estimates the distortion as a function of the task failure rates. timing model: for each trial we also record the execution time of the program. we then use regression to obtain a model that estimates the execution time as a function of the task failure rates. we have applied this approach to several benchmark applications selected from the jade benchmark suite. our results show that the models are quite accurate and that the estimated distortion and con dence intervals are small enough to provide useful distortion bounds in practice. and it is possible to use models developed on one set of inputs to accurately predict both timing and distortion properties of executions on other inputs. potential uses we see several potential uses for our models. some of these uses simply make existing programs more robust against errors and faults. other uses enable developers to deliberately introduce failures or accept the presence of errors to achieve other goals. we anticipate transient hardware faults for which the recovery mechanism is simply to discard the task, then continue on to execute other tasks on the same hardware platform. we also anticipate tolerating partial failures within a distributed computing platform hosting a distributed execution of the program. software errors another potential use is to provide distortion bounds for techniques that tolerate otherwise fatal software errors by discarding tasks that experience such errors. once developers experience the ability of this technique to provide tight distortion bounds for computations that acceptably recover from inadvertent software errors by discarding tasks, they may start to consciously exploit the technique by purposefully omitting complex code otherwise required to handle rare special cases. we are already aware of several systems that apply such approaches on an ad hoc basis with no distortion bounds. the potential bene ts include reduced development time and simpler delivered software systems. reducing computation time and resources it is possible to analyze the distortion and timing models to obtain strategies that optimally fail tasks to obtain the maximum execution time reduction while minimizing the resulting output distortion. users with large computations may use these strategies to reduce the amount of time or hardware resources required to obtain a desired result while staying within appropriate accuracy bounds. this assumption does not hold if the task failures are caused by software errors and or hardware faults in this case, tasks may execute for some time before failing. the primary anticipated use of the timing models is therefore to enable strategies that manage the time accuracy tradeoff by preemptively failing tasks before they begin execution. assumptions and fault model because our statistical models use the number of failed tasks to predict the distortion, they are appropriate for situations in which there is no correlation between the likelihood that a task will fail and the effect of that failure on the distortion. if there is such a correlation, the statistical model may not be accurate. this could happen with hardware faults, for example, if tasks that take a longer time to execute have a larger effect on the nal result. it could happen with software errors, for example, if the magnitude of a taskcontribution to the nal result is correlated with the likelihood that the task will encounter a software error. as with any statistical technique, one must understand the underlying assumptions to appropriately apply the technique to the situation at hand. we obtain our statistical models by sampling executions of the program running on several different training inputs. we anticipate usage scenarios in which these models are then used to provide distortion bounds estimates for the program running on production inputs. our approach is therefore appropriate for situations in which the effect of task failures on the distortion is not systematically different for the production inputs as compared with the training inputs. scope we have implemented a system that automatically produces distortion and timing models and applied this system to a collection of programs written in the jade implicitly parallel programming language. based on our experience with these programs, we have identi ed a general computational pattern that interacts well with our approach. nal result, then combine the contributions to obtain the result. this pattern is also present in many graphics and information retrieval programs. if each task either generates or combines contributions, the net effect of task failures is simply to discard some of the contributions. our results indicate that the distortion associated with discarding some of these contributions is often quite small and that our approach is quite effective at enabling computations that exhibit this pattern to execute through task failures to produce outputs with good distortion bounds. we anticipate that our approach will also work well for other computations that have this general pattern. finally, we note that our technique obtains the accuracy and timing models by sampling many different executions of the program running on a several training inputs. we anticipate usage scenarios in which this sampling overhead is pro tably amortized over the production runs, either because the production runs take substantially longer to execute than the training runs or because the program will be used for many more production runs than training runs. it may also be feasible to perform the training runs during a lead time between when the program is developed and when it is deployed into production. contributions this paper makes the following contributions: basic concept: it introduces, for the rst time, the concept of using continuous distortion and timing models to characterize the behavior of software systems in the face of failures. our results show that we are able to obtain accurate models that precisely capture the distortion and timing responses of these computations as a function of the task block failure rates. characterization: we analyze the underlying properties of the programs that are responsible for their behavior and identify a general computational pattern that interacts well with our approach. in section # we provide an example that illustrates how our technique operations. section # presents the methodology we use to obtain our models. section # presents our experience applying our technique to a set of scienti. we present related work in section # and conclude in section #. the amount of available resources is a central factor in the existence of virtually all living organisms. mechanisms that adapt the operation of the organism to variations in resource availability occur widely throughout nature. for example, during prolonged starvation, the human body preserves muscle mass by shifting its fuel source from proteins to ketone bodies. peripheral vasoconstriction, which mini mizes heat loss by limiting the ow of blood to the extremeties, is a standard response to hypothermia. the nasal turbinates in dehydrated camels extract moisture from exhaled respiratory air, thereby limiting water loss and enhancing the ability of the camel to survive in dessicated environments. all of these mechanisms take the organism away from its preferred operating mode but enable the organism to degrade its operation gracefully to enhance its survival prospects in resource poor environments. the vast majority of computer programs, in contrast, execute with essentially no exibility in the resources they consume. standard programming language semantics entails the execution of every computation the program attempts to perform. if the memory allocator fails to return a valid reference to an allocated block of memory, the program typically fails immediately with a thrown exception, failed error check, or memory addressing error. this inability to adapt to changes in the underlying operating environment impairs the exibility, robustness, and resilience of almost all currently deployed software systems. reduced resource computing encompasses a set of mechanisms that execute programs with only a subset of the resources that the standard programming language semantics and execution environment provides. reduced resource computing mechanisms include: discarding tasks: parallel computations are often structured as a collection of tasks. discarding tasks produces new computations that execute only a subset of the tasks in the original computation. loop perforation: loop perforation transforms loops to execute only a subset of the iterations in the original computation. different loop perforation strategies in clude modulo perforation, truncation perforation, and random perforation. xed size buffer for a given dynamic allocation site. at each allocation, it returns the next ele ment in the buffer, wrapping back around to the rst element when it reaches the end of the buffer. if the number of live objects allocated at the site is larger than the number of elements in the buffer, cyclic memory allocation produces new computations that execute with only a subset of the memory required to execute the original computation. resource reduction in practice unsurprisingly, these mechanisms almost always change the output that the program produces. so they are appropriate only for computations that have some exibility in the output they produce. examples of such computations include many numerical and scienti. computations, sensory applications that involve lossy encoding and decoding, many machine learning, statistical inference, and nance computations, and information retrieval computations. the relevant question is whether these kinds of computations are still able to deliver acceptable output after resource reduction. interestingly enough, our empirical results show that many of these computations contain components that can successfully tolerate the above resource reduction mechanisms the computation still produces acceptably accurate outputs after the application of these mechanisms to these components. and these resource reduction mechanisms can often endow computations with a range of capabilities that are typically otherwise available only through the manual development of new algorithms. speci cally, discarding tasks has been shown to enable computations to tolerate task failures without retry, to produce accuracy and per formance models that make it possible to purposefully and productively navigate induced accuracy versus performance trade off spaces, and to eliminate barrier idling at the end of parallel loops. cyclic memory al location has been shown to eliminate otherwise potentially fatal memory leaks. loop perforation has been shown to reduce the overall execution time of the computation and to enable techniques that dynamically control the computation to meet real time deadlines in the face of clock rate changes and processor failures. a key to the successful application of these mechanisms in practice is identifying the components that can successfully tolerate resource reduction, then applying resource reduction only to those components. this empirical fact leads to usage scenarios in which the resource reduction mechanisms generate a search space of programs close to the original programs. an automated search of this space nds the components that can tolerate resource reduction, with resource reduction con ned to those components when the computation executes. the remaining components execute with the full set of resources with which they were originally designed to operate. the resulting effect is conceptually similar to the mechanisms that biological organisms use to deal with reduced resources, which direct the delivery of scarce resources to those critical functions most necessary for survival. inherent redundancy the success of reduced resource computing shows that many computations, like biological organisms, have inherent sources of redundancy that enable them to operate successfully in the face of reduced resources. note, however, that these sources of redundancy were not explicitly engineered into the computation they emerge as an unintended consequence of the way the computation was formulated. in this paper we analyze various sources of redundancy that enable these computations to tolerate resource reduction. the result of this analysis is several general computational patterns that interact in very reasonable ways with the different resource reduction mechanisms. viewing our computations through the prism of these patterns helped us understand the behavior we were observing; we anticipate that recognizing these patterns in other computations will facilitate the prediction of how these other computations will react to resource reduction. in the future, trends such as the increasing importance of energy consumption, the need to dynamically adapt to computing platforms with uctuating performance, load, and power characteristics, and the move to more distributed, less reliable computing platforms will increase the need for computations that can execute successfully across platforms with a range of available resources. initially, we expect developers to let automated techniques nd and exploit patterns in existing applications that interact well with resource reduction. they may then move on to deploying such patterns into existing applications to enhance their ability to function effectively in a range of environments. ultimately, we expect developers to engineer software systems from the start around patterns that interact well with resource reduction in much the same way that developers now work with more traditional design patterns in all phases of the engineering process. contributions this paper makes the following contributions: computational patterns: it identi es computational patterns that interact well with resource reduction mechanisms such as discarding tasks, perforating loops, and dynamically allocating memory out of xed size buffers. understanding these patterns can help developers develop a conceptual framework that they can use to reason about the interaction of their applications with various resource reduction mechanisms. model computations: it presents concrete manifestations of the general patterns in the form of simple model computations. these model computations are designed to capture the essential properties of more complex realworld applications that enable these applications to operate successfully in the presence of resource reduction mechanisms. in this way, the model computations can give developers simple, concrete examples that can help them think productively about the structure of their applications and how that structure can affect the way their applications will respond to the application of different resource reduction mechanisms. the model computations can also serve as the foundation for static analyses that recognize computations that interact well with resource reduction mechanisms. such analyses could produce statistical models that precisely characterize the effect of resource reduction mechanisms on the application at hand, thereby making it possible to automatically apply resource reduction mechanisms to obtain applications with known statistical accuracy properties in the presence of resource reduction. simulations: it presents the results of simulations that use the model computations to quantitatively explore the impact of resource reduction on the accuracy of the results that the computations produce. the simulation results can help developers to better estimate and or analyze the likely quantitative accuracy consequences of applying resource reduction mechanisms to their own applications. relationship to applications: it relates the structure of the model computations and the simulation accuracy results back to characteristics of speci. understanding these relationships can help developers better understand the relationships between the model computations and their own applications. a new model of computation: standard models of computation are based on formal logic. in these models, the computation is rigidly xed by the application source code, with formulas in discrete formal logic characterizing the relationship between the input and output. this paper, in contrast, promotes a new and fun damentally different model in which the computation is exible and dynamic, able to adapt to varying amounts of resources, and characterized by continuous statistical relationships between the input, output, and amount of resources that the computation consumes. of course, almost every program has some hard logical correctness requirements even a video encoder, for example, must produce a correctly formatted video le. we therefore anticipate the development of new hybrid analysis approaches which verify appropriate hard logical correctness properties using standard program analysis techniques but use new statistical techniques to analyze those parts of the computation whose results can vary as long as they stay within acceptable statistical accuracy bounds. we present a new technique, early phase termination, for eliminating idle processors in parallel computations that use barrier synchronization. this technique simply terminates each parallel phaseas soon as there are too few remaining tasks to keep all of the processors busy. although this technique completely eliminates the idling that would other wise occur at barrier synchronization points, it may also change the computation and therefore the result that the computation produces. we address this issue by providing probabilistic distortion models that characterize how the use of early phase termination distorts the result that the computation produces. our experimental results show that for our set of benchmark applications, early phase termination can improve the performance of the parallel computation, the distortion is small and the distortion models provide accurate and tight distortion bounds. these bounds can enable users to evaluate the effect of early phase termination and confidently accept results from parallel computations that use this technique if they find the distortion bounds to be acceptable. finally, we identify a general computational pattern that works well with early phase termination and explain why computations that exhibit this pattern can tolerate the early termination of parallel tasks without producing unacceptable results. the synchronization that accomplishes this temporal separation is called barrier synchronization because it imposes a temporal barrier that separates the two phases. many parallel programs exhibit a parallelism pattern consisting of alternating parallel and serial phases. each parallel phase consists of a set of tasks that can execute in parallel. when all of the tasks nish, a serial phase consisting of a single task continues the computation until the start of the next parallel phase. each serial phase typically accesses data from all of the tasks in the preceding parallel phase. these tasks must therefore nish before the subsequent serial phase can begin. a well known issue with barrier synchronization is barrier idling, which occurs at the end of the parallel phase when there are few tasks left to execute. if there is a mismatch in processor speeds or task sizes, processors may be left idle as they wait for the remaining tasks to nish. this idling can limit the performance of the parallel computation. we propose a simple solution, early phase termination, to barrier idling: when there are too few tasks to keep all of the processors busy at the end of the parallel phase, simply terminate the remaining tasks and proceed immediately to the subsequent serial phase. it may, however, change the computation and therefore the result that the computation produces. there are two potential questions: how much does the result change, and how predictable is the change if the change is suf ciently small and predictable, users may be willing to accept the perturbed result in exchange for the increased performance associated with the elimination of barrier synchronization. distortion models to enable the user to evaluate the potential impact of early phase termination on the results of their computations, we build a statistical distortion model that characterizes the effect of terminating tasks. we obtain and use this model as follows: task decomposition: the developer speci es the task decomposition by identifying task blocks in the program. each task block consists of a block of code whose execution corresponds to a task in the computation. note that a given task block may execute many times during the course of a computation and may therefore generate many tasks into the computation. phase identi cation: the developer identi es the parallel and serial phases in the computation. baseline: we obtain several sample inputs for which the program is known to generate correct output. we run the program on these inputs, executing every task to completion, and record the outputs that it generates. criticality testing: we con gure the execution platform to randomly skip executions of selected tasks at target skip rates. we then select each task block in the program in turn, skip executions of that task block at a targeted rate, and observe the resulting output distortion. if the task skips produce unacceptable distortion or cause the computation to fail, we mark the task block as critical, otherwise we mark the task block as skippable. distortion model: given a set of skippable task blocks, we run repeated trials in which we randomly select a target task skip rate for each skippable task block, execute the computation, then record both the observed task skip rates and the resulting output distortion. we then use regression to obtain a probabilistic model that estimates the distortion as a function of the task skip rates. production runs: we use early phase termination in the production runs to eliminate barrier idling. we use the resulting observed proportion of early terminated tasks and the distortion model to obtain an estimated distortion and con dence interval around the distortion. the estimated distortion and con dence interval allow the user to determine whether or not to accept the result. we have applied this approach to several benchmark applications selected from the jade benchmark suite. our results show that the models are quite accurate and that the estimated distortion and con dence intervals are small enough to provide useful distortion bounds in practice. and it is possible to use models developed on one set of inputs to accurately predict distortion properties for other inputs. finally, our results indicate that the elimination of barrier idling can reduce the overall execution time and increase the performance of our benchmark applications. computation pattern based on our experience with our benchmark applications, we have identi ed a general computation pattern that interacts well with our approach. speci cally, the parallel phases in our applications generate many contributions to. nal result, then combine the contributions to obtain the result. if each parallel task either generates or combines contributions, the net effect of terminating tasks early is simply to discard some of the contributions. our results indicate that the distortion associated with discarding these contributions is often quite small. identifying this pattern provides a conceptual framework that can help users better understand the effect of early phase termination on their computations. one of our benchmark applications, for example, performs a monte carlo simulation in which each set of trials comprises a contribution. the net effect of early phase termination is simply to drop some of the trials. another application traces a set of rays through a medium to compute the density of different parts of the medium. the net effect of early phase termination is simply to drop some of the traced rays. we anticipate that this kind of understanding, which translates the effect of early phase termination back into concepts from the underlying application domain, can help users determine if they are comfortable using the technique. in the two examples above, it is clear that, in practice, both computations have enough redundancy to easily tolerate the loss of some of the contributions without substantially impairing the utility of the nal result. indeed, users may nd that understanding the effects of early phase termination at the level of the application domain may provide a more compelling case for its use than the probabilistic distortion models. a broader perspective to place early phase termination in a broader perspective, consider that almost all scienti. computations are inherently inaccurate in that they are designed to produce an approximation to an ideal result rather than the ideal result itself. many computational chemistry programs, for example, use classical or semi empirical methods instead of ab initio quantum mechanics. computations discretize conceptually continuous elds to enable the representation and computation of approximate solutions on digital computers, with the granularity of the discretization determined, in part, by factors such as the amount of available memory and computational power. as these examples illustrate, accuracy versus performance tradeoffs strongly in uence the form and basic approach of almost all scienti. the key question for such computations, then, is not correctness or incorrectness, but accuracy and pragmatic feasibility does the computation produce a result that is close enough to the ideal result within an acceptable time frame, and, if so, can the user determine that the result is acceptably close the ideal result viewed from this perspective, early phase termination is simply yet another technique for obtaining acceptable performance at the potential cost of some accuracy. and the probabilistic distortion models enable the user to evaluate the effect of the technique and determine whether or not the nal result is acceptably accurate. it is worth considering how these issues might play out in different application domains. many information retrieval computations, for example, map a subcomputation over a set of discrete items, then combine the results, with the overall computation able to tolerate the loss of some of the subcomputations. because of the ability of the human sensory system to tolerate noise and other processing artifacts, computer graphics and other sensory processing computations can often drop subcomputations without unacceptably degrading the nal output. as these examples illustrate, early phase termination may be feasible in practice for a wide range of computations, including computations for which probabilistic distortion models may not be readily available. contributions this paper makes the following contributions: elimination of barrier idling: it introduces the concept of eliminating barrier idling by terminating tasks at the end of parallel phases. distortion models: it introduces the use of probabilistic distortion models for characterizing the effect of early phase termination on the result that the program generates. these models provide an estimate of the distortion and probabilistic accuracy bounds around this distortion estimate explanation: it identi es a computation pattern that works well with early phase termination and explains why computations that exhibit this pattern can terminate tasks before they complete and still produce acceptable results. experimental results: it provides experimental results for our set of benchmark applications. these results indicate that the distortion and accuracy bounds are small enough for practical use and that eliminating barrier idling can improve the overall parallel performance. the remainder of the paper is structured as follows. in section # we provide an example that illustrates our programming model. section # presents the methodology we use to obtain the distortion models. section # presents our experience applying our technique to a set of scienti. we present related work in section # and conclude in section #. approximate computing is an approach where reduced accuracy of results is traded off for increased speed, throughput, or both. loss of accuracy is not permissible in all computing domains, but there are a growing number of data intensive domains where the output of programs need not be perfectly correct to provide useful results or even noticeable differences to the end user. these soft domains include multimedia processing, machine learning, and data mining analysis. an important challenge with approximate computing is transparency to insulate both software and hardware developers from the time, cost, and difficulty of using approximation. this paper proposes a software only system, paraprox, for realizing transparent approximation of data parallel programs that operates on commodity hardware systems. paraprox starts with a data parallel kernel implemented using opencl or cuda and creates a parameterized approximate kernel that is tuned at runtime to maximize performance subject to a target output quality that is supplied by the user. approximate kernels are created by recognizing common computation idioms found in data parallel programs and substituting approximate implementations in their place. across a set of soft data parallel applications with at most quality degradation, paraprox yields an average performance gain of on a nvidia gtx gpu and on an intel core quad core processor compared to accurate execution on each platform. over the past few years, the information technology industry has experienced a massive growth in the amount of data that it collects from consumers. analysts reported that in year# alone the industry gathered a staggering zettabytes of information, and they estimate that by year#, consumers will generate times this gure. most major businesses that host such large scale data intensive applications, including google, amazon, and microsoft, frequently invest in new, larger data centers containing thousands of multicore servers. however, it seems that such investments in new hardware alone may not translate to the computation capability required to keep up with the deluge of data. rather, it may be necessary to consider using alternative programming models that exploit the data parallel computing abilities of existing servers in order to address this problem. this paper focuses on applying one such model, approximate computing, where the accuracy of results is traded off for computation speed, to solve the problem of processing big data. approximation is applicable in domains where some degree of variation or error can be tolerated in the result of computation. for domains where some loss of accuracy during computation may cause catastrophic failure, eg, cryptography, approximation should not be applied. however, there are many important domains where approximation can greatly improve application performance, including multimedia processing, machine learning, data analysis, and gaming. video processing algorithms are prime candidates for approximation as occasional variation in results do not cause the failure of their overall operation. for example, a consumer using a mobile device can tolerate occasional dropped frames or a small loss in resolution during video playback, especially when this allows video playback to occur seamlessly. machine learning and data analysis applications also provide opportunities to exploit approximation to improve performance, particularly when such programs are operat figure #: the data parallel patterns that paraprox targets: map scatter gather reduction scan stencil partition. in this situation, processing the entire dataset may be infeasible, but by sampling the input data, programs in these domains can produce representative results in a reasonable amount of time. improving performance by applying approximation has been identi ed as an important goal by prior works. these works have studied this topic and proposed new programming models, compiler systems, and runtime systems to systematically manage approximation. we categorize the prior works based on these limitations: programmer based: in these systems, the programmer must write different approximate versions of a program and a runtime system decides which version to run. although the programmer may best understand how his code works, writing different versions of the same program with varying levels of approximation is neither easy nor practical to be applied generally. hardware based: these approaches introduce hardware modi cations such as imprecise arithmetic units, register les, or accelerators. although these systems work for general algorithms, they cannot be readily utilized without manufacturing new hardware. furthermore, having both exact and approximate versions of the same hardware increases the hardware design complexity and the dif culty of validating and verifying such hardware. software based: previous software based approximation techniques do not face the problems of the other two categories as they remove the burden of writing several versions of the program from the programmer, and can be used with existing, commodity systems. however, with past approaches, one solution does not. each of these solutions works only for a small set of applications. they either cannot achieve a desired amount of performance improvement or generate unacceptable computation errors for applications that they were not explicitly built to handle. to address these issues, this paper proposes a software framework called paraprox. paraprox identi es common patterns found in data parallel programs and uses a customdesigned approximation technique for each detected pattern. paraprox enables the programmer to write software once and run it on a variety of modern processors, without manually tuning code for different hardware targets. it is applicable to a wide range of applications as it determines the proper approximation optimizations that can be applied to each input program. because paraprox does not apply a single solution to all programs, it overcomes the aforementioned limitation of prior software based approaches. in this work, we identify different patterns commonly found in data parallel workloads and we propose a specialized approximation optimization for each pattern. we closely study data parallel programs because they are well tted for execution on prevalent multi core architectures such as cpus and gpus. paraprox is capable of targeting any data parallel architecture, provided that the underlying runtime supports such hardware. overall, paraprox enables the programmer to implement a kernel once using the opencl or cuda data parallel languages and, depending on the target output quality speci ed for the kernel, tradeoff accuracy for performance. to control the ef ciency, accuracy, and performance of the system, each optimization allows some variables to be dynamically varied. after paraprox generates the approximate kernels, a runtime system tunes the aforementioned variables to get the best performance possible while meeting the constraints of theoq. to automatically create approximate kernels, paraprox utilizes four optimization techniques which target six data parallel patterns: map, scatter gather, reduction, scan, stencil, and partition. paraprox applies approximate memoization to map and scatter gather patterns where computations are replaced by memory accesses. for reduction patterns, paraprox uses sampling plus adjustment to compute the output by only computing the reduction of a subset of the data. the stencil partition approximation algorithm is based on the assumption that adjacent locations in an input array are typically similar in value for such patterns. therefore, paraprox accesses a subset of values in the input array and replicates that subset to construct an approximate version of the array. for scan patterns, paraprox only performs the scan operation on a subset of the input array and uses the results to predict the results for the rest of the array. contributions of this work are as follows: pattern based compilation system for approximate execu tion. automatic detection of data parallel patterns in opencl and cuda kernels. approximation optimizations which are speci cally designed for six common data parallel computation patterns. the ability to control performance and accuracy tradeoffs for each optimization at runtime using dynamic tuning parameters. the rest of the paper is organized as follows. section # explains how the paraprox framework operates. approximate optimizations used by paraprox are discussed in section #. the results of using paraprox for various benchmarks and architectures are presented in section #. limitations of paraproxframework are discussed in section #. section # discusses the related work in this area and how paraprox is different from previous work. section # concludes this paper and summarizes its contributions and ndings. approximate computing, where computation accuracy is traded off for better performance or higher data throughput, is one solution that can help data processing keep pace with the current and growing overabundance of information. for particular domains such as multimedia and learning algorithms, approximation is commonly used today. we consider automation to be essential to provide transparent approximation and we show that larger benefits can be achieved by constructing the approximation techniques to fit the underlying hardware. our target platform is the gpu because of its high performance capabilities and difficult programming challenges that can be alleviated with proper automation. our approach, sage, combines a static compiler that automatically generates a set of cuda kernels with varying levels of approximation with a run time system that iteratively selects among the available kernels to achieve speedup while adhering to a target output quality set by the user. the sage compiler employs three optimization techniques to generate approximate kernels that exploit the gpu microarchitecture: selective discarding of atomic operations, data packing, and thread fusion. across a set of machine learning and image processing kernels, sage approximation yields an average of speedup with less than quality loss compared to the accurate execution on a nvidia gtx gpu. for example, while trying to smooth an image, the exact output value of a pixel can vary. to keep up with information growth, companies such as microsoft, google and amazon are investing in larger data centers with thousands of machines equipped with multi core processors to provide the necessary processing capability on a yearly basis. the latest industry reports show that in the next decade the amount of information will expand by a factor of while the number of servers will only grow by a factor of. at this rate, it will become more expensive for companies to provide the compute and storage capacity required to keep pace with the growth of information. to address this issue, one promising solution is to perform approximate computations on massively data parallel architectures, such as gpus, and trade the accuracy of the results for computation throughput. there are many domains where it is acceptable to use approximation techniques. in such cases some variation in the output is acceptable, and some degree of quality degradation is tolerable. many image, audio, and video processing algorithms use approximation techniques to compress and encode multimedia data to various degrees that provide tradeoffs between size and correctness such as lossy compression techniques. if the output quality is acceptable for the user or the quality degradation is not perceivable, approximation can be employed to improve the performance. as shown in figure #, smoothed images with and quality are not discernible from the original image but differences can be seen when enlarged. in the machine learning domain, exact learning and inference is often computationally intractable due to the large size of input data. we believe that as the amount of information continues to grow, approximation techniques will become ubiquitous to make processing such information feasible. conflicts per warp memory accesses per thread high cost of serialization memory bandwidth limitation slowdown figure #: three gpu characteristics that sageoptimizations exploit. these experiments are performed on a nvidia gtx gpu. shows how accessing the same element by atomic instructions affects the performance for the histogram kernel. illustrates how the number of memory accesses impacts performance while the number of computational instructions per thread remains the same for a synthetic benchmark. shows how the number of thread blocks impacts the performance of the blackscholes kernel. the idea of approximate computing is not a new one and previous works have studied this topic in the context of more traditional cpus and proposed new programming models, compiler systems, and run time systems to manage approximation. in this work, we instead focus on approximation for gpus. gpus represent affordable but powerful compute engines that can be used for many of the domains that are amenable to approximation. there are several common bottlenecks on gpus that can be alleviated with approximation. these include the high cost of serialization, memory bandwidth limitations, and diminishing returns in performance as the degree of multithreading increases. because many variables affect each of these characteristics, it is very dif cult and time consuming for a programmer to manually implement and tune a kernel. our proposed framework for performing systematic run time approximation on gpus, sage, enables the programmer to implement a program once in cuda, and depending on the target output quality speci ed for the program, trade the accuracy for performance based on the evaluation metric provided by the user. sage has two phases: of ine compilation and run time kernel management. during of ine compilation, sage performs approximation optimizations on each kernel to create multiple versions with varying degrees of accuracy. at run time, sage uses a greedy algorithm to tune the parameters of the approximate kernels to identify con gurations with high performance and a quality that satis es theoq. this approach reduces the overhead of tuning as measuring the quality and performance for all possible con gurations can be expensive. since the behavior of approximate kernels may change during run time, sage periodically performs a calibration to check the output quality and performance and updates the kernel con guration accordingly. to automatically create approximate cuda kernels, sage utilizes three optimization techniques. the rst optimization targets atomic operations, which are frequently used in kernels where threads must sequentialize writes to a common variable. the atomic operation optimization selectively skips atomic operations that cause frequent collisions and thus cause poor performance as threads are sequentialized. the next optimization, data packing, reduces the number of bits needed to represent input arrays, thereby sacri cing precision to reduce the number of high latency memory operations. the third optimization, thread fusion, eliminates some thread computations by combining adjacent threads into one and replicating the output of one of the original threads. a common theme in these optimizations is to exploit the speci. microarchitectural characteristics of the gpu to achieve higher performance gains than general methods, such as ignoring a random subset of the input data or loop iterations, which are unaware of the underlying hardware. in summary, the main contributions of this work are: the rst static compilation and run time system for automatic approximate execution on gpus. approximation optimizations that are utilized to automatically generate kernels with variable accuracy. a greedy parameter tuning approach that is utilized to determine the tuning parameters for approximate versions. a dynamic calibration system that monitors the output quality during execution to maintain quality with a high degree of con dence, and takes corrective actions to stay within the bounds of target quality for each kernel. the rest of the paper is organized as follows. section # discusses why sage chooses these three approximation optimizations. approximation optimizations used by sage are discussed in section #. the results of using sage for various benchmarks are presented in section #. section # discusses the related work in this area and how sage is different from previous works. the summary and conclusion of this work is outlined in section #. to mitigate this, approximate methods are widely used to learn realistic models from large data sets by trading off computation time for accuracy. however, in the context of gpus, previous approximation techniques have two limitations: the programmer is responsible for implementing and tuning most aspects of the approximation, and approximation is generally not cognizant of the hardware upon which it is run. section # explains how the sage framework operates. recent work has explored exposing this trade off in programming models. a key challenge, though, is how to isolate parts of the program that must be precise from those that can be approximated so that a program functions correctly even as quality of service degrades. using these types, the system automatically maps approximate variables to low power storage, uses low power operations, and even applies more energy efficient algorithms provided by the programmer. in addition, the system can statically guarantee isolation of the precise program component from the approximate component. this allows a programmer to control explicitly how information flows from approximate data to precise data. importantly, employing static analysis eliminates the need for dynamic checks, further improving energy savings. as a proof of concept, we develop enerj, an extension to java that adds approximate data types. we also propose a hardware architecture that offers explicit approximate storage and computation. we port several applications to enerj and show that our extensions are expressive and effective; a small number of annotations lead to significant potential energy savings at very little accuracy cost. energy is increasingly a first order concern in computer systems. exploiting energy accuracy trade offs is an attractive choice in applications that can tolerate inaccuracies. we propose using type qualifiers to declare data that may be subject to approximate computation. rst order constraint in mobile systems, and power cooling costs largely dominate the cost of equipment permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. more fundamentally, current trends point toward a utilization wall, in which the amount of active die area is limited by how much power can be fed to a chip. much of the focus in reducing energy consumption has been on low power architectures, performance power trade offs, and resource management. while those techniques are effective and can be applied without software knowledge, exposing energy considerations at the programming language level can enable a whole new set of energy optimizations. this work is a step in that direction. recent research has begun to explore energy accuracy trade offs in general purpose programs. a key observation is that systems spend a signi cant amount of energy guaranteeing correctness. consequently, a system can save energy by exposing faults to the application. importantly, these studies universally show that applications have portions that are more resilient and portions that are critical and must be protected from error. however, an error in a jump table could lead to a crash, and even small errors in the image le format might make the output unreadable. while approximate computation can save a signi cant amount of energy, distinguishing between the critical and non critical portions of a program is dif cult. these annotations, however, do not offer any guarantee that the fundamental operation of the program is not compromised. in other words, these annotations are either unsafe and may lead to unacceptable program behavior or need dynamic checks that end up consuming energy. we need a way to allow programmers to compose programs from approximate and precise components safely. moreover, we need to guarantee safety statically to avoid spending energy checking properties at runtime. the key insight in this paper is the application of type based information ow tracking ideas to address these problems. this paper proposes a model for approximate programming that is both safe and general. we use a type system that isolates the precise portion of the program from the approximate portion. the programmer must explicitly delineate ow from approximate data to precise data. the model is thus safe in that it guarantees precise computation unless given explicit programmer permission. safety is statically enforced and no dynamic checks are required, minimizing the overheads imposed by the language. we present enerj, a language for principled approximate computing. enerj extends java with type quali ers that distinguish between approximate and precise data types. data annotated with the approximate quali er can be stored approximately and computations involving it can be performed approximately. enerj also provides endorsements, which are programmer speci ed points at which approximate to precise data ow may occur. we formalize a core of enerj and prove a non interference property in the absence of endorsements. our programming model is general in that it uni es approximate data storage, approximate computation, and approximate algorithms. programmers use a single abstraction to apply all three forms of approximation. the model is also high level and portable: the implementation is entirely responsible for choosing the energy saving mechanisms to employ and when to do so, guaranteeing correctness for precise data and best effort for the rest. while enerj is designed to support general approximation strategies and therefore ensure full portability and backwardcompatibility, we demonstrate its effectiveness using a proposed approximation aware architecture with approximate memory and imprecise functional units. we have ported several applications to enerj to demonstrate that a small amount of annotation can allow a program to save a large amount of energy while not compromising quality of service signi cantly. we rst detail the enerj language extensions in section #. section # formalizes a core of the language, allowing us to prove a non interference property. the full formalism and proof are presented in an accompanying technical report. next, section # describes hypothetical hardware for executing enerj programs. while other execution substrates are possible, this proposed model provides a basis for the evaluation in sections and; there, we assess enerjexpressiveness and potential energy savings. the type checker and simulation infrastructure used in our evaluation are available at http: sampa cs washington edu. energy consumption is an increasing concern in many computer systems. many studies have shown that a variety of applications are resilient to hardware and software errors during execution. for example, an image renderer can tolerate errors in the pixel data it outputs a small number of erroneous pixels may be acceptable or even undetectable. the language supports programming constructs for algorithmic approximation, in which the programmer produces different implementations of functionality for approximate and precise data. section # presents related work and section # concludes. prior proposals have used annotations on code blocks and data allocation sites. memories today expose an all or nothing correctness model that incurs significant costs in performance, energy, area, and design complexity. but not all applications need high precision storage for all of their data structures all of the time. this paper proposes mechanisms that enable applications to store data approximately and shows that doing so can improve the performance, lifetime, or density of solid state memories. the first allows errors in multi level cells by reducing the number of programming pulses used to write them. the second mechanism mitigates wear out failures and extends memory endurance by mapping approximate data onto blocks that have exhausted their hardware error correction resources. simulations show that reduced precision writes in multi level phase change memory cells can be faster on average and using failed blocks can improve array lifetime by on average with quality loss under. techniques under the umbrella of approximate computing exploit this tolerance to trade. many common applications have intrinsic tolerance to inaccuracies in computation. applications in domains like computer vision, media processing, machine learning, and sensor data analysis can see large. ciency gains in permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than the author must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. jacob nelson university of washington nelson cs washington edu luis ceze university of washington luisceze cs washington edu exchange for small compromises on computational accuracy. extends to storage: tolerance to errors in both transient and persistent data is present in a wide range of software, from servers to mobile devices. meanwhile, the semiconductor industry is beginning to encounter limits to further scaling of common memory technologies like dram and flash. as a result, new memory technologies and techniques are emerging. multi level cells, which pack more than one bit of information in a single cell, are already commonplace and phase change memory is imminent. but both pcm and flash wear out over time as cells degrade and become unusable. furthermore, multi level cells are slower to write due to the need for tightly controlled iterative programming. memories traditionally address wear out issues and implement multi level cell operation in ways that ensure perfect data integrity of the time. this has signi cant costs in performance, energy, area, and complexity. these costs are exacerbated as memories move to smaller device feature sizes along with more process variation. by relaxing the requirement for perfectly precise storage and exploiting the inherent error tolerance of approximate applications failure prone and multi level memories can gain back performance, energy, and capacity. in this paper, we propose techniques that exploit data accuracy trade. in essence, we advocate exposing storage errors up to the application with the goal of making data storage more. we make this safe by: exploiting application level inherent tolerance to inaccuracies; and providing an interface that lets the application control which pieces of data can be subject to inaccuracies while. ering error free operation for the rest of the data. the rst technique uses multi level cells in a way that enables higher density or better performance at the cost of occasional inaccurate data retrieval. the second technique uses blocks with failed bits to store approximate data; to mitigate the. ect of failed bits on overall value precision, we prioritize the correction of higher order bits. approximate storage applies to both persistent storage as well as transient data stored in main memory. we explore the techniques in the context of pcm, which may be used for persistent storage or as main memory, but the techniques generalize to other technologies such as flash. we simulate main memory benchmarks and persistent storage datasets and nd that our techniques improve write latencies by or extend device lifetime by on average while trading. we begin by describing the programming models and hardware software interfaces we assume for main memory and persistent approximate storage. next, sections and describe our two approximate storage techniques in detail. section # describes our evaluation of the techniques using a variety of error tolerant benchmarks and section # gives the results for these experiments. finally, we enumerate related work on storage and approximate computing and conclude. control and memory divergence between threads in the same execution bundle, or warp, can significantly throttle the performance of gpu applications. we exploit the observation that many gpu applications exhibit error tolerance to propose branch and data herding. branch herding eliminates control divergence by forcing all threads in a warp to take the same control path. data herding eliminates memory divergence by forcing each thread in a warp to load from the same memory block. to safely and efficiently support branch and data herding, we propose a static analysis and compiler framework to prevent exceptions when control and data errors are introduced, a profiling framework that aims to maximize performance while maintaining acceptable output quality, and hardware optimizations to improve the performance benefits of exploiting error tolerance through branch and data herding. our software implementation of branch herding on nvidia geforce gtx improves performance by up to for a suite of nvidia cuda sdk and parboil benchmarks. our hardware implementation of branch herding improves performance by up to. observed output quality degradation is minimal for several applications that exhibit error tolerance, especially for visual computing applications. due to the evolution of technology constraints, especially energy constraints which may lead to heterogeneous multi cores, and the increasing number of defects, the design of defect tolerant accelerators for heterogeneous multi cores may become a major micro architecture research issue. and the emergence of high performance applications implementing recognition and mining tasks, for which competitive ann based algorithms exist, drastically expands the potential application scope of a hardware ann accelerator. however, while the error tolerance of ann algorithms is well documented, there are few in depth attempts at demonstrating that an actual hardware ann would be tolerant to faulty transistors. most fault models are abstract and cannot demonstrate that the error tolerance of ann algorithms can be translated into the defect tolerance of hardware ann accelerators. in this article, we introduce a hardware ann geared towards defect tolerance and energy efficiency, by spatially expanding the ann. most custom circuits are highly defect sensitive, a single transistor can wreck such circuits. on the contrary, artificial neural networks are inherently error tolerant algorithms. in order to precisely assess the defect tolerance capability of this hardware ann, we introduce defects at the level of transistors, and then assess the impact of such defects on the hardware ann functional behavior. we empirically show that the conceptual error tolerance of neural networks does translate into the defect tolerance of hardware neural networks, paving the way for their introduction in heterogeneous multi cores as intrinsically defect tolerant and energy efficient accelerators. approximate computing leverages the intrinsic resilience of applications to inexactness in their computations, to achieve a desirable trade off between efficiency and acceptable quality of results. to broaden the applicability of approximate computing, we propose quality programmable processors, in which the notion of quality is explicitly codified in the hw sw interface, ie, the instruction set. the isa of a quality programmable processor contains instructions associated with quality fields to specify the accuracy level that must be met during their execution. we show that this ability to control the accuracy of instruction execution greatly enhances the scope of approximate computing, allowing it to be applied to larger parts of programs. the micro architecture of a quality programmable processor contains hardware mechanisms that translate the instruction level quality specifications into energy savings. additionally, it may expose the actual error incurred during the execution of each instruction back to software. as a first embodiment of quality programmable processors, we present the design of quora, an energy efficient, quality programmable vector processor. quora utilizes a tiered hierarchy of processing elements that provide distinctly different energy vs. quality trade offs, and uses hardware mechanisms based on precision scaling with error monitoring and compensation to facilitate quality programmable execution. we evaluate an implementation of quora with processing elements in nm technology. the results demonstrate that leveraging quality programmability leads to savings in energy for virtually no loss in application output quality, and energy savings for modest impact on output quality. our work suggests that quality programmable processors are a significant step towards bringing approximate computing to the mainstream.