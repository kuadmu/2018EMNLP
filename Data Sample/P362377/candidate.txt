this paper analyzes the causes of packet loss in a node urban multi hop network. the patterns and causes of loss are important in the design of routing and error correction protocols, as well as in network planning the paper makes the following observations. the distribution of inter node loss rates is relatively uniform over the whole range of loss rates; there is no clear threshold separating in range and out of range. most links have relatively stable loss rates from one second to the next, though a small minority have very bursty losses at that time scale. signal to noise ratio and distance have little predictive value for loss rate. the large number of links with intermediate loss rates is probably due to multi path fading rather than attenuation or interference the phenomena discussed here are all well known. the contributions of this paper are an understanding of their relative importance, of how they interact, and of the implications for mac and routing protocol design. radio link quality estimation in wireless sensor networks has a fundamental impact on the network performance and also affects the design of higher layer protocols. therefore, for about a decade, it has been attracting a vast array of research works. reported works on link quality estimation are typically based on different assumptions, consider different scenarios, and provide radically different results. this article provides a comprehensive survey on related literature, covering the characteristics of low power links, the fundamental concepts of link quality estimation in wsns, a taxonomy of existing link quality estimators, and their performance analysis. to the best of our knowledge, this is the first survey tackling in detail link quality estimation in wsns. we believe our efforts will serve as a reference to orient researchers and system designers in this area. the propagation of radio signals is affected by several factors that contribute to the degradation of its quality. the effects of these factors are even more signi cant on the propagation of wireless signals with low power radios, typically used in wireless sensor networks. consequently, radio links in wsns are often unpredictable. in fact, their quality uctuates over time and space, and connectivity is typically asymmetric. nowadays, it is well known that three factors lead to link unreliability: the environment, which leads to multipath propagation effects and contributes to background noise, the interference, which results from concurrent transmissions within a wireless network or between cohabiting wireless networks and other electromagnetic sources; and hardware transceivers, which may distort sent and received signals due to their internal noise. in wsns, these radio transceivers transmit low power signals, which makes radiated signals more prone to noise, interference, and multipath distortion. furthermore, they rely on antennas with nonideal radiation patterns, which leads to anisotropic behavior. in the literature, several research papers focused on the statistical characterization of low power links through estimation theory, which is commonly known as link quality estimation, to study the behavior of low power links. link quality estimation in wsns is a fundamental building block for several mechanisms and network protocols. for instance, routing protocols rely on link quality estimation to overcome low power links unreliability and maintain the correct network operation. delivering data over links with high quality improves the network throughput by limiting packet loss and maximizes its lifetime by minimizing the number of retransmissions and avoiding route reselection triggered by links failure. link quality estimation also plays a crucial role for topologycontrol mechanisms to maintain the stability of the topology. high quality links are long lived, therefore, ef cient topology control mechanisms rely on the aggregation of high quality links to maintain robust network connectivity for long periods, thus avoiding unwanted transient topology breakdown. link quality estimation in wsns is a challenging problem due to the lossy and dynamic behavior of the links. therefore, it is vital for wsn protocol designers to correctly account for low power link characteristics. this article lls this gap and provides a comprehensive survey of the most relevant key observations drawn from empirical studies on low power links in wsns. such observations are useful for the design of ef cient link quality estimators as well as other mechanisms at higher layers, as they heavily depend on the underlying radio links. this article aims at providing wsn researchers and practitioners with a useful understanding of low power links. to this end, we start with an overview of the most common wsn radio technology, presented in section #. next, we analyse the empirical characterization of low power links in section #, and discuss their statistical estimation in section #. section # presents a novel taxonomy and classi cation of the existing link quality estimators, whereas section # discusses their performance. overall, we make four contributions: we present a comprehensive survey on low power link characteristics. we overview the fundamental concepts of link quality estimation in wsns. we present a taxonomy of existing link quality estimators. we discuss the performance of existing link quality estimators, based on existing simulation and experimental work. as such, they are closer to reality than other models. however, nearly all theoretic work in the sinr model depends on the assumption of smooth geometric decay, one that is true in free space but is far off in actual environments. we present a simple solution that allows the modeling of arbitrary static situations by moving from geometry to arbitrary decay spaces. the complexity of a setting is captured by a metricity parameter that indicates how far the decay space is from satisfying the triangular inequality. for distributed algorithms, that to date have appeared to necessarily depend on the planarity, we indicate how they can be adapted to arbitrary decay spaces at a cost in time complexity that depends on a fading parameter of the decay space. in particular, for decay spaces that are doubling, the parameter is constant bounded. finally, we explore the dependence on in the approximability of core problems. in particular, we observe that the capacity maximization problem has exponential upper and lower bounds in terms of in general decay spaces. in euclidean metrics and related growth bounded decay spaces, the performance depends on the exact metricity definition, with a polynomial upper bound in terms of, but an exponential lower bound in terms of a variant parameter. the upper bound result is the first approximation of a capacity type sinr problem that is subexponential in. signal strength models of wireless communications capture the gradual fading of signals and the additivity of interference. the challenge is to model realistic environments, including walls, obstacles, reflections and anisotropic antennas, without making the models algorithmically impractical or analytically intractable. all results that hold in the sinr model in general metrics carry over to decay spaces, with the resulting time complexity and approximation depending on in the same way that the original results depends on the path loss term. in spite of the apparent complexity of such models, various fundamental problems have been resolved analytically in recent years. these models also seem essential for studying certain properties of wireless networks, such as capacity, or connectivity and aggregation, which can be achieved in logarithmic rounds in worst case. nearly all theoretic work in signal strength models have been done in the geo sinr model that assumes that signals decay as a smooth polynomial function of distance. this assumption about decay is true in free space, but turns out to be far. in actual environments, as shown by a long history of experimental studies. quoting a recent meta study, link quality is not correlated with distance. this questions the wisdom of studying sinr models analytically, given the added. one hope might be that results in the basic sinr model could eventually carry some insights that would be of use in more detailed models that capture more of reality. however, real environments consist of assortments of walls, ceilings and obstacles, as well as complex interactions involving re ections, shadowing, multi path signals, and anisotropic antennas. it might seem near impossible to capture all of that without making the resulting models hopelessly impractical for algorithm design and or analytically intractable. both modify the signal strength multiplicatively by an exponentially distributed random variable. these models are highly useful both for generating instance and for average case analysis. for handling actual instances, their utility necessarily depends on the suitability of the assumptions. the alternative view, with deep roots in computer science theory, is to allow for worst case behavior and obtain guarantees that hold for all instances. the aim of this work is to make such anycase analysis feasible, while avoiding assumptions that may or may not be re ected in actual instances. we present a simple solution that allows the modeling of arbitrary static situations by moving from geometry to arbitrary decay spaces. the decay between two ordered nodes is the relative reduction in the strength of a signal sent from the rst node to the second. these decays can be obtained by signal strength measurements which almost any cheap widget can perform today capturing the truth on the ground. the complexity of a setting is captured by a metricity parameter. that indicates how far the decay space is from satisfying the triangular inequality. the simple metricity de nition has wide ranging implications: all results that hold in the sinr model in general metrics carry over to decay spaces; the resulting time complexity and approximation depending on. in the same way that the original results depends on the path loss term. for distributed algorithms, that to date have appeared to necessarily depend on the planarity, we introduce a fading parameter of the decay space and indicate they can be adapted to arbitrary decay spaces at a cost in time complexity that depend on a fading parameter of the decay space. in particular, for decay spaces that are doubling, the parameter is constant bounded. in euclidean metrics and related growth bounded decay spaces, the performance depends on the exact metricity de nition, with a polynomial upper bound in terms of, but an exponential lower bound in terms of a variant parameter. one may ask if we are being led to yet another model that will later been shown unrealistic. fortunately, numerous experimental studies have veri ed the remaining key assumptions in wide range of situations and technology: additivity of interference, sinr capture. thus, we may nally be reaching a wireless model that is a close approximation of the reality at the phy mac layer, yet usable algorithmically and analytically. that said, one should not discount the value of abstractions or the potentially value of simple models. also, modeling dynamic and mobile situations, which is outside the scope of our work, remains a highly important issue. some positive results hold in that model, eg, distributed power assignment of feasible sets, reductions involving rayleigh fading, and special cases of capacity maximization. however, for most problems of interest, extremely strong inapproximability results hold. thus, it is essential to use near metric properties of the decay space. the concept of inductive independence has heralded a more systematic approach to sinr analysis, and can by itself be seen as parameter of the decay space. additionally, we veri ed that the key properties of the sinr formula in decay spaces closely approximate reality, even in highly complex environments, in alignment with previous experimental results. we address the core requirement of fading for distributed algorithms, introduce a parameter that extends their reach to arbitrary spaces, and prove constant upper bounds in spaces with bounded doubling dimension. concluding with some observations and open issues in sec. the most common are log normal shadowing and rayleigh fading for addressing long and short distance variability, respectively. in a sibling paper, we introduced decay spaces and metricity with a focus on validation by measurements in two testbeds. the impact of metricity parameters on approximability is treated in sec. signal strength models of wireless communications capture the gradual fading of signals and the additivity of interference that are consistent with electro magnetic theory. various stochastic extensions of geometric path loss have been proposed to address the observed variability in signal propagation. in particular, we observe that the capacity problem has exponential upper and lower bounds in terms of. ectiveness, and invariability of wireless conditions in static environments. the abstract sinr model captures, like decay spaces, arbitrary pairwise path loss. fading metrics were identi ed to capture the main property required from the planar setting. we found that the metricity parameter did appear to re ect the complexity of the environment. we also examined the impact of the availability of multiple channels and found that this can signi cantly reduce the measured metricity. in the next section, we introduce decay spaces, and indicate how previous results in metric spaces carry over. the introduction of general metrics was a signi cant step in extending sinr theory beyond geometric assumptions. same holds forindependence in the case of uniform power. we study the impact on networks of rf interference from devices such as zigbee and cordless phones that increasingly crowd the ghz ism band, and from devices such as wireless camera jammers and non compliant devices that seek to disrupt operation. our experiments show that commodity equipment is surprisingly vulnerable to certain patterns of weak or narrow band interference. this enables us to disrupt a link with an interfering signal whose power is year# times weaker than the victim signals, or to shut down a multiple ap, multiple channel managed network at a location with a single radio interferer. we identify several factors that lead to these vulnerabilities, ranging from mac layer driver implementation strategies to phy layer radio frequency implementation strategies. our results further show that these factors are not overcome by simply changing operational parameters with the exception of frequency shifts. this leads us to explore rapid channel hopping as a strategy to withstand rf interference. we prototype a channel hopping design using prism nics, and find that it can sustain throughput at levels of rf interference well above that needed to disrupt unmodified links, and at a reasonable cost in terms of switching overheads. combined, these results clarify significantly the centralized complexity of wireless communication problems. the capacity of a wireless network is the maximum possible amount of simultaneous communication, taking interference into account. given is a set of links, each a sender receiver pair located in a metric space, and an assignment of power to the senders. we seek a maximum subset of links that are feasible in the sinr model: namely, the signal received on each link should be larger than the sum of the interferences from the other links. we give a constant factor approximation that holds for any length monotone, sub linear power assignment and any distance metric. we use this to give essentially tight characterizations of capacity maximization under power control using oblivious power assignments. specifically, we show that the mean power assignment is optimal for capacity maximization of bi directional links, and give a tight approximation of scheduling bi directional links with power control using oblivious power. for uni directional links we give a nearly optimal approximation to the power control problem using mean power, where is the ratio of longest and shortest links. in modern wireless networks devices are able to set the power for each transmission carried out. we present the first algorithm achieving a constant factor approximation in fading metrics. the best previous results depend on further network parameters such as the ratio of the maximum and the minimum distance between a sender and its receiver. expressed only in terms of, they are approximations. furthermore, existing approaches work well together with the algorithm allowing it to be used in singlehop and multi hop scheduling scenarios. our algorithm still achieves an approximation if we only assume to have a general metric space rather than a fading metric. experimental but also theoretical results indicate that such power control can improve the network capacity significantly. we study this problem in the physical interference model using sinr constraints. in the sinr capacity maximization problem, we are givenpairs of senders and receivers, located in a metric space. the algorithm shall select a subset of these pairs and choose a power level for each of them with the objective of maximizing the number of simultaneous communications. this is, the selected pairs have to satisfy the sinr constraints with respect to the chosen powers. all analytical and simulation research on ad hoc wireless networks must necessarily model radio propagation using simplifying assumptions. we provide a comprehensive review of six assumptions that are still part of many ad hoc network simulation studies, despite increasing awareness of the need to represent more realistic features, including hills, obstacles, link asymmetries, and unpredictable fading. we use an extensive set of measurements from a large outdoor routing experiment to demonstrate the weakness of these assumptions, and show how these assumptions cause simulation results to differ significantly from experimental results. we close with a series of recommendations for researchers, whether they develop protocols, analytic models, or simulators for ad hoc wireless networks. accurate interference models are important for use in transmission scheduling algorithms in wireless networks. in this work, we perform extensive modeling and experimentation on two node telosb motes testbeds one indoor and the other outdoor to compare a suite of interference models for their modeling accuracies. we first empirically build and validate the physical interference model via a packet reception rate vs. we then similarly instantiate other simpler models, such as hop based, range based, protocol model, etc. the modeling accuracies are then evaluated on the two testbeds using transmission scheduling experiments. we observe that while the physical interference model is the most accurate, it is still far from perfect, providing a percentile error about, depending on the scenario. the accuracy of the other models is worse and scenario specific. the second best model trails the physical model by roughly percentile points for similar accuracy targets. somewhat similar throughput performance differential between models is also observed when used with greedy scheduling algorithms. carrying on further, we look closely into the the two incarnations of the physical model thresholded and graded. we show via solving the one shot scheduling problem, that the graded version can improve expected throughput over the thresholded version by scheduling imperfect links. in a wireless network it is important to understand the nature of the joint interference generated at a receiver by multiple concurrent transmitters. prior experimental work using older generation mote class radios have showed systematic deviations between estimation and direct measurement of the joint interference power, thus questioning whether the standard assumption that received signal powers are additive is applicable in practice. we, however, show via extensive experimentation that on newer generation radios, the additive assumption is valid, particularly at the low power end. observations about radio behaviors in wireless networks are quite platform speci c. there is a growing interest in understanding and modeling interference in wireless networks. while many algorithmic studies have used simple interference models based on network topology or physical node distance, focus has recently shifted towards more realistic models, based on signal to interference plus noise ratio. essentially, in sinr model, success of a packet reception depends on the ratio of the signal power on one hand and the aggregated interference and noise power on permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. the sinr model is driven by the fundamentals of radio receiver behavior at the physical layer. while realistic, the sinr model is no longer pairwise as the other simpler models, where the interference is modeled between the link in question and only one other interfering link. in sinr model, interference is considered in an aggregated form as the sum of all interference powers from all interfering links. this subtlety not only makes algorithm design complex, but also makes instantiation of the model harder. prior research hasdonesigni cantempirical work in understanding theinterferenceproperties using thesinr modelforlowpowerwirelesslinksusing the rstgeneration motes. however, they reported several interesting observations that cannot be easily supported by radio fundamentals. in particular, they reportedthataggregatedorjointsignalpowercannot be modeled by simply summing the individual signal powers. this obviously makes modeling aggregated interference much harder. in our work, weperform a careful set of measurements on alatergeneration mote and report our ndings. we nd, incontrasttotheobservationsin, additive model worksquitewellinpracticebarring measurement noises. to makethisobservation, we repeatsimilarexperimentsin, albeit on the newer mote platform, and then back up the observation with more elaborate experiments and measurements so that a fairly robust conclusion is possible. for brevity, we particularly emphasize on the low power end. much of our paper describes the details of the experimental setup, methodology and observations. our experience in this work points out that speci. later generation hardware is likely to have lesser imperfections and are prone to lesser measurement errors and are thus likely to track theory more closely. the research community must do serious validation work in various platforms before embracing or questioning any model. to date, topology control in wireless ad hoc and sensor networks the study of how to compute from the given communication network a subgraph with certain beneficial properties has been considered as a static problem only; the time required to actually schedule the links of a computed topology without message collision was generally ignored. in this paper we analyze topology control in the context of the physical signal to interference plus noise ratio model, focusing on the question of how and how fast the links of a resulting topology can actually be realized over time for this purpose, we define and study a generalized version of the sinr model and obtain theoretical upper bounds on the scheduling complexity of arbitrary topologies in wireless networks. our result thus bridges the gap between static graph based interference models and the physical sinr model. based on these results, we also show that when it comes to scheduling, requiring the communication links to be symmetric may imply significantly higher costs as opposed to topologies allowing unidirectional links. note that our results also yield the rst general scaling laws on the scheduling complexity of an arbitrary set of communication requests. a common complaint within the networking community in general, and the ad hoc and sensor networking community in particular, is that there exists a wide chasm between theory and practice. what tends to be overlooked, however, is the fact that there are seemingly insurmountable gaps even within the theory community itself a particularly distinct divide reveals itself with the question of the underlying communication models. on the one hand, the graph theoretical ly oriented theory community has tried to come up with. cient network protocols for problems such as routing, clustering, data gathering, or topology control. typically, these analytical results are derived in a type of static graph model that captures certain aspects of wireless communication while abstracting away many others. on the other hand, there has been a large body of theoretical work taking a more communication or information theoretic approach. often based on the physical signal to interferenceplus noise ratio model of communication, these papers derive fundamental scaling laws that describe the theoretically achievable capacity in di erent modalities of communication and model assumptions. it is the goal of this paper to analytically identify previously unknown ties between these two approaches. speci cally, we study topology control until now an inherently graph theoretic notion and analyze its impact in the information theoretic sinr model. in a very general sense, topology control in wireless ad hoc and sensor networks can be considered the task of given a network connectivity graph computing a subgraph with speci. desired properties, such as connectivity, short stretches, sparsity, low interference, or low node degree. ort towards achieving and combining more and more of these properties. all these approaches have in common, however, that they model wireless networks as static graphs, hence neglecting one of the most crucial aspects of wireless communication: eventually messages or, even more exactly, radio signals will have to be sent over these static links in the topologies selected by a topology control algorithm, that is, the static graph of communication links must be scheduled on the physical layer. in general, not all of these messages can be sent simultaneously, as mutual interference may prevent proper message reception. hence, what has been inherently lacking in the study of topology control so far is the notion of time required to actually realize the selected links, that is, to successfully transmit messages over them. in particular, it has not been clear even from a theoretical point of view whether the graph theoretic measures of topology control really bear any signi cance when it comes to actually scheduling messages in an sinr environment. in this paper, we demonstrate and prove the existence of fundamental theoretical ties between topology control and the theoretically achievable. consider an arbitrary topology computed by a topology control algorithm, or more generally a set of communication requests. in one time slot, only a subset of all the desired communication links can be scheduled in parallel; and in every subsequent time slot, a subset of the remaining unscheduled links may be scheduled, until nally all links are scheduled. against this background, it is clearly advantageous to nd a short schedule, ideally one with minimal length. this allows for higher bandwidth and, ultimately, higher throughput. we call this measure describing the minimal amount of time required to physical ly establish a set of communication requests or a desired network topology the scheduling complexity of the topology. in this paper, we show that the initially mentioned gap in the eld of topology control is neither inherent nor inevitable. speci cally, we study two problems: first, what is the scheduling complexity of arbitrary topologies, that is, what is the time required to physically schedule an arbitrary set of requests and second, which fundamental graph theoretic measures determine the scheduling complexity in other words, what properties should a topology have such that provably. cient scheduling becomes possible strongly coupled with the rst problem is the question of how we can actually nd a schedule which enable a fast realization of a topology. interestingly, studying these questions also reveals the intricacy of the interplay between scheduling complexity and transmission power assignment. more speci cally, we make the following contributions: as the main technical result of this paper we present a scheduling algorithm which assigns transmission power levels to the network nodes and schedules all links of an arbitrary network topology. the subsequent analysis reveals that this algorithm computes a schedule of length, whereis the number of nodes and iin is a previously de ned static interference measure that re ects a static property of wireless network graphs. intriguingly, in spite of its static nature, the measure iin appears to play an important role concerning the scheduling complexity of arbitrary network topologies. our result proves that topology control algorithms choosing good static topologies topologies with small iin allow for faster scheduling on the physical sinr communication level. we show that there exists an inherent gap with respect to link symmetry: network topologies preserving connectivity of the given communication network using unidirectional links have signi cantly lower iin values and can therefore be scheduled much faster than connectivitypreserving topologies using exclusively symmetric links. this result sheds new light on the question of practicality of directed as opposed to symmetric links in wireless ad hoc and sensor networks: it shows that demanding communication links to be symmetric theoretically incurs a high overhead when it comes to scheduling. in this paper we explicitly analyze topology control with special emphasis on the physical de nition of interference, or more speci cally the signal to interference plus noise ratio. simply expressed, this characterization of interference re ects the fact that a radio signal can be correctly decoded by the intended receiver only if the ratio between the sensed power of the actual signal to be received and the sum of all power levels experienced due to other signals concurrently transmitted is above a certain hardware dependent threshold. in reality, obstacles to signal propagation can shadow, re ect, scatter, and di ract radio signals. ects, we extend the sinr approach by a generalized signal propagation model; in particular, we allow the received signal strength to deviate from the theoretically computed value in the absence of any obstacles by a constant factor which is re ected in the subsequent analysis. finally, combining our novel scheduling algorithm with a known low interference topology control algorithm for strong connectivity with iin. we show that in the physical model of communication, the scheduling complexity of connectivity is, thus improving the bound given in by a logarithmic factor. as a general remark, we would like to point out that we are aware of the fact that primarily due to its missing locality, as shown later in the paper, our scheduling algorithm does not lend itself to direct implementation as a network protocol. instead, we believe that the algorithm is of great theoretical interest, as it proves. rst upper bound on the scheduling complexity of arbitrary topologies in wireless networks. speci cally, it explicitly shows that fast scheduling of arbitrary topologies is theoretically possible even in worstcase wireless networks when assigning proper power levels to the nodes. the paper is organized as follows: after giving an overview of related work in the following section, we formally introduce the considered communication model in section # and de ne the concepts of scheduling complexity and iin interference in section #. in sections and we present the scheduling algorithm for arbitrary network topologies and its analysis, respectively. section # nally demonstrates the existence of a gap with respect to scheduling complexity between topologies allowing unidirectional links and topologies containing exclusively symmetric connections. we present a measurement based study of interference among links in a static, ieee, multi hop wireless network. interference is a key cause of performance degradation in such networks. to improve, or to even estimate the performance of these networks, one must have some knowledge of which links in the network interfere with one another, and to what extent. however, the problem of estimating the interference among links of a multi hop wireless network is a challenging one. accurate modeling of radio signal propagation is difficult since many environment and hardware specific factors must be considered. empirically testing every group of links is not practical: a network withnodes can have links, and even if we consider only pairwise interference, we may have to potentially test pairs. given these difficulties, much of the previous work on wireless networks has assumed that information about interference in the network is either known, or that it can be approximated using simple heuristics. we test these heuristics in our testbed and find them to be inaccurate. we then propose a simple, empirical estimation methodology that can predict pairwise interference using only measurements. our methodology is applicable to any wireless network that uses omni directional antennas. the predictions made by our methodology match well with the observed pairwise interference among links in our node, based testbed. we develop a general model to estimate the throughput and goodput between arbitrary pairs of nodes in the presence of interference from other nodes in a wireless network. compared to existing measurement based models, our model advances the state of the art in three important ways. first, it goes beyond pairwise interference and models interference among an arbitrary number of senders. second, it goes beyond broadcast transmissions and models the more common case of unicast transmissions. third, it goes beyond homogeneous nodes and models the general case of heterogeneous nodes with different traffic demands and different radio characteristics. using simulations and measurements from two different wireless testbeds, we show that the predictions of our model are accurate in a wide range of scenarios. our model is based on measurements from the underlying network itself and is thus more accurate than abstract models of rf propagation such as those based on distance. the seed measurements are easy to gather, requiring only measurements in annode networks. due to the broadcast nature of the medium, transmissions from one sender interfere with the transmission and reception capabilities of other nodes. understanding and managing interference is essential to the performance of wireless networks. channel assignment, transmit power control, routing, transport protocols, and network diagnosis. unfortunately, the state of the art in estimating the impact of interference is rather primitive. much of the existing work is based on simple, abstract models of radio propagation. while such models may predict the asymptotic behavior, they can be highly inaccurate in any given network. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. this has prompted researchers to devise models that are seeded using measurements from the underlying network. they are then used to predict the impact of interference in more complex con gurations such as multiple transmitting nodes. this is a promising direction because it makes no assumptions about the nature of radio propagation which has proven dif cult to model in real environments. however, the existing measurement based models are quite limited. they do not apply to con gurations that have more than two senders or two ows, have unicast traf, or have senders with nite demands. the only way today to predict network behavior under these general con gurations is to actually measure it. but measurement alone is insuf cient because it lacks predictive power and scalability. while it can accurately predict the performance of the measured con guration, it cannot predict performance for other con gurations. to optimize network performance, one often needs to predict the performance of many alternative con gurations. since measuring all possible con gurations is not feasible, it is necessary to develop a model to estimate network performance under arbitrary con gurations. in this paper, we develop a general model of interference in heterogeneous multihop wireless networks with asymmetric link quality and non binary interference relationships. it then estimates the rate at which each sender will transmit and the rate at which each receiver will successfully receive packets. compared to existing measurement based models, we advance the state of art in three important ways. first, we go beyond the case of two senders and model interference among an arbitrary number of senders. second, we go beyond broadcast transmissions and model the more common case of unicast transmissions. unicast transmissions introduce additional complexities due to retransmissions, exponential backoff, possibly asymmetric link qualities, and collisions with not only data packets but also ack packets. third, we go beyond the case of in nite traf. demands and model the more realistic case of nite demands. most real networks have heterogeneous nodes with varying traf. our model consists of three major components: annode markov model for capturing interactions among an arbitrary number of broadcast senders. the model provides a simple yet accurate approximation to the distributed coordination function. it is more general than previous models and can support multihop wireless networks, unsaturated demands, asymmetric link quality, and non binary interference relationships. these measurements are usually collected in a simple con guration, such as each node sending by itself. demand and received signal strength between pairs of nodes, which requires only measurements in an nnode network. this is challenging due to complex interactions among nodes. for instance, the sending rate of nodedepends on those of all other nodes, which in turn depend on the sending rate ofitself. we evaluate our models for the base case of two senders that broadcast packets simultaneously. we present practical models for the physical layer behaviors of packet reception and carrier sense with interference in static wireless networks. these models use measurements of a real network rather than abstract rf propagation models as the basis for accuracy in complex environments. seeding our models requirestrials in annode network, in which each sender transmits in turn and receivers measure rssi values and packet counts, both of which are easily obtainable. the models then predict packet delivery and throughput in the same network for different sets of transmitters with the same node placements. we find that they are effective at predicting when there will be significant interference effects. across many predictions, we obtain an rms error for a and of a half and a third, respectively, of a measurement based model that ignores interference. common protocols such as make conservative scheduling decisions, serializing the transmissions of senders who can hear each other in case there is harmful interference. most explorations of protocols with respect to inter permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. of analysis and simulation: the ability to explore a large space of con gurations with reproducible results. this undermines the value of experiments by making it dif cult to meaningfully compare results. rst step in that direction in which we derive a practical model of packet delivery under interference. we then formulate low level models for packet reception and carrier sense by relating the traditional notion of sinr to our measurements. wireless networks such as have enjoyed an unprecedented adoption rate in recent years, and their deployed base continues to grow. originally envisioned to support mobile devices, wireless has also proved popular in more static settings that involve pcs and laptops in homes and of ces because it removes the need for wires. a fundamental issue in these networks is interference, in which transmissions from one sender receiver pair affect those of other pairs. interference de nes the spatial boundaries for spectrum reuse, and it directly impacts the assignment of senders to channels, network capacity, and routing choices. it is thus startling that packet delivery under interference is poorly understood for real networks. ference use simple abstract models that may assume signal propagation is a simple function of distance, that coverage of radios is circular, that interference range is twice the transmission range, and so forth. unfortunately, empirical data from experimental wireless networks has shown that all of these models are largely inaccurate. rf propagation in realistic environments is suf ciently complex that the only existing feasible method for estimating packet delivery between two nodes is to measure it. as a response, there has been a shift towards experimental wireless networks in which packet delivery and higher level metrics have been measured for real radios working in particular network and protocol designs. this approach is valuable because it mitigates the problem of unrealistic rf models; it has improved the understanding of wireless behaviors. it is too time consuming to run experimental networks in a wide variety of settings, and results in one setting do not necessarily predict results in a different setting or even at a later time. in our work, we ask whether it is possible to combine the strengths of both methods: can we use simple measurements on a wireless network to capture its rf characteristics and then predict how it will perform when running under different settings this paper is. we use measurements on a running network to seed our model because predicting rf propagation in a complex environment is a hopelessly challenging task. to ensure that our model is applicable to real networks, we rely on only received signal strength indicator values and pair wise delivery counts, since both are easily obtained using commodity wireless cards. we record this information when there is a single sender as observed at all receivers, which requirestrials to obtain parameters for annode network. we investigate characteristics, both in a controlled setting with attenuators and on a building network, to provide a foundation for the models. these models are in turn fed into a higher layer system model that predicts packet delivery and interference for the same node placements but different sets of transmitters. we view this as a foundation for exploring other higher level design choices, such as rts cts exchanges, routing and channel assignments. we have evaluated the base case of our models, in which two senders compete to transmit xed size broadcast packets at a set bit rate, on our in building testbed. we nd that they are effective at identifying the situations in which there will be signi cant interference and predicting the magnitude of the effect. across many random trials the rms error of our throughput predictions is figure #: our wireless testbed, consisting of fteen ag nodes. of the channel bitrate for a and for b. this is comparable to the temporal variability we observe in the wireless medium. in contrast, the rms error of a naive model that ignores interference is two or three times higher, for a and for, with predictions that are often poor when there is signi cant interference. to further demonstrate the utility of our models, we show how they can be used to predict the con ict graph of a network. we view these results as promising, and are hopeful that future work will extend them to cover a larger fraction of the many transmission options: more than two senders, mixtures of packet sizes and rates, unicast traf. the rest of this paper is organized as follows. the network environments we experiment with are described in section #. we study wireless characteristics to support our models in section #. we undertake a systematic experimental study of the effects of concurrent packet transmissions in low power wireless networks. our measurements, conducted with mica motes equipped with cc radios, confirm that guaranteeing successful packet reception with high probability in the presence of concurrent transmissions requires that the signal to interference plus noise ratio exceed a critical threshold. however, we find a significant variation of about db in the threshold for groups of radios operating at different transmission powers. we find that it is harder to estimate the level of interference in the presence of multiple interferers. we also find that the measured sinr threshold generally increases with the number of interferers. our study offers a better understanding of concurrent transmissions and suggests richer interference models and useful guidelines to improve the design and analysis of higher layer protocols. experimental studies have demonstrated that the behavior of real links in low power wireless networks deviates to a large extent from the ideal binary model used in several simulation studies. in particular, there is a large transitional region in wireless link quality that is characterized by significant levels of unreliability and asymmetry, significantly impacting the performance of higher layer protocols. we provide a comprehensive analysis of the root causes of unreliability and asymmetry. in particular, we derive expressions for the distribution, expectation, and variance of the packet reception rate as a function of distance, as well as for the location and extent of the transitional region. these expressions incorporate important environmental and radio parameters such as the path loss exponent and shadowing variance of the channel, and the modulation, encoding, and hardware variance of the radios. wireless sensor network protocols are often evaluated through simulations that make simplifying assumptions about the link layer, such as the ideal this work has been supported in part by the national science foundation under grants cans, cans, cans, and cc. an earlier version of this article appeared in the year# proceedings of the ieee sencon. authoraddress:niga zamalloa, krishnamachari, department of electrical engineeringsystems, university of southern california, los angeles, ca; mail: usc edu. permission to make digital or hard copies part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or direct commercial advantage and that copies show this notice on the rst page or initial screen of a display along with the full citation. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior speci. permissions may be requested from the publications dept, acm, inc, penn plaza, suite, new york, ny year# usa, fax, or permissions acm org. year# acm year# year# art doi year# http: doi acm org year# binary model. in this model, packets are received only within the circular radio range of the transmitter. however, the real characteristics of low power wireless links differ greatly from those of the ideal model; chie. among these differences are the unreliable and asymmetric nature of real links. the signi cant differences between the ideal model and real behavior can lead to erroneous performance evaluation of upper layer protocols. several studies have classi ed low power wireless links in three distinct reception regions: connected, transitional, and disconnected. in the connected region, links are often of good quality, stable, and symmetric. by contrast, the transitional region is characterized by the presence of unreliable and asymmetric links; and the disconnected region presents no practical links for transmission. unfortunately, the transitional region is often quite signi cant in size, and in dense deployments such as those envisioned for sensor networks, a large number of the links in the network can be unreliable. recent studies have shown that unreliable and asymmetric links can have a major impact on the performance of upper layer protocols. in ganesan et al, it is shown that the dynamics of even the simplest ooding mechanism can be signi cantly affected due to asymmetric and occasional long distance links. in kotz et al, it is argued that routing structures formed by taking into account unreliable links can be signi cantly different from structures based on the simple binary model. similarly, the authors of zhou et al report that such unreliable links can have a negative impact on routing protocols, particularly geographic forwarding schemes. other works have proposed mechanisms to take advantage of nodes in the transitional region. for instance, the authors of de couto et al found that protocols using a traditional minimum hop count metric perform poorly in terms of throughput, and that a new metric called etx, which uses nodes in the transitional region, has better performance. the signi cant impact of real link characteristics on the performance of upper layer protocols has created increased understanding for the need for realistic link layer models for wireless sensor networks. in order to address this need, some recent works have proposed new link models based on empirical data. however, these models do not provide signi cant mathematical insight into how channel and radio dynamics affect link unreliability and asymmetry. also, some of these works are valid only for the speci. channel and radio parameters used in the deployment. in this study, we use analytical tools from communication theory, simulations, and experiments to present an in depth analysis of unreliable and asymmetric links in low power multihop wireless networks. the main contributions of this work are twofold. first, our work allows quantifying the impact of the wireless environment and radio characteristics on link reliability and asymmetry. second, we propose a systematic way to generalize models for the link layer so as to enhance simulation accuracy. organization topic section related work channel dynamics link model impact on link reliability expectation and variance of prr comparison with available models hardware variance hardware variance model impact on link asymmetry impact on link reliability empirical validation we also derive expressions for both the packet reception rate as a function of distance, and for the size of the transitional region. these expressions incorporate several radio parameters such as modulation, encoding, output power, frame size, receiver noise oor, and hardware variance, as well as important channel parameters, namely, the path loss exponent and log normal variance. section # studies the impact of multipaths on link reliability. first, we present a model for the packet reception rate as a function of distance in section #. based on this model, in section # we study the impact of channel and radio parameters on link reliability by analyzing their effect on the extent of the transitional region. then, in section # we present approximate expressions for the expectation and variance of the packet reception rate as a function of distance. the section ends with a comparison of available link models with the one proposed in this work. we study the impact of hardware variance in section #. hardware variance has already been identi ed as the cause of link asymmetry, and in addition, we also show that it can play a signi cant role on the extent of the transitional region. in section #, we present a model for hardware variance. based on this model, the impact of hardware variance on link asymmetry and reliability is quanti ed in sections and, respectively. finally, in section # we present empirical measurements, based on a testbed of mica motes, which validate some analytical insights of our work. before proceeding, we present the scope of our work. our study is focused on static and neither low dynamic environments and considers interference effects nor the nonisotropic property of radio coverage. however, it can be complemented with other research efforts to incorporate these dynamics. for instance, in son et al the authors focus on interference in wireless sensor networks, cerpa et al study some temporal properties and zhou et al provide an interesting model for the nonisotropic characteristic of radio coverage; the models presented in these works can be used to complement ours. a presents some guidelines on how to combine the nonisotropic rim model with our work. wireless sensor networks promise fine grain monitoring in a wide variety of environments. many of these environments can be harsh for wireless communication. from a networking perspective, the most basic aspect of wireless communication is the packet delivery performance: the spatio temporal characteristics of packet loss, and its environmental dependence. these factors will deeply impact the performance of data acquisition from these networks in this paper, we report on a systematic medium scale measurement of packet delivery in three different environments: an indoor office building, a habitat with moderate foliage, and an open parking lot. our findings have interesting implications for the design and evaluation of routing and medium access protocols for sensor networks. finally, in our harsher environments, nearly of the links exhibit asymmetric packet loss. wireless communication has the reputation of being notoriously unpredictable. the quality of wireless communication depends on the environment, the part of the frequency this work is supported in part by nsf grant ccr for the center for embedded systems. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. spectrum under use, the particular modulation schemes under use, and possibly on the communicating devices themselves. communication quality can vary dramatically over time, and has been reputed to change with slight spatial displacements. all of these are true to a greater degree for ad hoc communication than for wireless communication to a base station. given this, and the paucity of large scale deployments, it is perhaps not surprising that there have been no medium to large scale measurements of ad hoc wireless systems; one expects measurement studies to reveal high variability in performance, and one suspects that such studies will be non representative. wireless sensor networks are predicted on ad hoc wireless communications. perhaps more than other ad hoc wireless systems, these networks can expect highly variable wireless communication. they will be deployed in harsh, inaccessible, environments which, almost by de nition will exhibit signi cant multi path communication. many of the current sensor platforms use low power radios which do not have enough frequency diversity to reject multi path propagation. finally, these networks will be fairly densely deployed. given the potential impact of these networks, and despite the anecdotal evidence of variability in wireless communication, we argue that it is imperative that we get a quantitative understanding of wireless communication in sensor networks, however imperfect. using up to mica motes, we systematically evaluate the most basic aspect of wireless communication in a sensor network: packet delivery. particularly for energy constrained networks, packet delivery performance is important, since that translates to network lifetime. sensor networks are predicated using lowpower rf transceivers in a multi hop fashion. cient than one single hop over a long range link. poor cumulative packet delivery performance across multiple hops may degrade performance of data transport and expend signi cant energy. depending on the kind of application, it might signi cantly undermine application level performance. finally, understanding the dynamic range of packet delivery performance is important for evaluating almost all sensor network communication protocols. we study packet delivery performance at two layers of the communication stack. at the physical layer and in the absence of interfering transmissions, packet delivery performance is largely a function of the environment, the particular physical layer coding scheme, and perhaps individual receiver characteristics. we place a simple linear topology, with a single sender, in three di erent environments: an. ce building, a local habitat, and an open parking lot. cacy of packet delivery under di erent transmit powers and physical layer codings. at the medium access layer, interfering transmissions contribute to poor packet delivery performance. many mac layers contain mechanisms, such as carrier sense and linklayer retransmissions, to counteract these. cacy of such mechanisms in our three environments discussed above. our measurements uncover a variety of interesting phenomena. there are heavy tails in the distributions of packet loss, both at the physical layer and at the mac layer. in our indoor experiments at the physical layer, for example, fully half of the links experienced more than packet loss, and a third more than. at the physical layer, this variability can be characterized by the existence of a gray area within the communication range of a node: receivers in this gray area are likely to experience choppy packet reception, and in some environments, this gray area is almost a third of the communication range. the gray area is also distinguished by signi cant variability in packet reception over time. relatively sophisticated physical layer coding schemes are able to mask some of the variability, but with a loss in bandwidth. at the mac layer, link layer retransmissions are unable to reduce the variability; packet losses at the mac layer also exhibit heavy tails. ciency of the mac layer is low: to of communication energy is wasted in overcoming packet collisions and environmental. taken together, this appears to paint a somewhat pessimistic picture of wireless communication for sensor networks. however, we contend that there might be a simple set of mechanisms that can greatly improve packet delivery in the environments that sensor networks are targeted for. such topology control mechanisms would carefully discard poorly performing neighbors or neighbors to whom asymmetric links exist. this represents a departure from traditional lower layer design, where decisions are made at packet granularity. at least for static sensor networks, because pathological loss performance depends upon spatial positioning, it is meaningful to make decisions at the granularity of links to neighbors. from our experiments, we discover that the variance in received signal strength is largely random; however, it exhibits a continuous change with incremental changes in direction. with empirical data obtained from the mica and micaz platforms, we establish a radio model for simulation, called the radio irregularity model. with this model, we investigate the impact of radio irregularity on several upper layer protocols, including mac, routing, localization and topology control. our results show that radio irregularity has a relatively larger impact on the routing layer than the mac layer. it also shows that radio irregularity leads to larger localization errors and makes it harder to maintain communication connectivity in topology control. in this article, we investigate the impact of radio irregularity on wireless sensor networks. radio irregularity is a common phenomenon that arises from multiple factors, such as variance in rf sending power and different path losses, depending on the direction of propagation. this model is the first to bridge the discrepancy between the spherical radio models used by simulators and the physical reality of radio signals. to deal with these issues, we present eight solutions to deal with radio irregularity. the results obtained from both the simulations and a running testbed demonstrate that our solutions greatly improve system performance in the presence of radio irregularity. radio irregularity is a common and non negligible phenomenon in wireless sensor networks. or direct commercial advantage and that copies show this notice on the rst page or initial screen of a display along with the full citation. permissions may be requested from publications dept, acm, inc, year# broadway, new york, ny year# usa, fax: or permissions acm org. the impact of radio irregularity on protocol performance can be investigated through a running system. rim takes into account both the anisotropic properties of the propagation media and the heterogeneous properties of devices. with the help of the rim model, we explore the impact of radio irregularity on mac, routing, localization, and topology control performance. among the protocols we evaluate, we nd that radio irregularity has a signi cant impact on routing, localization, and topology control protocols, but a relatively small impact on the mac protocols. we propose several potential solutions to deal with radio irregularity in wireless sensor networks. experimental data collected from the berkeley mote platform and make some general conclusions about radio irregularity. we then use the rim model in simulations to analyze the impact of radio irregularity on mac protocols in section #, routing protocols in section #, localization protocols in section #, and topology control protocols in section #. it results in irregularity in radio range and variations in packet loss in different directions, and is considered as an essential reason this work was supported by the national science foundation, and by the darpa ixo of ces under the nest project. authors address: computer science department, university of virginia, charlottesville, va year#; email: virginia edu. permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior speci. year# acm year# for asymmetric links as viewed by upper layers in the protocol stack. several empirical studies on the berkeley mote platform have shown that the radio range varies signi cantly in different directions and the percentage of asymmetric links in a system varies depending on the average distance between nodes. however, few researchers have actually pursued this direction, because of two reasons: first, the complexity and cost of performance evaluations on a running system escalate when sensor networks scale up to thousands or more nodes. second, repeatable results of radio performance are extremely hard to obtain from uncontrolled environments, hence leading to dif culties in system tuning and performance evaluation. as a result, simulation techniques are used as an ef cient alternative to evaluate protocol performance. unfortunately, most existing simulations do not take radio irregularity, a common phenomenon in wireless communication, into account. the spherical radio patterns assumed by simulators such as zeng et al may not approximate real radio properties well enough, and hence may lead to an inaccurate estimation of application performance. several researchers have already shown extensive evidence of radio irregularity in wireless communication. their main focus is to observe and quantify such phenomena. this article is distinguished from the previous ones for the initiative in bridging the gap between spherical radio models used by simulators and the physical reality of radio signals. we rst verify the presence of radio irregularity using empirical data obtained from mica and micaz platforms. the results demonstrate that the radio pattern is largely random; however, it exhibits continuous change with incremental changes in direction. based on experimental data, a radio model for simulations, called the radio irregularity model, is formulated. we also nd that location based routing protocols, such as geographic forwarding perform worse in the presence of radio irregularity than on demand protocols, such as aodv and dsr. we evaluate the symmetric geographic forwarding solution in simulation, and implement the asymmetry detection service as well as the bounded distance forwarding solution in running systems with mica devices. our results illustrate that our solutions do succeed in alleviating the performance penalties due to radio irregularity. the rest of this article is organized as follows: we brie. analyze the causes and impact of radio irregularity in section #. in section #, we describe acm transactions on sensor networks, vol. based on these conclusions, we propose the rim radio model in section #. solutions to deal with radio irregularity are proposed and evaluated in section #. finally, we conclude the article in section #.