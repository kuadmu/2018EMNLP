evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance. we present a real world study of modeling the behavior of web search users to predict web search result preferences. accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks. our key insight to improving robustness of interpreting implicit feedback is to model query dependent deviations from the expected noisy user behavior. we show that our model of clickthrough interpretation improves prediction accuracy over state of the art clickthrough methods. we generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone. we report results of a large scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods. relevance measurement is crucial to web search and to information retrieval in general. traditionally, search relevance is measured by using human assessors to judge the relevance of querydocument pairs. however, explicit human ratings are expensive and difficult to obtain. at the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results. if we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems. recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search. however, most traditional permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. susan dumais robert ragno microsoft research microsoft research sdumais microsoft com rragno microsoft com ir work was performed over controlled test collections and carefully selected query sets and tasks. therefore, it is not clear whether these techniques will work for general real world web search. a significant distinction is that web search is not controlled. individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered. but the amount of the user interaction data is orders of magnitude larger than anything available in a non web search setting. by using the aggregated behavior of large numbers of users we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting. furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage. hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions. automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings. we present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results. our contributions include: a distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions. extensions of existing clickthrough strategies to include richer browsing and interaction features. a thorough evaluation of our user behavior models, as well as of previously published state of the art techniques, over a large set of web search sessions. we discuss our results and outline future directions and various applications of this work in section #, which concludes the paper. we review a query log of hundreds of millions of queries that constitute the total query traffic for an entire week of a general purpose commercial web search service. previously, query logs have been studied from a single, cumulative view. in contrast, our analysis shows changes in popularity and uniqueness of topically categorized queries across the hours of the day. we examine query traffic on an hourly basis by matching it against lists of queries that have been topically pre categorized by human editors. we show that query traffic from particular topical categories differs both from the query stream as a whole and from other categories. this analysis provides valuable insight for improving retrieval effectiveness and efficiency. it is also relevant to the development of enhanced query disambiguation, routing, and caching algorithms. after reviewing a weekworth of data hundreds of millions of queries we have found, not surprisingly, that: permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. we hypothesized that traffic behavior for some categories would change over time and that others would remain stable. understanding how queries change over time is critical to developing effective, efficient search services. we are unaware of any log analysis that studies differences in the query stream over the hours in a day; much less how those differences are manifested within topical categories. we focus on circadian changes in popularity and uniqueness of topical categories. emphasis on changing query stream characteristics over this longitudinal aspect of query logs distinguishes this work from prior static log analysis, surveyed in. we began with the hypothesis that there are very different characteristics during peak hours and off peak hours during a day. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. the number of queries issued is substantially lower during non peak hours than peak hours. however, we knew little about how often queries are repeated from one hour of the day to the next. after examining the behavior of millions of queries from one hour of the day to the next we have found the less obvious result: the average number of query repetitions in an hour does not change significantly on an hourly basis throughout the day. most queries appear no more than several times per hour. these queries consistently account for a large portion of total query volume throughout the course of the day. the queries received during peak hours are more similar to each other than their non peak hour counterparts. we also analyze the queries representing different topics using a topical categorization of our query stream. these cover approximately of the total query volume. for different categories, we examined their traffic characteristics: some topical categories vary substantially more in popularity than others as we move through an average day. some topics are more popular during particular times of the day, while others have a more constant level of interest over time. the query sets for different categories have differing similarity over time. the level of similarity between the actual query sets received within topical categories varies differently according to category. this leads us to believe that predictive algorithms that are able to estimate the likelihood of a query being repeated may well be possible. this could have a significant impact on future cache management and load balancing algorithms. such algorithms could improve retrieval effectiveness by assisting in query disambiguation, making it easier to determine what information need is being expressed by a query at a given time. they could also assist research in search efficiency that takes into account query arrival rates. our analysis covers the entirety of the tens of millions of queries each day in the search log from america online over a complete week in december. this represents a population of tens of millions of users searching for a wide variety of topics. section # reviews the prior work in query log analysis. section # describes our analysis of overall query traffic. section # describes our analysis of trends in categorized queries. finally, in section # we present our conclusions and directions for future work. furthermore, many web documents such as local news stories, lottery results, and sports team fan pages may not correspond to physical addresses, but the location of the user still plays an important role in document relevance. in this paper, we show how to infer a more general location relevance which uses not only physical location but a more general notion of locations of interest for web pages. our results show that a substantial fraction of web search queries can be significantly improved by incorporating location based features. personalization of search results offers the potential for significant improvements in web search. among the many observable user attributes, approximate user location is particularly simple for search engines to obtain and allows personalization even for a first time web search user. however, acting on user location information is difficult, since few web documents include an address that can be interpreted as constraining the locations where the document is relevant. we compute this information using implicit user behavioral data, characterize the most location centric pages, and show how location information can be incorporated into web search ranking. user behavior provides many cues to improve the relevance of search results through personalization. one aspect of user behavior that provides especially strong signals for delivering better relevance is an individual history of queries and clicked documents. our findings have implications for the design of search systems that leverage user behavior to personalize the search experience. we also characterize how the relative contribution of each model changes throughout the duration of a session. previous studies have explored how short term behavior or long term behavior can be predictive of relevance. ours is the first study to assess how short term behavior and long term behavior interact, and how each may be used in isolation or in combination to optimally contribute to gains in relevance through search personalization. our key findings include: historic behavior provides substantial benefits at the start of a search session; short term session behavior contributes the majority of gains in an extended search session; and the combination of session and historic behavior out performs using either alone. search personalization improves retrieval effectiveness by tailor ing the ranking of results for individual users based on models of their interests. to construct the profiles necessary for search personalization, evidence of a userinterests can be mined from observed past behaviors. an important determinant of the success of personalization is the behavioral information that is used to construct user profiles. alt hough there has been some work examining the effect of different contextual sources for modeling user interests, another critical aspect of personalization is the timespan of the behavioral information used for profile construction. short term profiles capture recent interactions but lack users long term interests. a principled investigation of the im pact of short and long term behavior on search personalization is lacking and we address that shortcoming with the research pre sented here. in this paper, we investigate how users long term search activity history interacts with their short term search session behavior. we characterize these interactions using a framework for modeling behavior from different timespans and predicting search rele vance. we explore the effectiveness of user profiles developed based on different temporal views. we make the following unique contributions with this research: propose a novel unified modeling framework that provides an integrative view of different parameters of personalization and controls key aspects such as the features generated from behavior and decay factors employed. section # describes our unified framework for combining a user historical behavior with their session activity and outlines the features and model training. section # describes the experiment, including the data and methodology. long term profiles represent long term interests but may not ade quately represent searcher needs for the current task. study dynamics in the relative contribution to personaliza tion of short and long term models over the course of a ses sion. we present findings in section #, discuss them and their implications in section #, and conclude in section #. this behavior can be sourced from the short term or the long term. these studies have shown that personalization is important but often care must be taken in how it is applied, eg, we may only want to personal ize queries which have high click entropy. earlier at tempts to address this challenge leveraged different representa tions for each source or made ad hoc decisions around how to weight distant actions. although each model makes use of sets of search activity gathered over different durations, the same feature set is used for each time span to remove that source of variation, and decay factors are studied in a principled manner. we evaluate the success of these models via a large search log containing queries, results, and clicks, enabling us to compare the performance of each personalized ranker rela tive to that of a high quality commercial search engine in a man ner similar to previous personalization research. as part of our analysis, we confirm intuitions that long term behavior is useful at the start of a session and that short term models yield benefit as the session proceeds. provide new findings on search personalization, such as the special properties of the first query in the session, and the strong performance of models that learn to combine short and long term features for each query, rather than simply ag gregating all features; suggesting that individual queries dif ferentially benefit from short and long term personalization. the remainder of this paper is structured as follows. section # presents related work on model based user behavior analysis for search personalization. we examine two basic sources for implicit relevance feedback on the segment level for search personalization: eye tracking and display time. a controlled study has been conducted where participants had to view documents in front of an eye tracker, query a search engine, and give explicit relevance ratings for the results. we examined the performance of the basic implicit feedback methods with respect to improved ranking and compared their performance to a pseudo relevance feedback baseline on the segment level and the original ranking of a web search engine. our results show that feedback based on display time on the segment level is much coarser than feedback from eye tracking. but surprisingly, for re ranking and query expansion it did work as well as eye tracking based feedback. all behavior based methods performed significantly better than our non behavior based baseline and especially improved poor initial rankings of the web search engine. the study shows that segment level display time yields comparable results as eye tracking based feedback. thus, it should be considered in future personalization systems as an inexpensive but precise method for implicit feedback. personalization has quite some history in information retrieval research. yet, this is a very hard problem and it has been identi ed as one of the major challenges in information retrieval research lately. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. the goal of interpreting user interaction data is to infer user perceived relevance of documents or content. it has been shown that explicitly asking the user for relevance feedback is very useful for individually improving the accuracy of search. however, asking for explicit relevance feedback is an additional burden to the user and demands extra cognitive. therefore, automatically inferring relevance by interpreting user behavior is a highly promising approach. until now, research in this area has primarily focused on user interaction data that is easy to get, ie, coming from keyboard and mouse input allowing for measures such as click through, display time, scrolling, mouse movements, etc. in most cases, these measures have been used to estimate object level relevance, thus relevance for entire documents. recently, research has been conducted to shed light on how additional measures like eye movements or emotions might relate to user intent and user assessment of individually perceived relevance. this kind of precise feedback has been proven to be particularly. but eye trackers with su cient precision are too expensive nowadays so that such feedback data will not be available in a common workplace in the near future. scrolling behavior, however, is simple to observe so that display times of di erent document segments are easy to compute. inexpensive, yet accurate feedback from display time would be highly valuable since it can be simply logged by browser toolbars. in this paper we examine the relationship between segmentlevel display time and segment level feedback from an eye tracker in the context of information retrieval tasks. how do they compare to each other and to a pseudo relevance feedback baseline on the segment level how much can they improve the ranking of a large commercial web search engine through re ranking and query expansion to answer those questions after providing some background information, we rst explain the technical basis of the applied implicit feedback methods. then, the study design is described, followed by a detailed analysis of the results and a conclusion. especially feedback generated from eye tracking can be applied on the segment level, eg, for speci. information like context and interaction data and adapting the search process according to individual user needs and preferences. as with any application of machine learning, web search ranking requires labeled data. the labels usually come in the form of relevance assessments made by editors. click logs can also provide an important source of implicit feedback and can be used as a cheap proxy for editorial labels. the main difficulty however comes from the so called position bias urls appearing in lower positions are less likely to be clicked even if they are relevant. in this paper, we propose a dynamic bayesian network which aims at providing us with unbiased estimation of the relevance from the click logs. experiments show that the proposed click model outperforms other existing click models in predicting both click through rate and relevance. web page ranking has been traditionally based on hand designed ranking functions such as bm. with the inclusion of thousands of features for ranking, hand tuning of ranking function becomes intractable. several machine learning algorithms have been applied to automatically optimize ranking functions. machine learned ranking requires a large number of training examples, with relevance labels indicating the degree of relevance for each querydocument pair. the cost of the editorial labeling is usually quite expensive. moreover, the relevance labels of the training examples could change over time. for example, if the query is time sensitive or recurrent, a search engine is expected to return the copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. most up to date documents sites to the users. however, it would be prohibitive to keep all the relevance labels up to date. click logs embed important information about user satisfaction with a search engine and can provide a highly valuable source of relevance information. compare to editorial labels, clicks are much cheaper to obtain and always re ect current relevance. clicks have been used in multiple ways by a search engine: to tune search parameters, to evaluate di erent ranking functions, or as signals to directly in uence ranking. however, clicks are known to be biased, by the presentation order, the appearance of the documents, and the reputation of individual sites. many studies have attempted to account the position bias of click. carterette and jones proposed to model the relationship between clicks and relevance so that clicks can be used to unbiasedly evaluate search engine when lack of editorial relevance judgment. other research attempted to model user click behavior during search so that future clicks may be accurately predicted based on observations of past clicks. two di erent types of the click models are position models and the cascade model. a position model assumes that a click depends on both relevance and examination. each rank has a certain probability of being examined, which decays by rank and depends only on rank. a click on a url indicates that the url is examined and considered relevant by the user. however this model treats the individual urls in a search result page independently and fails to capture the interaction among urls in the examination probability. take for example two equally relevant urls for a query: a user may only click on the top one, feel satis ed, and then leave the search result page. in this case, the positional bias cannot fully explain the lack of clicks for the second url. the cascade model assumes that users examine the results sequentially and stop as soon as a relevant document is clicked. here, the probability of examination is indirectly determined by two factors: the rank of the url and the relevance of all previous urls. the cascade model makes a strong assumption that there is only one click per search and hence it could not explain the abandoned search or search with more than one clicks. even though the cascade model is quite restrictive, the authors of that paper showed that we refer to url as a shorthand for the entire display block consisting of the title, abstract and url of the corresponding result. it can predict click through rates more accurately than the position models described above. none of the above models distinguish perceived relevance and actual relevance. because users cannot examine the content of a document until they click on the url, the decision to click is made based on perceived relevance. while there is a strong correlation between perceived relevance and actual relevance, there are also many cases where they di er. in this paper, a dynamic bayesian network model is proposed to model the users browsing behavior. as in the position model, we assume that a click occurs if and only if the user has examined the url and deemed it relevant. similar to the cascade model, our model assumes that users make a linear transversal through the results and decide whether to click based on the perceived relevance of the document. the user chooses to examine the next url if he she is unsatis ed with the clicked url. our model di ers from the cascade model in two aspects: because a click does not necessarily mean that the user is satis ed with the clicked document, we attempt to distinguish the perceived relevance and actual relevance. even though human movement and mobility patterns have a high degree of freedom and variation, they also exhibit structural patterns due to geographic and social constraints. using cell phone location data, as well as data from two online location based social networks, we aim to understand what basic laws govern human motion and dynamics. we find that humans experience a combination of periodic movement that is geographically limited and seemingly random jumps correlated with their social networks. short ranged travel is periodic both spatially and temporally and not effected by the social network structure, while long distance travel is more influenced by social network ties. we show that social relationships can explain about to of all human movement, while periodic behavior explains to. based on our findings, we develop a model of human mobility that combines periodic short range movements with travel due to the social network structure. we show that our model reliably predicts the locations and dynamics of future human movement and gives an order of magnitude better performance than present models of human mobility. while we would like to believe that our movement and mobility patterns have a high degree of freedom and variation, at a global scale human mobility exhibits structural patterns subject to geographic and social constraints. studies have also explored how social networks are embed into the underlying geography, and how geo data can be used to infer social ties. we study the relation between human geographic movement, its temporal dynamics, and the ties of the social network. overall, social relationships can explain about of human movement in cell phone data and up to of movement in location based social networks, while periodic movement behavior explains about to. previous studies have mostly focused on at most two out of these three aspects. knowledge of users locations can help improve large scale systems, such as cloud computing, content based delivery networks, and location based recommendations. we observe that people generally move periodically within a bounded region but occasionally travel long distance. despite strong correlation between friendship and mobility, there are limits in using friendship alone to predict mobility. we capture the temporal dynamics of transitioning between these locations with a dayspeci. we observe strong robustness of the model and agreement of results between the cell phone and the location based social network check in data. one would expect that people exhibit strong periodic behavior in their movement as they move back and forth between their homes and workplaces. mobility is probably also constrained geographically by the distance one can travel within a day. moreover, mobility may further be shaped by our social relationships as we may be more likely to visit places that our friends and people similar to us visited in the past. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. even though the above are some of the most fundamental questions and hypotheses about the dynamics of human mobility, answers to them remain largely unknown mostly due to the fact that reliable large scale human mobility data has been hard to obtain. recently, however, location based online social networking applications have emerged, where users share their current location by checking in on websites such as foursquare, facebook, gowalla, etc. while traditionally records of calls made by cell phones have been used to track the location of the cell phone towers associated with the calls, location based social networks provide an important new dimension in understanding human mobility. in particular, while cell phone data provides coarse location accuracy, location based social networks provide locationspeci. data, as one can distinguish between a check in to the of ce on the nd oor and a check in to a coffee shop on the st oor of the same building. on the other hand, check ins to location based social networks are usually sporadic while cell phone data provides better temporal resolution as a user checks in whenever she makes or receives a call. both types of data also contain network information. location based social networks maintain explicit friendship networks, while in mobile phones the network can be inferred from the communication network. this data allows for studying the three main aspects of human mobility: geographic movement, temporal dynamics and the social network. for example, research has focused on building models of human movement and dynamics, understanding the role of geographic distance and social interaction and the change of our perception of space. in contrast, we study the interaction of all three aspects of human mobility to devise a coherent model of human movement and dynamics. in the broader context, understanding and modeling human mobility has many applications and consequences. more broadly, accurate models of human mobility are essential for urban planning, understanding human migration patterns, and spread of diseases. in particular, we analyze the role of geography and daily routine on human mobility patterns as well as the effect of social ties, ie, friends that one travels to meet. we seek to identify the fundamental factors that govern human mobility and aim to answer questions, such as, how likely is a person going to a place because she has a friend there how likely is a person to make a new friend by going to a place how does this probability increase or decrease when she travels long distances from home overview of results: empirical ndings. weuse thedata from two popular online location based social networks, gowalla and brightkite, as well as a trace of million mobile phone users from a european country. people are generally more likely to visit a distant place if it is in proximity of an existing friend. thus, perhaps surprisingly, the short ranged travel is less impacted by the social network structure, while if a person travels a long distance then they are more likely to travel near an existing friend. overall, we nd that the in uence of friendship on onemobility is two times stronger than the in uence of mobility on creating new friendships. we also note strong agreement and robustness of the patterns of human mobility between the cell phone and the location based social network check in data. generally, there are both bene ts and limits to using mobility of friends to predict an individuallocation. we nd that users are most likely to check in right after a friend has checked in to the same place, and the probability drops off following a power law as the time difference increases. we show that a similarity of movement trajectories is a strong indication of a tie in the social network. for example, of the people have less than of their check ins visited by a friend prior to their own check in. building on our empirical ndings we develop a periodic social mobility model for predicting mobility of individuals. in particular, we build on the observation that people show strong periodic behavior throughout certain periods of the day alternating between primary and secondary locations on weekdays, and home and social network driven locations on weekends. our model has three components: a model of spatial locations that a user regularly visits, a model of temporal movement between these locations, and a model of movement that is in uenced by the ties of the social network. we model user locations with a mixture of gaussians centered at home and work locations. on top of this we use a model of social movement that governs user behavior over the weekends and weeknights. we develop an expectation maximization based parameter estimation method and evaluate the predictive power of the periodic social mobility model using three evaluation metrics. experiments show that our model outperforms current mobility models for more than a factor of two on all three metrics. in particular, our model predicts the exact user location at any time with accuracy, with an average relative distance error of for cell phone locations, and for social check ins. models of human mobility consider movement either as a diffusive process, or a stochastic process centered about a single xed point. our model considers human mobility as a time varying stochastic process around several xed points. this additional exibility of our model leads to a factor of two better predictive accuracy. more specialized mobility models have also been considered for wireless networks to model user transition between wireless access points. similarly, there have also been attempts to capture the periodicity of human mobility using gps position traces which are mostly governed by the physical embedding of road networks. while gps and wireless networking data allow for constantly tracing the user location, such studies have been limited to a relatively small number of users and small geographic areas. in contrast, our cell phone data covers two million users of a large country and check ins from location based social networks span the entire planet. aggregated search refers to the integration of content from specialized corpora or verticals into web search results. aggregation improves search when the user has vertical intent but may not be aware of or desire vertical search. in this paper, we address the issue of integrating search results from a news vertical into web search results. news is particularly challenging because, given a query, the appropriate decision to integrate news content or not changes with time. our system adapts to news intent in two ways. first, by inspecting the dynamics of the news collection and query volume, we can track development of and interest in topics. second, by using click feedback, we can quickly recover from system errors. we define several click based metrics which allow a system to be monitored and tuned without annotator effort. the web is comprised of data covering a variety of media, sources, and topics. the ease of classifying certain content types such as news or images has motivated the construction of specialized sub collections or verticals. despite the existence of many vertical search engines, searchers may still use a portal search engine even when the query is handled better by a vertical search engine. in these cases, the searcher may permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. figure #: integrating news content into web results. express explicit intent for vertical content; or, the searcherintent may be implicit. when a general web search engine has access to or maintains vertical search engines, one important task becomes the detection and presentation of relevant vertical results. this process is referred to as aggregated search. in this paper, we address the issue of integrating search results from a news vertical into web search results. news results are presented above the top web result in a small box we refer to as the news display or view. we present an example news display in figure #. in response to a news display, a searcher may either click on a displayed link to news content or skip the display without clicking a displayed link to news content. news is particularly challenging because, given a query, the appropriate decision to integrate news content or not changes with time. in the news index, topics emerge and decay with respect to content production. a system which only models evolving topics in the news index may waste modeling. ort on topics which searchers never request or, even worse, topics which searchers do not believe are newsworthy. a system which only models evolving query volume will not be able to separate queries requiring news displays from those which are merely popular. we present a system which integrates both massive document and query approaches to modeling events. speci cally, we will train a classi er to distinguish between newsworthy and non newsworthy queries. when training a classi er for any task, one requires a training set. for our task, such a data set would consist of queries manually classi ed as deserving a news display or not. for humans, making this decision requires knowledge about topical events being queried for as well as topical events being written about at the time when the query was issued. while not impossible, such a classi cation task for a modest number of days would be extremely expensive and potentially unreliable. to address this, we de ne several click based metrics which allow a system to be monitored and tuned without annotator. we will demonstrate that this feedback is su ciently related to manual labels so as to allow it to be used as a surrogate for training. our system adapts to the dynamics of the news integration problem. we present modules for assessing the newsworthiness of a query, recovering from system errors, and supporting query similarity. in query logs, news intent emerges and decays with respect to content demand. of growing interest in the area of improving the search experience is the collection of implicit user behavior measures as indications of user interest and user satisfaction. we developed an instrumented browser to collect a variety of measures of user activity and also to ask for explicit judgments of the relevance of individual pages visited and entire search sessions. the best models for individual pages combined clickthrough, time spent on the search result page, and how a user exited a result or ended a search session. rather than having to submit explicit user feedback, which can be costly in time and resources and alter the pattern of use within the search experience, some research has explored the collection of implicit measures as an efficient and useful alternative to collecting explicit measure of interest from users this research article describes a recent study with two main objectives. the first was to test whether there is an association between explicit ratings of user satisfaction and implicit measures of user interest. the second was to understand what implicit measures were most strongly associated with user satisfaction. the data was collected in a workplace setting to improve the generalizability of the results results were analyzed using traditional methods as well as a new usage behavior pattern analysis. we found that there was an association between implicit measures of user activity and the user explicit satisfaction ratings. behavioral patterns can also be used to predict user satisfaction for search sessions. fine grained search interactions in the desktop setting, such as mouse cursor movements and scrolling, have been shown valuable for understanding user intent, attention, and their preferences for web search results. as web search on smart phones and tablets becomes increasingly popular, previously validated desktop interaction models have to be adapted for the available touch interactions such as pinching and swiping, and for the different device form factors. in this paper, we present, to our knowledge, the first in depth study of modeling interactions on touch enabled device for improving web search ranking. in particular, we evaluate a variety of touch interactions on a smart phone as implicit relevance feedback, and compare them with the corresponding fine grained interactions on a desktop computer with mouse and keyboard as the primary input devices. our experiments are based on a dataset collected from two user studies with users in total, using a specially instrumented version of a popular mobile browser to capture the interaction data. we report a detailed analysis of the similarities and differences of fine grained search interactions between the desktop and the smart phone modalities, and identify novel patterns of touch interactions indicative of result relevance. finally, we demonstrate significant improvements to search ranking quality by mining touch interaction data. accurately estimating document relevance is at the core of web search or information retrieval in general. implicit feedback from users are good indicators of document relevance, among which, dwell time, or the time user spends on visiting the clicked web search result document, has been found to be indicative of document relevance and has been successfully applied in various other web search applications. however, dwell time may be misleading a user may spend seconds reading a relevant paragraph in a document or struggling nding the information and skimming through during the seconds. in previous studies, ne grained interactions such as mouse cursor movements and scrolling were found to be indicative of searcher interests and preferences. in particular, postclick behavior were found to capture the different viewing patterns of relevant and non relevant document, resulting in a more accurate prediction of intrinsic document relevance, compared to using dwell time information alone. for example, extensive and slow mouse movements were found to correlate with reading a relevant document while fast scrolling are found to correlate with skimming a non relevant document. recently, touch enabled mobile devices, such as smart phones and tables, have become an increasingly popular modality for users to search and browse the web. despite the success of modeling ne grained interactions on a personal computer with a mouse and a keyboard as the primary input devices, ne grained web search interactions on these new devices, where users zoom and swipe instead of using a mouse, are less understood. while these touch interactions may also provide additional signals of document relevance beyond dwell time, we are not aware of any reports of modeling these ne grained touch interactions as implicit relevance feedback. in particular, we identify behavioral patterns that correspond to viewing relevant and nonrelevant documents. using these insights, we design a variety of touch interaction features to capture these patterns, and analyze the correlations of these feature values with the document relevance. we also compare the differences and similarities of the interaction patterns across these two modalities. finally, we develop relevance prediction models based on the touch interaction features, and demonstrate signi cant improvements over using dwell time information alone. in summary, our contributions include: a characterization of patterns of examination and touch interactions that correspond to viewing a relevant or non relevant web search result document. mti, a novel model of relevance prediction that captures ne grained web search interactions on touch enabled mobile devices. a comparison of the differences and similarities of ne grained web search interactions on a touch enabled mobile device and a desktop pc. empirical evidence that mti is more effective than using dwell time information alone, for ranking web search result documents using the estimated relevance. survey the background and related work to put our contribution in context. perhaps surprisingly, despite qac being widely used, users interactions with it are poorly understood. query auto completion suggests possible queries to web search users from the moment they start entering a query. this popular feature of web search engines is thought to reduce physical and cognitive effort when formulating a query. this allows us to provide a first look at how users interact with qac. due to this strong position bias, ranking quality affects qac usage. we present the results of an in depth user study of user interactions with qac in web search. while study participants completed web search tasks, we recorded their interactions using eye tracking and client side logging. we specifically focus on the effects of qac ranking, by controlling the quality of the ranking in a within subject design. we identify a strong position bias that is consistent across ranking conditions. we also find an effect on task completion, in particular on the number of result pages visited. we show how these effects can be explained by a combination of searchers behavior patterns, namely monitoring or ignoring qac, and searching for spelling support or complete queries to express a search intent. we conclude the paper with a discussion of the important implications of our findings for qac evaluation. query auto completion is a popular feature of todayweb search engines, and domain speci. by suggesting a selection of possible completions based on the query also referred to as query completion, query sug gestion, and real time query expansion. despite the popularity of qac, users interactions with it are poorly understood. ranking quality is found to affect suggestion use. a descriptive analysis of user interactions with qac. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. information retrieval metrics, such as mean reciprocal rank, have been proposed as evaluation metrics for qac, but without further insights in user interactions with qac, it is not clear to what degree their underlying assumptions are warranted. or commercial advantage and that copies bear this notice and the full citation on the rst page. figure #: example snapshots of query formulation with qac for search tasks and, overlaid with heat maps of eye xations for all participants on each task. as an example, the visualizations in figure # show where searchers looked while formulating queries for two of the search tasks in our study. alternatively, this behavior could be explained by examination bias, ie, by users expectations to nd the best suggestions towards the top of the list. while position bias in interactions with qac was identi ed in previous log studies, our study can distinguish between these alternative explanations. the main contributions of this paper are: an eye tracking study of user interactions with qac; this includes the extension of the methodology for eye tracking studies of user interactions with search engines, analysis and a discussion of potential limitations of the developed approach. we hypothesize that a combination of these patterns can explain the observed ranking effects. copyrights for components of this work owned by others than the author must be honored. our results show that the focus on top suggestions is due to examination bias, not ranking quality. a user has typed so far, qac can help users formulate more effective queries in less time and with less effort. the resulting query formulation support is thought to reduce the effort of interacting with the search engine, leading to a more enjoyable and effective user experience. for instance, qac may help users avoid spelling mistakes, discover relevant search terms, and avoid issuing overly ambiguous queries. how many suggestions do users consider while formulating a query does position bias affect query selection, possibly resulting in suboptimal queries how does the quality of qac affect user interactions and, inversely, can observed behavior be used to infer qac quality in this paper we present the results of an eye tracking study designed to shed light on these questions. we focus on the role that qac ranking quality plays in this process. we see that in both tasks searchers examine primarily the topranked suggestions, in particular at ranks to. this behavior could be due to the high quality of qac rankings. query they had in mind, and this query is shown towards the top of the suggestion list, then examining lower ranked suggestions would be unnecessary. an experimental comparison of the effects of qac ranking quality on users search behavior. we nd a strong position bias that is consistent across ranking conditions. finally, we observe an effect on number of results visited to solve a search task, suggesting a link with query effectiveness. we identify common behavior patterns that generalize across participants, namely monitoring, ignoring, and searching. a detailed discussion of the implications of our ndings for the evaluation of qac systems, especially for the choice of evaluation metric. next, our experiment methodology is presented in section # and our analysis method is described in section #. after presenting our results in section #, we discuss their implications, and potential limitations of our study, in section #. this article examines the reliability of implicit feedback generated from clickthrough data and query reformulations in world wide web search. analyzing the users decision process using eyetracking and comparing implicit feedback against manual relevance judgments, we conclude that clicks are informative but biased. while this makes the interpretation of clicks as absolute relevance judgments difficult, we show that relative preferences derived from clicks are reasonably accurate on average. we find that such relative preferences are accurate not only between results from an individual query, but across multiple sets of results within chains of query reformulations. the idea of adapting a retrieval system to particular groups of users and particular collections of documents promises further improvements in retrieval quality for at least two reasons. second, as evident from the trec evaluations, differences between document collections make it necessary to tune retrieval functions with respect to the collection for optimum retrieval performance. since manually adapting a retrieval function is time consuming or even impractical, research on automatic adaptation using machine learning is receiving much attention. however, a great bottleneck in the application of machine learning techniques is the availability of training data. in contrast to explicit feedback, such implicit feedback has the advantage that it can be collected at much lower cost, in much larger quantities, and without burden on the user of the retrieval system. in this article we analyze which types of implicit feedback can be reliably extracted from observed user behavior, in particular clickthrough data in world wide web search. following and extending prior work reported in radlinski and joachims, joachims et al, and granka et al, we analyze implicit feedback from within individual queries as well as across multiple consecutive queries about the same information need. the feedback strategies across query chains exploit that users typically reformulate their query multiple times before their information need is satis ed. we elaborate on the query chain strategies proposed in radlinski and joachims, as well as propose and explore additional strategies. to evaluate the reliability of these implicit feedback signals, we conducted a user study. the study was designed to analyze how users interact with the list of ranked results from the google search engine and how their behavior can be interpreted as relevance judgments. first, we used eye tracking to understand how users behave on googleresults page. do users scan the results from top to bottom how many abstracts do they read before clicking how does their behavior change, if we arti cially manipulate googleranking answers acm transactions on information systems, vol. evaluating accuracy of implicit feedback in web search to these questions give insight into the users decision process and suggest in how far clicks are the result of an informed decision. based on these results, we propose several strategies for generating feedback from clicks and query reformulations. to evaluate the degree to which feedback signals indicate relevance, we compared the implicit feedback against explicit feedback we collected manually. the study presented in this article is different in at least two respects from previous work assessing the reliability of implicit feedback. second, we evaluate relative preference signals derived from user behavior. this is in contrast to previous studies that primarily evaluated absolute feedback. our results show that users make informed decisions among the abstracts they observe and that clicks re ect relevance judgments. however, we show that clicking decisions are biased in at least two ways. first, we show that there is a trust bias which leads to more clicks on links ranked highly by google, even if those abstracts are less relevant than other abstracts the user viewed. second, there is a quality of context bias: the users clicking decision is not only in uenced by the relevance of the clicked link, but also by the overall quality of the other abstracts in the ranking. this shows that clicks have to be interpreted relative to the order of presentation and relative to the other abstracts. we propose several strategies for extracting such relative relevance judgments from clicks and show that they accurately agree with explicit relevance judgments collected manually. first, our study provides detailed insight into the users decision making process through the use of eyetracking. first, a one size ts all retrieval function is necessarily a compromise in environments with heterogeneous users and is therefore likely to act suboptimally for many users. in this article we explore and evaluate strategies for how to automatically generate training examples for learning retrieval functions from observed user behavior. however, implicit feedback is more dif cult to interpret and potentially noisy. we performed two types of analysis in this study. web search has seen two big changes recently: rapid growth in mobile search traffic, and an increasing trend towards providing answer like results for relatively simple information needs. such results display the answer or relevant information on the search page itself without requiring a user to click. while clicks on organic search results have been used extensively to infer result relevance and search satisfaction, clicks on answer like results are often rare, making it challenging to evaluate answer quality. together, these call for better measurement and understanding of search satisfaction on mobile devices. in this paper, we studied whether tracking the browser viewport on mobile phones could enable accurate measurement of user attention at scale, and provide good measurement of search satisfaction in the absence of clicks. focusing on answer like results in web search, we designed a lab study to systematically vary answer presence and relevance, obtained satisfaction ratings from users, and simultaneously recorded eye gaze and viewport data as users performed search tasks. using this ground truth, we identified increased scrolling past answer and increased time below answer as clear, measurable signals of user dissatisfaction with answers. while the viewport may contain three to four results at any given time, we found strong correlations between gaze duration and viewport duration on a per result basis, and that the average user attention is focused on the top half of the phone screen, suggesting that we may be able to scalably and reliably identify which specific result the user is looking at, from viewport data alone. recent years have witnessed a rapid explosion in the usage of mobile devices on the web. according to recent surveys, web browsing on mobile devices increased ve fold from three years ago to in april year#; and a signi cant amount of permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. drw google com vidhyan google com figure #: an example of the search results page showing knowledge graph result. the yellow area indicates current position of the browserviewport. another recent change in search is the increasing trend towards providing answer like results for simple information needs that are popular on mobile. such results display the answer or relevant information on the search page itself without requiring the user to click. instant information is desirable on mobile devices, but poses a challenge while clicks on organic search results have been extensively used to infer result relevance and search satisfaction, answer like results often do not receive clicks, which makes it dif cult to evaluate answer quality and search satisfaction. and answer like results in search warrants better understanding of user attention and satisfaction in search on mobile devices. search behavior on mobile devices can be different than on desktop for several reasons. unlike traditional desktop computers with large displays and mouse keyboard interactions, touch enabled mobile devices have small displays and offer a variety of touch interactions, including touching, swiping and zooming. as a result, user experience and search behavior on mobile devices is different for example, due to the lack of a physical keyboard, users tend to issue shorter queries than on the desktops. compared to large desktop displays, the displays on mobile phones are small, and limit the amount of information that the user can view simultaneously. we introduce viewport as the portion of the web page that is visible on the phone screen at a given point in time. viewport coordinates are recorded in the web page coordinate system, since the small displays on mobile phones limit the number of visible search results to, viewport tracking could be used to better measure users attention on a web page, as was recently recognized by some researchers. to the best of our knowledge, there is no quantitative evaluation or validation of viewport data in how well it can approximate user attention on mobile devices, or be used to detect search satisfaction. in this paper we test the utility of viewport signals. to approximate attention from viewport tracking, we measure the result view time the duration for which a search result appeared within the viewport. in desktop settings, the amount of time user spent gazing on a particular result was shown to be useful for inferring result relevance, predicting future clicks, improving ranking, estimating snippet attractiveness and whole page quality. while cursor hovers do not exist on mo bile devices, these ndings suggest that measurement of viewing time of results on mobile could lead to several useful applications in relevance estimation and whole page optimization. in this paper we demonstrate how viewport metrics can be used to measure user attention, and detect search satisfaction. result seen by the user from viewport data alone. we begin by surveying related work in eye tracking for search on desktops and user behavior for search on mobile devices. we then describe our experiment and user study, followed by the analysis of searcherattention and satisfaction on mobile phones. we conclude with a discussion reviewing the ndings and limitations of this study, along with suggestions for future work. query abandonment by search engine users is generally considered to be a negative signal. in this paper, we explore the concept of good abandonment. we define a good abandonment as an abandoned query for which the user information need was successfully addressed by the search results page, with no need to click on a result or refine the query. we present an analysis of abandoned internet search queries across two modalities in three locales. the goal is to approximate the prevalence of good abandonment, and to identify types of information needs that may lead to good abandonment, across different locales and modalities. our study has three key findings: first, queries potentially indicating good abandonment make up a significant portion of all abandoned queries. second, the good abandonment rate from mobile search is significantly higher than that from pc search, across all locales tested. third, classified by type of information need, the major classes of good abandonment vary dramatically by both locale and modality. our findings imply that it is a mistake to uniformly consider query abandonment as a negative signal. further, there is a potential opportunity for search engines to drive additional good abandonment, especially for mobile search users, by improving search features and result snippets. the information retrieval community has a long tradition of using user clicks on search results as a positive signal. clicks have been used successfully to learn ranking functions and to evaluate comparative algorithms in a or interleaved experiments. it has been considered to be an indicator of user dissatisfaction if users choose not to click on any results, or worse, abandon their query by neither clicking a result nor issuing a query re nement. internet search engines have added features over the past several years that attempt to answer users information needs directly on the search results page, without requiring a click on any of the results. leading engines now provide a large array of these features for basic information needs such as weather, stock quotes, local business addresses and phone numbers, images, current news headlines, ight information, package delivery tracking, and many others. in addition, the result snippets returned by search engines have improved over time and may often answer information needs directly. in this paper, we explore the concept of good abandonment. we de ne a good abandonment as an abandoned query for which the userinformation need was successfully addressed by the search results page, with no need to click on a result or re ne the query. we present an analysis of abandoned queries sampled from googlesearch logs. speci cally, we analyze abandoned queries from three countries across two modalities for a total of six query streams. we are particularly interested in mobile search and how it compares to pc search with respect to abandonment. we anticipate that there may be di erences for several reasons. first, on mobile devices even current top tier devices such as the apple iphone opening web pages is often slow and clunky, with formatting issues, usability issues, and content omissions. therefore we postulate that users might want to avoid opening pages, and instead formulate queries in a way that may return answers directly within search results. second, anecdotally we hear from users about a quick answer in a bar type of use case for mobile search. here, users are out with friends, and use mobile search to answer questions that come up in conversation whatthe weather going to be like tomorrow, what time does the movie start tonight, what year was this celebrity born, etc. this use case, if real, would potentially drive good abandonment on mobile search. third, mobile devices are inherently local, and to the extent that internet search engines provide local business addresses and phone numbers, we might anticipate a high rate of good abandonment for queries seeking these types of information. first, we nd that queries potentially indicating good abandonment make up a signi cant portion of all abandoned queries, ranging from to across the set of locales and modalities we analyzed. second, we nd that the good abandonment rate from mobile search is signi cantly higher than that from pc search, again across all three locales. this appears to be a meaningful, robust di erence between how users interact with mobile search versus search on a pc. third, by hand classifying abandoned queries by the type of information need they represent, we identify the major classes of good abandonment, di erences across locales and modalities, and perhaps most importantly, largest opportunities for internet search engines to drive additional good abandonment for their users. section # describes the methodology we used to sample and classify abandoned queries, and de nes the categories and codings used. section # discusses the implications of our ndings for internet search engines, and concludes with some pointers to future research. what is the potential opportunity for personalization in this paper, we propose a new way to personalize search, personalization with backoff. ideally, classes would be defined by market segments, demographics and surrogate variables such as time and geography. how many pages are there on the web more less big bets on clusters in the clouds could be wiped out if a small cache of a few million urls could capture much of the value. language modeling techniques are applied to msn search logs to estimate entropy. the perplexity is surprisingly small: millions, not billions. entropy is a powerful tool for sizing challenges and opportunities. if we have relevant data for a particular user, we should use it. but if we dont, back off to larger and larger classes of similar users. as a proof of concept, we use the first few bytes of the ip address to define classes. the coefficients of each backoff class are estimated with an em algorithm. how hard is search how hard are query suggestion mechanisms like auto complete how much does personalization help all these difficult questions can be answered by estimation of entropy from search logs. could this work was done when the rst author was on a summer internship at microsoft research. how big is english one can nd simple answers on the covers of many dictionaries, but we would feel more comfortable with answersfroma moreauthoritativesourcethana marketingdepartment. many academics have contributed to this discussionfrom manyperspectives: education, psychology, statistics, linguistics, and engineering. chomsky and shannon proposed two di erent ways to think about such questions: chomsky: languageisin nite shannon: bitsper character these two answers are very di erent. chomskyanswer isaboutthetotal numberof words; andshannon sansweris abouttheperplexity, orthedi culty of using alanguage. using achomskian argument, wecould argue that there are in nitely many urls. for example, one could write a spider trap such as successor aspx which links to successor aspx which links to successor aspx. although there are a lot of pages out there, there are not that many pages thatpeople actuallygoto. but the logs are tiny, far less thancarlsagan sbillions andbillions. how hard is search how much does personalization help http: www timeanddate com calendar monthly html year year# month country personalization personalization is a hot topic, with a large body of work, not only in the scienti. many people use personalized search products every day. literature such as, you ll have to re ne the query considerably by adding a keyword like sigir. why does personalization help it is useful to know your audience. depending on the user, this query could be looking for the sports arena or thefood additive. the search engine could do a better jobanswering ambiguous queries like this if it had access to demographic data and or log data such as click logs. acs can refer to the american chemical society, the american cancer society, the americancollegeofsurgeons and more. acronyms take on special meanings inside many large organizations andprivateenterprises. and of course, it means other things to other people including: montessori school ofraleigh, momservicerepresentative and my sports radio. pss is a stock ticker for payless shoes, as well an abbreviation of several di erent companies: physicians sales and service, phoenix simulation software, personal search syndication, professional sound system, etc. but inside microsoft, pss refers to product support services. it helps to know your audience in order to know: what the terminology means which questions are likely to come up, and which answers are likely to be appreciated. personalization with backoff but whatif wedo nothavedatafor aparticular user this paper takes a backo. to larger and larger groups of similar users. it would be even better to group customers by market segments and or collaborative ltering. an advertiser such asford, for example, has a wide range ofproducts. for example, the rm may wish to target small trucks to a rural audience and hybrids to a green audience. companies would like to know if they are talking to college students, teenagers, parents with young children, etc. it is useful to know the class of your audience. to higher bytes of ip addresses is better than personalization or no personalization. too little personalization misses the opportunity and too much runs into sparse data. market segments are typically de ned interms of surrogate variablessuch asgeography, and time of day and day of week. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. the crawler can easily consume all available time and space. the bigger the web, the harder the search. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. these large investments be wiped out if a small cache of a few million pages could capture much of the value what if someone found a way to squeeze much of the value of the cluster into a desktop or a mobile device is search more like aneverest expedition or a walk inthepark related questions come up in language. a dictionary could coveralot of words, but not all of themare actively used. inadditiontointentionally maliciousspidertraps, thereare perfectly benign examples such as calendars, where there are in nitely many pages, one for each month, with links fromeachmonth tothenext. itisalltooeasy tobuild aweb crawlerthat ndsitself attempting to materializeanin nite set with nite resources. with all the talk about the long tail, one would think that the web was astronomical. as we will see, entropy is a powerful tool for sizing challenges and opportunities. the rst few pages of results are dominated by the commercial practice. if we have the relevant data for a particular user, we should use it. as a proof of concept, users are grouped into equivalence classes based on the most signi cant bytes of their ip address. personalization is then conducted by combining estimates based on all four bytes of the ip address, the rst three bytes, the rst two, and so on. customers are assigned to equivalence classes based on pro le features such as age, income, occupation, etc. it is useful for an advertiser to know who it is talking to so that it can target the message appropriately to the audience. some products are more attractive to some customers, and other products are more attractive to other customers. we nd that a little bit of personalization is better than too much or too little. instead of assigning each customertohis own class, it is common to assign customers to market segments. these surrogate variables are easy to work with, and hopefully, they are well correlated with the more sensitive demographic variables such as those mentioned above. how many pages are there on the web more less how hard is search how much does personalization help all are di cult but crucial questions to search business. search engines make large investments in expensive computer centers in the cloud to index billions of pages. thispaper will estimate entropy of urls based on logs from microsoft swww live com. we nd thatittakesjust bits to guess the next url. a query for personalized search returns millions of page hits. forexample, for mostpeople, msr means mountain safety research, but inside microsoft, it means microsoft research. if we do not have data for a particular user, back. we use correlation of clicks as our offline metric and show that click preference target has a better correlation than human judgments based models. modern web search engines are federated a user query is sent to the numerous specialized search engines called verticals like web, news, image, video, etc. and the results returned by these engines are then aggregated and composed into a search result page and presented to the user. for a specific query, multiple verticals could be relevant, which makes the placement of these vertical results within blocks of textual web results challenging: how do we represent, assess, and compare the relevance of these heterogeneous entities in this paper we present a machine learning framework for serp composition in the presence of multiple relevant verticals. first, instead of using the traditional label generation method of human judgment guidelines and trained judges, we use a randomized online auditioning system that allows us to evaluate triples of the form query, web block, vertical. we use a pairwise click preference to evaluate whether the web block or the vertical block had a better users engagement. next, we use a hinged feature vector that contains features from the web block to create a common reference frame and augment it with features representing the specific vertical judged by the user. a gradient boosted decision tree is then learned from the training data. for the final composition of the serp, we place a vertical result at a slot if the score is higher than a computed threshold. the thresholds are algorithmically determined to guarantee specific coverage for verticals at each slot. furthermore, on online tests for news and image verticals we show higher user engagement for both head and tail queries. in a sense, each vertical provides few data blocks. these blocks compete on their location on the serp. the core of web search is displaying links to relevant web pages for a given query. however, modern web search engines, have access to several alternative data sources and it is now the expectation of users to have a blend of multiple types of information in the search result page. for example, many users who type the query cool cars are interested in seeing images of cars, whereas users who type the query weather are interested in the weather forecast at their current location. therefore, web search engines are federated search engines in the sense that they have multiple data sources and they need to rank the relevancy of different types of data items and display them accordingly. in many cases these different data sources are referred to as verticals and the challenge could be presented as identifying and ranking the relevancy of different verticals given a query. the main challenge we address is the problem of page composition. given the different types of information, the goal is to layout the serp with the most relevant information. since users are more likely to see and interact with items presented at the top of the page, it is essential to rank the different information blocks and place them on the page accordingly. when viewed as a ranking problem, there is a big difference between the core ranking problem and the federated search type of ranking problem. in the core ranking problem one has to compare object of the same nature, ie, web pages, and rank them according to their relevancy. however, in the federated search problem, one has to compare objects of different nature, eg, web pages to images, and value their relevancy. the different object cannot be represented using similar features to allow them to be ranked using the same machinery used to rank web documents. for example, web pages have properties such as number of outgoing links, bm and so on. however, these features do not exist in images. at the same time, images have properties such as size, color palette and other features which do not have natural equivalent in the world of web documents. therefore, the main challenge we are interested at is how to merge, or integrate, heterogeneous results. another problem in federated search comes from the process of collecting judgments to be used for choosing, or training, a ranking model. the common method to collect such labels is to hire human judges to score each query result pair. recently, a lot of attention was given to obtaining labels by analyzing users click logs. the problem is that when the objects are of different type, it is hard to calibrate the scales of the scores to generate a uni ed scale. for example, can you compare an excellent picture of dogs to an excellent web document about dogs to overcome these challenges, we propose a new point of view on the federated search problem in web search. instead of trying to associate an absolute score to each vertical, we measure the relevancy in comparison to the core web results. since the core web results are the pivot around which the rest of the serp results are presented, we use it as an anchor. the question we are trying to solve now becomes: given a set of links to web pages and a set of say, images, should the images be presented above the web results, bellow the web results or not at all. when collecting judgments, we present the two blocks, web results and vertical results, and ask judges to quantify the relative quality of the two blocks. in the same way, when ranking, we always have a web block and an vertical block coming from an alternative source. we have a uni ed representation for all query web block vertical triples which allows us to use machine learning technique to layout the serp. therefore, this novel point of view solves the two main challenges discussed above. in this paper we present a detailed explanation of our solution as well as the results of experimenting with it in one of the leading web search services. we show that this solution signi cantly improves over previous propositions. we also show how explicit feedback, from human judges, and implicit feedback, from clicks, can be provided when learning these models to further improve the results. this paper presents a novel approach for using clickthrough data to learn ranked retrieval functions for web search results. we observe that users searching the web often perform a sequence, or chain, of queries with a similar information need. using query chains, we generate new types of preference judgments from search engine logs, thus taking advantage of user intelligence in reformulating queries. to validate our method we perform a controlled user study comparing generated preference judgments to explicit relevance judgments. we also implemented a real world search engine to test our approach, using a modified ranking svm to learn an improved ranking function from preference data. our results demonstrate significant improvements in the ranking given by the search engine. the learned rankings outperform both a static ranking function, as well as one trained without considering query chains. query auto completion is one of the most prominent features of modern search engines. the list of query candidates is generated according to the prefix entered by the user in the search box and is updated on each new key stroke. query prefixes tend to be short and ambiguous, and existing models mostly rely on the past popularity of matching candidates for ranking. however, the popularity of certain queries may vary drastically across different demographics and users. for instance, while instagram and imdb have comparable popularities overall and are both legitimate candidates to show for prefix, the former is noticeably more popular among young female users, and the latter is more likely to be issued by men. in this paper, we present a supervised framework for personalizing auto completion ranking. we introduce a novel labelling strategy for generating offline training labels that can be used for learning personalized rankers. we compare the effectiveness of several user specific and demographic based features and show that among them, the user long term search history and location are the most effective for personalizing auto completion rankers. we perform our experiments on the publicly available aol query logs, and also on the larger scale logs of bing. the results suggest that supervised rankers enhanced by personalization features can significantly outperform the existing popularity based base lines, in terms of mean reciprocal rank by up to. the two exceptions are the work by bar yossef and kraus, and weber and castillo. the query frequencies of instagram and imdb according to google trends. all four suggestions are popular queries with comparable historical frequencies. auto completion is among the rst services that the users interact with as they search and form their queries. following each new character entered in the query box, search engines lter suggestions that match the updated pre, and suggest the top ranked candidates to the user. the rst step is often facilitated by using data structures such as pre. in the second step, those suggestions that match the pre. the likelihood values are often approximated with respect to aggregated past frequencies although other approaches that rank suggestions according to their predicted future popularities have been also explored. in majority of previous work, the likelihood of qac suggestions are computed globally and are considered to be the same for all users. hence for a given pre, all users are presented with the same set of suggestions. bar yossef and kraus added a session bias parameter in auto completion ranking by comparing candidates with the queries recently submitted by the user. however, the notion of likelihood for a query does not vary across users, or demographic groups, plus their work is not applicable on single query sessions that account for no less than of the search tra. weber and castillo discussed the di erences in query distributions across various demographics and brie. covered query completion by focusing on predicting the second query term. in essence, they build a conditional probabilistic model for common phrases based on a set of demographic features. their model is based on simple aggregation over di erent demographics and does not address the sparsity issues as more features are added. weber and castillo do not consider any user speci. feature, and do not report the results for more general scenarios where only the rst few characters of queries are entered. in this paper, we propose a novel supervised framework for learning to personalize auto completion rankings. we are motivated by the previous studies that demonstrated that the query likelihoods vary drastically between di erent demographic groups and individual users. inspired by these observations we develop several features based on users age, gender, location, short and long history for personalizing auto completion figure #: the default auto completion candidates for pre. according to the us market version of google com. was typed in private browsing mode and the snapshot was taken on wed, jan, year#. for instance, consider the auto completion candidates returned by google for pre. in particular, the frequency distributions for instagram, and imdb are demonstrated in figure # according to google trends the depicted trends suggest similar likelihoods for both imdb and instagram, although the popularity of the latter is rising. in general, in the absence of any information about the user, the ranking of qac candidates in figure # look reasonable although models based on temporal query frequency trends may boost the position of instagram. the question we are addressing in this work, is how this ranking can be further improved if there are some additional information available about the user. figure # compares the likelihood of instagram and imdb among di erent demographics of users according to yahoo clues at the bottom of figure # the same analysis is repeated using the query logs of bing search engine which we use as one of the testbeds in our experiments. the overall trends are remarkably similar; instagram is mostly popular among young female users below the age of. in contrast, the query imdb is issued more often by male users particularly those between the age of to. hence, going back to figure #, if we knew that the person issuing the query was an under female user, perhaps the original order of qac candidates could be improved by boosting instagram. then again, if the previous query submitted by the user in the session was about ipad covers boosting ipad could be possibly better. we investigate how such additional information can be used in a supervised framework for personalizing autocompletion. to train personalized auto completion rankers, we introduce a new strategy for generating training labels from previous queries in the logs. our experiments on two large scale query logs suggest that integrating demographic and personalized features can signi cantly improve the effectiveness of auto completion. the remainder of this paper is organized as follows; we continue by covering the related work in section #. was submitted to the us market version of google com on january, year#, in private browsing mode. http: www google com trends http: clues yahoo com, discontinued in march year# figure #: the likelihood of instagram and imdb in queries submitted by di erent demographics according to yahoo clues. the likelihood of instagram and imdb in queries submitted by the logged in users of bing. framework for personalized auto completion is described in section #. section # discusses our testbed data, and the features used for personalization. the evaluation results are presented in section #. finally, we conclude in section # and suggest a few directions for future work. depending on previous user interaction with the repeated results, and the details of the session, we show that sometimes the repeated results should be promoted, while some other times they should be demoted. analysing search logs from two different commercial search engines, we find that results are repeated in about of multi query search sessions, and that users engage differently with repeats than with results shown for the first time. we demonstrate how statistics about result repetition within search sessions can be incorporated into ranking for personalizing search results. our results on query logs of two large scale commercial search engines suggest that we successfully promote documents that are more likely to be clicked by the user in the future while maintaining performance over standard measures of non personalized relevance. web search engines frequently show the same documents repeatedly for different queries within the same search session, in essence forgetting when the same documents were already shown to users. when interacting with web search engines, people frequently encounter the same results for di erent queries, both within a single session and across multiple sessions. there are times when this repetition may be intentional, permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. white microsoft research ryenw microsoft com filip radlinski microsoft liprad microsoft com as is the case with re nding, where users purposely seek the same web document. however, as we will show, repeated results are shown to users much more often than can be explained by re nding alone. rather, results are often shown multiple times as users try to nd the solutions to satisfy information needs and most web search engines exhibit a form of amnesia, without taking longer interactions with users into account. previous research has largely treated sequences of queries independently, or as weakly interacting by building models of user tasks or interests, rather than as a single continuing conversation between the user and search engine. making this problem even more interesting, repetition of results can intuitively be interpreted in a few distinct ways. consider a document shown twice to a user for two related queries. the rst time it was returned, the document must have been not noticed, noticed and considered but not clicked, or clicked. if we can detect the rst case, we may want to promote the document to help the user notice it as recurring and potentially relevant. if we can detect the second case, we may want to demote it. the third case is most interesting, as the user may be attempting to re nd the document, or the user may have decided that it is nonrelevant. our analysis shows that repetition is both frequent, and has a substantial. about of multi query sessions include at least one result being shown in the top results more than once. but unnoticed results are about more likely to be clicked when shown again later. conversely, results previously clicked once are to less likely to be clicked when shown again while results previously clicked three times are up to more likely to be clicked when shown again. we conjecture that a method that can leverage the skip and click behavior of users to automatically promote and demote results could lead to better ranking. we therefore proposecube, a context aware ranker enhanced by features generated from a userinteraction with repeatedlydisplayed search results. ine using log data from two large commercial search engines, and online via atesting with users of a commercial search engine. after presenting related work in section #, and based on an analysis of logs in section #, we build models that leverage features of these repeated results and other features such as query similarity to predict click preferences between result pairs. section # presents our re ranking approach, and describes our evaluation data and metrics. we follow that by discussing our experimental results in section #, and providing concluding remarks in section #. this leads us to the two key challenges we address in this paper: first, how do we know when a later query is similar enough to an earlier query to warrant inclusion considering previous displays of repeated documents second, how do we di erentiate between re nding and non relevance before describing our approach, it is important to verify that web results are often shown repeatedly, and that behavior on them is di erent. when a result is skipped once, depending on position, it is to less likely to be clicked later compared to the expected clickthrough rate at the same position. the results show signi cant gains fromcube over competitive baselines. this paper presents an algorithm that predicts with very high accuracy which web search result a user will click for one sixth of all web queries. prediction is done via a straightforward form of personalization that takes advantage of the fact that people often use search engines to re find previously viewed resources. in our approach, an individual past navigational behavior is identified via query log analysis and used to forecast identical future navigational behavior by the same individual. we compare the potential value of personal navigation with general navigation identified using aggregate user behavior. although consistent navigational behavior across users can be useful for identifying a subset of navigational queries, different people often use the same queries to navigate to different resources. this is true even for queries comprised of unambiguous company names or urls and typically thought of as navigational. we build an understanding of what personal navigation looks like, and identify ways to improve its coverage and accuracy by taking advantage of people consistency over time and across groups of individuals. one common way that web search engines are used is to navigate to particular information resources. for example, a person looking to buy a book on web search and data mining may, instead of searching directly for a book on the topic, issue the query amazon in order to navigate to the amazon com website where a relevant book can then be identified and purchased. over of all queries are navigational in nature, according to an in situ survey of people actively searching the web. if search engines are able to identify that a query is navigational, and to identify the queryintended navigational target, they can use that information provide significant benefit to their users. at a most basic level, they can display the target in a prominent manner that is easy for users to find and select. additionally, this can be done quickly via better caching for navigational queries, and the interface can be designed to help support the desired permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. intent by providing, for example, links directly into the sitecontent or access to appropriate meta data or site functionality. search engines may also be able to provide their users with more appropriate advertisements. several approaches have been explored to identify navigational queries, including analysis of the query string and behavioral data. but while some queries are used to navigate to a particular resource by all who issue them, there are many more queries with navigational intent where the intent or intended resource is not obvious, even when it seems like it should be from the query string. for example, the reader of this paper may use a search engine to navigate to the wsdm year# homepage via the query wsdm, while a person interested in country music in the midwest may use the same query to navigate to the wsdm fm radio station homepage. others may not use the query wsdm for navigation at all, but rather issue it with an informational intent to learn more about web services distributed management. to truly understand whether a particular instance of query is navigational requires understanding the individual userintent when they issue it. we find it is possible to easily and accurately identify a significant portion of queries with navigational intent and the associated target by using an individualpast search behavior via an approach that we call personal navigation. we identify personal navigational behavior once a user has used a query to navigate to a particular result twice before. for example, someone who has searched for wsdm several times and clicked on http: wsdm org every time they did can be expected to click on the same result the next time they issue the query. personal navigation presents a real opportunity for search engines to take a first step into safe, low risk web search personalization. most personalization approaches rely on explicit or inferred user profiles to guess what new content might be of interest to a user for a given query. here we look at how to capture the lowhanging fruit of personalizing results for repeat queries. our ability to reliably identify navigational intent for queries that appear informational suggests navigational behavior may be more common than previously believed. what is more, there is the potential to significantly benefit users with the identification of these queries, as the identified targets are more likely to be ranked low in the result list than typical clicked search results. after a brief description of the query logs used for our analysis, we explore general navigational behavior where everyone is assumed to use the same query to navigate to the same result. we expose several flaws in this approach, and introduce personal navigation as an alternative. we present a straightforward algorithm for identifying personal navigation behavior, and show that many queries can be easily and accurately identified in this way. we explore how our ability to predict personal navigation is impacted by the consistency of the behavior over time and across individuals, and conclude with a discussion of how repeat behavior can be used to improve the search experience. many techniques for improving search result quality have been proposed. typically, these techniques increase average effectiveness by devising advanced ranking features and or by developing sophisticated learning to rank algorithms. however, while these approaches typically improve average performance of search results relative to simple baselines, they often ignore the important issue of robustness. that is, although achieving an average gain overall, the new models often hurt performance on many queries. this limits their application in real world retrieval scenarios. given that robustness is an important measure that can negatively impact user satisfaction, we present a unified framework for jointly optimizing effectiveness and robustness. we propose an objective that captures the tradeoff between these two competing measures and demonstrate how we can jointly optimize for these two measures in a principled learning framework. experiments indicate that ranking models learned this way significantly decreased the worst ranking failures while maintaining strong average effectiveness on par with current state of the art models. most commonly, they apply well developed machine learning algorithms to construct ranking models from training data by optimizing a given ir metric. ective, these approaches have mostly ignored the important issue of robustness in addition to performing well on average, the model should, with permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. high probability, not perform very poorly for any individual query. ectiveness and robustness are competing forces that counteract each other: models optimized for effectiveness alone may not meet the strict robustness requirement for individual queries. this paper introduces learning robust ranking models for search via risk sensitive optimization, by exploiting and optimizing the tradeo. at a basic level, this framework learns ranking models whose. ectiveness and robustness can be thought of in terms of absolute performance, one can specialize these concepts to a case where we desire to perform well relative to a particular baseline for example a personalized system vs. in this case, we can specialize the notions of effectiveness and robustness to say that we wish to have high average gain relative to the baseline while at the say time incurring low risk relative to the baseline. we develop a uni ed learning to rank framework for jointly optimizing both gain and risk by introducing a novel tradeo. metric that decomposes gain into reward and risk relative to a baseline. while this objective could be integrated into many learning models, we show how to extend and generalize a state of the art algorithm lambdamart to optimize the new objective. we empirically demonstrate that models learned this way outperform both a selective personalization approach and standard learning to rank models in terms of achieving an optimal balance between gain and risk. moreover, our results also provide important insights into why our learned models are more robust in terms of the learning convergence property of the new model as compared to standard. ective ranking models for search have been proposed in recent years. while the learned ranking models can be highly. web searchers frequently transition from desktop computers and laptops to mobile devices, and vice versa. little is known about the nature of cross device search tasks, yet they represent an important opportunity for search engines to help their users, especially those on the target device. for example, the search engine could save the current session and re instate it post switch, or it could capitalize on down time between devices to proactively re trieve content on behalf of the searcher. in this paper, we present a log based study to define and characterize cross device search be havior and predict the resumption of cross device tasks. using data from a large commercial search engine, we show that there are dis cernible and noteworthy patterns of search behavior associated with device transitions. we also develop learned models for predicting task resumption on the target device using behavioral, topical, geo spatial, and temporal features. our findings show that our models can attain strong prediction accuracy and have direct implications for the development of tools to help people search more effectively in a multi device world. modern search engines have started shifting their goal from simply achieving better result ranking for individual queries to assisting users in completing such tasks. figure # presents an example of cross device behavior for a fictitious user searching for information permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. according to our analysis of device switching, around of switches involve contiguous tasks. one solution is sharing all search history across all devices. cross device behavior has been studied in the human factors community, but not with an emphasis on search. section # describes related work on desktop and mobile search, multi device usage, and cross session search tasks. the findings of the prediction experiments are presented in section #. search tasks can involve multiple queries across multiple sessions, and require significant effort to complete, especially if the task is complex. the recent proliferation of mobile devices such as smartphones and tablets allows web searchers to tackle these search tasks almost anytime, anywhere. on italian restaurants on his desktop before continuing the task on his mobile device. better support for task continuation could help the user resume his restaurant search when mobile. such contiguous cross device tasks those resumed soon on the post switch device are our focus in this paper. switching between devices may be expensive for contiguous tasks. the user has to remember what he was searching for and what has already been searched, which can be difficult when multiple search tasks are active simultaneously. however, this is insufficient since the user might not necessarily resume a search task with a previously searched query and managing onesearch history can be challenging from a smartphone. to provide a smooth transition among search devices and aid users in completing tasks on multiple devices, more sophisticated support is needed. cross session search tasks have been studied, but not across devices. mobile and desktop search have been studied separately, but the transitions between them have not been examined. a detailed study of cross device searching and the development of tools to support this activity are therefore timely and necessary. this paper makes a significant contribution as the first study in this important area. the specific research contributions of our work are fourfold: define cross device search tasks as a key research challenge and an opportunity for search engines to help people better perform search tasks that span devices. we demonstrate via empirical study the prevalence of cross device searches. characterize cross device task transitions, including identifying patterns in device transitions and exploring the temporal, geospatial, and topical aspects of cross device searching. develop predictive models to estimate which search tasks will be resumed following device switching. prediction occurs at different time points, including before a full device switch is observed and after device switching. the model integrates a rich set of features of cross device searching behavior. perform experiments using the search log from a large commercial search engine and show that our model significantly outperforms a task continuation baseline, based on prior work that lacks access to information on cross device behavior. the remainder of the paper is structured as follows. we characterize cross device searching in section # and present the details of our prediction models in section #. we discuss the findings and their implications in section # and conclude in section #. understanding the extent to which people search behaviors differ in terms of the interaction flow and information targeted is important in designing interfaces to help world wide web users search more effectively. in this paper we describe a longitudinal log based study that investigated variability in people. interaction behavior when engaged in search related activities on the web allwe analyze the search interactions of more than two thousand volunteer users over a five month period, with the aim of characterizing differences in their interaction styles allthe findings of our study suggest that there are dramatic differences in variability in key aspects of the interaction within and between users, and within and between the search queries they submit allour findings also suggest two classes of extreme user. whose search interaction is highly consistent or highly variable. lessons learned from these users can inform the design of tools to support effective web search interactions for everyone. search has emerged as a key enabling technology to facilitate access to information for the general user population of the world wide web. everyday, millions of users submit millions of queries to commercial search engines such as google, yahoo, and windows live search. drucker microsoft live labs one microsoft way redmond, wa usa sdrucker microsoft com example, when we can model and identify consistent behavior, we have a chance to adapt user interfaces to take advantage of predicted behavior. through the research in areas such as information foraging, sensemaking, orienteering search interface design, and information visualization, the research community is at the forefront of developing search technology that serves a diverse range of purposes. however, large scale commercial search engines have not yet been able to effectively apply this rich and varied research, and still favor the traditional ranked list style of result presentation. in this paper we present a study of interaction behavior for users engaged in web search activities that originate with the submission of a query to a search engine. to better understand what users are doing when they are searching, we place a particular emphasis on post query navigation trails. through client side logging of, users over a five month period, we gathered sufficient interaction log data to perform a detailed analysis of variability in search behavior within and between users and within and between the query statements they issued. understanding variability given a user or a query has a range of implications in areas such as the design of search interfaces, predictive document retrieval, and user modeling. although there has been related research on examining user trails, studying browsing behavior within web sites, developing user and task models, and investigating individual differences in user behavior, this is the first study to focus explicitly on behavioral variability in web search. in this investigation we wanted to characterize differences in the interaction styles of users, and better understand just how different users in particular, we focus on search interactions actually are. two research questions: how variable are search interactions within each user and between all users and how variable areooscurane tm caeiwes te fits a ap rahterhrslpsnt, hrhae search interactions within each query and between all queries to search interface is shown to all users for each query they submit. there is good reason for this: users benefit from familiarity with the interface, and the cost on interface designers is minimized. however, as users perform more tasks using search engines, there is a growing need to understand more precisely what users are doing during the search process. it is only through this understanding that we will be able to build more effective interfaces to cater to mrues qisnsacigtlsfor oeruraehserydn. copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. answer these questions we analyze interaction log data for a large sample of users and a set of queries sampled from the logs. in this analysis we focus on interaction patterns, features of the interaction such as time and structure, and features of the information that users interact with, such as the web domain. as well as providing a better understanding of behavioral variability in web search, the answers motivate the creation of a tailored set of design recommendations for supporting the most and least variable users and queries that can be offered as optional interface functionality for all web searchers. the driving force behind this research is a desire to improve the web search experience for all users. in the remainder of this paper we present a discussion of related work, describe the study performed, present the findings and their implications, and conclude.