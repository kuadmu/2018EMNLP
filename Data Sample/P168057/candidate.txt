we propose lmmh, a novel log linear modeling method that scales to massive data applications with billions of training records and several million potential predictors in a map reduce framework. our method exploits correlations in aggregates observed at multiple resolutions when working with multiple hierarchies; stable estimates at coarser resolution provide informative prior information to improve estimates at finer resolutions. other than prediction accuracy and scalability, our method has an inbuilt variable screening procedure based on a spike and slab prior that provides parsimony by removing non informative predictors without hurting predictive accuracy. extensive comparisons with other benchmark methods show significant improvements in prediction accuracy. we consider the problem of estimating rates of rare events for high dimensional, multivariate categorical data where several dimensions are hierarchical. such problems are routine in several data mining applications including computational advertising, our main focus in this paper. we perform large scale experiments on data from real computational advertising applications and illustrate our approach on datasets with several billion records and hundreds of millions of predictors. jointly estimating occurence rates of rare events for large number of attribute combinations is an important data mining permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not madeordistributedforpro. problem that arises in several applications like computational advertising, disease mapping, ecology, adverse drug reaction, and many others. ingeneral, a small sample size correction obtained by properly poolinginformation acrossdifferentdata aggregatesprovidebetter estimates. infact, aggregatingdata reduce variancedue tolarger sample size but introduces bias; disaggregation on the other hand reducebiasbutincurs more variance. whendataishierarchical, borrowing strength from aggregates across multiple dimensions and multipleresolutions oftenleadtoestimates withagoodbias variance tradeoff. howtoperform suchborrowinginanaccurate and scalablefashion when working withhighdimensionaldataistheproblem we addressin thispaper. such data are commonplace in many scenarios including computational advertising, our main motivating applicationinthispaper. sub discipline that is at the foundation of buildinglarge scale automated systems to select advertisements in online advertising applications. an important goal in online advertising is to nd the best match between agivenuser in agivencontext and a suitable ad. different variations of the problem arise depending on the context considered. forinstance, in sponsored searchthe contextis aqueryissued by the user; in contextual and display advertising the context is a publisherpage visitedbythe user andso on. thede nition of what constitutesa best match isacomplexoneandinvolvesmaximizing value for users, publishers and advertisers. however, one key input thatis often required tofacilitategood matches are estimates of rare eventslike click rates and conversion rates. infact, a significant fraction of online advertising is performance based whereby anadvertiserpaysif anuserperforming aweb search or visiting a publisherpage respondspositivelyto the ad. thepositive response is typically measured in terms of click through rate on ads. revenue modelsbased onconversionrates wherepayment ismadewhenusersperformsomepositiveaction on the advertiserlandingpage are alsobecomingpopular. in thispaper weprovide a novel log linear model to estimate such ratesinhighdimensional andlarge scale computational advertising problems. first, success rates aretypicallylow, especiallyindisplay and contextual advertising. second, although massive amounts of data is obtained from large scale advertising copyright acm. furthermore, data on new cells are frequently added to the system. however, the number ofpublishers, ads and usersin the systemis typicallylarge. thedatadistributionamong cellsisalsounbalanced sincethebestad matches typically tend to be more concentrated. hence, it is usual to see a small number of cells accountfor alargefraction ofdata with the remaining sparsely distributed among a large number of cells. weprovide estimation methodsthatcanexploit correlations when considering cross product of such multi dimensional hierarchical categorical variables. in fact, our method borrows from estimates at coarser resolutions when there is sparseness at ner resolutions. for instance, if users from manhattan have seen a honda accord ad on new york times, times in the past and clicked times, we do not use pooling and provide an estimate that is close to year#. consider another scenario wheresanjose mercury news site saw only visits from users in sunnyvale california and obtained clicks, an estimate of is perhaps not asreliableinthiscase herewemay wantto borrowstrength from aggregate click rate estimate of all california visits to mercury news. but how much should we borrow and from where should weborrow underdifferent sample size scenarios in the example above, isitbest toborrowfrom user visitsincalifornia or isitbettertoborrowfrom user visitsto allnews sitesincalifornia or shall we use some other aggregates the answer oftendepends on correlations among cells at ner resolutions that have common ancestor cells at coarser resolutions. weprovide a solution to this problem through a novel statistical method which weshall call lmmh in the rest of thepaper. other than estimation accuracy of rates, it is also desirable to have a parsimonious model. diskaccessforlarge modelsis an optionbutithas an adverseimpact on throughput, often not acceptableinlarge scale systems. our method achieves both competing objectives of accuracy and size. finally, our model tting procedure have to scale to massive amounts of data that are routine in computational advertising applications. massive in this paper would refer to applications with terabytes oftrainingdata and millions ofpotentialpredictors. more explicity, theentiretraining datacannot tinmemory using commodity hardware, computing paradigms like map reduce provide an attractive way to scale computations in such scenarios. in fact, we demonstrate scalability by tting models to datasets obtained from a real world display advertising system that consists of several billions records with hundreds of millions of cells. weproposelmmh, anovel statistical method to estimate rates of rare events withhighdimensional, multivariate and hierarchical categorical data. lmmh improves prediction by exploiting correlations in aggregates and extends previous work for a single hierarchy. our screeningprocedureispartof the model andbased ona spikeand slab priorthatensuresparsimony withouthurting accuracy weprovideascalablemodel ttingprocedurethrough a sequential one at a time update iteratedconditionamodes model tting algorithm in a map reduce framework. we illustrate our method on largedatasets obtained from a real world computational advertising system. therestof thepaperisorganized asfollows webeginwitha description ofdata underlying our method in section # with model description and tting in section # experiments are described in section and we end with abriefdiscussionin section. or commercial advantage and that copies bearthisnoticeand thefull citation onthe rstpage tocopy otherwise, to republish, topost on servers or to redistribute tolists, requiresprior speci. the main dif culty in such simultaneous rate estimation is the paucity of data and absence of events at ne resolutions, it is common to observe a large fraction of cells with zero or afew event occurences. hence rate estimates obtained independently for each cell are often unreliable and noisy. speci cally, wedescribe rate estimation methods that exploit cell correlations in data that is hierarchical along more than one dimensions. systems, thedimensionality ofthe attribute space is large and typically consists of cells that canpotentially run into several millions. infact, to see this curse ofdimensionalityissue, note that an addisplay is aninteraction among elementsinthetryad andtypicallygenerates a response. thus, thereis often extremedata sparseness at the nest resolution cells for which estimates are required to perform good ad matches. the main assumption we make is that although each dimension is ahigh dimensional categorical variable withlarge number of values, they arehierarchical and hence facilitate data pooling at multiple resolutions. for instance, publishers maybe arrangedinahierarchybased onurlpre xrollups, users maybe characterized by several attributes some of which are hierarchical andthereis a naturalhierarchyfor adsthat aggregatesinto campaigns whichinturn are aggregatedinto advertisers. this is a non trivial problem withhigh dimensional multivariatecategoricaldata. models with large number of parameters have high memory requirements that makes cost effective online ad selection dif cult. the idea employed is simple and based on thresholding cells that borrow too much strength from ancestors are pruned and completely fallback on ancestors. this eliminates the need to store parameters for pruned cells and signi cantly reduces the model size. however, the thresholding operationis not applied as a post processing step to model outputbutisinfact abuilt infeature ofthe modelitself. thisprovides aprincipled modelingframework to obtain models that areboth accurate andparsimonious. we provide a scalable and simple model tting algorithm for lmmh that scalesgracefullyto massivedata mining applicationsin a mapreduce framework. besides accuracy, we also addresstheissue ofparsimonyto reduce memory requirements for online scoring by taking recourse to a novel variable screening procedure. we show that incorporating user behavior data can significantly improve ordering of top results in real web search setting. we examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features. we report results of a large scale evaluation over, queries and million user interactions with a popular web search engine. we show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as relative to the original performance. implicit relevance feedback for ranking and personalization has become an active area of research. recent work by joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process. millions of users interact with search engines daily. they issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions. these interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments. our motivation for this work is to understand how implicit feedback can be used in a large scale operational environment to permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. how does it compare to and compliment evidence from page content, anchor text, or link based features such as inlinks or pagerank while it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies. our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy and a web search engine already uses hundreds of features and is heavily tuned. to this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine. the specific contributions of this paper include: analysis of alternatives for incorporating user behavior into web search ranking. an application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine. a large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback. we summarize our findings and discuss extensions to the current work in section #, which concludes the paper. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. as with any application of machine learning, web search ranking requires labeled data. the labels usually come in the form of relevance assessments made by editors. click logs can also provide an important source of implicit feedback and can be used as a cheap proxy for editorial labels. the main difficulty however comes from the so called position bias urls appearing in lower positions are less likely to be clicked even if they are relevant. in this paper, we propose a dynamic bayesian network which aims at providing us with unbiased estimation of the relevance from the click logs. experiments show that the proposed click model outperforms other existing click models in predicting both click through rate and relevance. web page ranking has been traditionally based on hand designed ranking functions such as bm. with the inclusion of thousands of features for ranking, hand tuning of ranking function becomes intractable. several machine learning algorithms have been applied to automatically optimize ranking functions. machine learned ranking requires a large number of training examples, with relevance labels indicating the degree of relevance for each querydocument pair. the cost of the editorial labeling is usually quite expensive. moreover, the relevance labels of the training examples could change over time. for example, if the query is time sensitive or recurrent, a search engine is expected to return the copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. most up to date documents sites to the users. however, it would be prohibitive to keep all the relevance labels up to date. click logs embed important information about user satisfaction with a search engine and can provide a highly valuable source of relevance information. compare to editorial labels, clicks are much cheaper to obtain and always re ect current relevance. clicks have been used in multiple ways by a search engine: to tune search parameters, to evaluate di erent ranking functions, or as signals to directly in uence ranking. however, clicks are known to be biased, by the presentation order, the appearance of the documents, and the reputation of individual sites. many studies have attempted to account the position bias of click. carterette and jones proposed to model the relationship between clicks and relevance so that clicks can be used to unbiasedly evaluate search engine when lack of editorial relevance judgment. other research attempted to model user click behavior during search so that future clicks may be accurately predicted based on observations of past clicks. two di erent types of the click models are position models and the cascade model. a position model assumes that a click depends on both relevance and examination. each rank has a certain probability of being examined, which decays by rank and depends only on rank. a click on a url indicates that the url is examined and considered relevant by the user. however this model treats the individual urls in a search result page independently and fails to capture the interaction among urls in the examination probability. take for example two equally relevant urls for a query: a user may only click on the top one, feel satis ed, and then leave the search result page. in this case, the positional bias cannot fully explain the lack of clicks for the second url. the cascade model assumes that users examine the results sequentially and stop as soon as a relevant document is clicked. here, the probability of examination is indirectly determined by two factors: the rank of the url and the relevance of all previous urls. the cascade model makes a strong assumption that there is only one click per search and hence it could not explain the abandoned search or search with more than one clicks. even though the cascade model is quite restrictive, the authors of that paper showed that we refer to url as a shorthand for the entire display block consisting of the title, abstract and url of the corresponding result. it can predict click through rates more accurately than the position models described above. none of the above models distinguish perceived relevance and actual relevance. because users cannot examine the content of a document until they click on the url, the decision to click is made based on perceived relevance. while there is a strong correlation between perceived relevance and actual relevance, there are also many cases where they di er. in this paper, a dynamic bayesian network model is proposed to model the users browsing behavior. as in the position model, we assume that a click occurs if and only if the user has examined the url and deemed it relevant. similar to the cascade model, our model assumes that users make a linear transversal through the results and decide whether to click based on the perceived relevance of the document. the user chooses to examine the next url if he she is unsatis ed with the clicked url. our model di ers from the cascade model in two aspects: because a click does not necessarily mean that the user is satis ed with the clicked document, we attempt to distinguish the perceived relevance and actual relevance. a key source of bias is presentation order: the probability of click is influenced by a document position in the results page. this paper focuses on explaining that bias, modelling how probability of click depends on position. we carry out a large data gathering effort, where we perturb the ranking of a major search engine, to see how clicks are affected. we then explore which of the four hypotheses best explains the real world position effects, and compare these to a simple logistic regression model. a cascade model, where users view results from top to bottom and leave as soon as they see a worthwhile document, is our best explanation for position bias in early ranks. search engine click logs provide an invaluable source of relevance information, but this information is biased. we propose four simple hypotheses about how position bias might arise. the data are not well explained by simple position models, where some users click indiscriminately on rank or there is a simple decay of attention over ranks. as people search the web, certain of their actions can be logged by a search engine. they can also be indicative of success or failure of the engine. or commercial advantage and that copies bear this notice and the full citation on the rst page. these record which results page elements were selected for which query. click log information can be fed back into the engine, to tune search parameters or even used as direct evidence to in uence ranking. a fundamental problem in click data is position bias. the probability of a document being clicked depends not only on its relevance, but on its position in the results page. in top results lists, the probability of observing a click decays with rank. eye tracking experiments show that the user is less likely to examine results near the bottom of the list, although click probability decays faster than examination probability so there are probably additional sources of bias. our approach is to consider several such hypotheses for how position bias arises, formalising each as a simple probabilistic model. we then collect click data from a major web search engine, while deliberately ipping positions of documents in the ranked list. we nally evaluate the position bias models using the ip data, to see which is the best explanation of real world position. although our experiment involves ips, our goal is to model position bias so we can correct for it, without relying on ips. with such a model it should be possible to process a click log and extract estimates of a search resultabsolute click relevance. patterns of behaviour in logs can give an idea of the scope of user activity. when deciding which search results to permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. bill ramsey microsoft research, redmond usa brams microsoft com present, click logs are of particular interest. such estimates could be used in applications where an estimate of probability of click is useful such as ad ranking or evaluation. we propose a new model to interpret the clickthrough logs of a web search engine. this model is based on explicit assumptions on the user behavior. in particular, we draw conclusions on a document relevance by observing the user behavior after he examined the document and not based on whether a user clicks or not a document url. this results in a model based on intrinsic relevance, as opposed to perceived relevance. we use the model to predict document relevance and then use this as feature for a learning to rank machine learning algorithm. comparing the ranking functions obtained by training the algorithm with and without the new feature we observe surprisingly good results. this is particularly notable given that the baseline we use is the heavily optimized ranking function of a leading commercial search engine. a deeper analysis shows that the new feature is particularly helpful for non navigational queries and queries with a large abandonment rate or a large average number of queries per session. this is important because these types of query is considered to be the most difficult to solve. search engine click logs provide an invaluable source of relevance information but this information is biased because we ignore which documents from the result list the users have actually seen before and after they clicked. otherwise, we could estimate document relevance by simple counting. in this paper, we propose a set of assumptions on user browsing behavior that allows the estimation of the probability that a document is seen, thereby providing an unbiased estimate of document relevance. our solution outperforms very significantly all previous models. they also explain why documents situated just after a very relevant document are clicked more often. to train, test and compare our model to the best alternatives described in the literature, we gather a large set of real data and proceed to an extensive cross validation experiment. as a side effect, we gain insight into the browsing behavior of users and we can compare it to the conclusions of an eye tracking experiments by joachims et al. in particular, our findings confirm that a user almost always see the document directly after a clicked document. users permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bearthisnoticeand thefull citationonthe rstpage tocopy otherwise, to republish, topost on servers or to redistribute tolists, requiresprior speci. benjamin piwowarski yahoo research latin america bpiwowar yahoo inc com areincreasingly understood tobethedrivingforceof theinternet and many initiatives are aimed at empowering them. social search, as its name implies, supposes participation from users who tag, bookmark, andcomment their search results. in addition to this information explicitly provided by users, there is a much larger source of implicit data which is collectedby search engines. itis apoll of millions of users over an enormous variety of topics. examples of applicationsincludewebpersonalization, web spam detection, query term recommendation. unlike human tags and bookmarks, implicit feedback is also not biased towards socially active web users. that is, the data is collected from all users, notjust usersthat choosetoedit a wikipage, orjoin a social network such asmyspace orfriendster. click data seems the perfect source of information when deciding whichdocuments to showin answerto a query. this information can be fed back into the engine, to tune search parameters or even used asdirect evidencetoin uence ranking. nevertheless, they cannot be used without further processing: a fundamental problem is the position bias. the probability of a document being clicked depends not only on its relevance, but on other factors as its position in the result page. contributions user activity models within web search can be broadly divided in three categories: analysis models where the aim istogaininsightintotypical userbehavior, modelsthat try topredictthenext useraction, and eventually models that estimate the attractiveness or perceived relevance of a document independently of the layout in uence. this work focusses on thelatter, usingas the only source ofinformation the web search logs produced by the search engines. yet users do not browse the whole list and documents situated earlier in the rankinghave ahigherprobability ofbeing examined. as a consequence, they also have a higher probability of being clicked independently of how relevant they are. if we could estimatetheprobability that adocumentis examined by the user, we could estimate its relevance as the ratio of the number of times a user clicked on the document to the expected number of times the document is examined. the main contribution of this work is a model of user browsing behavior when consulting a page of search results. this model estimates the probability of examination of a documentgiven the rankofthedocument andthedistance to the last clicked document. our model sheds light on user behavior, is in agreement with the user experiments ofgranka et al and extends andquanti esthe user model ofjoachims et al. in section # we review the literature for click models and we present our contributions. in section # we compare the predicting abilities on unseen data of the di erent models. westudy in moredetailstheimplications of theuserbrowsing model and we relate the ndings with the eye tracking experiments of insection. social search is quickly gaining acceptance as a promising way of harnessing the common knowledge of millions of users to help each other and search more. arguably, this is a long term trend that started with kleinberg idea of hubs and authorities, which proposed that a hyperlink from one document to another was a vote in favor of the document linked to, an idea in practice exploited in the pagerank algorithm. thisfeedbackprovidesdetailed and valuable information about users interactions with the system as theissuedquery, thepresentedurls, the selected documents and their ranking. it has been used in many ways to mine user interests and preferences. it can be thought as the result of users voting in favor of the documents they nd interesting. in top results lists, the probability of observing a click decays with rank. eye tracking experiments show that a user is less likely to examine results near the bottom of the list, although click probability decays faster than examination probability so there are probably additional sources of bias. experiments also show that a document is not clicked with the same frequency if situated after a highly relevant or a mediocre document. if the users looked with attention all the documents in the ranking list, the relevance of one of them could be estimated simply by counting thenumberof timesitisselected. we investigate how users interact with the results page of a www search engine using eye tracking. the goal is to gain insight into how users browse the presented abstracts and how they select links for further exploration. such understanding is valuable for improved interface design, as well as for more accurate interpretations of implicit feedback for machine learning. the following presents initial results, focusing on the amount of time spent viewing the presented abstracts, the total number of abstract viewed, as well as measures of how thoroughly searchers evaluate their results set. how do users interact with the list of ranked results of www search engines do they read the abstracts sequentially from top to bottom, or do they skip links how many of the results do users evaluate before clicking on a link or reformulating the search the answers to these questions will be beneficial in at least three ways. first, they provide the basis for improved interfaces. second, they suggest more targeted metrics for evaluating the retrieval performance in www search. and third, they help interpreting implicit feedback like clickthrough and reading times for machine learning of improved retrieval functions. in particular, better understanding of user behavior will allow us to draw more accurate inferences about how implicit feedback relates to relative relevance judgments. the following presents the results of an eye tracking study that we conducted. previous studies have analyzed directly observable data like query word frequency. however, unlike eyetracking, these measurements can at best give indirect evidence of how users perceive and respond to the search results. to the best of our knowledge, only one previous study has used eye tracking in the context of information retrieval evaluation. this study attempted to use eye movements to infer the relevancy of documents in the retrieval phase of an information search. the researchers linked relevancy judgments to increases in pupil diameter, as a larger diameter typically signifies high interest in the content matter. however, the sample size and search tasks in this experiment were not robust enough to generate predictable patterns of user search and scanning behavior, which is what our study is able to attain. given a terabyte click log, can we build an efficient and effective click model it is commonly believed that web search click logs are a gold mine for search business, because they reflect users preference over web documents presented by the search engine. click models provide a principled approach to inferring user perceived relevance of web documents, which can be leveraged in numerous applications in search businesses. due to the huge volume of click data, scalability is a must. we present the click chain model, which is based on a solid, bayesian framework. it is both scalable and incremental, perfectly meeting the computational challenges imposed by the voluminous click logs that constantly grow. we conduct an extensive experimental study on a data set containing million query sessions obtained in july year# from a commercial search engine. ccm consistently outperforms two state of the art competitors in a number of metrics, with over better log likelihood, over better click perplexity and much more robust prediction of the first and the last clicked position. important attributes of these search activities are automatically logged as implicit user feedbacks. web search click logs are probably the most extensive, albeit indirect, surveys on user experience, which can be part of this work was done when the rst author was on a summer internship with microsoft research. for example, the topic of utilizing click data to optimize search ranker has been well explored and evaluated by quite a few academic and industrial researchers since the beginning of this century. joachims et al carried out eye tracking experiments to study participants decision process as they scan through search results, and further compared implicit click feedback against explicit relevance judgments. they found that clicks are accurate enough as relative judgement to indicate userpreferences for certain pairs of documents, but they are not reliable as absolute relevance judgement, ie, clicks are informative but biased. a particular example is that users tend to click more on web documents in higher positions even if the ranking is reversed. richardson et al proposed the examination hypothesis to explain the position bias of clicks. under this hypothesis, a web document must be examined before being clicked, and user perceived document relevance is de ned as the conditional probability of being clicked after being examined. click models provide a principled way of integrating knowledge of user search behaviors to infer user perceived relevance of web documents, which can be leveraged in a number of search related applications, including: automated ranking alterations: the top part of ranking can be adjusted based on the inferred relevance so that they are aligned with users preference. search quality metrics: the inferred relevance and user examination probabilities can be used to compose search quality metrics, which correlate with user satisfaction. adaptive search: when the meaning of a query changes over time, so do user click patterns. based on the inferred relevance that shifts with click data, the search engine can be adaptive. judge of the judges: the inferred rst party relevance judgement could be contrasted reconciled with well trained human judges for improved quality. online advertising: the user interaction model can be adapted to a number of sponsored search applications such as ad auctions. an ideal model of clicks should, in addition to enabling reliable relevance inference, have two other important properties scalability and incremental computation; scalability enables processing of large amounts of clicklogs data and the incremental computation enables updating the model as new data are recorded. two click models are recently proposed which are based on the same examination hypothesis but with di erent assumptions about user behaviors. the user browsing model proposed by dupret and piwowarski is demonstrated to outperform the cascade model in predicting click probabilities. however, the iterative nature of the inference algorithm of ubm requires multiple scans of the data, which not only increases the computation cost but renders incremental update obscure as well. cient than ubm, but its performance on tail queries could be further improved. in this paper, we propose the click chain model, that has the following desirable properties: foundation: it is based on a solid, bayesian framework. a closed form representation of the relevance posterior can be derived from the proposed approximation inference scheme. scalability: it is fast and nimble, with excellent scalability with respect to both time and space; it can also work in an incremental fashion. ccm consistently shows performance improvements over two of the state of the art competitors in a number of evaluation metrics such as log likelihood, click perplexity and click prediction robustness. algorithms for relevance inference, parameter estimation and incremental computation are detailed in section #. billions of queries are submitted to search engines on the web every day. these attributes include, for each query session, the query string, the time stamp, the list of web documents shown in the search result and whether each document is clicked or not. copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. extracting key statistics or patterns from these tera byte logs is of much interest to both search engine providers, who could obtain objective measures of user experience and useful features to improve their services, and to world wide web researchers, who could better understand user behavior and calibrate their hypotheses and models. a number of studies have been conducted previously on analyzing user behavior in web search and their relationship to click data. top ranked documents may have more chance to be examined than those ranked below, regardless of their relevance. craswell et al further proposed the cascade model for describing mathematically how the rst click arises when users linearly scan through search results. however, the cascade model assumes that users abandon the query session after the rst click and hence does not provide a complete picture of how multiple clicks arise in a query session and how to estimate document relevance from such data. the dependent click model which appears in our previous work is naturally incremental, and is an order of magnitude more. the rest of this paper is organized as follows. we rst survey existing click models in section #, and then present ccm in section #. recent advances in click models have adopted the examination hypothesis which distinguishes document relevance from position bias. specifically, users with different search intents may submit the same query to the search engine but expect different search results. this bias has not been considered in previous works such as ubm, dbn and ccm. modeling a user click through behavior in click logs is a challenging task due to the well known position bias problem. in this paper, we revisit the examination hypothesis and observe that user clicks cannot be completely explained by relevance and position bias. thus, there might be a bias between user search intent and the query formulated by the user, which can lead to the diversity in user clicks. in this paper, we propose a new intent hypothesis as a complement to the examination hypothesis. this hypothesis is used to characterize the bias between the user search intent and the query in each search session. this hypothesis is very general and can be applied to most of the existing click models to improve their capacities in learning unbiased relevance. experimental results demonstrate that after adopting the intent hypothesis, click models can better interpret user clicks and achieve a significant ndcg improvement. ected by intent bias, relevance and position bias. copyright is held by the international world wide web conference committee. thus, the click through rate is not a proper measure of relevance. the examination hypothesis assumes that, if a document has been examined, the click through rate of the document for a given query is a constant number whose value is determined by the relevance between the query and the document. we argue that users with di erent search intents, however, may submit the same query to the search engine. in other words, a single query may not truly re ect user search intent. figure # describes the triangular relationship among intent, query and document, where the edge connecting the two entities measures the degree of match between them. the search engine receives the query and returns a list of ranked documents, while the relevance measures the degree of match between a query and a document. third, because the intent hypothesis is general, we apply it to two typical click models, ubm and dbn, and adopt a bayesian inference method to model the intent hypothesis. click through logs record user activities on search pages and encode user preferences of search results. click through this work was done when the authors were interns at microsoft research asia. distribution of these papers is limited to classroom use, and personal use by others. logs can be collected at a very low cost, and the analysis of them can help to understand the userlatest preference tendencies. naturally, many studies have attempted to discover user preferences from click through logs to improve the relevance of search results. it is well known that clicks are informative but biased, and it is a challenging task to estimate unbiased relevance from click through logs. ecting user clicks is the so called position bias: a document appearing in a higher position is more likely to attract user clicks even though it is not as relevant as other documents in lower positions. this bias was rst noticed by granka et al in their eye tracking experiment and some follow up investigations have been made to alleviate this bias so that the unbiased relevance can be inferred from the clicks. richardson et al proposed to increase the relevance of the documents in lower positions by using a multiplicative factor. this idea was later formalized as the examination hypothesis, which assumes that the user will click a search result only after examining its search snippet. in other words, given an examined document, only its relevance determines the user click. the examination hypothesis decouples document relevance from position bias where the position bias is formulated as the probability that a document is examined by a user. recently, many interesting studies have been made to re ne click models using the examination hypothesis. ubm, dbn, ccm, bbm, gcm are typical models which can extend the capabilities of the examination hypothesis. a user submits this query because she wants to browse general information about ipad, and the results from apple com or wikipedia com are attractive to her. in contrast, another user who submits the same query may look for information such as user reviews or feedback on ipad. in this situation, search results like technical reviews and discussion forums are more likely to be clicked. this example indicates that the attractiveness of a search result is not only in uenced by its relevance but also determined by the userintrinsic search intent behind the query. intent bias click probability relevance figure #: the triangular relationship among intent, query and document. the edge connecting two entities measures the degree of match between two entities. we design an experiment to validate that the relevance between a query and a document is not a constant number. in the experiment, we collect search sessions, and partition them into two groups according to di erent search intents. we note that after eliminating the position bias effect, most queries have signi cantly di erent clickthrough rates on two intent groups. in other words, the clickthrough rate of an examined document varies greatly across di erent search sessions due to the diversity in search intent. each user presumably has an intrinsic search intent before submitting a query. when a user comes to a search engine, she formulates a query according to her search intent and submits it to the search engine. the intent bias measures how well the query matches the intent, ie, the degree of match between the intent and the query. the user examines each document and, if a document better satis es her information need, she is more likely to click this document. the triangular relationship suggests that the user click is determined by both intent bias and relevance. if a user does not clearly express her information need in the input query, there is a large intent bias. thus, the user is unlikely to click the document that does not meet her search intent, even if the document is very relevant to the query. the examination hypothesis can be considered as a simpli ed case, that it regards the search intent and the input query as equivalent and ignores the intent bias. thus, the relevance between a query and a document may be mistakenly estimated when only the examination hypothesis is adopted. in this paper, we incorporate the concepts of intent and intent bias to propose a novel hypothesis, the intent hypothesis, to explain how user clicks are. the intent hypothesis can enhance the analytical power of the examination hypothesis, characterize search intent diversity and interpret user clicks better. click models that adopt the intent hypothesis can estimate more accurate and unbiased relevance. first, we empirically demonstrate the limitations of the examination hypothesis and suggest that the position bias is not the only bias. second, we propose the novel intent hypothesis to enhance the capability of modeling user search behavior. this inference method is capable of learning on very large scale click though logs. finally, the experiment has been conducted on million queries and one billion search sessions, and the results illustrate the advantages of adopting the intent hypothesis. this paper is organized as follows: in section #, we brie. review the previous research on click models including their speci cations and hypotheses. in section #, we empirically validate that the examination hypothesis can not well interpret real click through data. in section #, we propose our intent hypothesis and its inference method. in section #, the experiment on real datasets shows the advantages of adopting the proposed hypothesis. in section #, we analyze the intent bias that we estimated from the experiment and discover some insightful results. this paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. while previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. this makes them difficult and expensive to apply. the goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query log of the search engine in connection with the log of links the users clicked on in the presented ranking. such clickthrough data is available in abundance and can be recorded at very low cost. taking a support vector machine approach, this paper presents a method for learning retrieval functions. from a theoretical perspective, this method is shown to be well founded in a risk minimization framework. furthermore, it is shown to be feasible even for large sets of queries and features. the theoretical results are verified in a controlled experiment. it shows that the method can effectively adapt the retrieval function of a meta search engine to a particular group of users, outperforming google in terms of retrieval quality after only a couple of hundred training examples. which www page does a user actually want to re trieve when he types some keywords into a search engine there are typically thousands of pages that contain these words, but the user is interested in a much smaller subset. if we knew the set of pages actually relevant to the user query, we could use this as training data for optimizing the retrieval function. unfortunately, experience shows that users are only rarely willing to give explicit feedback. however, this paper argues that sufficient information is already hidden in the logfiles of www search engines. ceive millions of queries per day, such data is available in abundance. compared to explicit feedback data, which is typically elicited in laborious user studies, any information that can be extracted from logfiles is virtually free and sub stantially more timely. this leads to a problem of learning with preference examples like for query, document, should be ranked higher than document db. more generally, will formulate the problem of learning a ranking function over a finite domain in terms of empirical risk minimization. for this formulation, will present a support vector machine algorithm that leads to a convex program and that can be extended to non linear ranking functions. it starts with a defi nition of what clickthrough data is, how it can be recorded, and how it can be used to generate training examples in the form of preferences. section # then introduces a gen eral framework for learning retrieval functions, leading to an svm algorithm for learning parameterized orderings in section #. section # evaluates the method based on experi mental results. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific pemaission and or a fee. one could simply ask the user for feedback. since major search engines re permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. this paper presents an approach to learning retrieval func tions by analyzing which links the users click on in the pre sented ranking. experi ments show that the method can successfully learn a highly effective retrieval function for a meta search engine. this paper examines the reliability of implicit feedback generated from clickthrough data in www search. analyzing the users decision process using eyetracking and comparing implicit feedback against manual relevance judgments, we conclude that clicks are informative but biased. while this makes the interpretation of clicks as absolute relevance judgments difficult, we show that relative preferences derived from clicks are reasonably accurate on average. the idea of adapting a retrieval system to particular groups of users and particular collections of documents promises further improvements in retrieval quality for at least two reasons. second, as evident from the trec evaluations, differences between document collections make it necessary to tune retrieval functions with respect to the collection for optimum retrieval performance. since manually adapting a retrieval function is time consuming or even impractical, research on automatic adaptation using machine learning is receiving much attention. ithaca, ny, usa cornell edu however, a great bottleneck in the application of machine learning techniques is the availability of training data. in this paper we explore and evaluate strategies for how to automatically generate training examples for learning retrieval functions from observed user behavior. however, implicit feedback is more di cult to interpret and potentially noisy. in this paper we analyze which types of implicit feedback can be reliably extracted from observed user behavior, in particular clickthrough data in www search. the study is designed to analyze how users interact with the list of ranked results from the google search engine and how their behavior can be interpreted as relevance judgments. first, we use eyetracking to understand how users behave on googleresults page. do users scan the results from top to bottom how many abstracts do they read before clicking how does their behavior change, if we arti cially manipulate googleranking answers to these questions give insight into the users decision process and suggest in how far clicks are the result of an informed decision. based on these results, we propose several strategies for generating feedback from clicks. to evaluate the degree to which feedback signals indicate relevance, we compare the implicit feedback against explicit feedback we collected manually. the study presented in this paper is di erent in at least two respects from previous work assessing the reliability of implicit feedback. first, our study provides detailed insight into the users decision making process through the use of eyetracking. second, we evaluate relative preference signals derived from user behavior. this is in contrast to previous studies that primarily evaluated absolute feedback. our results show that users make informed decisions among the abstracts they observe and that clicks re ect relevance judgments. however, we show that clicking decisions are biased in at least two ways. first, we show that there is a trust bias which leads to more clicks on links ranked highly by google, even if those abstracts are less relevant than other abstracts the user viewed. second, there is a quality bias: the users clicking decision is not only in uenced by the relevance of the clicked link, but also by the overall quality of the other abstracts in the ranking. we propose several strategies for extracting such relative relevance judgments from clicks and show that they accurately agree with explicit relevance judgments collected manually. first, a one size ts all retrieval function is necessarily a compromise in environments with heterogeneous users and is therefore likely to act suboptimally for many users. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. in contrast to explicit feedback, such implicit feedback has the advantage that it can be collected at much lower cost, in much larger quantities, and without burden on the user of the retrieval system. to evaluate the reliability of implicit feedback signals, we conducted a user study. we performed two types of analysis in this study. this shows that clicks have to be interpreted relative to the order of presentation and relative to the other abstracts. on the second click log set, spanning a quarter of petabyte data, we showcase the scalability of bbm: we implemented it on a commercial mapreduce cluster, and it took only hours to compute the relevance for billion distinct query url pairs. given a quarter of petabyte click log data, how can we estimate the relevance of each url for a given query in this paper, we propose the bayesian browsing model, a new modeling technique with following advantages: it does exact inference; it is single pass and parallelizable; it is effective. we present two sets of experiments to test model effectiveness and efficiency. on the first set of over million search instances of million distinct queries, bbm out performs the state of the art competitor by in log likelihood while being times faster. or commercial advantage and that copies bear this notice and the full citation on the rst page. as such information is not available in the log, we need to rst understand how users examine search results and make decisions on clicks. in practice, it is common to have tens to hundreds of terabytes log data accumulated at the server side every day. in this paper, we propose the bayesian browsing model, which builds upon the state of the art user browsing model and scales to petabyte scale data with easy incremental updates. section # introduces preliminaries and discusses previous click models. section # covers related work, and the paper is concluded in section #. web search has become indispensable from everyday life: questions ranging from navigating to live com to how to bleach wine stains are all directed to search engines. while responding to users information needs, search engines also log down the interaction with users, a typical form of which is what urls are presented and which are clicked in each search result. such click log constitutes an invaluable source of user feedbacks, which can be leveraged in many searchrelated applications, eg, query recommendation, permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. learning to rank, and personalized search, just to name a few. a central question in click log analysis is to infer userperceived relevance for each query url pair. the task could be as trivial as counting the click through rate, should the search engine know the snippets of which urls are examined by the user in addition to those clicked. thanks to previous work in this direction, it is well known that a number of factors must be considered in an accurate interpretation of user clicks, such as the position bias of examination and clicks, and dependency between clicks over di erent documents in the same search result page. quite a few statistical models have been recently developed to leverage user clicks for inferring userperceived relevance, some representatives of which are the cascade model, dependent click model, and user browsing model. we will review them with details in section #. this data stream nature of click logs imposes two computational challenges for click modeling. first, the scalability: a click model must scale comfortably to terabyte scale or even petabyte scale data. even better, we would expect the modeling to be highly parallelizable for better scalability. second, the capability to be incrementally updated, ie, incremental learning: for not losing sync with the evolving world wide web, a click model has to be continuously updated. considering the ever growing search volume, we would expect a practical click model to be single pass, which minimizes the storage and io cost for revisiting historical data. the previous models, despite of their di erent user behavior assumptions, all follow the point estimation philosophy: the estimated relevance for each query url pair is a single number normalized to, and a larger value indicates stronger relevance. while this single number estimate could su ce for many usages, it nevertheless falls short of capacity for broader applications, for example, it is unspeci ed how to estimate the probability that url ui is preferred to uj for the same query while their relevance are ri and rj respectively. existing learning to rank algorithms derive pairwise preference relationship either from human ratings or from certain heuristics. to the best of our knowledge, no principled approach has been proposed to compute a preference probability, which could support broader applications in both web search and online advertising. by virtue of the bayesian approach to modeling the document relevance, the preference probability between multiple documents is well de ned and can be computed based on document relevance posteriors. moreover, we present an exact inference algorithm for bbm that exploits the particular probabilistic dependency in the model, and reveals the relevance posterior in closed form after a single pass over the log data. in summary, we make the following contributions in this study: we propose bbm, together with an exact inference algorithm that derives the relevance posterior in closed form, which facilitates single pass, incremental computation. we compare bbm with a state of the art model in a real world data set of over million search instances for million distinct queries. we implement bbm on a commercial mapreduce cluster, and it takes only hours to compute relevance for more than billion query url pairs by mining a terabyte click log. the rest of the paper is organized as follows. we elaborate on bbm and relevant algorithms in section #. performance comparison results are reported in section #, and the petabyte scale experiment is presented in section #. tagging plays an important role in many recent websites. recommender systems can help to suggest a user the tags he might want to use for tagging a specific item. factorization models based on the tucker decomposition model have been shown to provide high quality tag recommendations outperforming other approaches like pagerank, folkrank, collaborative filtering, etc. the problem with td models is the cubic core tensor resulting in a cubic runtime in the factorization dimension for prediction and learning. in this paper, we present the factorization model pitf which is a special case of the td model with linear runtime both for learning and prediction. the model is learned with an adaption of the bayesian personalized ranking criterion which originally has been introduced for item recommendation. empirically, we show on real world datasets that this model outperforms td largely in runtime and even can achieve better prediction quality. besides our lab experiments, pitf has also won the ecml pkdd discovery challenge year# for graph based tag recommendation. pitf explicitly models the pairwise interactions between users, items and tags. tagging is an important feature of the web. it allows the user to annotate items resources like songs, pictures, bookmarks, etc. tagging helps the user to organize his items and facilitate eg, browsing and searching. tag recommenders assist the tagging process of a user by suggesting him a set of tags that he is likely to use for an item. personalized tag recommenders take the usertagging behaviour in the past into account when they recommend tags. that means each user is recommended a personalized list of tagsie, the suggested list of tags depends both on the user and the item. personalization makes sense as people tend to use di erent tags for tagging the same item. this can be seen in systems like last fm that have a nonpersonalized tag recommender but still the people use different tags. in an empirical example was shown where recent personalized tag recommenders outperform even the theoretical upper bound for any non personalized tag recommender. this work builds on the recent personalized tag recommender models using factorization models. the drawback of using full td is that the model equation is cubic in the factorization dimension. that makes td models using a high factorization dimension unfeasible for midsized and large datasets. in this paper, we present a new factorization model that explicitly models the pairwise interactions between users, items and tags. the advantage of this model is that the complexity of the model equation is linear in the number of factorization dimensions which makes it feasible for high dimensions. our experimental results also indicate that our pairwise interaction model clearly outperforms the cd model in prediction quality and slightly in runtime. furthermore for learning tag recommender models in general, we adapt the bayessian personalized ranking optimization criterion from item recommendation to tag recommendation. in all, our contributions are as follows: we extend the bayessian personalized ranking optimization criterion to the task of figure #: the observed data in a tagging system forms a ternary relationbetween users, itemsand tags. on the right side, the cubeslices per user are placed next to each other. note that only positive observations are made; there are no explicit observations of negative tagging events. tag recommendation and provide a learning algorithm based on stochastic gradient descent with bootstrap sampling. this optimization criterion and learning algorithm is generic and not limited to factorization models like td. these models like higher order singular value decomposition and ranking tensor factorization are based on the tucker decomposition model. rtf has shown to result in very good prediction quality. in statistics, another approach for tensor factorization with a model equation of linear complexity is the canonical decomposition aka parallel factor analysis. we will show that our model is a special case of both cd and td. search engine advertising has become a significant element of the web browsing experience. choosing the right ads for the query and the order in which they are displayed greatly affects the probability that a user will see and click on each ad. this ranking has a strong impact on the revenue the search engine receives from the ads. further, showing the user an ad that they prefer to click on improves user satisfaction. for these reasons, it is important to be able to accurately estimate the click through rate of ads in the system. for ads that have been displayed repeatedly, this is empirically measurable, but for new ads, other means must be used. we show that we can use features of ads, terms, and advertisers to learn a model that accurately predicts the click though rate for new ads. we also show that using our model improves the convergence and performance of an advertising system. as a result, our model increases both revenue and user satisfaction. most major search engines today are funded through textual advertising placed next to their search results. the market for these search advertisements has exploded in the last decade to billion, and is expected to double again by year#. the most notable example is google, which earned billion in revenue for the third quarter of year# from search advertising alone. though there are many forms of online advertising, in this paper we will restrict ourselves to the most common model: pay per copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. performance with a cost per click billing, which means the search engine is paid every time the ad is clicked by a user. google, yahoo, and microsoft all primarily use this model. to maximize revenue and user satisfaction, pay per performance systems must predict the expected user behavior for each displayed advertisement and must maximize the expectation that a user will act on it. the search system can make expected user behavior predictions based on historical click through performance of the ad. for example, if an ad has been displayed times in the past, and has received clicks, then the system could estimate its click through rate to be. this estimate, however, has very high variance, and may only reasonably be applied to ads that have been shown many times. this poses a particular problem when a new ad enters the system. a new ad has no historical information, so its expected click through rate is completely unknown. in this paper, we address the problem of estimating the probability that an ad will be clicked on, for newly created ads and advertising accounts. we show that we can use information about the ad itself, the page the ad points to, and statistics of related ads, to build a model that reasonably predicts the future ctr of that ad. previous studies on search engine click modeling have identified two presentation factors that affect users behavior: position bias: the same result will get a different number of clicks when displayed in different positions and externalities: the same result might get more clicks when displayed with results of relatively lower quality than when shown with higher quality results. in this paper we focus on analyzing the sequence of user actions to model users click behavior on sponsored listings shown on the search results page. we first show that temporal click sequences are good indicators of externalities in the advertising domain. we then describe the positional rationality hypothesis to explain both the position bias and the externalities, and based on this hypothesis we further propose the temporal click model, a bayesian framework that is scalable and computationally efficient. to the best of our knowledge, this is the first attempt in the literature to estimate positional bias, externalities and unbiased user perceived ad quality from user click logs in a combined model. we finally evaluate the proposed model on two real datasets, each containing over million ad impressions obtained from a commercial search engine. the experimental results show that tcm outperforms two other competitive methods at click prediction. commercial web search engines typicallygenerate revenue bypresentingsponsored results as well as organic web results to satisfy a user query. the most commonly employed payment model is pay per click, where an advertiser pays the search engine when a user clicks ontheir ad. the cost of a clickdepends on thequality andbid of competing ads, and is usuallydeterminedby a secondprice auction. marketsbybidding only onthesearch terms interesting to them, to explore di erent markets with minimal risk, and toiterate andimprovequickly their campaignsby using immediate feedback from performance. the combination of these features makes sponsored search one of the most attractive andpro table advertising approaches. the objective of the search enginein the sponsored search modelisto maximizeitsrevenueoverthelong term. this involves adelicatebalance ofpossibly con icting objectives: maximize the revenue per search, minimize thenegativeimpact of adsonthe user experience, and maximize the return on investment for advertisers. estimating the probability that users click on ads displayed in response to their queries is essential to sponsored search, because accurate predictions are necessary to address the objectives above. in particular, the click probability is a factor in ranking, placement, ltering, and pricing of ads. several studies have been published recently analyzing user clickbehaviorin organic web search andin sponsored search advertising. joachims et al conducted an eye tracking experiment to understandthedecision makingprocessofuserswhenbrowsing search engine results. an important nding of this study, now well known as position bias, is that users tend to click less on documents that are shown in lower positions, even when the results were presented in reverse order. to explain the position bias phenomenon, richardson et al proposed the examination hypothesis, which assumes that a result mustbe examinedbeforebeing clicked, and theprobability of being clicked after being examined depends on its user perceived quality. craswell et al later proposed the cascade hypothesis, which assumes that users always examine results in order from top to bottom. under this assumption, results displayed at the top are more likely to be examined than results shown at the bottom, regardless oftheirquality. thecascade modelproposedin makes another strong assumption: the user session ends after the rst click on a result. the right four graphs are statistical analyses for sad. most of the subsequent research on click modeling for search results have focused on relaxing this assumption. the click chain model and the bayesian browsing model both allow the user session to continue with a probability that is dependent on the relevance of the clicked ad. the dynamic bayesian network model proposed by chapelle and zhang alsolets the user session continue after a click, butin their model the probability of continuing the session depends on the quality of the landing page. their model is the rst to separate the perceived relevance of the ads from the actual quality of the landing page. the click models discussed so far do not account for the attractivenessor relevanceof the resultsbelowwhenconsidering the probability of click on a particular ad. in guo andchapelle models, theprobability of clickforthe results might be. ected by the results shown immediately above them, since the examination probability depends on the previous result quality. however, the click probabilities cannot be in uenced by the results presented below. this can be a signi cant limitation, especially in sponsored search, where the advertisers compete andpayforthe userattention. ect of the set of ads on the user behavior is widely accepted and referred to as externalities. ghosh et al proposed the rationality hypothesis to explain this behavior. under their hypothesis users are assumed to be rational. given a set of ads displayed, a user rst compares the qualities of the ads and clicks on the best one. kempe and mahdian later proposed a variation of the cascade model based on this hypothesis. instead of using the top to bottom order of scanning the ads, their model allows each user to have a di erent ordering over the positions. unfortunately both of these models were used only in the context of analyzing the auction mechanisms, therefore there are no results on the accuracy of the click models proposed, or even analysis to show that the externalities exist in the data. in this paper, our objective is to rst show that externalities are indeed present in the sponsored search domain and then investigate how to explain both position bias and externalities in a combined model. we exploit the temporal order of useractions for this analysis. our contributions can be summarized in three points. first, we present a statistical analysis demonstrating that temporal user click sequences are good indicators of ad externalities. second, we propose the positional rationality hypothesis which explains both position bias and externalities. third, based on this hypothesis we develop the temporal click model, which has the properties: foundation: to the best of our knowledge, this is the rst attempt in the literature to estimate positional bias, externalities and unbiased userperceived adquality fromuserclicklogsinacombinedmodel; framework: tcm is based on a strict bayesian framework. closed form representations ofthe adquality and user behavior posteriors can be derived using this framework, making it scalable and computationally. cient to handle the challenges imposed by the voluminous click logs;ectiveness: tcm consistently outperforms two stateof the art models in a number of metrics at click prediction. recent advances in search users click modeling consider both users search queries and click skip behavior on documents to infer the user perceived relevance. tcm characterizes user behavior related to a task as a collective whole. using these biases, tcm is more accurately able to capture user search behavior. most of these models, including dynamic bayesian networks and user browsing models, use probabilistic models to understand user click behavior based on individual queries. the user behavior is more complex when her actions to satisfy her information needs form a search session, which may include multiple queries and subsequent click behaviors on various items on search result pages. previous research is limited to treating each query within a search session in isolation, without paying attention to their dynamic interactions with other queries in a search session. investigating this problem, we consider the sequence of queries and their clicks in a search session as a task and propose a task centric click model. specifically, we identify and consider two new biases in tcm as the basis for user modeling. the first indicates that users tend to express their information needs incrementally in a task, and thus perform more clicks as their needs become clearer. the other illustrates that users tend to click fresh documents that are not included in the results of previous queries. extensive experimental results demonstrate that by considering all the task information collectively, tcm can better interpret user click behavior and achieve significant improvements in terms of ranking metrics of ndcg and perplexity. we conduct experiments with a large scale realworld dataset which shows that the tcm can be scaled up. while previous research seeks to model a userclick behavior based on browsing and click actions after she enters a single query, often several queries are entered sequentially and multiple search results obtained from di erent queries are clicked to accomplish a single search task. clearly, a typical search can include complex user behavior, including multiple queries and multiple clicks for each query, etc. in this paper, we consider the latter, and refer to a search session as a task. as mentioned above, the previous research considers query sessions only, but ignores other sources of information and their relations to the same task. the dbn model, for example, assumes that users are always satis ed with the last click of each query, without considering subsequent queries and clicks. the rst bias indicates that users tend to express their information needs incrementally and then perform more clicks as their needs become clearer. our experiments used more than million search tasks as the research dataset. search engine click through logs are an invaluable resource that can provide a rich source of data on user preferences in their search results. the analysis of click through logs can be used in many search related applications, such as web search ranking, predicting click through rate, or predicting user satisfaction. in analyzing click through logs, a central question is how to construct a click model to infer a userperceived relevance for each query document pair based on a massive amount of search click data. using a click model, a commercial search engine can develop a better understanding of search users behavior and provide improved user services. previous investigations of click models include dynamic bayesian networks, the user browsing model, the click chain model and the pure relevance model. a user may rst issue a query, examine the returned results and then click on some of them. if the existing results do not satisfy her information needs, she may narrow her search and reformulate her query to construct a new query. this process can be repeated until she nds the desired results or gives up. collectively, all the useractions provide an overall picture of the userintention as she interacts with the search engine. the multiple queries, clicked results, and underlying documents are all sources of information that can help reveal the usersearch intent. traditionally, user sessions are obtained from a consecutive sequence of user search and browsing actions within. these sessions can be partitioned into two categories: a session and a search session, where the former refers to the browsing actions for an individual query while the latter encompasses all queries and browsing actions that a user performs to satisfy her information need. thus, most previous research su ers from a lack of accuracy in many cases. the above line of thinking has led us to consider the advantage of a task centric click model in this paper for understanding and predicting click behavior. in this paper, we rst point out the necessity of modeling task level user behavior by letting the real data speak for itself. we then de ne and describe two new user biases that in uence a search task but have been ignored in previous investigations. the second bias illustrates that users tend to click on fresh documents that they have not seen before under the same task. we design our tcm using a probabilistic bayesian method to address these two biases. tcm is general enough to integrate most other existing click models. ectiveness of the tcm by comparing its performance to the dbn and ubm models. the experimental results show that, by considering all of the task information, the tcm can better model and interpret user click behavior and achieve signi cant improvements in terms of ndcg and perplexity. recent advances in click models have positioned them as an effective approach to the improvement of interpreting click data, and some typical works include ubm, dbn, ccm, etc. after formulating the knowledge of user search behavior into a set of model assumptions, each click model developed an inference method to estimate its parameters. the inference method plays a critical role in terms of accuracy in interpreting clicks, and we observe that different inference methods for a click model can lead to significant accuracy differences. in this paper, we propose a novel bayesian inference approach for click models. this approach regards click model under a unified framework, which has the following characteristics and advantages: this approach can be widely applied to existing click models, and we demonstrate how to infer dbn, ccm and ubm through it. this novel inference method is based on the bayesian framework which is more flexible in characterizing the uncertainty in clicks and brings higher generalization abilities. as a result, it not only excels in the inference methods originally developed in click models, but also provides a valid comparison among different models; in contrast to the previous click models, which are exclusively designed for the position bias, this approach is capable of capturing more sophisticated information such as bm and pagerank score into click models. this makes these models interpret click through data more accurately. experimental results illustrate that the click models integrated with more information can achieve significantly better performance on click perplexity and search ranking; because of the incremental nature of the bayesian learning, this approach is scalable to process large scale and constantly growing log data. in a commercial search engine, terabytes of click through logs are generated every day at very low cost. these clickthrough logs encode valuable user preferences with regard to search results and reveal the latest tendency of user click behaviors. naturally, many studies have attempted to discover user preferences from click through logs in order to improve web search ranking. indeed, after the pioneering work of joachims et al, which uses preferences automatically generated from click through logs to train a ranking function, many interesting works have been proposed to estimate document relevance from user clicks. it has been noticed in existing works that one major dif culty in estimating relevance from click data comes from a so called position bias: a document appearing in a higher position is more likely to attract user clicks even though it is not as relevant as documents in lower positions. richardson et al proposed to increase the relevance of documents in lower positions by a multiplicative factor. this idea was later formalized as the examination hypothesis and adopted in the position model. the examination hypothesis assumes the user will click a search result only after examining the search snippet. craswell et al extended the examination hypothesis and proposed the cascade model by assuming that the user will scan search results from top to bottom. dupret and piwowarski introduced the positional distance into their ubm model. recently, guo et al proposed the ccm model and chappell et al proposed the dbn model, both of which generalize the cascade figure #: the perplexity score on di erent query frequencies achieved by the ubm model with maximum log likelihood and maximum posteriori methods, respectively. model by assuming that the probability of examining the current document is related to the relevance of the document in the previous position. click models, such as ubm, dbn and ccm, have been demonstrated to be much more successful than the simple counting approach in interpreting click data. each of these models introduced a set of assumptions integrating the knowledge of user browsing and click behaviors and developed an inference method. the inference methods in existing click models are often di erent with each other. for example, ubm used the em algorithm to optimize the parameter through maximizing the likelihood function. dbn also used the em algorithm, however, it tried to maximize the posterior function. ccm was a bayesian approach and it approximated the posterior distribution through a multinomial distribution. we observe that the inference method plays an important role in terms of accuracy in click prediction. as illustrated in figure #, the click perplexity of ubm can be signi cantly improved for low frequent queries after switching from the maximum likelihood estimation to the maximum posterior estimation method. thus, the difference of the inference methods in click models makes the comparison di cult. we cannot identify that the performance di erence between two click models is due to either the model assumption or the inference method. moreover, we nd there is large space to develop a new inference approach for improving the existing model accuracy. in this paper, we propose a novel inference approach which can be widely applied to existing click models. the new approach is based on the bayesian framework. it replaces each probability variable in click models with a new variable following the gaussian distribution through a probit link function, such that both the prior and the posterior distribution of the bayesian learning can be approximated by gaussians. we show that this inference approach is computationally tractable for all click models in which the likelihood functions are in the multinomial form. this requirement is general enough to be tted into most of the existing click models. we call the new proposed inference approach the probit bayesian inference. accordingly, the pbi approach can provide valid evaluation to compare di erent click models. we apply pbi to three state of the art click models, such as ubm, dbn and ccm, and the experiments show that the new approach consistently achieves better performance than the original inference algorithm of these models. another challenge with previous click models is that they are designed for position bias exclusively. however, we observe that a click may be. ected by the relevance between a query and a snippet, which can be measured by the bm score. the pbi approach is capable of capturing more sophisticated information into a click model to interpret user clicks accurately. in this paper, we include seven measures such as bm and pagerank scores into the previous click models through pbi, and the experimental results demonstrate that the integration of these measures yields signi cant improvement in perplexity and relevance. furthermore, pbi is an incremental approach, thus it is natural to handle very large scale data set. the paper is organized as follows: section # brie. introduces previous works on click models including their speci cations and hypothesis. in section #, the pbi approach will be presented in detail. in section #, we give three examples on how the pbi approach is applied to the ubm, ccm and dbn click models. in section #, we demonstrate how to integrate additional measures into the click models. section # reports the experimental results and the conclusion follows.