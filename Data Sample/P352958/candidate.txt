we focus on the problem of query rewriting for sponsored search. we base rewrites on a historical click graph that records the ads that have been clicked on in response to past user queries. given a query, we first consider simrank as a way to identify queries similar to, ie, queries whose ads a user may be interested in. we argue that simrank fails to properly identify query similarities in our application, and we present two enhanced versions of simrank: one that exploits weights on click graph edges and another that exploits evidence. we experimentally evaluate our new schemes against simrank, using actual click graphs and queries form yahoo, and using a variety of metrics. our results show that the enhanced methods can yield more and better query rewrites. this paper presents a simple and intuitive method for mining search engine query logs to get fast query recommendations on a large scale industrial strength search engine. in order to get a more comprehensive solution, we combine two methods together. on the one hand, we study and model search engine users sequential search behavior, and interpret this consecutive search behavior as client side query refinement, that should form the basis for the search engine own query refinement process. on the other hand, we combine this method with a traditional content based similarity method to compensate for the high sparsity of real query log data, and more specifically, the shortness of most query sessions. to evaluate our method, we use one hundred day worth query logs from sina search engine to do off line mining. then we analyze three independent editors evaluations on a query test set. based on their judgement, our method was found to be effective for finding related queries, despite its simplicity. in addition to the subjective editors rating, we also perform tests based on actual anonymous user search sessions. providing related queries for search engine users can help them quickly nd the desired content. recently, some search engines started showing related search keywords in the bottom of the result page. their main purpose is to give search engine users a comprehensive recommendation when they search using a speci. recommending the most relevant search keywords set to users not only enhances the search enginehit rate, but also helps the user to nd the desired information more quickly. also, for some users who are not very familiar with a certain domain, we can use the queries that are used by previous similar searchers who may part of this work was done while the rst author was at search engined group, sina corporation, beijing, china have gradually re ned their query, hence turning into expert searchers, to help guide these novices in their search. that is, we can get query recommendations by mining the search engine query logs, which contain abundant information on past queries. search engine query log mining is a special type of web usage mining. in, a content ignorant approach and a graph based iterative clustering method was used to cluster both the urls and queries. later in, the authors presented a well rounded solution for query log clustering by combining content based clustering techniques and cross reference based clustering techniques. in, methods to get query recommendation by utilizing the click through data were presented. a reliable query similarity measure can be used in a variety of applications such as query recommendation, query expansion, and advertising. our approach relies on the concept of the query flow graph. the query flow graph aggregates query reformulations from many users: nodes in the graph represent queries, and two queries are connected if they are likely to appear as part of the same search goal. our query similarity measure is obtained by projecting the graph on a low dimensional euclidean space. our experiments show that the measure we obtain captures a notion of semantic similarity between queries and it is useful for diversifying query recommendations. defining a measure of similarity between queries is an interesting and difficult problem. in this paper, we exploit the information present in query logs in order to develop a measure of semantic similarity between queries. finding a measure of similarity between queries can be very useful to improve the services provided by search engines. first, the ability to identify similar queries is in the core of any query recommendation system. second, query similarity can be used for performing query expansion. additionally, a reliable notion of query similarity can be used for broad matching of advertisements to queries, or even for suggesting keywords to advertisers. however, de ning a part of this work was done while visiting yahoo research labs, barcelona permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. query similaritymeasureisnot aneasytask asit stronglydepends on user intent: syntactically similar queries may originate from completely di erent intents. issues such as polysemy, synonymy, high levels of noise, and the small amount of available information make the problem challenging. in such a complex setting, information extracted from query logs has shown to be. the information on how users interact with search engines has often been used toimprovethe user search experience. inparticular, querylog analysis is used to provide insights on how users re ne their queries, and what kind of search strategies they are using to locate the information they need. in this paper we describe a method of obtaining a querysimilarity measure, based onquery log analysis. our method relies on an aggregated representation of a query log by the means of a reformulation graph, which is known as the query ow graph. in this graph, nodes represent queries and twoqueries are connectedif they arelikely to appear as part of the same search goal. our mainintuitionisthatrelatedquerieswill tendtocluster in local neighborhoods of the graph. thus, we suggest projecting the query ow graph and then measure query similarity on the resulting geometric space. we use the resulting query similarity measure for diversifying query recommendations. query recommendation systems are provided by all major search engines and they aim at helping users to nd more easily what they are searching for. the idea is that a diversi cation algorithm requires a notion of query similarity, for which we use the proposed measure. our main contributions are summarized as follows: wedescribea methodfor measuring similaritybetween queries by projecting a query reformulation graph; we show that our similarity measure captures the human notion of related queries better than other measures on the original graph; weapply this method tothetask ofproducing diverse and useful recommendations; we show how to improve its. ciency further, by projectingonly the neighborhood of the input query. the rest of this paper is organized as follows: section # discusses related work. sections and describe the framework we use to de ne our measures of query similarity, and section # explores di erent variants for optimizing our measures. graph projection methods areknownto mapgraph nodesintogeometricspaces so that the distance distortion is minimized. the technique is general and it can be applied to other graphs obtained from query logs, for example, the clickgraph. section # describes our application for diversifyingquery recommendations andsection. topic modeling has been a key problem for document analysis. one of the canonical approaches for topic modeling is probabilistic latent semantic indexing, which maximizes the joint probability of documents and terms in the corpus. the major disadvantage of plsi is that it estimates the probability distribution of each document on the hidden topics independently and the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with overfitting. latent dirichlet allocation is proposed to overcome this problem by treating the probability distribution of each document over topics as a hidden random variable. both of these two methods discover the hidden topics in the euclidean space. however, there is no convincing evidence that the document space is euclidean, or flat. therefore, it is more natural and reasonable to assume that the document space is a manifold, either linear or nonlinear. in this paper, we consider the problem of topic modeling on intrinsic document manifold. specifically, we propose a novel algorithm called laplacian probabilistic latent semantic indexing for topic modeling. lapplsi models the document space as a submanifold embedded in the ambient space and directly performs the topic modeling on this document manifold in question. we compare the proposed lapplsi approach with plsi and lda on three text data sets. experimental results show that lapplsi provides better representation in the sense of semantic structure. despite its remarkable success in different domains, lsi has a number of de cits, mainly due to its unsatisfactory statistical formulation. this leads to serious problems with over tting. document representation has been a key problem for document analysis and processing. the vector space model might be one of the most popular models for document representation. in vsm, each document is represented as a bag of words. correspondingly, the inner product is used as the standard similarity measure for documents or documents and queries. unfortunately, it is well known that vsm has severe drawbacks, mainly due to the ambiguity of words and the personal style and individual differences in word usage. to deal with these problems, ir researchers have proposed several dimensionality reduction techniques, most notably latent semantic indexing. the general claim is that similarities between documents or between documents and queries can be more reliably estimated in the reduced latent space representation than in the original representation. lsi received a lot of attentions during these years and many variants of lsi have been proposed. to address this issue, hofmann proposed a generative probabilistic model named probabilistic latent semantic indexing. plsi models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of topics. each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on. this distribution is the reduced representation associated with the document. the major disadvantage of plsi is that it estimates the probability distribution of each document on the hidden topics independently and the number of parameters in the model grows linearly with the size of the corpus. latent dirichlet allocation is then proposed to overcome this problem by treating the probability distribution of each document over topics as aparameter hidden random variable rather than a large set of individual parameters, where theis the number of hidden topics. both of the above two topic modeling approaches discover the hidden topics in the euclidean space. however, there is no convincing evidence that the documents are actually sampled from a euclidean space. recent studies suggest that the documents are usually sampled from a nonlinear low dimensional manifold which is embedded in the high dimensional ambient space. thus, the local geometric structure is essential to reveal the hidden semantics in the corpora. in this paper, we propose a new algorithm called laplacian probabilistic latent semantic indexing. lapplsi considers the topic modeling on the document manifold. it models the document space as a submanifold embedded in the ambient space and directly perform the topic modeling on this document manifold in question. by discovering the local neighborhood structure, our algorithm can have more discriminating power than plsi and lda. speci cally, lapplsi rst builds an nearest neighbor graph to model the local document manifold structure. it is natural to assume that two suf ciently close documents have similar probability distribution over different topics. the nearest neighbor graph structure is then incorporated into the log likelihood maximization as a regularization term for lapplsi. in this way, the topic model estimated by lapplsi maximizes the joint probability over the corpus and simultaneously respects the local manifold structure. it is worthwhile to highlight several aspects of our proposed algorithm here: the conventional generative probabilistic modeling approaches, eg, plsi and lda, discover the hidden topics in the euclidean space. our approach considers the problem of topic modeling directly on the document manifold in question and discovers the hidden topics. lsi uses a singular value decomposition of the term document matrixto identify a linear subspace that captures most of the variance in the data set. dyadic data arises in many real world applications such as social network analysis and information retrieval. in order to discover the underlying or hidden structure in the dyadic data, many topic modeling techniques were proposed. the typical algorithms include probabilistic latent semantic analysis and latent dirichlet allocation. the probability density functions obtained by both of these two algorithms are supported on the euclidean space. however, many previous studies have shown naturally occurring data may reside on or close to an underlying submanifold. we introduce a probabilistic framework for modeling both the topical and geometrical structure of the dyadic data that explicitly takes into account the local manifold structure. specifically, the local manifold structure is modeled by a graph. the graph laplacian, analogous to the laplace beltrami operator on manifolds, is applied to smooth the probability density functions. as a result, the obtained probabilistic distributions are concentrated around the data manifold. experimental results on real data sets demonstrate the effectiveness of the proposed approach. dyadic data refers to domain where two sets of objects, row or column objects, are characterized by a matrix of numerical values which describe their mutual relationships. such data arises in many real world applications such as social network analysis and information retrieval. a common example is term document co appearing in proceedings of the th international conference on machine learning, montreal, canada, year#. in order to discover the underlying or hidden structure in the dyadic data, topic modeling techniques are usually applied to learn a probabilistic interpretation of the row and column objects. two of the most popular approaches for this purpose are probabilistic latent semantic indexing and latent dirichlet allocation. in the dyadic aspect model applied to text analysis, a corpus of document is modeled as a set of pairs, whereis a document index andis a word index. each document is represented as a unique distribution over thesettings of the latent variable. each setting of the latent variablecorresponds to an underlying topic. associated with each topic is a distribution over words in the vocabulary. thus, a document is seen as a distribution over topics where each topic is described by a different distribution over words. a word is generated for a document by choosing a topic and then selecting a word according to the distribution over words for the chosen topic. plsa has been shown to be a low perplexity language model and outperforms latent semantic indexing in terms of precision recall on a number of document collections. however, the number of parameters of plsa grows linearly with the number of documents, which suggests that plsa is prone to over tting. lda was introduced to address this problem by incorporating a dirichlet regularization on the underlying topics. these two approaches do yield impressive results on exploratory dyadic data analysis. however, both of them fails take into account the geometry of the spaces where the objects reside. the learned probability distributions are simply supported on the ambient spaces. recent studies have shown that naturally occurring data, such as texts and images, cannot possibly ll up the ambient euclidean space, rather it must concentrate around lower dimensional structures. the goal of this paper is to extract this kind of low dimensional structure and use it to regular the parameters can be estimated by maximizing the logize the learning of probability distributions. we construct likelihood a nearest neighbor graph to model the underlying manifold nm structure. the graph laplacian, analogous to the laplaceln logp beltrami operator on manifolds, is then used as a smooth ing operator applied to the conditional probability distri nmi butions pz. ments should have similar conditional probability distribullogp tions. we use kullback leibler divergence to measure the distance between two conditional probability distributions. the local consistency is incorporated into the probabilistic modeling framework through a regularizer. we discuss how to solve the regularized log likelihood maximization problem using expectation maximization techniques. the rest of the paper is organized as follows. section # provide a background of dyadic data analysis. our locallyconsistent topic modeling approach is introduced in section #. a variety experimental results are presented in section #. finally, we give concluding remarks in section #. query classification is a task that aims to classify web queries into topical categories. since queries are usually short in length and ambiguous, the same query may need to be classified to different categories according to different people perspectives. in this paper, we propose the personalized query classification task and develop an algorithm based on user preference learning as a solution. users preferences that are hidden in clickthrough logs are quite helpful for search engines to improve their understandings of users queries. we propose to connect query classification with users preference learning from clickthrough logs for pqc. to tackle the sparseness problem in clickthrough logs, we propose a collaborative ranking model to leverage similar users information. experiments on a real world clickthrough log data show that our proposed pqc algorithm can gain significant improvement compared with general qc as well as natural baselines. our method can be applied to a wide range of applications including personalized search and online advertising. with the exponentially increasing amount of information available on the internet, web search has become an indispensable tool for web users to gain their desired information. typically, web users submit a short query consisting of a few words to search engines and expect to satisfy their information need. however, because these queries are usually short and ambiguous, how to interpret them is not trivial. this problem has become a major research issue in ir community. in this work, we refer to the problem of classifying queries to a set of topical categories as the query classi cation problem, or qc for short. the importance of qc is emphasized by many services provided by web search. a direct application of qc is to provide better search result pages for users with interests of di erent categories. search result pages can be grouped according to the categories predicted by a qc algorithm to give better user experiences. online advertising services can be enhanced by qc results to promote di erent products more accurately. since web queries are usually short and ambiguous, one same query may belong to di erent categories for di erent people. this di culty is illustrated in table #, which shows the labeling results from three labelers in a query classi cation competition. in the table, the three human labelers, denoted as, and, respectively, gave each query at most ve labels in a ranked order. the average precision and score values of each labeler are evaluated against the labeling results of the other two labelers. the average values among the three labelers are around, which indicates that the categories of the same queries are di erent for di erent people. from this example, we can see that it is di cult for humans and search engines to predict to which categories the query belongs for a particular user. due to the di culty in solving the qc problem, search engines often treat queries as if they belong to all probable popular categories, in order to make the provided services. for example, the search results are often reorganized to make returned web pages diverse in topics. in online advertising, for example, when a user submits a query apple, the advertisements related to di erent interpretations of the term are returned, such as ipod, macintosh and apple. in many search services, no matter whether the user is a farmer or a table #: the score of each labeler evaluated based on scores of the other two labelers as ground truth on the kddcup dataset. avg precision figure #: an example result page from a commercial search engine with query. the returned web pages contain di erent topics ranging from microsoft software product. the rest of this paper is organized as follows. in section #, we will rst survey the related work. in section #, we will introduce our overall model for pqc as well as the algorithm for learning user preferences. then in section #, we will present our experimental results on real world datasets. section # concludes this paper and also provides the future work we plan to investigate on this problem. search engines can record which documents were clicked for which query, and use these query document pairs as soft relevance judgments. however, compared to the true judgments, click logs give noisy and sparse relevance information. we apply a markov random walk model to a large click log, producing a probabilistic ranking of documents for a given query. a key advantage of the model is its ability to retrieve relevant documents that have not yet been clicked for that query and rank those effectively. we conduct experiments on click logs from image search, comparing our random walk model to a different random walk, varying parameters such as walk length and self transition probability. the most effective combination is a long backward walk with high self transition probability. a search engine can track which of its search results were clicked for which query. for a popular system, these click records can amount to millions of query document pairs per day. each pair can be viewed as a weak indication of relevance: that the user decided to at least view the document, based on its description in the search results. although clicks are not real judgments, there is evidence that they are useful, for example as training data, as annotations, for query suggestion or directly as evidence for ranking. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. we can use the clicks of past users to improve the current search results. however, the clicked set of documents is likely to di er from the current userrelevant set. other di erences are due to presentation issues; for example, the user must decide whether to click based on a short summary and is in uenced by the ordering of results. for any given search, a large number of documents are never seen by the user, therefore not clicked. from the perspective of a user conducting a search, documents that are clicked but not relevant constitute noise in the click data. documents that are relevant but not clicked constitute sparsity in the click data. one class of approaches attempts to reduce noise in click data, by building a click model that may use additional information about the userbehaviour. these approaches can signi cantly reduce noise, by identifying some clicked documents as irrelevant. this paper focuses on the sparsity problem, although our model also has noise reduction properties. the model gives a probabilistic ranking of documents, which includes relevant documents that have not yet been clicked for the current query. the sparsity problem is evidenced by power law distributions observed in click logs. most queries in the click log have a small number of clicked documents. in such cases, it is useful to identify additional relevant documents. we rst describe the click information as a graph, and survey a range of click graph applications. then we detail our markov random walk model for nding relevant documents. the subsequent sections describe a real click dataset, and empirical evaluation of the new methods. some differences arise because we are aggregating clicks across users, who may simply disagree about which documents are relevant. for example, taking into account the userbrowsing patterns after clicking a document. query log analysis has received substantial attention in recent years, in which the click graph is an important technique for describing the relationship between queries and urls. state of the art approaches based on the raw click frequencies for modeling the click graph, however, are not noise eliminated. nor do they handle heterogeneous query url pairs well. in this paper, we investigate and develop a novel entropy biased framework for modeling click graphs. the intuition behind this model is that various query url pairs should be treated differently, ie, common clicks on less frequent but more specific urls are of greater value than common clicks on frequent and general urls. based on this intuition, we utilize the entropy information of the urls and introduce a new concept, namely the inverse query frequency, to weigh the importance of a click on a certain url. the iqf weighting scheme is never explicitly explored or statistically examined for any bipartite graphs in the information retrieval literature. we not only formally define and quantify this scheme, but also incorporate it with the click frequency and user frequency information on the click graph for an effective query representation. to illustrate our methodology, we conduct experiments with the aol query log data for query similarity analysis and query suggestion tasks. experimental results demonstrate that considerable improvements in performance are obtained with our entropy biased models. moreover, our method can also be applied to other bipartite graphs. recently query log analysis has been studied widely for improving search engines. such stud permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. ies mined the logs to improve numerous search enginecapabilities, such as query suggestion and classi cation, ranking, targetedadvertising, etc. theclickgraph, abipartite graphbetweenqueries andurls, is animportanttechnique for describing the information contained in the query logs, in which edges connect a query with the urls that were clicked by users as a result. an example of a click graph with queries and urls is depicted in figure #. the edges of the graph can capture some semantic relations between queries and urls. for example, queries map and travel are related to each other, since theyare co clickedwith some urls such as www mapquest com and so on. therefore, howtoutilizeand model theclickgraph torepresentqueries becomes an interesting and challenging problem. traditionally, the edge ofthe clickgraphis weightedbased on the raw clickfrequency from aquery to a url. the transition probability can be further determinedbythenormalized clickfrequency. takingthe edgefrom map to www mapquest com infigure asanexample, the rawclickfrequencyis and thenormalized click frequencyis. however, thetraditionalqueryrepresentation for the click graph has its own disadvantages. one of thesedisadvantagesisits robustness,e, aquery thathasa skewed click count on a certain url may exclusively in uence the click graph, such as navigational queries. ect on learning algorithms, previous work presented in simply identi ed some navigational queries and removed them from the click graph. unfortunately, the deletion of such queries leads to the loss of some information. another related problem is that the raw click frequencycanbeeasilymanipulatedasitispronetospamby some malicious clicks. to deal with these critical problems, weexploreanovel entropy biasedframework whichincorporates raw click frequencies and other information with the entropy information of the connected urls. the basic idea of the entropy biased model is that various query url pairs should be treated di erently. let us look atthequery map and its connected urls, which is shown in figure #. the click frequency from to is the same as the count from to. there is a critical question when only consider the raw click frequency: is a single click on di erent urls in the click graph equally important clearly not in this case, at an intuitive level, one click on may capture more meaningful information, or be more important than one click on. the key difference is that the connected urls are di erent: one url is www mapquest com, which is connected with queries; while anotherurlis www yahoo com, whichis connected with queries. suppose there is a url which is commonly clicked and connected with most of thequeries, this tends to increase the ambiguity of the url. however, if the url is clicked and connected with fewer queries, this tends to increase the speci city of the url. a frequently clicked url thus functions in retrieval as a nonspeci curl, eventhoughits meaning maybequitespeci. url is most likely to be more important for distinguishing the speci city of the query than another click on an ambiguous url. based on the above intuition, we introduce a new concept, denoted as the inverse query frequency, to weigh the importance of a click on a certain url, which can be extended and used for other bipartite graphs. consequently, we propose a novel entropy biased model, namely cf iqf model, torepresentthequery, which simultaneously combinestheinversequery frequency information with the raw click frequency. as the raw click frequency can be easily manipulated, we develop and use the number of users associated with the query url pair, namely the user frequency, instead of the raw click frequency toimprovethe resistanceagainst maliciousclick data. moreover, the inverse query frequency can be incorporated with the user frequency, as another entropy biased uf iqf model, toachievebetterperformance. toillustrate our methodology, we apply the entropy biased models to query similarity analysis and query suggestion tasks using the real world aol query log data. the main concern is to increase the precision of the topretrieved results. for thequery similarity analysis, wecomparesixdi erent models, includingfour models based ontheclickgraph and two models based on the query terms. it is shown that cf iqf model improvesovercf modelby up to, whileuf iqf over uf by up to. as expected, uf iqf and uf outperform cf iqf and cf respectively. in addition, uf iqf model signi cantly improves the traditional tf idf model by up to. for the query suggestion task, evaluation results also show that the entropy biased models outperform thebaseline models, indicatingthattheimprovementsinour proposed models are consistent and promising. the rest of this paper is organized as follows. in section # we present the proposed query representation models. section # describes two basic applications of these models, which are the query similarity analysis and query suggestion. we then describe and report the experimental evaluation in section #. finally, we present our conclusions and future work in section #. query recommendation has been recognized as an important mean to help users search and also improve the usability of search engines. existing approaches mainly focus on helping users refine their search queries and the recommendations typically stick to users search intent, named search interests in this paper. however, users may also have some vague or delitescent interests which they are unaware of until they are faced with one, named exploratory interests. these interests may be provoked within a search session when users read a web page from search results or even follow links on the page. by considering exploratory interests in query recommendation, we attract more user clicks on recommendations. this type of query recommendation has not been explicitly addressed in previous work. in this paper, we propose to recommend queries in a structured way for better satisfying both search and exploratory interests of users. specifically, we construct a query relation graph from query logs and social annotation data which capture two types of interests respectively. based on the query relation graph, we employ hitting time to rank possible recommendations, leverage a modularity based approach to group top recommendations into clusters, and label each cluster with social tags. empirical experimental results indicate that our structured approach to query recommendation with social annotation data can better satisfy users interests and significantly enhance users click behavior on recommendations. this paper addresses the problem of named entity recognition in query, which involves detection of the named entity in a given query and classification of the named entity into predefined classes. we consider contexts of a named entity as words of a document, and classes of the named entity as topics. the topic model is constructed by a novel and general learning method referred to as ws lda, which employs weakly supervised learning using partially labeled seed entities. experimental results show that the proposed method based on ws lda can accurately perform nerq, and outperform the baseline methods. the paper proposes taking a probabilistic approach to the task using query log data and latent dirichlet allocation. nerq is potentially useful in many applications in web search. classes of named entities can be, for instance, book, movie, game, and music. nerq is essentially useful for many applications in web search. according to our analysis, about of search queries contain named entities. as far as we know, there was no previous work on nerq. traditionally named entity recognition is mainly performed on natural language texts. however, direct application of exiting ner technologies to nerq would not perform well. in this paper, we propose a new probabilistic approach to nerq using query log data. without loss of generality, a query having one named entity is represented as a triple, wheredenotes named entity, context of, andclass of. then the goal of nerq here becomes to nd the triple for a given query, which has the largest joint probability pr. when there are multiple entities appearing in a query, our method can still be applied by viewing the more popular one as named entity and the rest as context. our approach is in part inspired by the work. they proposed a method for acquiring named entities from query log using templates. we employ a probabilistic model, while they take a deterministic approach in the sense that they assume that each named entity can only belong to one class. our contribution in this paper lies in the following points. in this paper we address a novel problem in web search, namely named entity recognition in query. in the task given a query we are to detect the named entity within the query and identify the most likely classes of the named entity. given query harry potter walkthrough, we detect harry potter as a named permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. this is because the context walkthrough strongly indicates that harry potter here is more likely to mean the harry potter game. identifying named entities in queries would help us to understand search intents better, and therefore provide better search. for example, in relevance search, we can improve ranking by treating named entity and context separately; in query suggestion, we can generate more relevant suggestions, eg, harry potter walkthrough. this is because queries are usually very short and are not necessarily in standard form, and thus the features are not su cient for performing accurate nerq. we have proposed a novel method for conducting nerq. the rest of the paper is organized as follows. entity and assign game to it as the most likely class, movie and book as less likely classes, and music as unlikely class. usually a supervised learning approach is exploited and a set of features is utilized. the joint probability is factorized and then estimated by using query log and lda. in the lda model, contexts of a named entity are represented as words of a document, classes of the named entity are represented as topics of the model. the alignment between model topics and prede ned classes needs to be guaranteed. to address this problem, we propose a weakly supervised learning method, referred to as ws lda, which can leverage the weak supervision from humans. here we only consider queries which contain single named entities. there are some di erences between their work and ours. we have developed a new topic modeling method with weakly supervised learning, ie, ws lda. section # de nes the problem of nerq and proposes a probabilistic approach to the task. the task of the kdd cup year# competition was to classify, internet user search queries into predefined categories. this task is easy to understand, but the lack of straightforward training set, subjective user intents of queries, poor information in short queries, and high noise level make the task very challenge in this paper, we summarize the competition task, the evaluation method, and the results of the competition. here we only highlight some key techniques used in submitted solutions. at the end, we also share the results of a survey conducted with this year cup participants. to facilitate research in this area, the task description, data, answer set, and related information of this kdd cup are published at the kdd cup year# web site: http: www acm org sigs sigkdd kdd kddcup html. the kdd cup year# competition was held in conjunction with the eleventh acm sigkdd international conference on knowledge discovery and data mining. the technical details of the solutions from the three award winning teams are available in their papers separately in this issue of sigkdd explorations. given the exponential growth of informationavailability in electronic form, search becomes one of the most important and effective approaches to finding correct relevant information to serve our needs. a user can type in key words in a search engine to find out where to buy a product and whether a certain price is a good price. a user can also find travel attractions to fit his her interests. if a user is interested in certain medicine, he she can find plenty related information including its usage and potential side effects. researchers can easily find the latest development of a research topic. to plan a hiking trip for a coming weekend, one can find the weather forecast simply through a search. these are just a few examples of how search can help a userdaily life. although researchers and industry practitioners have achieved tremendous success in developing smart search engines, we are zijian zheng was at amazon com during the time of kdd cup year#. still facing many great challenges as current search engines are not very accurate. the difference is still quite big between what search engines can do and what we expect them to do. it is not uncommon that a search engine returns irrelevant, misleading or, incorrect results after you type in a query. in another time, the relevance results are returned but down to the bottom of a long result list. due to the nature that huge amount data is available from each search engine and many problems of search can be turned into learning or modeling problems, there is a great potential for data mining techniques to contribute to the success of search. since late, researchers and practitioners have been studying search query data, trying to find search patterns, understanding search user intents, and providing personalized search. a survey on search related research is available. the other side of search being a difficult problem is that the information contained inside the data is often incomplete, fuzzy, and indirect. all of these present big challenges to the data mining community. in kdd cup year#, we presented one challenge problem: search query categorization. manning and schtüze discusses general methodologies and applications of text categorization. most work in this area has been focused on categorizing web pages or longer text or corpus. however, search query classification is very different in the sense that queries are usually very short on the one hand, and with implicit and subjective user intents on the other hand. therefore, how to automatically understand user search intents given the search queries would be very interesting to ir and text mining researchers. in this report, we first describe the competition task presented to the participants in section #, including a discussion on why this task is challenging. in section #, we present the evaluation method. then, we highlight the interesting techniques from the submissions in section #, and analyze the overall results as a whole in section #. computer search including internet search has become a part of many peopledaily life and work. the intents of search engine users are highly subjective. the detailed presentations of techniques from the three winning teams are available as three separate papers in this issue of sigkdd explorations. text classification and categorization is a well known topic in information retrieval and text mining fields. in this paper, aiming at providing semantically relevant queries for users, we develop a novel, effective and efficient two level query suggestion model by mining clickthrough data, in the form of two bipartite graphs extracted from the clickthrough data. based on this, we first propose a joint matrix factorization method which utilizes two bipartite graphs to learn the low rank query latent feature space, and then build a query similarity graph based on the features. due to the complexity of the web structure and the ambiguity of users inputs, most of the suggestion algorithms suffer from the problem of poor recommendation accuracy. for a given query raised by a specific user, the query suggestion technique aims to recommend relevant queries which potentially suit the information needs of that user. after that, we design an online ranking algorithm to propagate similarities on the query similarity graph, and finally recommend latent semantically relevant queries to users. experimental analysis on the clickthrough data of a commercial search engine shows the effectiveness and the efficiency of our method. or commercial advantage and that copies terface for web users to obtain any kind of information they may seek. queries containing ambiguous terms may confuse the search engine into retrieving web pages which do not satisfy the information needs of users. another consideration, as reported in, is that users tend to submit short queries consisting of only one or two terms under most circumstances, and short queries are more likely to be ambiguous. however, due to the commercial reasons, few public papers have been released to unveil the methods they adopt. in fact, clickthrough data is an ideal source for mining relevant queries. bear this notice and the full citation on the rst page. in this paper, by analyzing the clickthrough data, we develop a query suggestion framework using two level latent semantic analysis. then we build a query graph based on the representation of query space. we evaluate our model from di erent angles: first, it is assessed by a panel of three experts. ciency of our online query suggestion algorithm by measuring how much cpu time that it needs. the results show that our method is both. cient for improving the recommendation quality, as well as generating semantically related queries to users. the rst one is the ambiguity which commonly exists in the natural language. moreover, this noise is not easily removed by machine learning methods. in order to avoid these problems, some additional data sources are likely to be very helpful to improve the recommendation quality. however, most of these references extract only the query url bipartite graph of the clickthrough data for analysis, and ignore the information of users who issued the queries. we rst extract two bipartite graphs, which are user query and query url bipartite graphs. section # presents the similarity propagation model as well as the method for recommending queries. the rest of the paper is organized as follows. with the exponential growth of information on the world wide web, web search engines provide an indispensable in permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. although current commercial search engines have been proved to be successful for recommending the most relevant web pages to users, there are several outstanding issues that can potentially degrade the quality of search results, and these merit investigation. through the analysis of a commercial search enginequery logs recorded over three months in year#, we observe that of web queries are single term queries, and a further of web queries contain only two terms. thirdly, in most cases, the reason why users search is that they have little or even no knowledge about the topic they are searching for. in order to nd satisfactory answers, users have to rephrase their queries constantly. to overcome all of these problems, a valuable technique, query suggestion, has been employed by some famous commercial search engines, such as yahoo, live search, ask and google, to recommend relevant queries to users. typically, query suggestion is based on local and global document analysis, or anchor text analysis. however, these traditional methods have di culty summarizing the latent meaning of a web document due to the huge noise embedded in each web page. in the typical search scenario, a user initiates a query, and submits it to a search engine. the search engine returns a set of ranked related web pages or documents to this user. the user then clicks some pages of interest. some users even re ne their queries in order to nd the desired information. therefore, the collection of queries is likely to well re ect the relatedness of the target web pages. to copy otherwise, to http: www yahoo com republish, to post on servers or to redistribute to lists, requires prior speci. http: www live com permission and or a fee. http: www ask com cikm, october, year#, napa valley, california, usa. actually, users perform as the most important role in the clickthrough data, since all the queries are issued by the users, and which urls to click are also decided by the users. the connections between queries and urls are essentially bridged by di erent kinds of users. moreover, if two distinct users issued the similar set of queries, we can assume that these two users are very similar since they have similar information needs. from the above analysis, we cannot ignore the users in the clickthrough data. then we give solutions to the following two problems: how to learn the query latent feature space from these two bipartite graphs, and how to recommend semantically relevant queries to users as to the rst problem, we develop a joint matrix factorization method which fuse user query and query url bipartite graphs together to learn the low dimensional query latent feature space. in order to address the second problem, we develop a novel,ective, and. cient similarity propagation model, which not only suggests a list of queries relevant to the queries submitted by users, but also ranks the query list based on the similarity scores. we evaluate our model for query suggestion using clickthrough data of a commercial search engine. then, we evaluate it in terms of the ground truth extracted from the odp database. section # describes the construction of two bipartite graphs, and proposes a joint matrix factorization method of learning query latent feature space. in section #, we demonstrate the empirical analysis of our models and algo http: www dmoz org rithms. finally, conclusions and future work are given in section #. the proposed method bridges topic modeling and social network analysis, which leverages the power of both statistical topic models and discrete regularization. the output of this model well summarizes topics in text, maps a topic on the network, and discovers topical communities. in this paper, we formally define the problem of topic modeling with network structure. we propose a novel solution to this problem, which regularizes a statistical topic model with a harmonic regularizer based on a graph structure in the data. empirical experiments on two different genres of data show that our approach is effective, which improves text oriented methods as well as network oriented methods. the proposed model is general; it can be applied to any text collections with a mixture of topics and an associated network structure. with concrete selection of a topic model and a graph based regularizer, our model can be applied to text mining problems such as author topic analysis, community discovery, and spatial text mining. generating alternative queries, also known as query suggestion, has long been proved useful to help a user explore and express his information need. in many scenarios, such suggestions can be generated from a large scale graph of queries and other accessory information, such as the clickthrough. however, how to generate suggestions while ensuring their semantic consistency with the original query remains a challenging problem. in this work, we propose a novel query suggestion algorithm based on ranking queries with the hitting time on a large scale bipartite graph. without involvement of twisted heuristics or heavy tuning of parameters, this method clearly captures the semantic consistency between the suggested query and the original query. empirical experiments on a large scale query log of a commercial search engine and a scientific literature collection show that hitting time is effective to generate semantically consistent query suggestions. the proposed algorithm and its variations can successfully boost long tail queries, accommodating personalized query suggestion, as well as finding related authors in research. the explosive growth of web information has not only created a crucial challenge for search engine companies to handle large scale data, but also increased the di culty for a user to manage his information need. it has become increasingly di cult for a user to compose a succinct and precise query to present his search need. instead of pushing this burden to the users, it is common practice for a search engine to provide some types of query suggestions. this work was done when the rst author was on a summer internship at microsoft research. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. when a user types a query msg to the search engines, he will be provided with quite a few alternative potential queries. for example, he will be suggested msg chinese food, msg health, and other names for msg by google, and msg error, msg network, and msg seating chart by yahoo. there are also other query suggestion mechanisms which could automatically complete a query, and automatically correct spelling mistakes. such query suggestion mechanisms are usually developed based on morphological information of queries, or cooccurrence of one query word with other queries. although suchquery suggestions are proved useful in di erent ways, there is usually no guarantee that the suggested queries convey close semantic information with the original query. indeed, it is usually annoying for a researcher who searches for chris burges but is suggested with chris burgess or chris burge ministries. similarly, it is not very helpful to suggest kdd with kbb, kddi, ntt, and harry shum with harry potter. people searching for larry page maybe interested in sergey brin but not yellow page. a good query suggestion system should consider a handful of features, but in most cases it is important to ensure that the semantics of the suggested query do not drift too much from the original one. some users will issue the query msg to search for the sports center in newyork and others use it to search the food additive. msr could mean microsoft research, but also mountain safety research, or even mortgage servicing rights. without the constraint of semantics, a general suggestion to such ambiguous queries would easily be. another big challenge and opportunity for the current query suggestion systems lies in the suggestion of infrequent queries. it has been a well known theory in business that a company could sell less of more by boosting the long tail of the power law distribution. the same question lies in searchengine business, especially in advertising where customers bid for query terms. frequently clicked queries cost more and long tail queries cost less. if a well designed query suggestion system could route the traf all real examples are collected on feb. http: search yahoo com searchchris burges http: search live com results aspxchris burges http: search live com results aspxkdd http: search live com results aspxharry shum. and boost the clickthrough of long tail queries, there is a huge opportunity to maximize thebene ts forboth a search engine company and customers of its advertising system. is there a principled way to suggest semantically similar queries while also boosting long tail queries can such a method also provide a natural solution to personalization it is challenging because semantics is hard to de ne and both long tail queries andpersonalization usually su er from data sparsity. in this paper, we propose a uni ed approach to query suggestion, by computing the hitting time on a large scale bipartite graph of queries and clickthrough. despite its simplicity, this novel approach introduces quite a fewbene ts to query suggestion: the suggestions generated with the proposed algorithm are semantically similar to the original query; the suggestions generated do not have to occur with the original query; this approachboosts the long tail queries as suggestions; and this model provides a natural treatment for personalized query suggestion. empirical experiments on a large scale query log of a commercial search engine, as well as a public available scienti. bibliography dataset show that our proposed algorithm is. ective for semantically coherent query suggestion, which provides a potential new framework, or an important and novel feature for building a real query suggestion system. the approach of using hitting time is quite general, which could provide potential solutions to many other search related problems other than query suggestion. the rest of the paper is organized as follows. in section #, we formally introduce the concept of hitting time on a bipartite graph. in section #, we propose the algorithm of query suggestion using hitting time. we show our experiments and results in section #, introduce the related work in section #, and conclude in section #. net ix spends millions to look for an. ective way to suggest hard to nd movies. we address the problem of selecting these ads so that they are both relevant to the queries and profitable to the search engine, showing that optimizing ad relevance and revenue is not equivalent. selecting the best ads that satisfy these constraints also naturally incurs high computational costs, and time constraints can lead to reduced relevance and profitability. empirical evaluation shows that our method optimized for relevance matches a state of the art method while improving expected revenue. the primary business model behind web search is based on textual advertising, where contextually relevant ads are displayed alongside search results. an offine preprocessing phase leverages additional knowledge that is impractical to use in real time, and rewrites frequent queries in a way that subsequently facilitates fast and accurate online matching. we propose a novel two stage approach, which conducts most of the analysis ahead of time. when optimizing for revenue, we see even more substantial improvements in expected revenue. determining the similarity of short text snippets, such as search queries, works poorly with traditional document similarity measures, since there are often few, if any, terms in common between two short text snippets. we address this problem by introducing a novel method for measuring the similarity between short text snippets by leveraging web search results to provide greater context for the short texts. in this paper, we define such a similarity kernel function, mathematically analyze some of its properties, and provide examples of its efficacy. we also show the use of this kernel function in a large scale system for suggesting related queries to search engine users. in analyzing text, there are many situations in which we wish to determine how similar two short text snippets are. for example, there may be di erent ways to describe some concept or individual, such as united nations secretary general and ko. annan, and we would like to determine that there is a high degree of semantic similarity between these two text snippets. similarly, the snippets ai and arti cial intelligence are very similar with regard to their meaning, even though they may not share any actual terms in common. directly applying traditional document similarity measures, such as the widely used cosine coe cient, to copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. such short text snippets often produces inadequate results, however. indeed, in both the examples given previously, applying the cosine would yield a similarity of since each given text pair contains no common terms. even in cases where two snippets may share terms, they may be using the term in di erent contexts. consider the snippets graphical models and graphical interface. the rst uses graphical in reference to graph structures whereas the second uses the term to refer to graphic displays. thus, while the cosine score between these two snippets would be due to the shared lexical term graphical, at a semantic level the use of this shared term is not truly an indication of similarity between the snippets. to address this problem, we would like to have a method for measuring the similarity between such short text snippets that captures more of the semantic context of the snippets rather than simply measuring their term wise similarity. to help us achieve this goal, we can leverage the large volume of documents on the web to determine greater context for a short text snippet. by examining documents that contain the text snippet terms we can discover other contextual terms that help to provide a greater context for the original snippet and potentially resolve ambiguity in the use of terms with multiple meanings. our approach to this problem is relatively simple, but surprisingly quite powerful. we simply treat each snippet as a query to a web search engine in order to nd a number of documents that contain the terms in the original snippets. we then use these returned documents to create a context vector for the original snippet, where such a context vector contains many words that tend to occur in context with the original snippet terms. such context vectors can now be much more robustly compared with a measure such as the cosine to determine the similarity between the original text snippets. furthermore, since the cosine is a valid kernel, using this function in conjunction with the generated context vectors makes this similarity function applicable in any kernel based machine learning algorithm where text data is being processed. while there are many cases where getting a robust measure of similarity between short texts is important, one particularly useful application in the context of search is to suggest related queries to a user. in such an application, a user who issues a query to a search engine may nd it helpful to be provided with a list of semantically related queries that he or she may consider to further explore the related information space. by employing our short text similarity kernel, we could match the userinitial query against a large repository of existing user queries to determine other similar queries to suggest to the user. thus, the results of the similarity function can be directly employed in an end user application. the approach we take in constructing our similarity function has relations to previous work in both the information retrieval and machine learning communities. we explore these relations and put our work in the context of previous research in section #. we then formally de ne our similarity function in section # and present initial examples of its use in section #. this is followed by a mathematical analysis of the similarity function in section #. section # presents a system for related query suggestion using our similarity function, and an empirical evaluation of this system is given in section #. finally, in section # we provide some conclusions and directions for future work. we model documents as if they were generated by a two stage stochastic process. each author is represented by a probability distribution over topics, and each topic is represented as a probability distribution over words for that topic. the words in a multi author paper are assumed to be the result of a mixture of each authors topic mixture. the topic word and author topic distributions are learned from data in an unsupervised manner using a markov chain monte carlo algorithm. we apply the methodology to a large corpus of, abstracts and, authors from the well known citeseer digital library, and learn a model with topics. we discuss in detail the interpretation of the results discovered by the system including specific topic and author models, ranking of authors by topic and topics by author, significant trends in the computer science literature between year# and year#, parsing of abstracts by topics and authors and detection of unusual papers by specific authors. an online query interface to the model is also discussed that allows interactive exploration of author topic models for corpora such as citeseer. we propose a new unsupervised learning technique for extracting information from large text collections. an usual approach to address mismatching vocabulary problem is to augment the original query using dictionaries and other lexical resources and or by looking at pseudo relevant documents. either way, terms are added to form a new query that will be used to score all documents in a subsequent retrieval pass, and as consequence the original query focus may drift because of the newly added terms. we propose a new method to address the mismatching vocabulary problem, expanding original query terms only when necessary and complementing the user query for missing terms while scoring documents. it allows related semantic aspects to be included in a conservative and selective way, thus reducing the possibility of query drift. our results using replacements for themissing query termsin modified document and passages retrieval methods show significant improvement over the original ones. a user query for a retrieval system expresses both the userinformation need and the knowledge he she has about the query topic. all these surrounding factors in an information retrieval setting makes it hard to capture the marginal aspects of a query. in particular, a word used in a query can have di erent meanings or have other words that permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. clarke school of computer science university of waterloo waterloo, canada claclark plg uwaterloo ca may replace it in documents. this causes problems such as query drift and mismatching vocabulary that deteriorate the accuracy of the retrieval process. one way to address the mismatching problem is through automatic query expansion, where new terms are added to create a new expanded query to be submitted to the retrieval engine. on the other hand, aqe increases the chances for query drift. an alternative to handle the problem of mismatching vocabularies is through the use of translation language models and the methods used in cross language information retrieval. the lack of query terms in the documents is addressed by using one or more words in the document as a translation for query terms. in a sense, the translation of document words into query terms is not the same thing as expanding the query with extra terms. translation focuses on replacing the query term while the aqe focuses is on complementing the query with some other aspects. we take a di erent approach to address the mismatching vocabulary problem. unlike aqe and translation models, instead of augmenting the query to score documents, we use the original query and replace missing terms only when necessary. the idea is to use the original query terms to score documents as long as possible. this can be viewed as a kind of translation, however we do not try to translate query terms that are present in the document and, depending on how we choose the replacement terms, we can also capture relationship types other than translation. since the vocabulary changes from one document to another, it is likely that our approach will score documents using di erent queries from the original but forming a new query with as minimal change as possible from the original user query. in order to prevent the original query terms from being outweighed by replacement terms, we adjust the weights of replacement terms based on their relatedness to the missing query term. while our approach is a form of query expansion, it does not exclude the possibility that a traditional aqe could be performed later in the retrieval process. we apply the new method to passage retrieval and document retrieval, as described in section #. the method to nd replacements for the missing query terms is described in section #. our empirical results and discussions are presented in section #. in the same situation, traditional aqe will use one query for all documents, regardless of the mismatching vocabulary problem. query clustering is a process used to discover frequently asked questions or most popular topics on a search engine. this process is crucial for search engines based on question answering. because of the short lengths of queries, approaches based on keywords are not suitable for query clustering. this paper describes a new query clustering method that makes use of user logs which allow us to identify the documents the users have selected for a query. the similarity between two queries may be deduced from the common documents the users selected for them. our experiments show that a combination of both keywords and user logs is better than using either method alone. this is one of the main factors that affects the precision of the search engines. zhichun road haidian district, beijing, china; email: jrwen microsoft com; jian, nie, diro, university of montreal, cp, succursale center ville, montreal, c canada; email: nie iro umontreal ca; hong. zhang, microsoft research, asia, beijing sigma center no. permission to make digital hard copy of part or all of this work for personal or classroom use is granted without fee provided that the copies are not made or distributed for pro. year# acm year# traditional search engines, these new systems try to understand the userquestion in order to suggest similar questions that other people have asked and for which the system has the correct answers. in fact, the correct answers have been prepared or checked by human editors in most cases. the assumption behind such a system is that many people are interested in the same questions the frequently asked questions. it is assumed that if the system can correctly answer these questions, then an important part of users questions will be answered precisely. in order to cluster queries, two related problems have to be solved: how can human editors determine which questions queries are faqs how can a system judge if two questions queries are similar the rst question has to be answered in order to identify possible faqs so that human editors can prepare check their answers. the second question is closely related to the rst one. in this paper, we propose a new approach to query clustering based on user logs. the rst principle has been applied in our query clustering approach. both principles are used in combination with the traditional approaches based on query contents. in addition, we notice that many similar questions would have been separated into different acm transactions on information systems, vol. this study demonstrates the usefulness for a search engine, of user logs for query clustering, and the feasibility of an automatic tool to detect faqs. despite the fact that keywords are not always good descriptors of contents, most existing search engines still rely solely on the keywords contained in documents and queries to calculate their similarity. in many cases, the answers returned by search engines are not relevant to the userinformation need, although they do contain the same keywords as the query. faced with the increasing requirement for more precise information retrieval devices, a new generation of search engines or question answering systems have appeared on the web. wen, microsoft research, asia, beijing sigma center no. or commercial advantage, the copyright notice, the title of the publication, and its date appear, and notice is given that copying is by permission of the acm, inc. to copy otherwise, to republish, to post on servers, to redistribute to lists, requires prior speci. it is then guaranteed that, if one of the suggested questions is truly similar to that of the user, the answers provided by the system will be relevant. the queries submitted by users are very different, however, and they are not always well formed questions. it would be a tedious task to do this manually. what is needed is an automatic tool that helps the editors to identify faqs. this is the goal of the present study to cluster similar queries questions together in order to discover faqs. this question has to be answered before implementing any automatic tool. it represents the core problem that we will deal with in this paper. the classic approach to information retrieval would suggest a similarity calculation between queries according to their keywords. however, this approach has some known drawbacks due to the limitations of keywords. in the case of queries, in particular, the keyword based similarity calculation will be very inaccurate due to the short lengths of the queries. in particular, we make use of cross references between the users queries and the documents that the users have chosen to read. our hypothesis is that there is a stronger relationship between the queries and the selected documents than between these queries and other documents. our query clustering approach is based on the following principles: if users clicked on the same documents for different queries, then these queries are similar. if a set of documents is often selected for the same queries, then the terms in these documents are; to some extent, related to the terms of the queries. the second principle is being used in our work on the construction of a live thesaurus. our experimental results on query clustering show that many similar queries are actually clustered together by using our approach. clusters by traditional clustering approaches because they do not share any common keywords.