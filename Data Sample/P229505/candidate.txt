a number of online video social networks, out of which youtube is the most popular, provides features that allow users to post a video as a response to a discussion topic. these features open opportunities for users to introduce polluted content, or simply pollution, into the system. for instance, spammers may post an unrelated video as response to a popular one aiming at increasing the likelihood of the response being viewed by a larger number of users. moreover, opportunistic users promoters may try to gain visibility to a specific video by posting a large number of responses to boost the rank of the responded video, making it appear in the top lists maintained by the system. content pollution may jeopardize the trust of users on the system, thus compromising its success in promoting social interactions. in spite of that, the available literature is very limited in providing a deep understanding of this problem. in this paper, we go a step further by addressing the issue of detecting video spammers and promoters. towards that end, we manually build a test collection of real youtube users, classifying them as spammers, promoters, and legitimates. using our test collection, we provide a characterization of social and content attributes that may help distinguish each user class. we also investigate the feasibility of using a state of the art supervised classification algorithm to detect spammers and promoters, and assess its effectiveness in our test collection. we found that our approach is able to correctly identify the majority of the promoters, misclassifying only a small percentage of legitimate users. in contrast, although we are able to detect a significant fraction of spammers, they showed to be much harder to distinguish from legitimate users. with internet video sharing sites gaining popularity at a dazzling speed, the web is being transformed into a major channel for the delivery of multimedia. online video social networks, out of which youtube is the most popular, are distributing videos at a massive scale. as an example, according to comscore, in may year#, percent of the total. internet audience viewed online videos, being responsible for billion videos viewed on that month. additionally, with ten hours of videos uploaded every minute, youtube is also considered the second most searched site in the web. by allowing users to publicize and share their independently generated content, online video social networks may become susceptible to different types of malicious and opportunistic user actions. particularly, these systems usually offer three basic mechanisms for video retrieval: a search system, ranked lists of top videos, and social links between users and or videos. although appealing as mechanisms to ease content location and enrich online interaction, these mechanisms open opportunities for users to introduce polluted content, or simply pollution, into the system. as an example, video search systems can be fooled by malicious attacks in which users post their videos with several popular tags. opportunistic behavior on the other two mechanisms for video retrieval can be exempli ed by observing a youtube feature which allows users to post a video as a response to a video topic. some users, which we call spammers, may post an unrelated video as response to a popular video topic aiming at increasing the likelihood of the response being viewed by a larger number of users. additionaly, users we refer to as promoters may try to gain visibility to a speci. video by posting a large number of responses to boost the rank of the video topic, making it appear in the top lists maintained by youtube. promoters and spammers are driven by several goals, such as to spread advertise to generate sales, disseminate pornography, or just to compromise system reputation. polluted content may compromise user patience and satisfaction with the system since users cannot easily identify the pollution before watching at least a segment of it, which also consumes system resources, especially bandwidth. additionally, promoters can further negatively impact system aspects, since promoted videos that quickly reach high rankings are strong candidates to be kept in caches or in content distribution networks. in this paper, we address the issue of detecting video spammers and promoters. to do it, we crawled a large user data set from youtube site, containing more than thousands users. then, fabricio is supported by uol, through uol bolsa pesquisa program, process number year# a. we created a labeled collection with users manually classi ed as legitimate, spammers and promoters. after that, we conducted a study about the collected user behavior attributes aiming at understanding their relative discriminative power in distinguishing between legitimate users and the two different types of polluters envisioned. using attributes based on the userpro le, the usersocial behavior in the system, and the videos posted by the user as well as her target videos, we investigated the feasibility of applying a supervised learning method to identify polluters. we found that our approach is able to correctly identify the majority of the promoters, misclassifying only a small percentage of legitimate users. in contrast, although we are able to detect a signi cant fraction of spammers, they showed to be much harder to distinguish from legitimate users. these results motivated us to investigate a hierarchical classi cation approach, which explores different classi cation tradeoffs and provides more exibility for the application of different actions to the detected polluters. the rest of the paper is organized as follows. section # describes our crawling strategy and the test collection built from the crawled dataset. section # investigates a set of user attributes and their ability to distinguish promoters, spammers and legitimate users. section # describes and evaluates our strategies to detect promoters and spammers. finally, section # offers conclusions and directions for future work. we analyze the information credibility of news propagated through twitter, a popular microblogging service. previous research has shown that most of the messages posted on twitter are truthful, but the service is also used to spread misinformation and false rumors, often unintentionally. on this paper we focus on automatic methods for assessing the credibility of a given set of tweets. specifically, we analyze microblog postings related to trending topics, and classify them as credible or not credible, based on features extracted from them. we use features from users posting and re posting behavior, from the text of the posts, and from citations to external sources. we evaluate our methods using a significant number of human assessments about the credibility of items on a recent sample of twitter postings. our results shows that there are measurable differences in the way messages propagate, that can be used to classify them automatically as credible or not credible, with precision and recall in the range of to. over years ago, fogg and tseng described credibility as a perceived quality composed of multiple dimensions. twitter is a micro blogging service that counts with millions of users from all over the world. it allows users to post and exchange character long messages, which are also known as tweets. twitter is used through a wide variety of clients, from which a large portion of active users correspond to mobile users. tweets can be published by sendingmails, sending sms text messages and http: blog twitter com year# evolving ecosystem html copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. directly from smartphones using a wide array of web based services. therefore, twitter facilitates real time propagation of information to a large group of users. this makes it an ideal environment for the dissemination of breaking news directly from the news source and or geographical location of events. for instance, in an emergency situation, some users generate information either by providing rst person observations or by bringing relevant knowledge from external sources into twitter. cial and reputable sources is considered valuable and actively sought and propagated. from this pool of information, other users synthesize and elaborate to produce derived interpretations in a continuous process. this process can gather, lter, and propagate information very rapidly, but it may not be able to separate true information from false rumors. indeed, in we observed that immediately after the year# earthquake in chile, when information from. cial sources was scarce, several rumors posted and re posted on twitter contributed to increase the sense of chaos and insecurity in the local population. however, we also observed that information which turned out to be false, was much more questioned than information which ended up being true. this seems to indicate that the social network somehow tends to favor valid information, over false rumors. the focus of our research is the credibility of information spread through social media networks. in this paper we use credibility in the sense of believability:ering reasonable grounds for being believed we rst ask users to state if they consider that a certain set of messages corresponds to a newsworthy event. next, for those messages considered as related to newsworthy events, we ask another group of users to state if they believe those messages are likely to be true or false. our main objective is to determine if we can automatically assess the level of credibility of content posted on twitter. our primary hypothesis is that there are signals available in the social media environment itself that enable users to assess information credibility. in this context we de ne social media credibility as the aspect of information credibility that can be assessed using only the information available in a social media platform. http: www merriam webster com dictionary credible acm. our method is based on supervised learning, and the rst step is to build a dataset for studying credibility on twitter. we rst extract a set of relevant discussion topics by studying bursts of activity. then, each topic is labeled by a group of human assessors according to whether it corresponds to a newsworthy information event or to informal conversation. after the dataset is created, each item of the former class is assessed on its level of credibility by another group of judges. next, we extract relevant features from each labeled topic and use them to build a classi er that attempts to automatically determine if a topic corresponds to a newsworthy information event, and then to automatically assess its level of credibility. finally, section # presents our conclusions and directions for future work. the next section outlines previous work related to our current research. gathering and labeling more data to periodically retrain the classifier is expensive. the standard method for combating spam, either in email or on the web, is to train a classifier on manually labeled instances. as the spammers change their tactics, the performance of such classifiers tends to decrease over time. we present a method based on an ensemble of classifiers that can detect when its performance might be degrading and retrain itself, all without manual intervention. experiments with a real world dataset from the blog domain show that our methods can significantly reduce the number of times classifiers are retrained when compared to a fixed retraining schedule, and they maintain classification accuracy even in the absence of manually labeled examples. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. tim oates and tim finin university of maryland, baltimore county year# hilltop circle baltimore, md year# umbc edu account, classi er performance will decrease over time. first, we use mutual agreement between classi ers in the ensemble to detect possible changes in classi er accuracy. the mutual agreement between a pair of classi ers is the fraction of time they assign an instance the same class label. we show, using real data from the blog domain, that changes in mutual agreement are indicative of decreased classi cation accuracy. second, we use the output of the ensemble as a proxy for the true label for new instances and retrain individual classi ers identi ed as possibly weak via mutual agreement. there is signi cant work on using ensembles to tackle concept drift. both methods rely on a stream of labeled examples, whereas ours does not. as we build more robust classi ers to detect spam, the methods used by spammers evolve so that they can continue to get their messages through. this leads to changes in the distribution of the data seen by classi ers used to weed out spam. such changes can also occur naturally, for example, as topics change in a blog or one company is bought by another and the internal email tra. unless this non stationarity of the data is taken into permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. for example, one common method used by spammers is to add text from legitimate sources to their communications in an attempt to get past the classi ers as false negatives. when many of these messages get through, classi ers are often updated, which leads to an arms race of sorts. first, how does one determine when classi er performance has degraded second, how are the classi ers retrained the most common answers to these questions involve a continual stream of labeled examples, such as current emails labeled according to whether or not they are spam. these labeled examples can be used to test the accuracy of the classi ers and to retrain them when accuracy becomes too low. the problem with this approach is that obtaining labeled examples is expensive. in this paper we present a method based on an ensemble of classi ers that automatically, without human intervention, deals with the two questions above. note that the true label need not be known to compute mutual agreement. these approaches are motivated by the fact that it is often useful to have multiple opinions before making a decision. presented an ensemble method for concept drift by assigning weights to classi ers based on their accuracies. extended this work by dynamically creating and removing weighted experts in response to changes in performance. presented another ensemble based method for concept drift, claiming it to have low computational cost and still be comparable with other ensemble methods. presented a random decision tree ensemble based engine to mine data streams. they demonstrated the ability to detect concept drift on the. and discussed ways to combine old data with new data for computing optimal models. we present a new algorithm for duplicate document detection thatuses collection statistics. we compare our approach with thestate of the art approach using multiple collections. thesecollections include a mb, web document collectiondeveloped by excite home and three nist collections. the first nistcollection consists of mb, la times documents, which isroughly similar in the number of documents to theexcite at; home collection. the other two collections are both gb and are the, web document collection and the trec disks and, document collection. we show that our approachcalledmatch, scales in terms of the number of documents andworks well for documents of all sizes. we compared our solution tothe state of the art and found that in addition to improvedaccuracy of detection, our approach executed in roughly one fifththe time. the tremendous growth of the internet has spurred the existence of data portals for nearly every topic. some of these portals are of general interest; some are highly domain speci c. independent of the focus, the vast majority of the portals obtain data, loosely called documents, from multiple sources. obtaining data from multiple input sources typically results in duplication. the detection of duplicate documents within a collection has recently become an area of great interest and is the focus of our described effort. typically, inverted indexes are used to support ef cient query processing in information search and retrieval engines. storing duplicate documents affects both the accuracy and ef ciency of the search engine. retrieving duplicate documents in response to a userquery clearly lowers the number of valid responses provided to the user, hence lowering the accuracy of the userresponse set. furthermore, processing duplicates necessitates additional computation this work was partially supported by the national science foundation under the national young investigator program. authoraddress: information retrieval laboratory, illinois institute of technology, west st street, chicago, il; email: abdur ir iit edu; ophir cs iit edu; grossman iit edu; mcatherm comcast net. permission to make digital hard copy of part or all of this work for personal or classroom use is granted without fee provided that the copies are not made or distributed for pro. or commercial advantage, the copyright notice, the title of the publication, and its date appear, and notice is given that copying is by permission of the acm, inc. to copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior speci. year# acm year# without introducing any additional bene t. hence, the processing ef ciency of the userquery is lowered. a problem introduced by the indexing of duplicate documents is potentially skewed collection statistics. collection statistics are often used as part of the similarity computation of a query to a document. hence, the biasing of collection statistics may affect the overall precision of the entire system. simply put, not only is a given userperformance compromised by the existence of duplicates, but also the overall retrieval accuracy of the engine is jeopardized. the de nition of what constitutes a duplicate is unclear. for instance, a duplicate can be de ned as the exact syntactic terms, without formatting differences. throughout our efforts however, we adhere to the de nition previously referred to as a measure of resemblance. the general notion is that if a document contains roughly the same semantic content it is a duplicate whether or not it is a precise syntactic match. when searching web documents, one might think that, at least, matching urlwould identify exact matches. however, many web sites use dynamic presentation wherein the content changes depending on the region or other variables. in addition, data providers often create several names for one site in an attempt to attract users with different interests or perspectives. for instance, www fox com, onsale channel com, and www realtv com all point to an advertisement for real tv. while the previous examples are for web documents, the same holds true for other collections where multiple document sources populate a single document collection. the national center for complimentary and alternative medicine, part of the national institutes of health supports a search engine for medical data whose inputs come from multiple medical data sources. given the nature of the data, duplicates are common. since unique document identi ers are not possible across the different sources, the detection of duplicate information is essential in producing non redundant results. a previously proposed solution is the digital syntactic clustering algorithm and its super shingle variant. while these algorithms are commonly used, they have ef ciency problems. one reported run took ten cpu days to process a thirty million document collection. additionally, dsc ss and dsc are known to perform poorly on small documents. given that the average size of a document on the web is around kb, working with small documents is imperative. our algorithm, called iit match ormatch for short, lters documents based on term collection statistics. our results show thatmatch is ve to six times faster than the dsc ss algorithm. furthermore, we show thatmatch does not ignore small documents and places each document into at most one duplicate set. other approaches place potentially duplicate documents in multiple clusters. hence, it is harder for a user to detect the actual duplicates. finally, the sets of duplicates we detect are usually tighter than dsc because we require an exact match for the terms remaining after our ltration process. however, like other approaches, we still identify non exact duplicates. this paper introduces a web image dataset created by nus lab for media search. the dataset includes: images and the associated tags from flickr, with a total of, unique tags; six types of low level features extracted from these images, including color histogram, color correlogram, edge direction histogram, wavelet texture, block wise color moments extracted over fixed grid partitions, and bag of words based on sift descriptions; and ground truth for concepts that can be used for evaluation. based on this dataset, we highlight characteristics of web image collections and identify four research issues on web image annotation and retrieval. we also provide the baseline results for web image annotation by learning from the tags using the traditionalnn algorithm. the benchmark results indicate that it is possible to learn effective models from sufficiently large image dataset to facilitate general image retrieval. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. as flickr and picasa, are popular in daily life. for example, there are more than, images being uploaded to flickr every minute. er us great opportunity to freely acquire a large number of images with annotated tags. in this paper, we present four research issues on mining thecommunity contributedimages and tagsforimageannotation and retrieval. to http: www ickr com http: picasa google com figure #: the number of tags per image these ends, we construct a benchmark dataset to focus research. to our knowledge, this is the largest real world web image dataset comprising over, images with over, user provided tags, and ground truth of concepts for the entire dataset. digital images have become more easily accessible following the rapid advances in digital photography, networking and storage technologies. or commercial advantage and that copies bear this notice and the full citation on the rst page. during peak times, up to, images are being served per second, and the record for the number of images uploaded per day exceeds million images. when users sharetheirimages, they typicallygive several tags to describe the contents of their images. for example, what can we do with millions of images and their associated tags how can general image indexing and search bene. from the community shared images and tags in fact, how to improve the performance of existing image annotation and retrieval approaches by using machine learning and other arti cial intelligent technologies has attracted much attention in multimedia research community. ective, a large number of balanced labeled samples is required, which typically comesfromusersduring aninteractive manualprocess. ort, many semi supervised learning or active learning approaches have been proposed. nevertheless, there is still a need to manually annotate many images to train the learning models. on the other hand, the image sharing sites. the tags for the images are collectively annotated by a large group of heterogeneous users. it is believed that although most tags are correct, there are many noisy and missing tags. thus if we can learn the accurate models from these user shared images together with their associated noisy tags, then much manual. in this case, content based image annotation and retrieval can bene. much from the community contributed images and tags. the issues are: how to utilize the community contributed images and tags to annotate nontaggedimages. howtoleveragethe modelslearnedfrom these images and associated tags to improve the retrieval of webimages withtags or surroundingtext. howto ensure tag completion which means the removal of the noise in the tag set and the enrichment of missing tags. ective training set for each concept and the overall concept network from the available information sources. the dataset includes a set of images crawled from flickr, together with their associated tags, as well as the ground truth for concepts for these images. we also extract six low level visual features, including color histogram in lab color space, color correlogram in hsv color space, edge distribution histogram, wavelettexture, dblock wise lab based color moments extracted over xedgridpartitions, and bag of visual words. for the image annotation task, we also provide a baseline using thenn algorithm. the set of low level features for images, their associated tags, ground truth, and the baseline results can be downloaded at http: lms comp nus edu sg research nus wide htm. the dataset is much larger than the popularly availablecorel andcaltech datasets. the rest of the paper is organized as follows. section # introducesthecrawledimages and tagsfromflickr andhow we pre process them, including the removal of duplicate images and useless tags. section # introduces thede nitionsof the conceptsfor evaluation andhow we manually annotate the ground truth. the levels of noise in di erent tags are analyzed in section #. in section #, we describe the de nitions of four research challenges for web image annotation and retrieval, and present the benchmark results. finally, section # contains the conclusion and discussion for future work. somephoto sharingwebsites, such permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. out of these archives, severalquestions naturally arisefor multimedia research. section # describes the extraction of the low level features for images. like the web, twitter has become a target for link farming, where users, especially spammers, try to acquire large numbers of follower links in the social network. recently, twitter has emerged as a popular platform for discovering real time information on the web, such as news stories and people reaction to them. acquiring followers not only increases the size of a user direct audience, but also contributes to the perceived influence of the user, which in turn impacts the ranking of the user tweets by search engines. in this paper, we first investigate link farming in the twitter network and then explore mechanisms to discourage the activity. to this end, we conducted a detailed analysis of links acquired by over, spammer accounts suspended by twitter. we find that link farming is wide spread and that a majority of spammers links are farmed from a small fraction of twitter users, the social capitalists, who are themselves seeking to amass social capital and links by following back anyone who follows them. our findings shed light on the social dynamics that are at the root of the link farming problem in twitter network and they have important implications for future designs of link spam defenses. in particular, we show that a simple user ranking scheme that penalizes users for connecting to spammers can effectively address the problem by disincentivizing users from linking with other users simply to gain influence. so spammers attempt to enhancetheirin uencescore by acquiring links in the social network. finally, we explore mechanisms to deter link farming in the future. recently, the twitter social network has emerged as a popular platform for discovering real time information on the copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. www year#, april, year#, lyon, france acm year#. gummadi mpi sws, germany web, suchascurrent events, newsstories, andpeople sopinion about them. traditional media, celebrities, and marketers are increasingly using twitter to directly reach audiences in the millions. furthermore, millions of individual users are sharing the information they discover over twitter, making it an important source of breaking news during emergencieslikerevolutionsanddisasters. recent estimates suggest that million active twitter users post million tweets containing morethan millionurls daily. as the information shared over twitter grows rapidly, search is increasingly being used to nd interesting trending topicsand recent news. astwitterbecomesmoreaccessible via search, it has also started to attract the attention of spammers, who strive to get their tweets appear near the top of the search results. search engines rank tweets based not only on the content of the tweet, but also onhowin uentialthe user whopostedthetweetis. the exactdetails of the in uence metric depend on the search engine, but all of them critically depend on the userconnectivity in the social graph. the more followers a user has, the more likely his tweets are to be ranked highly. similar to the web, where some websites exchange reciprocal links with other sites to improve their ranking by search engines, spammers try to in ltrate the twitter network by building social relationships they follow other users and try to get others to follow them. we refer to this process of reciprocal exchange of links between unrelated users to gain in uence in the network as link farming. while link farming in the web graph has been well studied and understood, thereis no existing work on link farming in the twitter social network to the best of our knowledge. furthermore, unlike the web, where a link from web page a to web pageimplies thats content is relevant to acontent, the meaning of a social link between two users is unknown to anyone but the users themselves. this makes it considerably harder to detect and analyze link farming activities in twitter. inthispaper, weinvestigatethevulnerability of thetwitter social network to link farming. speci cally, we focus on better understanding the users who establish links to spammers and the potential reasons for their behavior. to this end, we gathered data of, spammer accounts suspended by twitter and conduct a detailed analysis of the users who connect to them we also used extensive data from a previous measurement study that included a complete snapshot of thetwitter network and thecompletehistory oftweetspostedby all users as ofaugust. our analysis reveals surprising social dynamics that drive link farming in twitter. when we started the study, we expected that spammers in twitter would be farming links in two ways: rst, by targeting lay twitter users with very few followers, who then reciprocate out of social etiquette, and second, from other spammers and fake accounts that have been explicitly created for the purpose of farming links. in sharp contrast to our expectation, we found that a majority of farmed links come from a small number oflegitimate, popular, and highly active twitter users. this is very di erent from the web, wherepopularpages wouldrarelypoint to spampages. we conjecture that the twitter users engaging in link farming are social capitalists, whosegoalistoamasssocial capital andpromote theirlegitimate content ontwitter. examples of social capitalists range from popular bloggers of social media and internet technologies to celebrities like britney spears and from politicians like barack obama to businesses like jetblue airways. we show that social capitaliststendto reciprocate toanyonewho connects to them, to increase their social capital. unfortunately, spammers exploit this behavior of capitalists to farm links in the twitter network and promote spam content. ective solution to ght link spam should take into consideration the current incentive structure of the twitter network that encourages social capitalists to collude with other social capitalists, including users who they might not know. inspired by ideas from spam defense strategiesproposedfor thewebgraph, wepropose a ranking system, called collusionrank that penalizes users for connecting to spammers. collusionrank disincentivizes users from colluding with people that are unknown to them, who might potentially be spammers. we show that, even when only a small fraction of all spammers are identi ed, collusionrank successfully lowers the in uence scores of the other spammers in the network. we find that of million urls posted to the site point to phishing, malware, and scams listed on popular blacklists. we analyze the accounts that send spam and find evidence that it originates from previously legitimate accounts that have been compromised and are now being puppeteered by spammers. using clickthrough data, we analyze spammers use of features unique to twitter and the degree that they affect the success of spam. we find that twitter is a highly successful platform for coercing users to visit spam pages, with a clickthrough rate of, compared to much lower rates previously reported for email spam. we group spam urls into campaigns and identify trends that uniquely distinguish phishing, malware, and spam, to gain an insight into the underlying techniques used to attract users. given the absence of spam filtering on twitter, we examine whether the use of url blacklists would help to significantly stem the spread of twitter spam. our results indicate that blacklists are too slow at identifying new threats, allowing more than of visitors to view a page before it becomes blacklisted. we also find that even if blacklist delays were reduced, the use by spammers of url shortening services for obfuscation negates the potential gains unless tools that use blacklists develop more sophisticated spam filtering. in this work we present a characterization of spam on twitter. within the last few years, twitter has developed a following of million users that post to the site over one billion times per month. promising users great diets and more friends, or simply stealing accounts, spam has become a pervasive problem throughout twitter. notable attacks on twitter include the brute force guessing of weak passwords that led to exploitation of compromised accounts to advertise diet pills. phishing is also a signi cant concern on twitter, leading the site to completely redesign the sending of private messages between users to help mitigate attacks. even though twitter is vigilant at notifying users and works to stop phishing, spammers continue to create and compromise accounts, sending messages from them to fool users into clicking on scams and harmful links. despite an increase in volume of unsolicited messages, twitter currently lacks. ltering mechanism to prevent spam, with the exception of malware, blocked using googlesafebrowsing api. instead, twitter has developed a loose set of heuristics to quantify spamming activity, such as excessive account creation or requests to befriend other users. using these methods along with user generated reports of spamming and abusive behavior, the site suspends offending accounts, withdrawing their presence from the twittersphere along with all of the accountmessages. in this paper we describe our ndings from a large scale effort to characterize spam on twitter. after collecting a month long sample of twitter data, we examine over million public tweets and crawl million unique urls. analyzing the content of spam messages, we provide a breakdown of techniques employed by spammers to exhort twitter users to click on links. by studying the accounts involved in spamming, we nd evidence that spammers primarily abuse compromised accounts in their spamming activity, rather than accounts generated solely for the purpose of spamming, which are signi cantly less prevalent. using clickthrough data generated from spam urls, we examine the success of twitter spam at enticing over million users into visiting spam web pages. we nd that the success of spam is directly tied to having a large audience and a variety of accounts to spam from, while use of certain twitter speci. overall, we nd that of messages advertised on twitter will be clicked, almost two orders of magnitude higher than email spam. given the absence of spam ltering on twitter, we examine whether the use of url blacklists would help to signi cantly stem the spread of twitter spam. by measuring the time period between a blacklist agging a spam url and its appearance on twitter, we nd that blacklists in fact lag behind twitter, with the majority of spam messages appearing days before the urls embedded in the messages become agged. in contrast, we nd over of visits to spam urls occur within the rst two days of posting, indicating that blacklist lag time is too long to protect a signi cant number of users against spam. we also examine how spammers can employ url shortening services to completely evade blacklists, a current problem for twittermalware detection. in summary, the contributions of this paper are: we present the rst in depth look at spam on twitter, based on a detailed analysis of tweets containing over million distinct urls pointing to blacklisted scams, phishing and malware. we analyze the clickthrough rate for spam on twitter, nding that of users exposed to spam urls click though to the spam web site. we identify a diversity of spam campaigns exploiting a range of twitter features to attract audiences, including large scale phishing attacks and targeted scams. lter for urls posted on twitter, nding that blacklists are currently too slow to stop harmful links from receiving thousands of clicks. we develop techniques to identify and analyze two types of spamming accounts on twitter; those created primarily for spamming and accounts compromised by spammers. we organize the remainder of the paper as follows. section # presents a brief background on spam and an overview of twitter. section # describes the data we have collected, and section # discusses trends we nd in spam tweets, the users who send them, and the clickthrough rate for urls in tweets. section # discusses techniques for grouping spam into campaigns and examples of successful campaigns. section # presents our evaluation of blacklists, followed by conclusions in section #. as celebrities such as oprah, ashton kutcher, and justin bieber attract throngs of twitter followers, spammers have been quick to adapt their operations to target twitter with scams, malware, and phishing attacks. using an assortment of url blacklists to identify spam, we nd over million urls that direct users to scams, malware, and phishing sites roughly of all links posted to twitter. while microblogging has emerged as an important information sharing and communication platform, it has also become a convenient venue for spammers to overwhelm other users with unwanted content. currently, spammer detection in microblogging focuses on using social networking information, but little on content analysis due to the distinct nature of microblogging messages. second, the texts in microblogging are short and noisy. as we know, spammer detection has been extensively studied for years in various media, eg, emails, sms and the web. motivated by abundant resources available in the other media, we investigate whether we can take advantage of the existing resources for spammer detection in microblogging. while people accept that texts in microblogging are different from those in other media, there is no quantitative analysis to show how different they are. in this paper, we first perform a comprehensive linguistic study to compare spam across different media. inspired by the findings, we present an optimization formulation that enables the design of spammer detection in microblogging using knowledge from external media. we conduct experiments on real world twitter datasets to verify whether email, sms and web spam resources help and how different media help for spammer detection in microblogging. microblogging a style of communicating through shortform content has emerged as a popular social networking platform. microblogging systems have been increasingly used for large scale information dissemination and sharing in various elds such as marketing, journalism or public relations. with microblogginggrowing popularity, activities of spamming have become rampant in launching various attacks in the medium. for example, the spammers spread ads to generate sales, disseminate pornography, viruses, phishing, or simply to compromise a systemreputation. to improve user experience and the overall value of a system, it is essential to detect spammers in microblogging. existing methods for spammer detection in social media focus on using social networking information. these network based methods characterize the spammers by analyzing the network features, eg, social status. the assumption behind this strategy is that it is di cult for the spammers to establish a large number of social relations with legitimate users. di erent from other social media sites, in microblogging, users can follow anyone without prior consent from the followee. many users just follow back when they are followed by someone for the sake of courtesy. so, the spammers can easily enhance their in uence score to fool the system. in this case, content analysis could complement network based methods in spammer detection; thus, we explore the use of content information in this work. a straightforward way to perform content based spammer detection is to model this task as a supervised learning problem. ective textual features from the messages and build a classi er or a regressor based on the features. given a new user, the built model can output a class label or score to determine whether it is a spammer based on microblogging messages the user posted. content based methods become di cult to be directly applied due to the distinct features of microblogging data. first, in microblogging, it is time consuming and labor intensive to obtain labeled data, which is essential in building an. given the size and dynamic nature of microblogging, a manual labeling process is neither scalable nor sensible. second, the texts in microblogging are short and noisy; thus, we lack su cient aggregated information to evaluate the given messages. these present great challenges to directly making use of existing content based methods for. while the problem of spamming in microblogging is relatively new, it has been extensively studied for years in other platforms, eg, email communication, sms and the web. similarly, the spammers in these platforms unfairly overwhelm other users by spreading unwanted information, which leads to phishing, malware, and scams. also, it has been reported in natural language processing literature that microblogging is not as noisy as was expected. although microblogging is an informal communication medium, it has been shown to be similar to other platforms and it is seemingly possible to employ nlp tools to clean it. motivated by the previous ndings, we explore the possibility of using knowledge learned from other platforms to facilitate spammer detection in the context of microblogging. in this paper, we explore the use of resources available in other media to help spammer detection in microblogging. ects of the knowledge learned from different media on spammer detection in twitter. the remainder of this paper is organized as follows. in section #, we conduct a quantitative study to examine the di erences between spam corpora in di erent media from a linguistic perspective. in section #, we formally de ne the problem of leveraging knowledge across media for spammer detection in microblogging. in section #, we propose a novel framework for the problem we study. in section #, we report empirical results on real world datasets. in section #, we review existing literature related to our work. in section #, we conclude this work and present some future work. we present the conceptual framework of the social honeypot project for uncovering social spammers who target online communities and initial empirical results from twitter and myspace. two of the key components of the social honeypot project are: the deployment of social honeypots for harvesting deceptive spam profiles from social networking communities; and statistical analysis of the properties of these spam profiles for creating spam classifiers to actively filter out existing and new spammers. this paper aims to detect users generating spam reviews or review spammers. we identify several characteristic behaviors of review spammers and model these behaviors so as to detect the spammers. in particular, we seek to model the following behaviors. first, spammers may target specific products or product groups in order to maximize their impact. second, they tend to deviate from the other reviewers in their ratings of products. we propose scoring methods to measure the degree of spam for each reviewer and apply them on an amazon review dataset. we then select a subset of highly suspicious reviewers for further scrutiny by our user evaluators with the help of a web based spammer evaluation software specially developed for user evaluation experiments. our results show that our proposed ranking and supervised methods are effective in discovering spammers and outperform other baseline method based on helpfulness votes alone. we finally show that the detected spammers have more significant impact on ratings compared with the unhelpful reviewers. motivation web spam refers to all forms of malicious manipulation of user generated data so as to in uence usage patterns of the permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. vhu qkdss\bought this cd player thinking that it looks nice and well designed. but, regretted soon after it does not produce great cd quality sound. now hate it and would like to replace it asap. product the worst cd playerhad purchased so far. \ vhu qkdss\bought this cd player thinking that it looks nice and well designed. but, regretted soon after it does not produce great cd quality sound. now hate it and would like to replace it asap. examples of web spam include search engine spam, email spam, and video spam. in this paper, we focus on spam found in online product review sites commonly known as review spam or opinion spam. review spam is designed to give unfair view of some products so as to in uence the consumers perception of the products by directly or indirectly in ating or damaging the productreputation. in, it was found that to of reviews essentially echo the earlier reviews and may potentially be in uenced by review spam. consider figure #a which illustrates a review for product by user mr unhappy. the review is highly negative with star rating in contrast with the high overall star rating. this review does not cause any alarm until we nd another highly negative review by the same user on a different product and both reviews are identical in content. since identical review content for di erent products re ects a strong bias or a lack of seriousness, and the userratings are very di erent from the rest, we consider the two reviews likely to be spam and the user likely to be a spammer. it is not clear how much review spam exists in online prod uct review sites but their existence causes several problems including unfair treatment of products either independently or in comparison with other similar products. ected products especially for review sites that also. when consumers rely on reviews from spammers to purchase products, they could be disappointed by purchased products not meeting their expectation, or misjudging good products. it is thus an important task to detect review spam and remove them so as to protect the genuine interests of consumers and product vendors. detecting review spam is a challenging task as no one knows exactly the amount of spam in existence. due to the openness of product review sites, spammers can pose as di erent users contributing spammed reviews making them harder to eradicate completely. spam reviews usually look perfectly normal until one compares them with other reviews of the same products to identify review comments not consistent with the latter. orts of additional comparisons by the users make the detection task tedious and non trivial. one approach taken by review site such as amazon com is to allow users to label or vote the reviews as helpful or not. orts and is subject to abuse by spammers. the state of the art approach to review spam detection is to treat the reviews as the target of detection. this approach represents a review by review, reviewer and productlevel features, and trains a classi er to distinguish spam reviews from non spam ones. however, these features may not provide direct evidence against the spammed review. for the example in figure #, we rely on comparison with other ratings on the same products by the contributor, and identical review content for di erent products by the contributor. both are behaviors of reviewer that deviate from normal practice and are highly suspicious of review manipulation. this suggests that one should focus on detecting spammers based on their spamming behaviors, instead of detecting spam reviews. in fact, the more spamming behaviors we can detect for a reviewer, the more likely the reviewer is a spammer. subsequently, the reviews of this reviewer can be removed to protect the interests of other review users. review spammer detection in this paper, we address the problem of review spammer detection, or nding users who are the source of spam reviews. unlike the approaches for spammed review detection, our proposed review spammer detection approach is user centric, and user behavior driven. a user centric approach is preferred over the review centric approach as gathering behavioral evidence of spammers is easier than that of spam reviews. a review involves only one reviewer and one product. a reviewer on the other hand may have reviewed a number of products and hence has contributed a number of reviews. the likelihood of nding evidence against spammers will be much higher. the user centric approach is also scalable as one can always incorporate new spamming behaviors as they emerge. the main building blocks of the spamming behavior detection step are the spamming behavior models based on di erent review patterns that suggest spamming. each model assigns a numeric spamming behavior score to each reviewer by measuring the extent to which the reviewer practises spamming behavior of a certain type. in this paper, we mainly rely on patterns of review content and ratings to de ne four di erent spamming behavior models, ie, targeting product; targeting group; general rating deviation; and early rating deviation. to assign an overall numeric spam score to each user, we combine the spam scores of the userdi erent spamming behaviors using linear weighted combination. the weights on the di erent component spam scores can be empirically de ned or learnt automatically. our proposed behavior models shy away from deep natural review text understanding and opinion extraction mainly to avoid high computational costs and performance pitfalls due to inaccurate text analysis. should text analysis be accurate enough to extract opinions from review text, our spamming behavior models can be extended to consider review content. nevertheless, content analysis can be computationally costly and we shall leave this to our future research. the remainder of the paper is organized as follows. section # describes the data representation and dataset used in this research. our proposed characterizations of target based and deviationbased spamming behaviors are given in sections and respectively. section # describes an user evaluation experiment of our proposed unsupervised spammer scoring methods. the supervised version of the spammer scoring method is given in section #. it is well known that many online reviews are not written by genuine users of products, but by spammers who write fake reviews to promote or demote some target products. although some existing works have been done to detect fake reviews and individual spammers, to our knowledge, no work has been done on detecting spammer groups. this paper focuses on this task and proposes an effective technique to detect such groups. opinion spamming refers to writing fake reviews that try to deliberately mislead human readers or automated opinion mining systems by giving undeserving positive opinions or unjust or false negative opinions to promote or demote some target products. the problem can be seen as a classification problem with two classes, spam and non spam. however, to obtain training data for model building by manually labeling reviews is very hard, if not impossible, as a spammer can easily craft a fake review that is just like an genuine review. duplicate reviews were used in as spam reviews for model building. however, many non duplicate reviews can be spam too. due to the labeling problem, studies have been made to find reviewers who behave in suspicious ways. for example, if a reviewer wrote all negative reviews about products of a brand but wrote all positive reviews about a competing brand, this reviewer is clearly a spam suspect. in this work, we focus on group spam, which has not been studied so far. a spammer group refers to a group of reviewers who works together writing fake reviews to promote or demote a set of target products. spammer groups are very damaging due to their sheer sizes. when a group is working collaboratively towards a product, it can take control of the sentiment for the product. this paper proposes a method to detect such groups, which consists of pattern mining to find candidate groups, assessing copyright is held by the author owner. them using criteria that indicate atypical behaviors of groups, and finally ranking the candidate groups. our experiment is based on a large set of amazon reviewers and their reviews. the user study shows that the proposed method is highly effective. search engines became a de facto place to start information acquisition on the web. though due to web spam phenomenon, search results are not always as good as desired. moreover, spam evolves that makes the problem of providing high quality search even more challenging. over the last decade research on adversarial information retrieval has gained a lot of interest both from academia and industry. in this paper we present a systematic review of web spam detection techniques with the focus on algorithms and underlying principles. we categorize all existing algorithms into three categories based on the type of information they use: content based methods, link based methods, and methods based on non traditional data such as user behaviour, clicks, http sessions. in turn, we perform a subcategorization of link based category into five groups based on ideas and principles used: labels propagation, link pruning and reweighting, labels refinement, graph regularization, and featurebased. we also define the concept of web spam numerically and provide a brief survey on various spam forms. finally, we summarize the observations and underlying principles applied for web spam detection. spam pervades any information system, be itmail or web, social, blog or reviews platform. the concept of web spam or spamdexing was rst introduced in year# and soon was recognized as one of the key challenges for search engine industry. recently, all major search engine companies have identi ed adversarial information retrieval as a top priority because of multiple negative. ects caused by spam and appearance of new challenges in this area of research. first, spam deteriorates the quality of search results and deprives legitimate websites of revenue that they might earn in the absence of spam. second, it weakens trust of a user in a search engine provider which is especially tangible issue due to zero cost of switching from one search provider to another. third, spam websites serve as means of malware and adult content dissemination and shing attacks. for instance, ranked million pages using pagerank algorithm and found that out of top results were pornographic websites, that achieved high ranking due to content and web link manipulation. last, it forces a search engine company to waste a signi cant amount of computational and storage resources. in year# the total worldwide nancial losses caused by spam were estimated at billion, in year# the same value was estimated already at billion. among new challenges which are emerging constantly one can highlight a rapid growth of the web and its heterogeneity, simpli cation of content creation tools and decrease in website maintenance cost, evolution of spam itself and hence appearance of new web spam strains that cannot be captured by previously successful methods. web spam phenomenon mainly takes place due to the following fact. the fraction of web page referrals that come from search engines is signi cant and, moreover, users tend to examine only top ranked results. thus, showed that for of the queries only the rst result page is requested and only the rst three to ve links are clicked. therefore, inclusion in the rst serp has a clear economic incentive due to an increase in website tra c. to achieve this goal website owners attempt to manipulate search engine rankings. this manipulation can take various forms such as the addition of a surrogate content on a page, excessive and undeserved link creation, cloaking, click fraud, and tag spam. we de ne these concepts in section #, following the work. generally speaking, web spam manifests itself as a web content generated deliberately for the purpose of triggering unjusti ably favourable relevance or importance of some web page or pages. it is worth mentioning that the necessity of dealing with the malicious content in a corpus is a key distinctive feature of adversarial information retrieval in comparison with the traditional information retrieval, where algorithms operate on a clean benchmark data set or in an intranet of a corporation. according to various studies the amount of web spam varies from to percent, which demonstrates the scope of the problem and suggests that solutions that require manual intervention will not scale. speci cally, shows that of english language web pages were classi ed as spam, reports of spam on a host level and estimates it as. another group of researchers study not only the cumulative amount of spam but its dis search engine result page. they report of spam in the english speaking internet, in japanese, in german, and in french. they also show that of pages in the biz domain and in the com domain are spam. this survey has two goals: rst, it aims to draw a clear roadmap of algorithms, principles and ideas used for web spam detection; second, it aims to build awareness and stimulate further research in the area of adversarial information retrieval. to the best of our knowledge there is no comprehensive but concise web spam mining survey with the focus on algorithms yet. this work complements existing surveys and a book on the topic of web spam. presents a web spam taxonomy and provides a broad coverage of various web spam forms and de nitions. enumerate spam forms and discuss challenges caused by spam phenomenon for web search engines. is a recent book, which provides the broadest coverage of web spam detection research available so far. we think that parallel research on email spam ghting and spam on social websites might also be relevant. presents a general framework for adversarial classi cation and approaches the problem from game theoretical perspective. in section # we provide a brief overview of web spam forms following. then we turn to content based mining methods in section #. in section # we provide a careful coverage of link based spam detection algorithms. in section # we consider approaches to ght against spam using clickthrough and user behaviour data, and by performing realtime http sessions analysis. finally, we summarize key principles underlying web spam mining algorithms in section # and conclude in section #. social network spam increases explosively with the rapid development and wide usage of various social networks on the internet. to timely detect spam in large social network sites, it is desirable to discover unsupervised schemes that can save the training cost of supervised schemes. in this work, we first show several limitations of existing unsupervised detection schemes. the main reason behind the limitations is that existing schemes heavily rely on spamming patterns that are constantly changing to avoid detection. motivated by our observations, we first propose a sybil defense based spam detection scheme sd that remarkably outperforms existing schemes by taking the social network relationship into consideration. in order to make it highly robust in facing an increased level of spam attacks, we further design an unsupervised spam detection scheme, called unik. instead of detecting spammers directly, unik works by deliberately removing non spammers from the network, leveraging both the social graph and the user link graph. the underpinning of unik is that while spammers constantly change their patterns to evade detection, non spammers do not have to do so and thus have a relatively non volatile pattern. unik has comparable performance to sd when it is applied to a large social network site, and outperforms sd significantly when the level of spam attacks increases. based on detection results of unik, we further analyze several identified spam campaigns in this social network site. the result shows that different spammer clusters demonstrate distinct characteristics, implying the volatility of spamming patterns and the ability of unik to automatically extract spam signatures. spam in online social networks increases quickly because of the viral distribution of information provided by massive social connections on the internet. ectiveness of email spam detection also contributes to such a trend. a study shows that email spam has dropped by half in year# and spammers are more aggressively targeting social networks and search engines. it is estimated that of social network users have been spammed in a survey conducted by sophos. to detect spam in online social networks, many supervised machine learning based methods have been proposed. for example, lee et al proposed to deploy honeypots in social networks, and apply machine learning to detect spam using captured spam as the training set. benevenuto et al suggested to detect promoters and spammers in a video social network with user based, video based, and social network based features. markines et al proposed to use six features at post, resource, or user level to capture spam in a social bookmarking system. however, supervised machine learning methods have some inherent limitations when being applied to a large social network site. specifically, labeling the training set is required for supervised learning, which incurs a high human labor cost. moreover, the labeling work has to be done repetitively to maintain. ectiveness for spam detection given the volatility of the spam content and some spam posting patterns. lastly, the supervised model always lags behind spam attacks with new patterns of spam content. di erent from supervised ones, unsupervised schemes that do not have the training cost have been proposed to detect spam in emails and social networks by directly leveraging the spamming patterns. for example, xie et al proposed autore to automatically extract spam url patterns based on the distributed and bursty patterns of botnet based email spam campaigns. applying this approach in detecting social spam, however, may su er from a high false negative rate since a number of spam posts in social networks are continuously sent over months instead of following the bursty pattern. gao et al identi ed spam by clustering posts based on text and url similarities and then expecting spam posts to form large clusters with bursty posting patterns. this approach assumes that spam clusters are not connected to non spam ones. however, spam posts may include non spam urls to increase their legitimacy as shown in our data and discussed in, which. ectively connects spam clusters to non spam clusters, making it highly dif cult to distinguish spam from non spam. thereby, it is desirable and imperative to design an unsupervised scheme that can address the limitations of existing schemes. in this work, we rst propose a sybil defense based spam detection scheme sd. in sd, a user graph is constructed by combining the social graph and the user link graph. the former represents the social relationship between active nonspammers, while the latter characterizes the spam link sharing activity of spammers. observing that spammers and non spammers usually form di erent communities in the user graph, sd applies community detection based sybil defense algorithm to the user graph and achieves better spam detection performance than existing schemes. ectiveness of sybil defense is sub ject to the spam attack intensity, sd does not perform well when the level of attacks increases. to improve the spam detection performance under an increased level of spam attacks, we further design a new unsupervised social network spam detection scheme, called unik. instead of picking out spam directly, unik works by capturing the properties of non spammers in the network rst, and then clustering suspicious spammers based on the landing pages they are advertising, leveraging both the social graph and the user link graph. the underpinning of unik is that while spammers constantly change their patterns to evade detection, non spammers do not have to do so and thus have a relatively non volatile pattern. unik rst constructs a user link graph connecting users who share url links. given that a spammer often uses di erent accounts to post spam urls, the user link graph constructed would include almost all spammers in the system, although non spammers who share urls are also included. unik then constructs the social graph according to the mutual social connections between users, and identi es non spammers with the help of the social graph. the urls mainly posted by these identi ed non spammers are collected as a url whitelist, which captures the patterns of non spam urls. by trimming non spam url edges matching the whitelist in the user link graph, unik isolates a large portion of nonspammers in the user link graph. finally, unik di erentiates spammers from non spammers with respect to the node degree in the trimmed user link graph and detects the majority of spammers. unik is expected to overcome the limitations of two existing unsupervised spam detection schemes, and sd, as unik exploits non spam patterns to detect spam. first, the autore scheme works by detecting spam that is sent with two patterns: distributed and bursty. correspondingly, spam that is typically posted in the same long duration as normal posts will not be detected. unik works by removing non spammers from the user link graph which covers most spammers, so it is able to detect most of the spam. second, the spam clustering scheme relies on the assumption that spam and non spam posts can be clustered into di erent groups utilizing the sharing urls between them. however, as shown in, spam content often includes non spam urls to increase the legitimacy, which. ectively breaks the assumption even if only a handful legitimate urls are included. unik overcomes this limitation by using the nonspam pattern to remove non spam urls from the user link graph, therefore it is robust to the spam attacks with legitimate urls. third, sd uses a sybil defense algorithm to cluster non spammers and spammers, whose performance is sub ject to the spam attack intensity. unik identi es nonspam url signatures based on the social graph and the url sharing pattern, and then removes non spam urls from the user link graph, thus its. ectiveness is maintained in spite of the spam attack intensity since non spam patterns are largely not. we evaluate the performance of sd and unik with a month dataset from a commercial social blog site. sd shows its superior performance compared to autore and the spam clustering scheme by reducing both the false positive rate and the false negative rate in detecting spam. unik also shows comparable performance to sd when being applied to the dataset. furthermore, unik maintains its performance when the spam attack increases, while the performance of sd degrades accordingly. based on the spam detection result of unik, we have identi ed a number of large spam campaigns in this social blog dataset. our analysis shows that di erent spam campaigns demonstrate distinct characteristics. on one hand, this indicates the ine ectiveness of some detection schemes relying on these spamming patterns. on the other hand, this also means that unik is able to. ectively group spammers into spam campaigns, which provides an opportunity to extract spam signatures from these campaigns and use them to detect spam in other systems. the rest of the paper is organized as follows. section # evaluates the limitations of existing work and motivates our work. section # presents the design, evaluation and analysis of sd. section # illustrates the design of unik, and section # evaluates its performance. section # analyzes the spammer clusters detected by unik. section # discusses other related work, and section # concludes this paper. in this study, we examine the abuse of online social networks at the hands of spammers through the lens of the tools, techniques, and support infrastructure they rely upon. to perform our analysis, we identify over million accounts suspended by twitter for disruptive activities over the course of seven months. in the process, we collect a dataset of billion tweets, million of which belong to spam accounts. we use our dataset to characterize the behavior and lifetime of spam accounts, the campaigns they execute, and the wide spread abuse of legitimate web services such as url shorteners and free web hosting. we also identify an emerging marketplace of illegitimate programs operated by spammers that include twitter account sellers, ad based url shorteners, and spam affiliate programs that help enable underground market diversification. our results show that of spam accounts identified by twitter are suspended within on day of their first tweet. because of these pressures, less than of accounts form social relationships with regular twitter users. instead, of accounts rely on hijacking trends, while of accounts use unsolicited mentions to reach an audience. in spite of daily account attrition, we show how five spam campaigns controlling thousand accounts combined are able to persist for months at a time, with each campaign enacting a unique spamming strategy. surprisingly, three of these campaigns send spam directing visitors to reputable store fronts, blurring the line regarding what constitutes spam on social networks. as twitter continues to grow in popularity, a spam marketplace has emerged that includes services selling fraudulent accounts, af liate programs that facilitate distributing twitter spam, as well as a cadre of spammers who execute large scale spam campaigns despite twitterefforts to thwart their operations. while social network spam has garnered a great deal of attention in the past year from researchers, most of the interest has involved developing tools to detect spam. these approaches rely on url blacklists, passive social networking spam traps, and even manual classi cation to generate datasets of twitter spam for developing a classi er that characterizes abusive behavior. these spam detection approaches however have not yet been used to analyze the tools and techniques of spammers, leaving the underground marketplace that capitalizes on twitter largely obscure. in this paper we characterize the illicit activities of twitter accounts controlled by spammers and evaluate the tools and techniques that underlie the social network spam distribution chain. this infrastructure includes automatically generated accounts created for the explicit purpose of soliciting spam; the emergence of spam as a service programs that connect twitter account controllers to marketers selling products; and nally the techniques required to maintain large scale spam campaigns despite twittercounter efforts. to perform the study, we aggregate over billion messages on twitter sent by million accounts during a seven month period from august, year# to march, year#. within this period, we identify accounts suspended by twitter for abusive behavior, including spam, aggressive friending, and other non spam related offenses. manual analysis indicates that an estimated of suspended accounts were in fact spammers, with the remaining suspended for mimicking news services and aggressive marketing. in total, our dataset consists of over million suspended accounts that we show to be spammers, and million spam tweets from these accounts. in contrast to previous studies, only of the urls we examine were ever caught by blacklists, and the accounts within our dataset are largely fraudulent, as opposed to compromised users. this enables us to provide a unique perspective on a subset of twitter spammers not previously examined. at the heart of the of the twitter spam craft is access to hundreds of accounts capable of reaching a wide audience. we nd that of accounts employed by spammers are suspended within a day of their rst post, and of accounts within three days. the countermeasures imposed by twittersuspension algorithm preclude the possibility of attempting to form meaningful relationships with legitimate users, with of spam accounts having fewer than followers. in place of distributing messages over the social graph, we nd that of spam accounts turn to unsolicited mentions, whereby a personalized message is sent to another ac copyright year# acm year#. count despite the absence of a social relationship. another of accounts rely on embedding hashtags in their messages, allowing spam to garner an audience from users who view popular twitter discussions via search and trending topics. beyond the characteristics of spam accounts, we explore ve of the largest twitter spam campaigns that range from days to months in duration, weaving together fraudulent accounts, diverse spam urls, distinct distribution techniques, and a multitude of monetization approaches. together, these campaigns control thousand account that generate of spam on twitter. surprisingly, three of the largest campaigns direct users to legitimate products appearing on amazon com via af liate links that generate income on a purchase, blurring the line regarding what constitutes spam. indeed, only one of the ve campaigns we analyze advertises content generally found in email spam, revealing a diverse group of miscreants in the underground space that go beyond email spammers. finally, within the amalgam of spam on twitter, we identify an emerging market of spam as a service. this marketplace includes af liate programs that operate as middlemen between spammers seeking to disseminate urls and af liates who control hundreds of twitter accounts. the most prominent af liate program, called clickbank, appeared in over million tweets sent from af liates participating in the program. other services include ad based url shorteners as well as account arbiters who sell the ability to tweet from thousands of accounts under a single servicecontrol. each of these services enables a diversi cation in the social network spam marketplace, allowing spammers to specialize exclusively in hosting content or acquiring twitter accounts. in summary, we frame our contributions as followers: we characterize the spamming tools and techniques of million suspended twitter accounts that sent million tweets. we examine a number of properties pertaining to fraudulent accounts, including the formation of social relationships, account duration, and dormancy periods. we evaluate the wide spread abuse of urls, shortening services, free web hosting, and public twitter clients by spammers. we provide an in depth analysis of ve of the largest spam campaigns targeting twitter, revealing a diverse set of strategies for reaching audiences and sustaining campaigns in twitterhostile environment. we identify an emerging marketplace of social network spam as a service and analyze its underlying infrastructure. popular internet services in recent years have shown that remarkable things can be achieved by harnessing the power of the masses using crowd sourcing systems. however, crowd sourcing systems can also pose a real challenge to existing security mechanisms deployed to protect internet services. thus they would perform poorly or be easily bypassed when attacks are generated by real users working in a crowd sourcing system. through measurements, we have found surprising evidence showing that not only do malicious crowd sourcing systems exist, but they are rapidly growing in both user base and total revenue. we analyze details of campaigns offered and performed in these sites, and evaluate their end to end effectiveness by running active, benign campaigns of our own. finally, we study and compare the source of workers on crowdturfing sites in different countries. many of these security techniques rely on the assumption that malicious activity is generated automatically by automated programs. we describe in this paper a significant effort to study and understand these crowdturfing systems in today internet. we use detailed crawls to extract data about the size and operational structure of these crowdturfing systems. our results suggest that campaigns on these systems are highly effective at reaching users, and their continuing growth poses a concrete threat to online communities both in the us and elsewhere. popular internet services in recent years have shown that remarkable things can be achieved by harnessing the power of the masses. by distributing tasks or questions to large numbers of internet users, these crowd sourcing systems have done everything from answering user questions, to translating books, creating photo tours, and predicting the behavior of stock mar copyright is held by the international world wide web conference committee. online services like amazonmechanical turk, rent a coder, freelancer, and innocentive have created open platforms to connect people with jobs and workers willing to perform them for various levels of compensation. on the other hand, crowd sourcing systems could pose a serious challenge to a number of security mechanisms deployed to protect internet services against automated scripts. technique used, they rely on a common assumption, that the malicious tasks in question cannot be performed by real humans en masse. this is an assumption that is easily broken by crowd sourcing systems dedicated to organizing works to perform malicious tasks. through measurements, we have found surprising evidence showing that not only do malicious crowd sourcing systems exist, but they are rapidly growing in both user base and revenue generated. we found significant evidence of these systems in a number of countries, including the us and india, but focus our study on two of the largest crowdtur ng systems with readily available data, both of which are hosted in and targeted users in china. we use readily available data to quantify both tasks and revenue owing through these systems, and observe that these sites are growing exponentially in both metrics. content on osns, microblogs, blogs, and online forums. make payment figure #: work and cash ow of a crowdtur ng campaign. finally, we study and compare the source of workers on crowdtur ng sites in different countries. we nd that crowdtur ng workers easily cross national borders, and workers in less developed countries often get paid through global payment services for performing tasks affecting us based networks. based online communities such as facebook, twitter, and google. these systems have already established roots in other countries, and are responsible for producing fake social network accounts that look indistinguishable from those of real users. a recent study shows that similar types of behavior are also on the rise in the usbased freelancer site. understanding the operation of these systems from both nancial and technical angles is the rst step to developing effective defenses to protect todayonline social networks and online communities. distribution of these papers is limited to classroom use, and personal use by others. for example, electronic marketplaces want to prevent scripts from automating auction bids, and online social networks want to detect and remove fake users that spread spam. detection techniques include different types of captchas, as well as machine learning that tries to detect abnormal user behavior, eg, near instantaneous responses to messages or highly bursty user events. because of their similarity with both traditional crowd sourcing systems and astrotur ng behavior, we refer to them as crowdturfing systems. more speci cally, we de ne crowdtur ng systems as systems where customers initiate campaigns, and a signi cant number of users obtain nancial compensation in exchange for performing simple tasks that go against accepted user policies. in this paper, we describe a signi cant effort to study and understand crowdtur ng systems in todayinternet. from anecdotal evidence, we learn that these systems are well known to young internet users in china, and have persisted despite threats from law enforcement agencies to shut them down. our study results in four key ndings on the operation and effectiveness of crowdtur ng systems. first, we used detailed crawls to extract data about the size and operational structure of these crowdtur ng systems. second, we study the types of tasks offered and performed in these sites, which include mass account creation, and posting of speci. tasks often ask users to post advertisements and positive comments about websites along with an url. we perform detailed analysis of tasks trying to start. information cascades on microblogging sites, and study the effectiveness of cascades as a function of the microblog social graph. third, we want to evaluate the end to end effectiveness of crowdtur ng campaigns. to do so, we created accounts on one of our target systems, and initiated a number of benign campaigns that provide unsolicited advertisements for legitimate businesses. by bouncing clicks through our redirection server, we log responses to advertisements generated by our campaigns, allowing us to quantify their effectiveness. our data shows that crowdtur ng campaigns can be cost effective at soliciting real user responses. this suggests that the continuing growth of crowdtur ng systems poses a real threat to. this study is the one of the rst to examine the organization and effectiveness of large scale crowdtur ng systems on the internet. finding duplicate and near duplicate images plays an important role on redundancy reduction for image storage, summarization and recommendation. this paper introduces how to speed up duplicate near duplicate image detection. image clustering was first applied to partition the images into multiple groups by using coarse visual features; pair wise image matching was further applied on the images within the same cluster by using fine visual features such as interesting point descriptors. our coarse to fine method can dramatically reduce the computation cost while achieving comparable detection accuracy rate. twitterrank measures the influence taking both the topical similarity between users and the link structure into account. this paper focuses on the problem of identifying influential users of micro blogging services. twitter, one of the most notable micro blogging services, employs a social networking model called following, in which each user can choose who she wants to follow to receive tweets from without requiring the latter to give permission first. in a dataset prepared for this study, it is observed that of the users in twitter follow more than of their followers, and of the users have of users they are following follow them back. our study reveals that the presence of reciprocity can be explained by phenomenon of homophily. based on this finding, twitterrank, an extension of pagerank algorithm, is proposed to measure the influence of users in twitter. experimental results show that twitterrank outperforms the one twitter currently uses and other related algorithms, including the original pagerank and topic sensitive pagerank. section # presents the experimental results, comparing twitterrank with the benchmark method currently used by twitter and other related algorithms. micro blogging is an emerging form of communication. it allows users to publish brief message updates, which can permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. wsdm, february, year#, new york city, new york, usa. be submitted in many di erent channels, including the web and text messaging service. one of the most notable micro blogging services is twitter. unlike other social network services that require users to grant friend links to other users befriending them, twitter employs a social networking model called following, in which each twitterer is allowed to choose who she wants to follow without seeking any permission. conversely, she may also be followed by others without granting permission rst. inoneinstanceof following relationship, the twitterer whose updates are being followed is called the friend, while the one who is following is called the follower. twitter has gained huge popularity since the rst day that it was launched. it has also drawn increasing interests from research community. there is previous work to study the topological and geographical properties of the social network formed by the twitterers and their followers. in this paper, we are interested in identifying the in uential twitterers. first, it potentially brings order to the real time web in that it allows the search results to be sorted by the authority in uence of the contributing twitterers giving a timely update of the thoughts of in uential twitterers. second, accordingto, twitter is also a marketing platform. targeting those in uential users will increase the. for example, a handphone manufacturer can engage those twitterers in uential in topics about it gadgets to potentially in uence more people. there are also applications that utilize twitter to gather opinions and information on particular topics. identifying in uential twitterers for interesting topics can improve the quality of the opinions gathered. currently, twitter and many other applications interpret a twittererin uence as the number of followers she has. however, is this really a good indicator of in uence in a dataset prepared for this study, it is observed that of the users follow more than of their followers, and of the user have of their friends follow them back. two seemingly con icting reasons can possibly another similar service is plurk. users in twitter are usually dubbed twitterers, and the short message updates published by the users tweets. in this paper, an in uential twitterer is one with certain authority within her social network. real time web: http: en wikipedia org wiki realtime web. first, the following relationship is so casual that each twitterer just randomly follows someone, and those being followed follow back just for the sake of courtesy. second, it might be the opposite, ie, the following relationship is a strong indicator of the similarity among users. in other words, a twitterer follows a friend because she is interested in the topics the friend publishes in tweets, and the friend follows back because she nds they share similar topic interest. this phenomenon is called homophily, which has been observed in many social networks. the cause of such reciprocity has important implication here. if it is caused by the rst reason, identifying the in uential twitterers based on following relationship would be rendered meaningless since the following relationship itself does not carry strong indication of in uence. on the other hand, the presence of homophily indicates that the following relationships between twitterers are related to their topical similarity. our study con rms that homophily does exist in the context of twitter. this justi es that there are some twitterers who do seriously follow someone because of common topical interests instead of just playing a number game. based on this observation, we propose a novel approach to measure the in uence of twitterers, known as twitterrank. the framework of the proposed approach is shown in figure #. first, topics that twitterers are interested in are distilled automatically by analyzing the content of their tweets based on the topics distilled, topic speci. finally, we apply our twitterrank algorithm, which is an extension of pagerank, to measure the in uence taking both the topical similarity between twitterers and the link structure into account. figure #: framework of the proposed approach this paper improves the state of the art by making two contributions. first, to the best of our knowledge, this paper is the rst to report homophily in twitter. second, it introduces twitterrank to measure the topic sensitive in uence of the twitterers. prior to this, a twittererin uence is often measured by her node in degree in the network, ie, the number of followers. however, as observed in previous social network analysis studies, in degree does not accurately capture the notion of in uence. pagerank improves over in degree by considering the link structure of the whole network. nevertheless, pagerank ignores the interests of twitterers, which. ects the way twitterers in uence one another. our proposed approach addresses the shortcomings of in degree and pagerank by taking into account both the link structure and topical similarity among twitterers. the rest of this paper is organized as follows: a twitter dataset has been prepared for the purpose of this study. section # describes in detail how the dataset is prepared. topic distillation and the phenomenon of homephily observed in the dataset is elaborated in section #, while twitterrank is proposed in section #. finally, section # concludes with directions for further research.