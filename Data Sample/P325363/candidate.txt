energy efficient computing is important in several systems ranging from embedded devices to large scale data centers. several application domains offer the opportunity to tradeoff quality of service solution for improvements in performance and reduction in energy consumption. programmers sometimes take advantage of such opportunities, albeit in an ad hoc manner and often without providing any qos guarantees. we propose a system called green that provides a simple and flexible framework that allows programmers to take advantage of such approximation opportunities in a systematic manner while providing statistical qos guarantees. green enables programmers to approximate expensive functions and loops and operates in two phases. in the calibration phase, it builds a model of the qos loss produced by the approximation. this model is used in the operational phase to make approximation decisions based on the qos constraints specified by the programmer. the operational phase also includes an adaptation function that occasionally monitors the runtime behavior and changes the approximation decisions and qos model to provide strong statistical qos guarantees. to evaluate the effectiveness of green, we implemented our system and language extensions using the phoenix compiler framework. our experiments using benchmarks from domains such as graphics, machine learning, signal processing, and finance, and an in production, real world web search engine, indicate that green can produce significant improvements in performance and energy consumption with small and controlled qos degradation. motivated by energy constraints, future heterogeneous multi cores may contain a variety of accelerators, each targeting a subset of the application spectrum. beyond energy, the growing number of faults steers accelerator research towards fault tolerant accelerators. in this article, we investigate a fault tolerant and energy efficient accelerator for signal processing applications. we depart from traditional designs by introducing an accelerator which relies on unary coding, a concept which is well adapted to the continuous real world inputs of signal processing applications. unary coding enables a number of atypical micro architecture choices which bring down area cost and energy; moreover, unary coding provides graceful output degradation as the amount of transient faults increases. we introduce a configurable hybrid digital analog micro architecture capable of implementing a broad set of signal processing applications based on these concepts, together with a back end optimizer which takes advantage of the special nature of these applications. for a set of five signal applications, we explore the different design tradeoffs and obtain an accelerator with an area cost of mm. on average, this accelerator requires only of the energy of an atom like core to implement similar tasks. we then evaluate the accelerator resilience to transient faults, and its ability to trade accuracy for energy savings. due to ever stringent technology constraints, especially energy and faults, the micro architecture community has been contemplating increasingly varied approaches for designing architectures which realize different tradeoffs between application exibility, area cost, energy ef ciency and fault tolerance. finding appropriate accelerators is both an open question and fast becoming one of the key challenges of our community. but in addition to technology constraints, there is a similarly important evolution in the systems these chips are used in, and the applications they are used for. our community is currently focused on general purpose computing, but a large array of more or less specialized devices and applications are in increasing demand for performance and sophisticated pro permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. examples include voice recognition which has recently become mainstream on smartphones, an increasing number of cameras which integrate facial expression recognition, or even self driving cars which require fast video and navigation processing, and a host of novel applications stemming from and based upon sensors such as the microsoft kinect, etc. these devices and applications may well drive the creation of less exible but highly ef cient micro architectures in the future. at the very least, they already, or may soon correspond, to applications with high enough volume to justify the introduction of related accelerators in multi core chips. a notable common feature of many of these emerging applications is that they continuously process real world, and often noisy, low frequency, input data, and they perform more or less sophisticated tasks on them. in other words, many of these tasks can be deemed signal processing tasks at large, where the signal nature can vary. features, they have become increasingly similar to full edged processors. for instance the ti has a clock frequency of ghz, a rich instruction set and a cache hierarchy. the rationale for this article is that continuous real world input data processing entails speci. properties which can be leveraged to better cope with the combined evolution of technology and applications. we particularly seek low cost solutions to enable their broad adoption in many devices, enough exibility to accommodate a broad set of signal processing tasks, high tolerance to transient faults, and low energy solutions. the starting point of our approach is to rely on unary coding for representing input data and any data circulating within the architecture. unary coding means that an integer valueis coded with a train ofpulses. this choice may come across as inef cient performance wise, since most architectures would transmit valuein a single cycle using log bits in parallel. both the time margin provided by low frequency input signal and by unary coding enable a set of atypical micro architecture design innovations, with signi cant bene ts in energy and fault tolerance; some of these innovations are nested, one enabling another, as explained thereafter. of unary coding is more progressive tolerance to faults than word based representations. errors occurring on data represented usingbits can exhibit high value variations depending on the magnitude of the bit which suffered the fault. with unary coding, losing one pulse induces a unit error variation, so the amplitude of the error increases progressively with the number of faults. two of the main architecture bene ts are: the switches in the fabric routing network can operate asynchronously, without even resorting to handshaking, the clock tree is smaller than in a usual fabric since fewer elements operate synchronously. of unary coding is the existence of a powerful, but very cost effective operator, for realizing complex signal processing tasks. recent research has shown that most elementary signal processing operations can not only be expressed, but also densely implemented, using a leaky integrate and fire spiking neuron. each operator can be built with just a few such neurons; however, to dispel any possible confusion, there is no learning involved, the weights of the synapses are precisely set, ie, programmed, to accomplish the desired function; the neuron is only used as an elementary operator. using a spiking neuron enables, in turn, additional micro architecture optimizations. a spiking neuron can be realized as a digital or analog circuit, but analog versions have been shown to be much denser and more energy ef cient, so we select that option. however, we avoid two of the main pitfalls of past attempts at designing analog architectures: the dif culty to chain and precisely control analog operators, and the dif culty to program such architectures. for the rst issue, we resort to a hybrid digitalanalog grid like architecture where the routing network and control is asynchronous but digital, while only the computational operators at the grid nodes are analog. the programming issue is overcome by creating a library of signal processing operations, themselves based on the unique nature of the elementary hardware operator. while such graphs can theoretically require a massive number of connections, unary coding enables again a set of unique back end compiler optimizations which can further, and drastically, reduce the overall cost of the architecture. unary coding and the low input signal frequency enable additional micro architecture bene ts. for instance, our fabric routing network is composed of time multiplexed one bit channels instead of multi bit channels. is that real world input data expressed as pulses does not need to be fetched from memory, it can directly come from sensors, saving again on cost and energy. this is possible because analog real world data can be converted into pulses more ef ciently than into digital words, and this conversion can thus be implemented within sensors themselves, or as a front end to the architecture. in this article, we investigate the design of a micro architecture based on these concepts, we propose a detailed implementation, we develop a back end compiler generator to automatically map application graphs onto the architecture, and which plays a signi cant role in keeping the architecture cost low, and we evaluate the main characteristics of this micro architecture on a set of signal processing applications. this architecture is less exible than a processor or an fpga, but far more exible than an asic, and thus capable of harnessing a broad range of applications. the computational unit of this architecture has been successfully taped out, and the full architecture presented in this paper is in the process of being taped out. in section #, we recall the basic concepts allowing to program signal processing applications as graphs of operators, and to implement these operators using analog neurons. in section #, we present the micro architecture and how real world input data characteristics are factored in the design. in section #, we discuss the back end generator, especially which mapping optimizations help keep the architecture cost low. in section #, we introduce the methodology and applications. in section #, we explore the optimal architecture con guration for our target applications, and evaluate the energy, fault tolerance and bandwidth of our architecture. in section #, we present the related work and conclude in section #. digital signal processors have been speci cally designed to cope with such inputs. however, while real world input data may be massive, its update frequency is typically very low, eg, hz for video, and rarely above mhz for most signals, except for high frequency radio signals. the ability to lose pulses with no signi cant impact on application behavior brings, in turn, cost and energy bene ts. a spiking neuron may be slower than a typical digital operator but it realizes more complex operations, such as integration, and the low input signal frequency provides ample time margin. a signal processing application is then expressed as a graph of such operations and mapped onto the architecture. for example, approximate computing applications can often naturally tolerate soft errors. rely allows developers to specify the reliability requirements for each value that a function produces. emerging high performance architectures are anticipated to contain unreliable components that may exhibit soft errors, which silently corrupt the results of computations. full detection and masking of soft errors is challenging, expensive, and, for some applications, unnecessary. we present rely a programming language that enables developers to reason about the quantitative reliability of an application namely, the probability that it produces the correct result when executed on unreliable hardware. we present a static quantitative reliability analysis that verifies quantitative requirements on the reliability of an application, enabling a developer to perform sound and verified reliability engineering. the analysis takes a rely program with a reliability specification and a hardware specification that characterizes the reliability of the underlying hardware components and verifies that the program satisfies its reliability specification when executed on the underlying unreliable hardware platform. we demonstrate the application of quantitative reliability analysis on six computations implemented in rely. many computations, however, can tolerate occasional unmasked errors. in contrast to existing approaches, which support only a binary distinction between critical and approximate regions, quantitative reliability can provide precise static probabilistic acceptability guarantees for computations that execute on unreliable hardware platforms. rely rely is an imperative language that enables developers to specify and verify quantitative reliability speci cations for programs that allocate data in unreliable memory regions and incorporate unreliable arithmetic logical operations. for example, a developer can declare a function signature int, where is the reliability speci cation fors return value. joint reliabilities serve as an abstraction of a functioninput distribution, which enables relyanalysis to be both modular and oblivious to the exact shape of the distributions. we present rely, a language that allows developers to specify reliability requirements for programs that allocate data in unreliable memory regions and use unreliable arithmetic logical operations. true false exp cmp exp bexp lop bexp bexp bexp. skipexpa a exp id id ife bexps; whilee bexp repeate. relylanguage syntax we present a dynamic semantics for rely via a probabilis eiebok eieegt tic small step operational semantics. this semantics is pa define width rameterized by a hardware reliability speci cation that char acterizes the probability that an unreliable arithmetic logical or memory read write operation executes correctly. we formalize the semantics of quantitative reliability as it relates to the probabilistic dynamic semantics of a rely program. we present a program analysis that veri es that the dynamic semantics of a rely program satis es its quantitative reliability speci cations. system reliability is a major challenge in the design of emerging architectures. energy ef ciency and circuit scaling are becoming major goals when designing new devices. however, aggressively pursuing these design goals can often increase the frequency of soft errors in small and large systems alike. researchers have developed numerous techniques for detecting and masking soft errors in both hardware and software. these techniques typically come at the price of increased execution time, increased energy consumption, or both. an approximate computation can often acceptably tolerate occasional errors in its execution and or the data that it manipulates. a checkable computation can be augmented with an ef cient checker that veri es the acceptability of the computationresults. if the checker does detect an error, it can reexecute the computation to obtain an acceptable result. for both approximate and checkable computations, operating without mechanisms that detect and mask soft errors can produce fast and energy ef cient execution that delivers acceptably accurate results often enough to satisfy the needs of their users despite the presence of unmasked soft errors. background researchers have identi ed a range of both approximate computations and checkable computations. their results show that it is possible to exploit these properties for a variety of purposes increased performance, reduced energy consumption, increased adaptability, and increased fault tolerance. one key aspect of such computations is that they typically contain critical regions and approximate regions. to support such computations, researchers have proposed energy ef cient architectures that, because they omit some error detection and correction mechanisms, may expose soft errors to the computation. a key aspect of these architectures is that they contain both reliable and unreliable components for executing the critical and approximate regions of a computation, respectively. the rationale behind this design is that developers can identify and separate the critical regions of the computation from the approximate regions of the computation. existing systems, tools, and type systems have focused on helping developers identify, separate, and reason about the binary distinction between critical and approximate regions. however, in practice, no computation can tolerate an unbounded accumulation of soft errors to execute acceptably, even the approximate regions must execute correctly with some minimum reliability. quantitative reliability we present a new programming language, rely, and an associated program analysis that computes the quantitative reliability of the computationie, the probability with which the computation produces a correct result when its approximate regions execute on unreliable hardware. more speci cally, given a hardware speci cation and a rely program, the analysis computes, for each value that the computation produces, a conservative probability that the value is computed correctly despite the possibility of soft errors. rely supports quantitative reliability speci cations for the results that functions produce. the symbolic expression is the joint reliability ofandnamely, the probability that they both simultaneously have the correct value on entry to the function. this speci cation states that the reliability of the return value ofmust be at least ofands reliability when the function was called. this is important because such exact shapes can be dif cult for developers to identify and specify and known tractable classes of probability distributions are not closed under many operations found in standard programming languages, which can complicate attempts to develop compositional program analyses that work with such exact shapes. rely assumes a simple machine model that consists of a processor and a main memory. the model includes unreliable arithmetic logical operations and unreliable physical memories. rely works with a hardware reliability speci cation that lists the probability with which each operation in the machine model executes correctly. rely is an imperative language with integer, logical, and oating point expressions, arrays, conditionals, while loops, and function calls. in addition to these standard language features, rely also allows a developer to allocate data in unreliable memories and write code that uses unreliable arithmetic logical operations. for example, the declaration intin urel allocates the variablein an unreliable memory named urel where both reads and writes ofmay fail with some probability. which is an unreliable addition of the values a andthat may produce an incorrect result. we have designed the semantics of rely to exploit the full availability of unreliable computation in an application. speci cally, rely only requires reliable computation at points where doing so ensures that programs are memory safe and exhibit control ow integrity. relysemantics models an abstract machine that consists of a heap and a stack. the stack consists of frames that contain references to the locations of each invoked functionvariables. to protect references from corruption, the stack is allocated in a reliable memory region and stack operationsie, pushing and popping frames execute reliably. to prevent out ofbounds memory accesses that may occur as a consequence of an unreliable array index computation, rely requires that each array read and write includes a bounds check; these bounds check computations also execute reliably. underlying mechanism to execute these operations reliably; one can use any applicable software or hardware technique. to prevent the execution from taking control ow edges that are not in the programstatic control ow graph, rely assumes that instructions are stored, fetched, and decoded reliably and control ow branch targets are computed reliably. quantitative reliability analysis given a rely program and a hardware reliability speci cation, relyanalysis uses a precondition generation approach to generate a symbolic reliability precondition for each function. a reliability precondition captures a set of constraints that is suf cient to ensure that a function satis es its reliability speci cation when executed on the underlying unreliable hardware platform. the reliability precondition is a conjunction of predicates of the form aoutr, where aout is a placeholder for a developer provided reliability speci cation for an output named out, is a real number between and, and the term is the joint reliability of a set of parameters. conceptually, each predicate speci es that the reliability given in the speci cation should be less than or equal to the reliability of a path that the program may take to compute the result. the analysis computes the reliability of a path from the probability that all operations along the path execute reliably. the speci cation is valid if the probabilities for all paths to computing a result exceed that of the resultspeci cation. to avoid the inherent intractability of considering all possible paths, rely uses a simpli cation procedure to reduce the precondition to one that characterizes the least reliable path through the function. one of the core challenges in designing rely and its analysis is dealing with unreliable computation within loops. the reliability of variables updated within a loop may depend on the number of iterations that the loop executes. speci cally, if a variable has a loop carried dependence and updates to that variable involve unreliable operations, then the variablereliability is a monotonically decreasing function of the number of iterations of the loop on each loop iteration the reliability of the variable degrades relative to its previous reliability. if a loop does not have a compile time bound on the maximum number of iterations, then the reliability of such a variable can, in principle, degrade arbitrarily, and the only conservative approximation of the reliability of such a variable is zero. to provide speci cation and veri cation exibility, rely provides two types of loop constructs: statically bounded while loops and statically unbounded while loops. statically bounded while loops allow a developer to provide a static bound on the maximum number of iterations of a loop. the dynamic semantics of such a loop is to exit if the number of executed iterations reaches this bound. this bound allows relyanalysis to soundly construct constraints on the reliability of variables modi ed within the loop by unrolling the loop for its maximum bound. statically unbounded while loops have the same dynamic semantics as standard while loops. in the absence of a static bound on the number of executed loop iterations, however, relyanalysis constructs a dependence graph of the loopbody to identify variables that are reliably updated speci cally, all operations that in uence these variables values are reliable. because the execution of the loop does not decrease the reliability of these variables, the analysis identi es that their reliabilities are unchanged. for the remaining, unreliably updated variables, relyanalysis conservatively sets their reliability to zero. in the last step of the analysis of a function, rely veri es that the functionspeci cations are consistent with its reliability precondition. because reliability speci cations are also of the formr, the nal precondition is a conjunction of predicates of the form, where is a reliability speci cation and is a path reliability. if these predicates are valid, then the reliability of each computed output is greater than that given by its speci cation. the validity problem for these predicates has a sound mapping to the conjunction of two simple constraint validity problems: inequalities between real numbers and set inclusion constraints over nite sets. checking the validity of a reliability precondition is therefore decidable and ef ciently checkable. case studies we have used rely to build unreliable versions of six building block computations for media processing, machine learning, and data analytics applications. our case studies illustrate how quantitative reliability enables a developer to use principled reasoning to relax the semantics of both approximate and checkable computations. for approximate computations, quantitative reliability allows a developer to reify and verify the results of the fault injection and accuracy explorations that are typically used to identify the minimum acceptable reliability of a computation. for checkable computations, quantitative reliability allows a developer to use the performance speci cations of both the computation and its checker to determine the computationoverall performance given that with some probability it may produce an incorrect answer and therefore needs to be reexecuted. contributions this paper presents the following contributions: quantitative reliability speci cations. we present quantitative reliability speci cations, which characterize the probability that a program executed on unreliable hardware produces the correct result, as a constructive method for developing applications. quantitative reliability speci cations enable developers who build applications for unreliable hardware architectures to perform sound and veri ed reliability engineering. speci cally, we de ne the quantitative reliability of a variable as the probability that its value in an unreliable execution of the program is the same as that in a fully reliable execution. we also de ne the semantics of a logical predicate language that can characterize the reliability of variables in a program. for each function in the program, the analysis computes a symbolic reliability precondition that characterizes the set of valid speci cations for the function. the analysis then veri es that the developer provided speci cations are valid according to the reliability precondition. we have used our rely implementation to develop unreliable versions of six building block computations for media processing, machine learning, and data analytics applications. these case studies illustrate how to use quantitative reliability to develop and reason about both approximate and checkable computations in a principled way. critical computation subgraphs can be accelerated by collapsing them into new instructions that are executed on specialized function units. the main problem with this approach is that a new processor must be generated for each application domain. in this work, we propose a strategy to transparent customization of the core computation capabilities of the processor without changing its instruction set. a congurable array of function units is added to the baseline processor that enables the acceleration of a wide range of data flow subgraphs. application specific instruction set extensions are an effective way of improving the performance of processors. collapsing the subgraphs simultaneously reduces the length of computation as well as the number of intermediate results stored in the register file. while new instructions can be designed automatically, there is a substantial amount of engineering cost incurred to verify and to implement the final custom processor. to exploit the array, the microarchitecture performs subgraph identification at run time, replacing them with new microcode instructions to configure and utilize the array. we compare the effectiveness of replacing subgraphs in the fill unit of a trace cache versus using a translation table during decode, and evaluate the tradeoffs between static and dynamic identification of subgraphs for instruction set customization. ective at improving performance, typically yielding several orders of magnitude speedup along with reduced energy consumption. or a change in standards, the application will usually no longer be able to take advantage of the asic. rewriting an application can be a large engineering burden. instruction set customization is another method for providing enhanced performance in processors. instruction set extensions also maintain a degree of system programmability, which enables them to be utilized with more exibility. instruction set extensions is that there are signi cant non recurring engineering costs associated with implementing them. for example, a new set of masks must be created to fabricate the chip, the chip must be reveri ed, and the new instructions must. the goal is to extract many of the bene ts of traditional instruction set customization without having to break open the processor design each time. the cca consists of an array of function units that can. several di erent strategies are proposed for accomplishing transparent instruction set customization. one strategy, a fully dynamic scheme, performs subgraph identi cation and instruction replacement in hardware. to reduce hardware complexity, a static strategy performs subgraph identi cation. the contributions of this paper are twofold: we present the design of the cca, which provides the functionality of common application speci. instruction set extensions in a single hardware unit. in embedded computing, a common method for providing performance improvement is to create customized hardware solutions for particular applications. unfortunately, there are also negative aspects to using asics. the primary problem is that asics only provide a hardwired solution, meaning that only a few applications will be able to fully bene. if an application changes, because of a bug. another drawback is that even when an application can utilize an asic, it must be speci cally rewritten to do so. extensions to an instruction set, the critical portions of an applicationdata ow graph can be accelerated by mapping them to specialized hardware. ective as asics, instruction set extensions improve performance and reduce energy consumption of processors. is that automation techniques, such as the ones used by arm optimode, tensilica, and arc, have been developed to allow the use of instruction set extensions without undue burden on hardware and software designers. the addition of instruction set extensions to a baseline processor brings along with it many of the issues associated with designing a brand new processor in the rst place. furthermore, extensions designed for one domain are often not useful in another, due to the diversity of computation causing the extensions to have only limited applicability. to overcome these problems, we focus on a strategy to customize the computation capabilities of a processor within the context of a general purpose instruction set, referred to as transparent instruction set customization. this is achieved by adding a con gurable compute accelerator to the baseline processor design that provides the functionality of a wide range of application speci. instruction set extensions in a single hardware unit. oaded to the cca and then replaced with microarchitectural instructions that con gure and utilize the array. subgraphs that are to be mapped onto the cca are marked in the program binary to facilitate simple cca con guration and replacement at run time by the hardware. a detailed analysis of the cca design shows that it implements the most common subgraphs while keeping control cost, delay, and area overhead to a minimum. we describe the hardware and software algorithms necessary to facilitate dynamic customization of a microarchitectural instruction stream. for example, embedded systems often have one or more applicationspeci. as technology scales ever further, device unreliability is creating excessive complexity for hardware to maintain the illusion of perfect operation. in this paper, we consider whether exposing hardware fault information to software and allowing software to control fault recovery simplifies hardware design and helps technology scaling. the combination of emerging applications and emerging many core architectures makes software recovery a viable alternative to hardware based fault recovery. emerging applications tend to have few io and memory side effects, which limits the amount of information that needs checkpointing, and they allow discarding individual sub computations with small qualitative impact. software recovery can harness these properties in ways that hardware recovery cannot. we describe relax, an architectural framework for software recovery of hardware faults. relax includes three core components: an isa extension that allows software to mark regions of code for software recovery, a hardware organization that simplifies reliability considerations and provides energy efficiency with hardware recovery support removed, and software support for compilers and programmers to utilize the relax isa. applying relax to counter the effects of process variation, our results show a energy efficiency improvement for parsec applications with only minimal source code changes and simpler hardware. as cmos technology scales, individual transistor components will soon consist of only a handful of atoms. at these permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page to copyotherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. sizes, transistors are extremely di cult to control in terms of their individual power and performance characteristics, their susceptibility to soft errors caused by particle strikes, the rate at which their performance degrades over time, and their manufacturability concerns commonly referred to as variability, soft errors, wear out, and yield, respectively. already, the illusion that hardware is perfect is becoming hard to maintain at the vlsi circuit design, cad, and manufacturing layers. ciency are lost due to the conservative voltage and frequency assumptions necessary to overcome unpredictability. this trend towards increasingly unreliable hardware has led to an abundance of work on hardware fault detection and recovery. additionally, researchers have explored architectural pruning and timing speculation as ways to mitigate chip design and manufacturing constraints. however, in all cases these proposals have focused on conventional applications running on conventional architectures, with a typical separation of hardware and software concerns. in this paper, we observe two complementary trends in emerging applications and architectures that favor a new overall architectural vision: hardware faults recovered in software. below, we explain these trends, articulate the challenges in designing an architecture with software recovery, and nally describe our proposed framework, relax. emerging applications applications that continue to drive increases in chip performance include computer vision, data mining, search, media processing, and data intensive scienti. many of these applications have two distinct characteristics that make them interesting from a reliability perspective. first, and a key observation unique to this work, is that many have few memory side. in particular, state modifying io operations are rare and memory operations are primarily loads, because the compute regions of these applications perform reductions over large amounts of data. second, for many emerging applications, a perfect answer is not attainable due to the inherent computational complexity of the problem and or noisy input data. therefore, they employ approximation techniques to maximize the qualitative usefulness of their output. this suggests that these applications might be error tolerant, which has been observed in prior work as well. in this paper, we speci cally explore the phenomenon that the application can discard computations in the event of an error. the concurrent architecture trend is that massively multicore architectures are emerging to meet the computational copyright year#acm. figure #: the evolution of hardware, architecture, and applications in the context of relax. these architectures often employ simple, in order cores to maximize throughput and energy. ciency with little or no support for speculative execution or bu ering. hence, the paradigm that hardware misspeculation recovery mechanisms can be repurposed for error recovery does not apply for these architectures. the valuable chip real estate that would otherwise be devoted to hardware recovery resources could be better spent elsewhere if software recovery were. ects and error tolerance that exists in large portions of emerging applications renders hardware recovery in exible, unnecessarily conservative, and too expensive for emerging many core architectures. figure # shows the evolutionary path to software recovery considering these trends in hardware, architecture, and applications. historically, traditional applications running on traditional superscalar processor architectures built with perfect cmos devices required no recovery. even with imperfect cmos, these applications still work best utilizing hardware recovery when running on traditional processor architectures. however, with emerging applications running on emerging many core architectures, hardware recovery introduces the ine ciencies we have described. in the future, while hardware substrates will be unreliable, we require mechanisms that provide exibility to software and keep the architecture simple. an architecture that exposes hardware errors to allow software recovery enables synergy between applications and architectures as shown in figure #. the design of a system architecture that allows such software recovery of hardware faults involves many important questions and challenges. the rst and most obvious question is whether changes to the isa are necessary. to answer this question, we refer to prior studies that show application tolerance to arbitrary instruction level errors is very poor. operations relating to control ow and memory accesses are failure prone and constitute a large percentage of application operations. for an architecture to allow reasonably ne grained software recovery without isa changes, it would be necessary for the hardware to somehow distinguish these critical operations from the non critical operations as it executes code. to date, no one has been able to propose an. the next logical question concerns what form isa support should take. software recovery of hardware faults has been proposed before in the context of software detection, using compiler automated triple modular redundancy. tmr makes sense when the overhead of detection is already very high, as is the case with comprehensive software detection. however, it is expensive and does not allow the application to exploit error tolerance. cient solution that allows an application to choose its own form of recovery is closer to ideal. we divide relax into three core components: an isa extension, hardware support to implement the relax isa, and software support for applications to use the relax isa. we discuss each component in a separate section: isa extension: in section #, we describe the relax isa extension, which enables software to register a fault handler for a region of code. the extension allows applications to encode behavior similar to the try catch behavior found in modern programming languages. the isa behavior is intuitive to programmers, and the compiler and hardware combine to make guarantees about the state of the program as the region is executed. we also provide a rigorous de nition of the isasemantics. hardware support: we cover the hardware support for relax in section #. the relax isasemantics allow hardware design simpli cation and provide energy. ciency by relaxing the reliability constraints of the hardware. we describe support for fault detection and discuss hardware organizations that support relax. we show that mechanisms such as aggressive voltage scaling, frequency overclocking, and turning. we also consider statically heterogeneous architectures, where cores are constructed with di erent reliability guarantees at design time. software support: in section #, we develop ac language level recovery construct to expose the relax isa extension to developers. we propose two key ideas: relax blocks to mark regions that may experience a hardware fault, and optional recover blocks to specify recovery code if a fault occurs. our results indicate promise for alternative forms of application support as well, such as automated support through compiler static analysis or pro le guided compilation. to support relax, we develop performance models to guide the development of relaxed applications. the models, discussed in section #, determine the. ciency of relax based on application and architecture characteristics and can be used to compute the achievable. ciency improvements for a given application, recovery behavior, and architecture combination. we evaluate relax in sections and, where we apply our language construct and relax compiler to real applications, and simulate how relax enables energy. ciency gains using process variation as a case study. we discuss directions for future work in section #, related work in section #, and nally we conclude in section #. to model single core scaling, we combine measurements from over processors to derive pareto optimal frontiers for area performance and power performance. since year#, processor designers have increased core counts to exploit moore law scaling, rather than focusing on single core performance. the failure of dennard scaling, to which the shift to multicore parts is partially a response, may soon limit multicore scaling just as single core scaling has been curtailed. this paper models multicore scaling limits by combining device scaling, single core scaling, and multicore scaling to measure the speedup potential for a set of parallel workloads for the next five technology generations. for device scaling, we use both the itrs projections and a set of more conservative device scaling parameters. finally, to model multicore scaling, we build a detailed performance model of upper bound performance and lower bound core power. the multicore designs we study include single threaded cpu like and massively threaded gpu like multicore chip organizations with symmetric, asymmetric, dynamic, and composed topologies. the study shows that regardless of chip organization and topology, multicore scaling is power limited to a degree not widely appreciated by the computing community. even at nm, of a fixed size chip must be powered off, and at nm, this number grows to more than. through year#, only average speedup is possible across commonly used parallel workloads, leaving a nearly fold gap from a target of doubled performance per generation. this paper considers all those factors together, projecting upper bound performance achievable through multicore scaling, and measuring the. moorelaw has been a fundamental driver of computing. for the past three decades, through device, circuit, microarchitecture, architec permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. ture, and compiler advances, moorelaw, coupled with dennard scaling, has resulted in commensurate exponential performance increases. the recent shift to multicore designs has aimed to increase the number of cores along with transistor count increases, and continue the proportional scaling of performance. as a result, architecture researchers have started focusing on core and year# core chips and related research topics and called for changes to the undergraduate curriculum to solve the parallel programming challenge for multicore designs at these scales. with the failure of dennard scaling and thus slowed supply voltage scaling core count scaling may be in jeopardy, which would leave the community with no clear scaling path to exploit continued transistor count increases. since future designs will be power limited, higher core counts must provide performance gains despite the worsening energy and speed scaling of transistors, and given the available parallelism in applications. by studying these characteristics together, it is possible to predict for how many additional technology generations multicore scaling will provide a clear bene t. ciency of devices is not scaling along with integration capacity, and since few applications have parallelism levels that can. ciently use a core or year# core chip, it is critical to understand how good multicore performance will be in the long term. in year#, will processors have times the performance of processors from year#, exploiting ve generations of core doubling such a study must consider devices, core microarchitectures, chip organizations, and benchmark characteristics, applying area and power limits at each technology node. ects of non ideal device scaling, including the percentage of dark silicon on future multicore chips. additional projections include best core organization, best chip level topology, and optimal number of cores. we consider technology scaling projections, single core design scaling, multicore design choices, actual application behavior, and microarchitectural features together. previous studies have also analyzed these features in various combinations, but not all together. this study builds and combines three models to project performance and fraction of dark silicon on xed size and xed power chips as listed below: device scaling model: area, frequency, and power requirements at future technology nodes through year#. core scaling model: power performance and area performance single core pareto frontiers derived from a large set of diverse microprocessor designs. multicore scaling model: area, power and perfor mance of any application for any chip topology for cpulike and gpu like multicore performance. devm corm: pareto frontiers at future technology nodes; any performance improvements for future cores will come only at the cost of area or power as de ned by these curves. cmpm devm corm and an exhaustive state space search: maximum multicore speedups for future technology nodes while enforcing area, power, and benchmark constraints. the results from this study provide detailed best case multicore performance speedups for future technologies considering real applications from the parsec benchmark suite. our results evaluating the parsec benchmarks and our upper bound analysis con rm the following intuitive arguments: contrary to conventional wisdom on performance improvements from using multicores, over ve technology generations, only average speedup is possible using itrs scaling. ii while transistor dimensions continue scaling, power limitations curtail the usable chip fraction. at nm, of the chip will be dark and at nm, over of the chip will not be utilized using itrs scaling. iii neither cpu like nor gpu like multicore designs are su cient to achieve the expected performance speedup levels. radical microarchitectural innovations are necessary to alter the power performance pareto frontier to deliver speed ups commensurate with moorelaw. disciplined approximate programming lets programmers declare which parts of a program can be computed approximately and consequently at a lower energy cost. the compiler proves statically that all approximate computation is properly isolated from precise computation. we describe an isa extension that provides approximate operations and storage, which give the hardware freedom to save energy at the cost of accuracy. the basis of our design is dual voltage operation, with a high voltage for precise operations and a low voltage for approximate operations. the hardware is then free to selectively apply approximate storage and approximate computation with no need to perform dynamic correctness checks. in this paper, we propose an efficient mapping of disciplined approximate programming onto hardware. we then propose truffle, a microarchitecture design that efficiently supports the isa extensions. the key aspect of the microarchitecture is its dependence on the instruction stream to determine when to use the low voltage. we evaluate the power savings potential of in order and out of order truffle configurations and explore the resulting quality of service degradation. we evaluate several applications and demonstrate energy savings up to. as improvements in per transistor speed and energy efficiency diminish, radical departures from conventional approaches are needed to continue improvements in the performance and energy efficiency of general purpose processors. general purpose approximate computing explores a third dimension error and trades the accuracy of computation for gains in both energy and performance. techniques to harvest large savings from small errors have proven elusive. for a set of diverse applications, npu acceleration provides whole application speedup of and energy savings of on average with average quality loss of at most. one such departure is approximate computing, where error in computation is acceptable and the traditional robust digital abstraction of near perfect accuracy is relaxed. conventional techniques in energy efficient computing navigate a design space defined by the two dimensions of performance and energy, and traditionally trade one for the other. this paper describes a new approach that uses machine learning based transformations to accelerate approximation tolerant programs. the core idea is to train a learning model how an approximable region of code code that can produce imprecise but acceptable results behaves and replace the original code region with an efficient computation of the learned model. we use neural networks to learn code behavior and approximate it. we describe the parrot algorithmic transformation, which leverages a simple programmer annotation to transform a code region from a von neumann model to a neural model. after the learning phase, the compiler replaces the original code with an invocation of a low power accelerator called a neural processing unit. the npu is tightly coupled to the processor pipeline to permit profitable acceleration even when small regions of code are transformed. offloading approximable code regions to npus is faster and more energy efficient than executing the original code. npus form a new class of accelerators and show that significant gains in both performance and efficiency are achievable when the traditional abstraction of near perfect accuracy is relaxed in general purpose computing. it is widely understood that energy efficiency now fundamentally limits microprocessor performance gains. in this post dennard scaling era, solutions that improve performance and efficiency while retaining as much generality as possible are highly desirable; hence the exploding interest in gpgpus and fpgas. these applications are common in mobile, embedded, and server systems and fall into four broad categories: applications with analog inputs. cmos scaling is no longer providing gains in efficiency commensurate with transistor density increases, as a result, both the semiconductor industry and the research community are increasingly focusing on specialized accelerators, which can provide large gains in efficiency and performance by restricting the workloads that benefit. recent work has quantified three orders of magnitude of difference in efficiency between general purpose processors and asics the community is facing an iron triangle in this respect; we can choose any two of performance, energy efficiency, and generality at the expense of the third. before the traditional trend of transistor scaling dennard scaling broke down, we were able to improve all three on a consistent basis for decades. such programmable accelerators exploit some characteristic of an application domain to achieve efficiency gains at the cost of generality. fpgas, for example, exploit copious, fine grained, and irregular parallelism while gpus exploit many threads and data level simd style parallelism. whether an application can use an accelerator effectively depends on the degree to which it exhibits the acceleratorrequired characteristics. tolerance to approximation is one such program characteristic. a growing body of recent work, has focused on approximation as a strategy for improving efficiency. large classes of applications can tolerate small errors in their outputs with no discernible loss in their quality of result. this category includes image processing, sensor data processing, voice recognition, etc, that operate on noisy real world data. they are inherently resilient to some noise and can handle an extra noise resulting from approximation. technology scaling has delivered on its promises of increasing device density on a single chip. however, the voltage scaling trend has failed to keep up, introducing tight power constraints on manufactured parts. in such a scenario, there is a need to incorporate energy efficient processing resources that can enable more computation within the same power budget. energy efficiency solutions in the past have typically relied on application specific hardware and accelerators. unfortunately, these approaches do not extend to general purpose applications due to their irregular and diverse code base. towards this end, we propose beret, an energy efficient co processor that can be configured to benefit a wide range of applications. our approach identifies recurring instruction sequences as phases of temporal regularity in a program execution, and maps suitable ones to the beret hardware, a three stage pipeline with a bundled execution model. this judicious off loading of program execution to a reduced complexity hardware demonstrates significant savings on instruction fetch, decode and register file accesses energy. on average, beret reduces energy consumption by a factor of for the program regions selected across a range of general purpose and media applications. the average energy savings for the entire application run was over a single issue in order processor. the traditional microprocessor was designed with an objective of running general purpose programs at a good performance, while treating ef ciency as a second order criteria. however, with a growing demand for on chip resource integration, longer battery life and lower heat dissipation in modern day devices, there is an emerging need to improve computational energy ef ciency. the trend in the silicon integration is also reinforcing this need for energyef cient architectures. over the years, transistor density and performance have continued to increase as per moorelaw, however, the threshold voltage has not kept up with this trend. as a result, the per transistor switching power has not witnessed the bene ts of scaling, causing a steady rise in power density. overall, this limits the number of resources that can be kept active on a die simultaneously. an instance of this trend can be already seen in intelnehalem generation of processors that boost the performance of one core, at the cost of slowing down shutting off the rest of them. while the importance of ef ciency today is being felt across all domains of computing, from datacenters cooling costs to smartphone battery lives, a majority of past works have focussed on the embedded application domain. these solutions have leveraged specialized hardware units, loop accelerators, and wide simd support to save energy. unfortunately, these specialization approaches do not directly extend to general purpose applications such as desktop workloads, spec integer suite, os utilities, library codes etc, for several reasons. first, these applications have a highly irregular program structure, and contain a large amount of control ow. for instance, the large, uncounted, non modulo schedulable loops in these applications cannot be mapped to las. second, these irregular codes exhibit little, if any, data level parallelism. this limits the applicability of simd support for energy savings. and nally, the generalpurpose application space is very diverse and constantly evolving, therefore, designing a custom hardware for each of these programs is not very cost effective. despite its shortcomings, specialized hardware like asics form an important design point in the space of techniques to improve performance and ef ciency. in fact, a recent work makes a case for function level asics in the context of irregular codes, claiming the large availability of dark silicon. the advantage here is that carefully customized data paths and long instruction ranges deliver highest levels of ef ciency. the disadvantage of asics is that each of them can handle only one application function. a second class of performance ef ciency solution that overcomes this challenge is the work on programmable functional units. pfus allow a small chain of operations to execute together using a applications with asic support remaining instruction applications gpp asics and gpp beret loop accelerators generality any application most applications single application range instructions instructions instructions figure #: solution classes to improve computational ef ciency of general purpose processors. range of dynamic instructions offloaded determines frequency of communication with the main core, and correlates well with energy savings. the pfu class covers custom instructions and subgraph accelerators that can target virtually any application, but work on small ranges. asics provide a much larger range, but are typically exclusive to an applicationfunction. beret is our proposed design point, which provides application exibility while also covering large instruction ranges. the advantage is its universal applicability to almost any program. however, the energy ef ciency gains are limited due to a small instruction range, and an emphasis on processor back end. studies have shown that a large fraction of application energy is consumed by the processor front end. the two solution classes discussed above fall into opposing extremes, with one providing large ef ciency gains, and the other. to bridge this gap, this paper proposes beret, a con gurable co processor that achieves signi cant energy savings for the selected program regions mapped onto it, without sacri cing performance. as the approach tries to bridge the gap between pfus and asics, it has to simultaneously achieve two objectives, increase instruction range relative to pfus, and make the design exible across applications unlike asics. for increasing the instruction range, the insight is to leverage recurring instructions sequence in a programexecution. such a sequence consists of instructions that repeatedly execute back toback with a high likelihood, despite the presence of intervening control instructions. these recurring sequences represent phases of temporal regularity in an otherwise irregular code, and make good candidates for mapping to beret. conceptually these are traces or frames, with an added requirement of forming a loop. hereonwards, we refer to them as hot traces or recurring traces. the recurring traces provide several bene ts such as long instruction ranges, predictable code behavior and appearance of structure to irregular codes, all of which help in designing a simple and ef cient co processor hardware. more importantly, as these traces are signi cantly shorter than the original unstructured loops, beret buffers them internally and eliminates redundant fetches and decodes. the second objective of beret is to support multiple applications. the insight here is to use a bundled execution model for running the traces. in this model, instead of executing one instruction at a time, beret uses compiler analysis to break down traces into bundles of instructions, and executes them sequentially. these bundles are essentially subgraphs from the trace wide data ow graph. further, our analysis of application traces demonstrated that many subgraph structures are common within as well as across applications. thus, given a diverse enough collection of subgraph execution blocks, our compilation scheme is able to break down any given recurring trace into constituent subgraphs from this collection. in terms of energy savings, a major advantage of this bundled execution is that it signi cantly cuts down on the redundant register reads and writes for the temporary variables. overall, we consider this bundled execution model a trade off design that lets us achieve ef ciency gains nearer to an application speci. data ow hardware while maintaining application universality of regular von neumann execution model. leveraging these two insights, beret is designed as a subgraphlevel execution pipeline for recurring instruction sequences that enables signi cant energy savings for general purpose programs. primarily, the energy savings come from large reductions in redundant fetches, decodes, and register reads and writes for temporary variables. beret also represents a hybrid accelerator design point in the ef ciency solution space where a large range of instructions are of oaded and most applications bene. recent advances in the neuroscientific understanding of the brain are bringing about a tantalizing opportunity for building synthetic machines that perform computation in ways that differ radically from traditional von neumann machines. these brain like architectures, which are premised on our understanding of how the human neocortex computes, are highly fault tolerant, averaging results over large numbers of potentially faulty components, yet manage to solve very difficult problems more reliably than traditional algorithms. a key principle of operation for these architectures is that of automatic abstraction: independent features are extracted from highly disordered inputs and are used to create abstract invariant representations of the external entities. this feature extraction is applied hierarchically, leading to increasing levels of abstraction at higher levels in the hierarchy. this paper describes and evaluates a biologically plausible computational model for this process, and highlights the inherent fault tolerance of the biologically inspired algorithm. we introduce a stuck at fault model for such cortical networks, and describe how this model maps to hardware faults that can occur on commodity gpgpu cores used to realize the model in software. we show experimentally that the model software implementation can intrinsically preserve its functionality in the presence of faulty hardware, without requiring any reprogramming or recompilation. this model is a first step towards developing a comprehensive and biologically plausible understanding of the computational algorithms and microarchitecture of computing systems that mimic the human cortex, and to applying them to the robust implementation of tasks on future computing systems built of faulty components. the originalvon neumann modelofa computing unit has been a relatively nice. for the technology evolutions of the past four decades. however, it is hard not to notice that this model is under growing pressure. the power dissipation bottleneck has made architects shift their focus to multi core architectures but the programming bottleneck of multi cores raises doubts on the ability to truly take advantage of many core systems. more recently, the reliability bottleneck brings a whole new set of challenges to the table. architects have attempted to meet all these challenges, but the proposed solutions progressively erode performance scalability. with such limitations, it now makes sense to investigate alternative models better suited to cope with this technology evolution. either upcoming ultra cmos technology, or alternative technologies like nanotubes, share some common properties. first, the number of individual transistors elements will continue to increase. second, these elements will not necessarily be muchfaster. third, theywill come with a growing numberof defects andfaults. now, when one considers these properties, it is hard not to observe that nature has found a way to harness a huge number of elements with similar properties to realize complex information processing tasks. the fact that biologists have made tremendous progress in understanding the working of parts of the brain is not yet wellknown to computer architects. considering the abilities of the brain, it is clear that computer architects should leverage this information, if only for special purpose computing systems. computer architects are uniquely positioned for this task because, unlike biologists, their goal is to steer this research towards useful computing systems and applications. although both the elementary components and the resulting biologically inspired computing systems are quite different from existing systems, similar approaches can and should be used to architect them. these approaches include understanding how to combine and control elementary components hierarchically into increasingly complex building blocks, de ning a programming approach for these computing systems, understanding their potential applications scope, understanding the appropriate modelling level to integrate billions of components without beingoverwhelmedby complexity nor missingkeyproperties, and so on. using the example of vision processing, and employing speci. literature, we show that it is now possible to rebuild replicate certain cortical sensory tasks out of elementary neurons, thereby providing a detailed explanation of how such functions can emerge, and operate, within the brain. next, we highlight that the corresponding architectures are based on a small set of structural and operating rules, which are local by nature, and that repetitively and hierarchically applying these rules results in architectures capable of increasingly complex tasks. consequently, these architectures have intrinsic scalability properties: they easily translate additional components into increasingly powerful processing tasks; unlike in traditional computers, scalability here means more complex functions rather than faster execution, though both can be related. these architectures also have programmability assets: they only rely on the repeated exposure to data, without resorting to the supervised training common in arti cial neural networks. gpgpubased software implementation of a hierarchical cortical network, modeled after the visual cortex, that is capable of complex visual recognition tasksina biologically plausiblefashion. within this context, this paper makes the following novel contributions: we describe how the random structure and permanent learning properties of the cortical network model are intrinsically robust to permanent defects, far beyond the capabilities of traditional computing systems. we introduce a stuck at fault model for cortical networks and show how it maps to permanent faults that can occur in current generation gpgpus that are emulating a cortical network in software. we show how to emulate these stuck atfaults ef ciently, using at speed gpgpu execution, and validate that approach against a detailed cycle accurate simulator. we demonstrate experimentally that the performance of the cortical network degrades gracefully with increasingly faulty hardware, and that the network is able to transparently recover lost functionality without anyreprogramming or recompilation. the desire to create novel computing systems, paired with recent advances in neuroscientific understanding of the brain, has led researchers to develop neuromorphic architectures that emulate the brain. to date, such models are developed, trained, and deployed on the same substrate. however, excessive co dependence between the substrate and the algorithm prevents portability, or at the very least requires reconstructing and retraining the model whenever the substrate changes. this paper proposes a well defined abstraction layer the neuromorphic instruction set architecture, or nisa that separates a neural application algorithmic specification from the underlying execution substrate, and describes the aivo framework, which demonstrates the concrete advantages of such an abstraction layer. aivo consists of a nisa implementation for a rate encoded neuromorphic system based on the cortical column abstraction, a state of the art integrated development and runtime environment, and various profile based optimization tools. aivo ide generates code for emulating cortical networks on the host cpu, multiple gpgpus, or as boolean functions. its runtime system can deploy and adaptively optimize cortical networks in a manner similar to conventional just in time compilers in managed runtime systems. we demonstrate the abilities of the nisa abstraction by constructing a cortical network model of the mammalian visual cortex, deploying on multiple execution substrates, and utilizing the various optimization tools we have created. for this hierarchical configuration, aivo profiling based network optimization tools reduce the memory footprint by and improve the execution time by a factor of on the host cpu. deploying the same network on a single gpgpu results in a speedup. we further demonstrate that a speedup of can be achieved by deploying a massively scaled cortical network across three gpgpus. finally, converting a trained hierarchical network toc boolean constructs on the host cpu results in speedup. understanding of the structural and operational aspects of various components of the human brain has signi cantly increased over the past few decades. this has led to the development of a number of biologically inspired software and hardware based computational models. most of these rely on the neocortex, the part of the brain that is evolutionarily the most recent and is unique to mammals, as their biological basis. these models implement some of the basic properties of the neocortex, including uniformity of structure, hierarchical arrangement, invariant representation of features, and feedback based prediction. even more impressive is the fact that successful learning algorithms are now being deployed on specially designed hardwares which attempts to capture the physical properties of the brain. although these models show success at various learning tasks, they also suffer from a number of problems. first, the intrinsic complication of modeling the brain can make such models quite hard to understand. second, many such models require explicit de nition of network connectivity and hierarchical arrangements, often using low level programming techniques. third, given that many such models scale to large sizes, debugging can become quite cumbersome. finally, as will be explained further in section #, many neuromorphic architectures strongly tie the proposed learning algorithm to the execution substrate. as such, these models can be quite dif cult to port and are unable to take advantage of automated pro le driven and machine speci. optimizations to improve performance, reduced resource utilization, and improved robustness. historically, general purpose von neumann computer systems suffered from a similar set of challenges until the introduction of standardized instruction set architectures. an isa creates a valuable layer of abstraction between an applicationalgorithmic speci cation and itexecution substrate, enabling separate development in each domain, including automated tools for generating and optimizing machine code while allowing the same software to run on multiple generations of hardware. this paper advocates the adoption of a similar abstraction layer for neurally inspired cortical models or neuromorphic computing systems and shows many of the same bene ts follow. cortical models typically rely on some variant of hebbian learning to train themselves to perform complex computational tasks. they encode the results of this training as a collection of synaptic weights, thresholds, connectivity, and other key parameters of neural components. once the model has learned the values for these parameters, it can be deployed on an appropriate execution substrate. in the conventional approach to developing and deploying learning algorithms, the development and deployment substrates are the same, which leads to dif culties and or compromises in the design and ef ciency of both. any changes to the execution substrate require recon guring, retraining, and possibly redesigning the entire network at the algorithmic level, while any attempts to optimize the network to better match the characteristics of the execution substrate must be done manually, in a cumbersome and error prone manner. instead, following the example of well de ned isas in conventional computing systems which separate the algorithm from the execution hardware this paper proposes adoption of a neuromorphic instruction set architecture, which forms an analogous implementation independent abstraction layer for neuromorphic systems. as a case study of a nisa based approach for developing neuromorphic systems, this paper introduces aivo, a neuromorphic framework which consists of a well de ned nisa, an integrated development environment, and several compelling optimization tools, as seen in figure #. the aivo nisa de nes an implementation of a cortically inspired computational model proposed by hashmi et al. the key enabling mechanism that aivo provides over prior approaches for modeling cortical networks is the nisa, which is used for specifying the structure, connectivity, semantics, state, and pro le metadata of the cortical network. the nisa is implemented as a virtual isa, and is deployed as an easily readable xml schema. this common intermediate representation allows the various components of the aivo system to communicate and inter operate smoothly, while providing a userfriendly, human readable, self documenting, and easily extensible persistent store for all cortical network structure and state. using the aivo ide, this paper shows how such learning models can be easily built, trained, and debugged through a user friendly interface. since the nisa approach allows a trained network to exist as a persistent contract between the learning algorithm and the execution substrate, we show how we are able to deploy our model on a single core cpu, multiple gpgpus, and even as functional boolean logic. furthermore, the pro ling information about the cortical network stored in the nisa, as well as knowledge of the deployment substrate, open up a number of optimizations for resource utilization and performance improvements. it should be noted that, as multiple isas exist in traditional computing systems, multiple nisas may be developed to accommodate speci. the contributions of this paper are as follows: we propose the nisa as a virtual isa which serves as a meaningful layer of abstraction between cortically inspired learning algorithms and available execution substrates. we implement hashmi et alcortical column model as the aivo nisa to demonstrate the ability of this abstraction layer. we present the aivo ide as a useful tool for building, debugging, and deploying large neocortically inspired networks. we demonstrate how the nisa allows for many algorithmic and substrate speci. optimizations and guarantees that such translations are safe. finally, we show how the nisa allows for easy deployment on cpu, gpgpu, or simple logic substrates. the rest of the paper is organized as follows: section # examines work related to learning models and development environments. section # provides some basic background information about the structure and functionality of the neocortex. section # details the biologically inspired computational model. section # provides a detailed discussion of the proposed neuromorphic isa abstraction. section # describes the aivo integrated development environment. section # demonstrates how the nisa abstraction allows us to deploy our learning algorithm on a host cpu, gpgpu, or as boolean logic. section # presents some of the high level optimizations the nisa enables. section # describes our experimentation methodology the results. section # concludes the paper and proposes our future work. overview of how the nisa allows independent development and high level optimizations of the learning algorithm, capable of deployment across multiple available substrates. energy has become a first class design constraint in computer systems. memory is a significant contributor to total system power. this paper introduces flikker, an application level technique to reduce refresh power in dram memories. flikker enables developers to specify critical and non critical data in programs and the runtime system allocates this data in separate parts of memory. the portion of memory containing critical data is refreshed at the regular refresh rate, while the portion containing non critical data is refreshed at substantially lower rates. this partitioning saves energy at the cost of a modest increase in data corruption in the non critical data. flikker thus exposes and leverages an interesting trade off between energy consumption and hardware correctness. we show that many applications are naturally tolerant to errors in the non critical data, and in the vast majority of cases, the errors have little or no impact on the application final outcome. we also find that flikker can save between of the power consumed by the memory sub system in a mobile device, with negligible impact on application performance. flikker is implemented almost entirely in software, and requires only modest changes to the hardware. rst class design constraint in many computer systems, particularly in mobile devices, clusters, and serverfarms. in a mobile phone, saving energy can extend battery life and enhance mobility. recently, mobile phones have morphed into general purpose computing platforms, often called smartphones. smartphones are typically used in short bursts over extended periods of time, ie, they are idle most of the time. nonetheless, they are always on as users expect to resume their applications in the state they were last used. hence, even when the permission to make digital or hard copies of all or part of this work for personal or classroomuseisgrantedwithout feeprovidedthat copies arenot madeordistributed forpro torcommercialadvantage andthatcopiesbearthisnoticeandthefullcitation onthe rstpage to copy otherwise, to republish, topostonserversorto redistribute tolists, requirespriorspeci cpermission and ora fee. phone is not being used, application state is stored in the phonememory to maintain responsiveness. this wastes power because dynamic random access memories leak charge and need to be refreshed periodically, or else theywill lose data. memory is a major part of overall system power in smartphones. measures of dram power as a fraction of overall power range from and depend on the application model, operating system, and underlying hardware. some smartphone application programming models, such as android, emphasize reducing application dram usage in idle mode. further, the memory capacity of smartphones has been steadily increasing and, as a result, memory power consumption will be even more important in the future. memory consumes power both when the device is active and when it is suspended. in standby mode, the refresh operation is the dominant consumer of power, and hence we focus on reducing refresh power in this paper. this paper proposes flikker, a software technique to save energy by reducing refresh power in drams. dram manufacturers typically set the refresh rate to be higher than the leakage rate of the fastest leaking memory cells. however, studies have shown that the leakage distribution of memory cells follows an exponential distribution, with a small fraction of the cells having signi cantly higher leakage rates than other cells. hence, the vast majority of the cells will retain their values even if the refresh rate of the memory chip is signi cantly reduced. flikker leverages this observation to obtain power reduction in dram memories at the cost of knowingly introducing a modest number of errors in application data. typical smartphone applications include games, audio video processing and productivity tasks such as email and web browsing. these applications are insensitiveto errorsinallbuta small portion of their data. we call such data critical data, as it is important for theoverall correctnessofthe application. forexample, in a video processing application, the data structure containing the list of frames is more important than the outputbuffer to which frames are rendered. therefore, this data structure would be considered as critical data. flikker enables the programmer to distinguish between critical and non critical data in applications. at runtime, flikker allocates the critical and non critical data in separate memory pages and reduces the refresh rate for pages containing non critical data at the cost of increasing the number of errors in these pages. pages containing critical data are refreshed at the regular rate and are hence free of errors. this differentiated allocation strategy enables flikker to achieve power savings with only marginal degradation of the applicationreliability. crt monitors occasionally exhibited ickering, ie, loss of resolution, when their refresh rates were lowered hence the name. our approach in flikker fundamentally differs from existing solutions for saving energy in low power systems. in these solutions, energy reduction is achieved by appropriately trading offperformance metrics, such as throughput latency, quality of service, or user response time, eg, in contrast, our approach explores a largeley unexplored trade off in system design, namely trading off energy consumption for data integrity at the application level. by intentionally lowering hardware correctness in an application aware manner, we show that it is possible to achieve signi cant power savings at the cost of a negligible reduction in application reliability. to the best of our knowledge, flikker is the rst software technique to intentionally introduce hardware errors for memory power savings based on the characteristics of the application. while we focus on mobile applications in this paper, we believe that flikker approach can also be applied to data center applications. aspects of flikker make it appealing for use in practice. first, flikker allows programmers to control what errors are exposed to the applications, and hence explicitly specify the trade offbetween power consumption and reliability. programmers can de ne what parts of the application are subject to errors, and take appropriate measures to handle the introduced errors. second, flikker requires only minor changes to the hardware in the form of interfaces to expose refresh rate controls to the software. current mobile drams already allow the software to specify how much of the memory should be refreshed, and we show that it is straightforward to enhance the pasr architecture to refresh different portions of the memory at different rates. finally, legacyapplications can work unmodi ed with flikker, as it can be selectively enabled or disabled on demand. hence, flikker can be incrementally deployed on new applications. we have evaluated flikker both using analytical and experimental methods on ve diverse applications representative of mobile workloads. we nd that flikker can save between to of the total dram power in mobile applications, with negligible degradation in reliability and performance. based on previous study of dram power as a fraction of total power consumption, this corresponds to of total power savings in a smartphone. we also nd that the effort required to deploy flikker is modest for the applications considered in the paper. a significant part of future microprocessor real estate will be dedicated to or caches. these on chip caches will heavily impact processor perfor mance, power dissipation, and thermal management strategies. there are a number of interconnect design considerations that influence power performance area characteristics of large caches, such as wire mod els, signaling strategy, router design, etc. yet, to date, there exists no analytical tool that takes all of these parameters into account to carry out a design space exploration for large caches and estimate an optimal organization. in this work, we implement two major extensions to the cacti cache modeling tool that focus on interconnect design for a large cache. second, we add the ability to model non uniform cache access. we not only adopt state of the art design space exploration strategies for nuca, we also enhance this exploration by considering on chip network contention and a wider spectrum of wiring and routing choices. we present a validation analysis of the new tool and present a case study to showcase how the tool can improve architecture research methodologies. keywords: cache models, non uniform cache archi tectures, memory hierarchies, on chip intercon nects. first, we add the ability to model different types of wires, such as rc based wires with different power delay characteristics and differential low swing buses. multi core processors will incorporate large and complex cache hierarchies. intel is already prototyping an core processor and there is speculation that entire dies in a package may be employed for large sram caches or dram main memory. therefore, it is expected that future processors will have to intelligently manage many megabytes of on chip cache. this work was supported in parts by nsf grant ccf and nsf career award ccf. cache into shared private domains, move data to improve locality and sharing, optimize the network parameters for ef cient communication between cores and cache banks. examples of ongoing research in these veins include. many cache evaluations employ the cacti cache access modeling tool to estimate delay, power, and area for a given cache size. the cacti estimates are invaluable in setting up baseline simulator parameters, computing temperatures of blocks neighboring the cache, evaluating the merits overheads of novel cache organizations, etc. while cacti produces reliable delay power area estimates for moderately sized caches, it does not model the requirements of large caches in suf cient detail. besides, the search space of the tool is limited and hence so is its application in power performance trade off studies. with much of future cache research focused on multi megabyte cache hierarchy design, this is a serious short coming. hence, in this work, we extend the cacti tool in many ways, with the primary goal of improving the delity of its large cache estimates. the tool can also aid in trade off analysis: for example, with a comprehensive design space exploration, cacti can identify cache con gurations that consume three times less power for about a delay penalty. the main enhancement provided in cacti is a very detailed modeling of the interconnect between cache banks. a large cache is typically partitioned into many smaller banks and an inter bank network is responsible for communicating addresses and data between banks and the cache controller. earlier versions of cacti have employed a simpletree network with global wires and have assumed uniform access times for every bank. recently, non uniform cache architectures have also been proposed that employ a packet switched network between banks and yield access times that are a function of where data blocks are found. we add support for such an architecture within cacti. whether we employ a packet switched ortree network, the delay and power of the network components dominate the overall cache access delay and power as the the rst four versions of cacti have been cited by more than year# papers and are also incorporated into other architectural simulators such as wattch. cache size tree delay percentagetree power percentage figure #. contribution oftree network to overall cache delay and power. figure # shows that thetree of the cacti model contributes an increasing percentage to the overall cache delay as the cache size is increased from to mb. its contribution to total cache power is also sizeable around. the inter bank network itself is sensitive to many parameters, especially the wire signaling strategy, wire parameters, topology, router con guration, etc. the new version of the tool carries out a design space exploration over these parameters to estimate a cache organization that optimizes a combination of power delay area metrics for uca and nuca architectures. network contention plays a non trivial role in determining the performance of an on chip network design. we also augment the design space exploration with empirical data on network contention. components of the tool are partially validated against detailed spice simulations. section # provides details on the new interconnect models and other enhancements integrated into cacti. a case study using cacti is discussed in section #. the intel montecito employs two mb private caches, one for each core. future research will likely explore architectural mechanisms to organize the or we thank the anonymous reviewers for their helpful suggestions. we also present an example case study to demonstrate how the tool can facilitate architectural evaluations. we present marss, an open source, fast, full system simulation tool built on qemu to support cycle accurate simulation of superscalar homogeneous and heterogeneous multicore processors. marss includes detailed models of coherent caches, interconnections, chipsets, memory and io devices. marss simulates the execution of all software components in the system, including unmodified binaries of applications, os and libraries. single and multicore processors implementing the instruction set architecture are deployed within many computing platforms today, starting from high end servers to desktops and ultimately down to mobile devices, including potential new products that target the smart phone market segment and beyond. the one clear advantage of using the processors in the full range of the product spectrum is to facilitate the rapid deployment of the wide variety of application binaries. it is thus important to have a full system simulation tool that incorporates realistic simulation models for other systems level components such as the chipset, dram, network interface cards and peripheral devices in addition to accurate simulation models for single and multicore processors implementing the isa. such a tool is useful for evaluating and developing products that will use current and emerging single and multicore chips. this paper introduces an open source full system simulation tool, called marss micro architectural and system simulator, which meets this critical need. the cpu simulation component of marss is based on an extended and modified version of ptlsim. the specific features of marss are: marss uses a cycle accurate simulation models for out oforder and in order single core and multicore cpus implementing the isa. these are integrated into the qemu full system emulation environment. marss supports seamless switching between the cycle permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. dac, june, year#, san diego, california, usa copyright; year# acm. accurate simulation mode and the native emulation mode of qemu, permitting the fast forwarding of simulation in the emulation mode to a region of interest where cycle accurate simulation is needed. unmodified operating systems can be booted on marss and the execution of unmodified binaries of applications and existing libraries can be simulated on marss. marss includes detailed, cycle accurate models of a contemporary memory hierarchy for single core and multicore processor chips, including coherent caches and a dram memory system. simulation speeds of to kilo instructions per second are realized in the cycle accurate simulation mode of multicore processor chips. marss permits system level data to be imported into the simulator from the underlying emulated system. this not only permits the use of realistic data but also enables users to gauge the effect of core designs on the rest of the system and vice versa. to the best of our knowledge, we are not aware of any existing public domain, open source simulator that rivals the characteristics of marss. this paper focuses on the implementation details of marss and evaluates the performance of various single threaded and multi threaded benchmarks and presents some example case studies using marss. this poster describes chimps, a toolflow that aims to provide software developers with a way to program hybrid cpu fpga platforms using familiar tools, languages, and techniques. chimps starts withand produces a specialized spatial dataflow architecture that supports coherent caches and the shared memory programming model. the toolflow is designed to abstract away the complex details of data movement and separate memories on the hybrid platforms, as well as take advantage of memory management and computation techniques unique to reconfigurable hardware. this poster focuses on the memory design for chimps, particularly the use of numerous small caches customized for various phases of program execution. applications compiled using chimps show performance improvements of more than on simple compute intensive kernels, and on the difficult to parallelize stswm application without any special optimizations compared to running only on the cpu. the toolflow supports full ansi, and produces hardware that runs on platforms that are expected to be available within one year. in this paper, we concentrate on the microarchitectural design of the simplest form of prisc a risc microprocessor with a single pfu that only evaluates combinational functions. with the inclusion of a single bit wide pfu whose hardware cost is less than that of a kilobyte sram, our study shows a improvement in processor performance on the specint benchmarks. this paper explores a novel way to incorporate hardware programmable resources into a processor microarchitecture to improve the performance of general purpose applications. through a coupling of compile time analysis routines and hardware synthesis tools, we automatically configure a given set of the hardware programmable functional units and thus augment the base instruction set architecture so that it better meets the instruction set needs of each application. we refer to this new class of general purpose computers as programmable instruction set computers. although similar in concept, the prisc approach differs from dynamically programmable microcode because in prisc we define entirely new primitive datapath operations. we briefly discuss the operating system and the programming language compilation techniques that are needed to successfully build prisc and, we present performance results from a proof of concept study. approximate computing, where computation accuracy is traded off for better performance or higher data throughput, is one solution that can help data processing keep pace with the current and growing overabundance of information. for particular domains such as multimedia and learning algorithms, approximation is commonly used today. we consider automation to be essential to provide transparent approximation and we show that larger benefits can be achieved by constructing the approximation techniques to fit the underlying hardware. our target platform is the gpu because of its high performance capabilities and difficult programming challenges that can be alleviated with proper automation. our approach, sage, combines a static compiler that automatically generates a set of cuda kernels with varying levels of approximation with a run time system that iteratively selects among the available kernels to achieve speedup while adhering to a target output quality set by the user. the sage compiler employs three optimization techniques to generate approximate kernels that exploit the gpu microarchitecture: selective discarding of atomic operations, data packing, and thread fusion. across a set of machine learning and image processing kernels, sage approximation yields an average of speedup with less than quality loss compared to the accurate execution on a nvidia gtx gpu. for example, while trying to smooth an image, the exact output value of a pixel can vary. to keep up with information growth, companies such as microsoft, google and amazon are investing in larger data centers with thousands of machines equipped with multi core processors to provide the necessary processing capability on a yearly basis. the latest industry reports show that in the next decade the amount of information will expand by a factor of while the number of servers will only grow by a factor of. at this rate, it will become more expensive for companies to provide the compute and storage capacity required to keep pace with the growth of information. to address this issue, one promising solution is to perform approximate computations on massively data parallel architectures, such as gpus, and trade the accuracy of the results for computation throughput. there are many domains where it is acceptable to use approximation techniques. in such cases some variation in the output is acceptable, and some degree of quality degradation is tolerable. many image, audio, and video processing algorithms use approximation techniques to compress and encode multimedia data to various degrees that provide tradeoffs between size and correctness such as lossy compression techniques. if the output quality is acceptable for the user or the quality degradation is not perceivable, approximation can be employed to improve the performance. as shown in figure #, smoothed images with and quality are not discernible from the original image but differences can be seen when enlarged. in the machine learning domain, exact learning and inference is often computationally intractable due to the large size of input data. we believe that as the amount of information continues to grow, approximation techniques will become ubiquitous to make processing such information feasible. conflicts per warp memory accesses per thread high cost of serialization memory bandwidth limitation slowdown figure #: three gpu characteristics that sageoptimizations exploit. these experiments are performed on a nvidia gtx gpu. shows how accessing the same element by atomic instructions affects the performance for the histogram kernel. illustrates how the number of memory accesses impacts performance while the number of computational instructions per thread remains the same for a synthetic benchmark. shows how the number of thread blocks impacts the performance of the blackscholes kernel. the idea of approximate computing is not a new one and previous works have studied this topic in the context of more traditional cpus and proposed new programming models, compiler systems, and run time systems to manage approximation. in this work, we instead focus on approximation for gpus. gpus represent affordable but powerful compute engines that can be used for many of the domains that are amenable to approximation. there are several common bottlenecks on gpus that can be alleviated with approximation. these include the high cost of serialization, memory bandwidth limitations, and diminishing returns in performance as the degree of multithreading increases. because many variables affect each of these characteristics, it is very dif cult and time consuming for a programmer to manually implement and tune a kernel. our proposed framework for performing systematic run time approximation on gpus, sage, enables the programmer to implement a program once in cuda, and depending on the target output quality speci ed for the program, trade the accuracy for performance based on the evaluation metric provided by the user. sage has two phases: of ine compilation and run time kernel management. during of ine compilation, sage performs approximation optimizations on each kernel to create multiple versions with varying degrees of accuracy. at run time, sage uses a greedy algorithm to tune the parameters of the approximate kernels to identify con gurations with high performance and a quality that satis es theoq. this approach reduces the overhead of tuning as measuring the quality and performance for all possible con gurations can be expensive. since the behavior of approximate kernels may change during run time, sage periodically performs a calibration to check the output quality and performance and updates the kernel con guration accordingly. to automatically create approximate cuda kernels, sage utilizes three optimization techniques. the rst optimization targets atomic operations, which are frequently used in kernels where threads must sequentialize writes to a common variable. the atomic operation optimization selectively skips atomic operations that cause frequent collisions and thus cause poor performance as threads are sequentialized. the next optimization, data packing, reduces the number of bits needed to represent input arrays, thereby sacri cing precision to reduce the number of high latency memory operations. the third optimization, thread fusion, eliminates some thread computations by combining adjacent threads into one and replicating the output of one of the original threads. a common theme in these optimizations is to exploit the speci. microarchitectural characteristics of the gpu to achieve higher performance gains than general methods, such as ignoring a random subset of the input data or loop iterations, which are unaware of the underlying hardware. in summary, the main contributions of this work are: the rst static compilation and run time system for automatic approximate execution on gpus. approximation optimizations that are utilized to automatically generate kernels with variable accuracy. a greedy parameter tuning approach that is utilized to determine the tuning parameters for approximate versions. a dynamic calibration system that monitors the output quality during execution to maintain quality with a high degree of con dence, and takes corrective actions to stay within the bounds of target quality for each kernel. the rest of the paper is organized as follows. section # discusses why sage chooses these three approximation optimizations. approximation optimizations used by sage are discussed in section #. the results of using sage for various benchmarks are presented in section #. section # discusses the related work in this area and how sage is different from previous works. the summary and conclusion of this work is outlined in section #. to mitigate this, approximate methods are widely used to learn realistic models from large data sets by trading off computation time for accuracy. however, in the context of gpus, previous approximation techniques have two limitations: the programmer is responsible for implementing and tuning most aspects of the approximation, and approximation is generally not cognizant of the hardware upon which it is run. section # explains how the sage framework operates. recent work has explored exposing this trade off in programming models. a key challenge, though, is how to isolate parts of the program that must be precise from those that can be approximated so that a program functions correctly even as quality of service degrades. using these types, the system automatically maps approximate variables to low power storage, uses low power operations, and even applies more energy efficient algorithms provided by the programmer. in addition, the system can statically guarantee isolation of the precise program component from the approximate component. this allows a programmer to control explicitly how information flows from approximate data to precise data. importantly, employing static analysis eliminates the need for dynamic checks, further improving energy savings. as a proof of concept, we develop enerj, an extension to java that adds approximate data types. we also propose a hardware architecture that offers explicit approximate storage and computation. we port several applications to enerj and show that our extensions are expressive and effective; a small number of annotations lead to significant potential energy savings at very little accuracy cost. energy is increasingly a first order concern in computer systems. exploiting energy accuracy trade offs is an attractive choice in applications that can tolerate inaccuracies. we propose using type qualifiers to declare data that may be subject to approximate computation. rst order constraint in mobile systems, and power cooling costs largely dominate the cost of equipment permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. more fundamentally, current trends point toward a utilization wall, in which the amount of active die area is limited by how much power can be fed to a chip. much of the focus in reducing energy consumption has been on low power architectures, performance power trade offs, and resource management. while those techniques are effective and can be applied without software knowledge, exposing energy considerations at the programming language level can enable a whole new set of energy optimizations. this work is a step in that direction. recent research has begun to explore energy accuracy trade offs in general purpose programs. a key observation is that systems spend a signi cant amount of energy guaranteeing correctness. consequently, a system can save energy by exposing faults to the application. importantly, these studies universally show that applications have portions that are more resilient and portions that are critical and must be protected from error. however, an error in a jump table could lead to a crash, and even small errors in the image le format might make the output unreadable. while approximate computation can save a signi cant amount of energy, distinguishing between the critical and non critical portions of a program is dif cult. these annotations, however, do not offer any guarantee that the fundamental operation of the program is not compromised. in other words, these annotations are either unsafe and may lead to unacceptable program behavior or need dynamic checks that end up consuming energy. we need a way to allow programmers to compose programs from approximate and precise components safely. moreover, we need to guarantee safety statically to avoid spending energy checking properties at runtime. the key insight in this paper is the application of type based information ow tracking ideas to address these problems. this paper proposes a model for approximate programming that is both safe and general. we use a type system that isolates the precise portion of the program from the approximate portion. the programmer must explicitly delineate ow from approximate data to precise data. the model is thus safe in that it guarantees precise computation unless given explicit programmer permission. safety is statically enforced and no dynamic checks are required, minimizing the overheads imposed by the language. we present enerj, a language for principled approximate computing. enerj extends java with type quali ers that distinguish between approximate and precise data types. data annotated with the approximate quali er can be stored approximately and computations involving it can be performed approximately. enerj also provides endorsements, which are programmer speci ed points at which approximate to precise data ow may occur. we formalize a core of enerj and prove a non interference property in the absence of endorsements. our programming model is general in that it uni es approximate data storage, approximate computation, and approximate algorithms. programmers use a single abstraction to apply all three forms of approximation. the model is also high level and portable: the implementation is entirely responsible for choosing the energy saving mechanisms to employ and when to do so, guaranteeing correctness for precise data and best effort for the rest. while enerj is designed to support general approximation strategies and therefore ensure full portability and backwardcompatibility, we demonstrate its effectiveness using a proposed approximation aware architecture with approximate memory and imprecise functional units. we have ported several applications to enerj to demonstrate that a small amount of annotation can allow a program to save a large amount of energy while not compromising quality of service signi cantly. we rst detail the enerj language extensions in section #. section # formalizes a core of the language, allowing us to prove a non interference property. the full formalism and proof are presented in an accompanying technical report. next, section # describes hypothetical hardware for executing enerj programs. while other execution substrates are possible, this proposed model provides a basis for the evaluation in sections and; there, we assess enerjexpressiveness and potential energy savings. the type checker and simulation infrastructure used in our evaluation are available at http: sampa cs washington edu. energy consumption is an increasing concern in many computer systems. many studies have shown that a variety of applications are resilient to hardware and software errors during execution. for example, an image renderer can tolerate errors in the pixel data it outputs a small number of erroneous pixels may be acceptable or even undetectable. the language supports programming constructs for algorithmic approximation, in which the programmer produces different implementations of functionality for approximate and precise data. section # presents related work and section # concludes. prior proposals have used annotations on code blocks and data allocation sites. memories today expose an all or nothing correctness model that incurs significant costs in performance, energy, area, and design complexity. but not all applications need high precision storage for all of their data structures all of the time. this paper proposes mechanisms that enable applications to store data approximately and shows that doing so can improve the performance, lifetime, or density of solid state memories. the first allows errors in multi level cells by reducing the number of programming pulses used to write them. the second mechanism mitigates wear out failures and extends memory endurance by mapping approximate data onto blocks that have exhausted their hardware error correction resources. simulations show that reduced precision writes in multi level phase change memory cells can be faster on average and using failed blocks can improve array lifetime by on average with quality loss under. techniques under the umbrella of approximate computing exploit this tolerance to trade. many common applications have intrinsic tolerance to inaccuracies in computation. applications in domains like computer vision, media processing, machine learning, and sensor data analysis can see large. ciency gains in permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than the author must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. jacob nelson university of washington nelson cs washington edu luis ceze university of washington luisceze cs washington edu exchange for small compromises on computational accuracy. extends to storage: tolerance to errors in both transient and persistent data is present in a wide range of software, from servers to mobile devices. meanwhile, the semiconductor industry is beginning to encounter limits to further scaling of common memory technologies like dram and flash. as a result, new memory technologies and techniques are emerging. multi level cells, which pack more than one bit of information in a single cell, are already commonplace and phase change memory is imminent. but both pcm and flash wear out over time as cells degrade and become unusable. furthermore, multi level cells are slower to write due to the need for tightly controlled iterative programming. memories traditionally address wear out issues and implement multi level cell operation in ways that ensure perfect data integrity of the time. this has signi cant costs in performance, energy, area, and complexity. these costs are exacerbated as memories move to smaller device feature sizes along with more process variation. by relaxing the requirement for perfectly precise storage and exploiting the inherent error tolerance of approximate applications failure prone and multi level memories can gain back performance, energy, and capacity. in this paper, we propose techniques that exploit data accuracy trade. in essence, we advocate exposing storage errors up to the application with the goal of making data storage more. we make this safe by: exploiting application level inherent tolerance to inaccuracies; and providing an interface that lets the application control which pieces of data can be subject to inaccuracies while. ering error free operation for the rest of the data. the rst technique uses multi level cells in a way that enables higher density or better performance at the cost of occasional inaccurate data retrieval. the second technique uses blocks with failed bits to store approximate data; to mitigate the. ect of failed bits on overall value precision, we prioritize the correction of higher order bits. approximate storage applies to both persistent storage as well as transient data stored in main memory. we explore the techniques in the context of pcm, which may be used for persistent storage or as main memory, but the techniques generalize to other technologies such as flash. we simulate main memory benchmarks and persistent storage datasets and nd that our techniques improve write latencies by or extend device lifetime by on average while trading. we begin by describing the programming models and hardware software interfaces we assume for main memory and persistent approximate storage. next, sections and describe our two approximate storage techniques in detail. section # describes our evaluation of the techniques using a variety of error tolerant benchmarks and section # gives the results for these experiments. finally, we enumerate related work on storage and approximate computing and conclude. due to the evolution of technology constraints, especially energy constraints which may lead to heterogeneous multi cores, and the increasing number of defects, the design of defect tolerant accelerators for heterogeneous multi cores may become a major micro architecture research issue. and the emergence of high performance applications implementing recognition and mining tasks, for which competitive ann based algorithms exist, drastically expands the potential application scope of a hardware ann accelerator. however, while the error tolerance of ann algorithms is well documented, there are few in depth attempts at demonstrating that an actual hardware ann would be tolerant to faulty transistors. most fault models are abstract and cannot demonstrate that the error tolerance of ann algorithms can be translated into the defect tolerance of hardware ann accelerators. in this article, we introduce a hardware ann geared towards defect tolerance and energy efficiency, by spatially expanding the ann. most custom circuits are highly defect sensitive, a single transistor can wreck such circuits. on the contrary, artificial neural networks are inherently error tolerant algorithms. in order to precisely assess the defect tolerance capability of this hardware ann, we introduce defects at the level of transistors, and then assess the impact of such defects on the hardware ann functional behavior. we empirically show that the conceptual error tolerance of neural networks does translate into the defect tolerance of hardware neural networks, paving the way for their introduction in heterogeneous multi cores as intrinsically defect tolerant and energy efficient accelerators. approximate computing leverages the intrinsic resilience of applications to inexactness in their computations, to achieve a desirable trade off between efficiency and acceptable quality of results. to broaden the applicability of approximate computing, we propose quality programmable processors, in which the notion of quality is explicitly codified in the hw sw interface, ie, the instruction set. the isa of a quality programmable processor contains instructions associated with quality fields to specify the accuracy level that must be met during their execution. we show that this ability to control the accuracy of instruction execution greatly enhances the scope of approximate computing, allowing it to be applied to larger parts of programs. the micro architecture of a quality programmable processor contains hardware mechanisms that translate the instruction level quality specifications into energy savings. additionally, it may expose the actual error incurred during the execution of each instruction back to software. as a first embodiment of quality programmable processors, we present the design of quora, an energy efficient, quality programmable vector processor. quora utilizes a tiered hierarchy of processing elements that provide distinctly different energy vs. quality trade offs, and uses hardware mechanisms based on precision scaling with error monitoring and compensation to facilitate quality programmable execution. we evaluate an implementation of quora with processing elements in nm technology. the results demonstrate that leveraging quality programmability leads to savings in energy for virtually no loss in application output quality, and energy savings for modest impact on output quality. our work suggests that quality programmable processors are a significant step towards bringing approximate computing to the mainstream. growing transistor counts, limited power budgets, and the breakdown of voltage scaling are currently conspiring to create a utilization wall that limits the fraction of a chip that can run at full speed at one time. in this regime, specialized, energy efficient processors can increase parallelism by reducing the per computation power requirements and allowing more computations to execute under the same power budget. to pursue this goal, this paper introduces conservation cores. conservation cores, orcores, are specialized processors that focus on reducing energy and energy delay instead of increasing performance. this focus on energy makescores an excellent match for many applications that would be poor candidates for hardware acceleration. we present a toolchain for automatically synthesizingcores from application source code and demonstrate that they can significantly reduce energy and energy delay for a wide range of applications. thecores support patching, a form of targeted reconfigurability, that allows them to adapt to new versions of the software they target. our results show that conservation cores can reduce energy consumption by up to for functions and by up to for whole applications, while patching can extend the useful lifetime of individualcores to match that of conventional processors. as power concerns continue to shape the landscape of generalpurpose computing, heterogeneous and specialized hardware has emerged as a recurrent theme in approaches that attack the power wall. in the current regime, transistor densities and speeds continue to increase with moore slaw, but limitson thresholdvoltage scaling have stopped the downward scaling of per transistor switching permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. consequently, the rate at which we can switch transistors is far outpacing our ability to dissipate the heat created by those transistors. the result is a technology imposed utilization wall that limits the fraction of the chip we can use at full speed at one time. our experiments with a nm tsmc process show that we can switch less than of a mm die at full frequency within an powerbudget. itrs roadmap projections and cmos scaling theory suggests that this percentage will decrease to less than in nm, and will continue to decrease by almost half with each process generation and even further with integration. the effects of the utilization wall are already indirectly apparent in modern processors: intelnehalem provides a turbo mode that powers off some cores in order to run others at higher speeds. another strong indication is that even though native transistor switching speeds have continued to double every two process generations, processor frequencies have not increased substantially over the last years. in this regime, reducing per operation energy translates directly into increased potential parallelism for the system: if a given computation can be made to consume less power at the same level of performance, other computations can be run in parallel without violating the powerbudget. this paper attacks the utilization wall with conservation cores. hardware circuits created for the purpose of reducing energy consumption on computationally intensive applications. since it is no longer possible to run the entire chip at full frequencyat once, it makes sense to customize the portions of the chip that are running so theywill be as ef cient as possible for the application at hand. in effect, conservation cores allow architects to trade area for energy in a processordesign. the utilization wall has made this trade off favorable, because moorelaw has made increases in transistor counts cheap, while poor cmos scaling has exhausted power budgets, making increases in power consumption very expensive. conservation cores have a different goal than conventional application speci. circuits, and we differentiate betweencores and the more common accelerators along several axes. first, accelerators focus on improving performance, at a potentially worse, equal, or better energy ef ciency. conservation cores, on the other hand, focus primarily on energy reduction. cores that are also accelerators are possible, but this work targets similar levels of performance, and focuses on reducing energy and energy delay, especially at advanced technology nodes wheredvfs is less effective for saving energy. shifting the focus from performance to ef ciencyallowscores to target a broader range of applications than accelerators. for codes with large amounts of parallelism and predictable communication patterns, since these codes map naturally onto hardware. thus, parallelism intensive regions of code that are hot are the best candidates for implementation as accelerators. on the other hand, cores are parallelism agnostic: hot code with a tight critical path, little parallelism, and or very poor memory behavior is an excellent candidate for acore, ascores can reduce the number of transistor toggles required to get through that code. for instance, our results show thatcores can deliver signi cant energy savings for irregular, integer applications that would be dif cult to automatically accelerate with specialized hardware. incorporatingcores into processors, especially at a scale large enough to save power across applications with multiple hot spots, raises a number of challenges: determining whichcores to build in order tobuildcores, we must be able to identify which pieces of code are the best candidates for conversion intocores. the code should account for a signi cant portion of runtime and energy, and stem from a relatively stable code base. transistor density continues to increase exponentially, but power dissipation per transistor is improving only slightly with each generation of moore law. given the constant chip level power budgets, this exponentially decreases the percentage of transistors that can switch at full frequency with each technology generation. hence, while the transistor budget continues to increase exponentially, the power budget has become the dominant limiting factor in processor design. in this regime, utilizing transistors to design specialized cores that optimize energy per computation becomes an effective approach to improve system performance. to trade transistors for energy efficiency in a scalable manner, we propose quasi specific cores, or qscores, specialized processors capable of executing multiple general purpose computations while providing an order of magnitude more energy efficiency than a general purpose processor. our approach exploits these similar code patterns to ensure that a small set of specialized cores support a large number of commonly used computations. our results show that qscores can provide better energy efficiency than general purpose processors while reducing the amount of specialized logic required to support the workload by up to. the qscores design flow is based on the insight that similar code patterns exist within and across applications. we evaluate qscores ability to target both a single application library as well as a diverse workload consisting of applications selected from different domains.