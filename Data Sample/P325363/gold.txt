this research lies at the intersection of general purpose approximate computing, accelerators, analog and digital neural hardware, neural based code acceleration, and limited precision learning. this work combines techniques in all these areas to provide a compilation work ow and the architecture circuit design that enables code acceleration with limited precision mixed signal neural hardware. in each area, we discuss the key related work that inspired our work. several studies have shown that diverse classes of applications are tolerant to imprecise execution. a growing body of work has explored relaxing the abstraction of full accuracy at the circuit and architecture level for gains in performance, energy, and resource utilization. these circuit and architecture studies, although proven successful, are limited to purely digital techniques. we explore how a mixed signal, analog digital approach can go beyond what digital approximate techniques offer. research on accelerators seeks to synthesize ef cient circuits or fpga con gurations to accelerate generalpurpose code. similarly, static specialization has shown signi cant ef ciency gains for irregular and legacy code. more recently, con gurable accelerators have been proposed that allow the main cpu to of oad certain code to a small, ef cient structure. this paper extends the prior work on digital accelerators with a new class of mixed signal, analog digital accelerators. there is an extensive body of work on hardware implementations of neural networks both in digital and analog. recent work has proposed higher level abstractions for implementation of neural networks. other work has examined fault tolerant hardware neural networks. in particular, temam uses datasets from the uci machine learning repository to explore fault tolerance of a hardware neural network design. in contrast, our compilation, neural network selection training framework, and architecture design aim at applying neural networks to general purpose code written in familiar programming models and languages, not explicitly written to utilize neural networks directly. a recent study shows that a number of applications can be manually reimplemented with explicit use of various kinds of neural networks. that study did not prescribe a programming work ow, nor a preferred hardware architecture. more recent work exposes analog spiking neurons as primitive operators. this work devises a new programming model that allows programmers to express digital signal processing applications as a graph of analog neurons and automatically maps the expressed graph to a tiled analog, spiking neural hardware. the work in is restricted to the domain of applications whose inputs are realworld signals that should be encoded as pulses. our approach addresses the long standing challenges of using analog computation by not imposing domain speci. limitations, and by providing analog circuitry that is integrated with a conventional digital processor in a way that does not require a new programming paradigm. the work in provides a complete survey of learning algorithms that consider limited precision neural hardware implementation. we tried various algorithms, but we found that cdlm was the most effective. more sophisticated limited precision learning techniques can improve the reported quality results in this paper and further con rm the feasibility and effectiveness of the mixed signal, approach for neural based code acceleration.