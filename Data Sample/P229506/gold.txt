more recently, huang et al discriminatively trained a deep structured semantic model on clickthrough data to maximize the conditional likelihood of the clicked documents for the corresponding queries. by training on query document pairs they generate a pair of models for projecting the queries and the documents, respectively, to the same embedding space. their experiments demonstrated better retrieval performance over other existing semantic models by directly optimizing for the document ranking task. they also proposed a word hashing technique for dealing with large vocabularies that are commonly associated with web corpora. the rst layer of their model maps the high dimensional term vectors corresponding to the input queries and documents into a lower dimensional letter basedgram vectors, while the subsequent layers learn a non linear projection of thisgram vectors to a low dimensional semantic space. the cosine similarity between the vectors of a query and a document represents their mutual relevance. while the dssm treats an input query as a raw term vector or a bag of words, shen et al added a convolutional pooling structure to the network architecture in the convolutional latent semantic model to capture richer contextual structures in the input text. the clsm model has been shown to perform better than dssm and many other existing state of the art techniques on various information retrieval tasks such as web document ranking and contextual entity search. in this paper, we study the vector offset technique in the context of the clsm outputs. most modern browsers, search engines, text editors and command shells implement some form of an auto completion feature to aid users in faster text entry. in web search, pre computed auto completion systems are popular, where the suggestions are typically ltered by exact pre. matching from a pre selected set of candidates and ranked according to past popularity. ranking suggestions by past frequency is commonly referred to as the mostpopularcompletion model and can be regarded as a maximum likelihood approximator. req weber and castillo and shokouhi showed how query distributions change across different user demographics and argued that qac systems based on personalization features can signi cantly outperform popularity based baselines. ranking suggestions based on temporal context has also been explored. the two qac related studies most relevant to our work have been done by shokouhi and kharitonov et al. to capture short term context, shokouhi relied on lettergram matches between the previous queries and the candidates, and trained a supervised ranking model for combining them with mpc and other noncontextual and user demographic features. kharitonov et al proposed a uni ed framework for contextualizing and diversifying the ranking of qac suggestions. their empirical evaluations show that by considering the userprevious query alone more than of the improvements can be achieved, as compared to additionally considering the document examination history and diversi cation context. given the previous query, their proposed model computes the expected probability of a given completion as follows, whereis an indicator variable whose value is if the user continues the current task, and otherwise. for our evaluation, we implement the supervised ranking framework proposed by shokouhi and include thegram similarity, the query frequency and the query pairwise frequency features among others as described in section #. we believe that the baseline used in our experiment is comparable with any state of the art baselines described in the literature for qac ranking. in web search, bennett et al investigated the impact of short term and long term user behaviour on relevance prediction, and showed that short term user history becomes more important as the session progresses. li et al evaluated dssm and convolutional dssm for modelling session context for web search. besides the primary ir task, qac as opposed to web ranking, our work differs from this study by going beyond computing the topical similarity using the existing models and explicitly modelling query reformulations as vectors. we also show the bene ts of optimizing a clsm model directly for capturing session context by training on session query pairs. yan et al proposed an approach that maps queries and clicks to latent search intents represented using open directory project categories for making context aware query recommendations. cao et al and liao et al have explored session context using latent concept clusters from click through bipartite graphs, while guo et al represented the userprevious queries using a regularized topic model. zhang et al proposed a task centric click model for characterizing user behaviour within a single search session. cao et al learnt a variable length hidden markov model from large scale search logs, whereas boldi et al studied random walks on query ow graphs for improved recommendations. lastly, previous studies on the relationships between neighbouring queries from a search session have been mostly focused on categorizing the reformulations based on broad manually de ned taxonomies or understanding the user goals behind com mon actions. motivated by the broad manually identi ed reformulation categories xiang et al and jiang et al designed simple features for supervised retrieval models. finally, guan et al use reinforcement learning for modifying term weights in response to the observed modi cations made to the query by the user. while clearly using session context for web search is a wellstudied topic, context sensitive query auto completion has been discussed less thoroughly in the literature. also, to the best of our knowledge this is the rst time that an explicit vector representation of query reformulations has been proposed and studied. latent semantic analysis, probabilistic lsa and latent dirichlet allocation are some of the well known models proposed to represent queries and documents in low dimensional space for semantic matching. unlike these models, which commonly use unsupervised learning, gao et al trained bi lingual topic models and linear discriminative projection models on clickthrough data, consisting of query and clicked documents. salakhutdinov and hinton used auto encoders to show that deep learning can be useful for extracting hierarchical semantic structures from queries and documents. an examination of the clsm model outputs reveals syntactic and semantic regularities in the distributed representation of queries. the regularities are akin to the ones reported by mikolov et al about the embeddings learnt by continuous space language models, where simple vector offsets between words were found to capture semantic and syntactic inter word relationships. mikolov et al further proposed models that can be trained on large scale datasets and extended the vector representations to phrases. unlike these continuous space language models, clsm can project multi word variable length queries into the embedding space. http: www dmoz org figure #: architecture of the convolutional latent semantic model. the model has an input layer that performs the word hashing, a convolutional layer, a max pooling layer, and an output layer that produces the nal semantic vector representation of the query.