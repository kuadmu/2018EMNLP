convolutional neural networks applications range from recognition and reasoning to intelligent text applications such as semantic text analysis and natural language processing applications. two key observations drive the design of a new architecture for cnn. first, cnn workloads exhibit a widely varying mix of three types of parallelism: parallelism within a convolution operation, intra output parallelism where multiple input sources are combined to create a single output, and inter output parallelism where multiple, independent outputs are computed simultaneously. workloads differ significantly across different cnn applications, and across different layers of a cnn. second, the number of processing elements in an architecture continues to scale much faster than off chip memory bandwidth of chips. based on these two observations, we show that for a given number of processing elements and off chip memory bandwidth, a new cnn hardware architecture that dynamically configures the hardware on the fly to match the specific mix of parallelism in a given workload gives the best throughput performance. our cnn compiler automatically translates high abstraction network specification into a parallel microprogram that is mapped, scheduled and executed by the coprocessor. compared to a ghz quad core, dual socket intel xeon, ghz gpu, and a mhz fpga implementation, our mhz dynamically configurable architecture is to faster. this is the first cnn architecture to achieve real time video stream processing on a wide range of object detection and recognition tasks. feed forward multilayer neural networks are computational models that are widely used in diverse domains such as video and image processing, medical diagnosis systems and financial forecasting. these computation models serve one of two roles: pattern recognition to provide a meaningful categorization of input patterns, or functional approximation where the models find a smooth function that approximates the actual mapping between input and output patterns. a vast majority of these computational models are still implemented in software on general purpose and embedded processors. however, these processors do not fully exploit the parallelism inherent in these computational models. numerous custom hardware implementations have also been proposed to parlay the abundant parallelism inherent in these computational models into significantly higher performance. traditional pattern recognition systems use two distinct steps to recognize individual patterns. first, a feature extractor transforms the input patterns into short strings of symbols that can be easily matched or compared. these features are relatively invariant with respect to transformations and distortions that do not change the inherent nature of the patterns. the feature extractor is usually rather specific to the task. it is also the focus of most design effort, because it is entirely hand crafted. second, a classifier categorizes the feature vectors into a few classes. the classifier is usually general purpose, and a standard, fully connected multi layer neural network can be automatically trained to do the classification. however, recognition accuracy is largely determined by the ability of the designer to extract an appropriate set of features. feed forward, multi layer artificial neural networks like the convolutional neural networks have found increasing use in several new applications because they have the potential to process vast amounts of labeled data to automatically learn and extract complex features. cnns are especially attractive because they can recognize visual patterns directly from pixel images with very minimal preprocessing of the image. cnns can easily recognize patterns with extreme variability. also, their recognition ability is not impaired by distortions or simple geometric transformations of the image. every connection between an input and a neuron is assigned a value called weight. each neuron computes a weighted sum of all its inputs, followed by a non linear or sigmoid function to restrict its output value within a reasonable range. neurons in the hidden layers are also called the hidden units. as an example, consider one layer of a multi layer neural network and the task of connecting year# inputs to hidden units to produce a set of output values. the year# inputs can be the year# pixel values in a image. the hidden units produce output values which can be interpreted as pixels in a output image. a fully connected neural network connects every input to every hidden unit, resulting in a network that has year#, different weights. learning these weights will require a very large number of training instances. furthermore, a fully connected architecture ignores the topology of the input image because input values can be presented in any order without affecting the outcome of the training. on the contrary, images have strong two dimensional local structure where pixels that are spatially nearby are highly correlated. therefore, each hidden unit in a cnn is connected to only on a small number of inputs, say inputs. these inputs, also called as the receptive field of the hidden unit, correspond to a pixel area in the input image. only weights are now necessary to connect the inputs to a hidden unit. since such an elementary feature detector is likely to be useful across other pixel areas in the input image, many different hidden units are used to cover the entire image. receptive fields of these hidden units can overlap, and the degree of overlap can be pre specified. hidden units, whose receptive fields are located at different places on the image, can be forced to detect the same elementary feature by reusing the same weights. hidden units that share the same set of weights form a plane, and outputs of units in a plane constitute the feature map. hidden units in a plane can together extract visual features like edges, corners etc. if we represent the distinct weights as a matrix, then outputs of all the hidden units can be computed as the convolution of the input image with the kernel matrix. cnn multicore blas gpu pcie cnp dc cnn mhz conv, bit port width, pci speedup of dc cnn compute time transfer time over ghz, core over ghz, core gpu over cnp automotive safety ms ms ms ms video surveillance ms ms ms ms face recognition ms ms ms ms mobile robot vision ms ms ms ms ms face detection ms ms ms ms table #: comparison with other cnn implementations. after the convolution step, we have a image in which a feature of interest has been detected, and the exact location of the feature becomes less important. the value of every hidden unit is also subjected to a squashing function. a simple way to reduce the precision with which the distinctive features are encoded in the image is to reduce the spatial resolution of the image by using sub sampling. this also reduces the sensitivity of the outputs to shifts or distortions. a typical cnn has multiple layers of hidden units to perform complex object recognition tasks. evaluating a trained cnn involves performing a huge number of convolutions with considerable data movement. convolution computation is a performance bottleneck, but reducing overheads of data movement is also necessary to accelerate the performance of a cnn. consider a simple object recognition application that is used on relatively high resolution streaming images. with a image, a cnn network that can be used to identify faces within all possible windows in the image runs at approximately frames per second on a ghz intel xeon processor when optimized using blas. multi threading this to and cores on quad core and dual quad core machines only improves the speed by a little over due to synchronization overheads, and the fact that different threads share common inputs. therefore, the most optimized software implementation on state of the art processors struggles to achieve about to frames per second when we analyze vga images. vga images are more realistic in practical use case scenarios such as security cameras. can the abundant parallelism in cnn workloads be leveraged by custom architectures to improve the feed forward processing speeds to be close to real time to answer this question, we investigate a dynamically reconfigurable architecture in which the hardware parallelism can be tailored to suit the parallelism offered by the specific application workload. related work implementation of the convolution operation in hardware has been studied extensively. both fpga and lsi implementations have been proposed. digital media processors with a large number of high speed multiply and accumulate units have been used to implement the convolution operation. lsi architectures using a mixed analog digital approach as well as several fpga based implementations have also been proposed. fast implementations of a convolution core are necessary but not sufficient to accelerate the cnn workload. a host processor can carve out the convolution operations from the cnn workload and off load the convolutions to the hardware implementation. for each off load, the host processor provides the image and weight values, and retrieves the convolved output image from the accelerator. some optimizations are possible to store kernel values or some intermediate data on the accelerator. however, for cnns used in practice, the data dependencies within a layer and across multiple network layers of the cnn, the management of significant amount of intermediate data over a slow host to hardware accelerator link, and the detailed orchestration of complex control and data flows by the host processor quickly offset performance gains obtained by using a fast hardware core only for convolutions. there are no reported lsi implementations of cnns but several software implementations on gpus exist. however, none of the prior implementations are able to process video feeds in real time. an fpga implementation of cnn was reported recently. this architecture uses one hardware convolver for data processing, and a general purpose soft processor for control, all implemented on a virtex fpga from xilinx. the self contained unit was developed for video processing in mobile robot applications. in contrast, we use a very different approach. we employ a configurable bank of hardware convolvers, a hardwired controller, and configurability hardware that dynamically configures hw resources on the fly to match the precise type and mix of parallelism in the cnn workload. in, no attempt is made to alter the hardware computing paths to match type or extent of parallelism in the workload. our contribution while several earlier efforts have implemented convolutions and neural networks in hardware, to our knowledge, this is the first effort to create a co processor architecture that automatically analyzes workloads and dynamically configures its hardware and software components to match the exact mix of different types of parallelism in the workload. our most novel contribution is dynamic configurability and a method to quickly match hw to workload characteristics. for the first time, we achieve a tipping point. we enable new, real time on line classification applications that were not possible before. a high abstraction level software api, along with a run time software component allows domain experts to easily specify and execute arbitrary convolutional neural network workloads. unlike prior work, our co processor architecture is forward scalable. as we scale the number of processing elements, and the available off chip memory bandwidth, cnn applications continue to automatically improve in performance. domain experts do not have to re write the application. instead, a software run time component determines the optimal configuration of processing elements for each layer of the cnn workload, and the coprocessor architecture is dynamically configured to match the workload characteristics. this allows the throughput and performance of the coprocessor architecture to be very close to the peak throughput of the individual processing elements. dynamic configurability in hardware is fast, and it is under the control of software. our coprocessor can easily implement different feed forward neural networks and classifiers such as hmax, dbn and hog methods. the rest of this document is organized as follows. in section #, we provide a background for cnns. in section #, we discuss the parallelism in cnn workloads. in section #, we present a motivating example that illustrates the benefits of a dynamically reconfigurable architecture. in section #, we describe the coprocessor architecture and section # describes dynamic configurability. we present architectural evaluation results in section #, and conclude in section #. machine learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems. at the same time, a small set of machine learning algorithms are proving to be state of the art across many applications. as architectures evolve towards heterogeneous multi cores composed of a mix of cores and accelerators, a machine learning accelerator can achieve the rare combination of efficiency and broad application scope. until now, most machine learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. however, recent state of the art cnns and dnns are characterized by their large size. in this study, we design an accelerator for large scale cnns and dnns, with a special emphasis on the impact of memory on accelerator design, performance and energy. we show that it is possible to design an accelerator with a high throughput, capable of performing gop in a small footprint of mm and mw; compared to a bit ghz simd processor, the accelerator is faster, and it can reduce the total energy by x. the accelerator characteristics are obtained after layout at nm. such a high throughput in a small footprint can open up the usage of state of the art machine learning algorithms in a broad set of systems and for a broad set of applications. as architectures evolve towards heterogeneous multi cores composed of a mix of cores and accelerators, designing accelerators which realize the best possible tradeoff between exibility and ef ciency is becoming a prominent issue. the rst question is for which category of applications one should primarily design accelerators together with the architecture trend towards accelerators, a second simultaneous and signi cant trend in high performance and embedded applications is developing: many of the emerging high performance and embedded applications, from image video audio recognition to automatic translation, business analytics, and all forms of robotics rely on machinelearning techniques. this trend even starts to percolate in our community where it turns out that about half of the benchmarks of parsec, a suite partly introduced to highlight the emergence of new types of applications, can be implemented using machine learning algorithms. this trend in application comes together with a third and equally remarkable trend in machine learning where a small number of techniques, based on neural networks, have been proved in the past few years to be state ofthe art across a broad range of applications. as a result, there is a unique opportunity to design accelerators which can realize the best of both worlds: signi cant application scope together with high performance and ef ciency due to the limited number of target algorithms. currently, these workloads are mostly executed on multicores using simd, on gpus, or on fpgas. however, the aforementioned trends have already been identi ed by a number of researchers who have proposed accelerators implementing convolutional neural networks or multi layer perceptrons; accelerators focusing on other domains, such as image processing, also propose ef cient implementations of some of the primitives used by machinelearning algorithms, such as convolutions. others have proposed asic implementations of convolutional neural networks, or of other custom neural network algorithms. however, all these works have rst, and successfully, focused on ef ciently implementing the computational primitives but they either voluntarily ignore memory transfers for the sake of simplicity, or they directly plug their computational accelerator to memory via a more or less sophisticated dma. while ef cient implementation of computational primitives is. rst and important step with promising results, inef cient memory transfers can potentially void the throughput, energy or cost advantages of accelerators, ie, an amdahllaw effect, and thus, they should become. rst order concern, just like in processors, rather than an element factored in accelerator design on a second step. unlike in processors though, one can factor in the speci. nature of memory transfers in target algorithms, just like it is done for accelerating computations. this is especially important in the domain of machine learning where there is a clear trend towards scaling up the size of neural networks in order to achieve better accuracy and more functionality. in this study, we investigate an accelerator design that can accommodate the most popular state of the art algorithms, ie, convolutional neural networks and deep neural networks. we focus the design of the accelerator on memory usage, and we investigate an accelerator architecture and control both to minimize memory transfers and to perform them as ef ciently as possible. we present a design at nm which can perform bit xed point operations in parallel every ns, ie, gop, in a mm, mw footprint. on of the largest layers found in recent cnns and dnns, this accelerator is faster and more energy ef cient on average than an bit simd core clocked at ghz. in summary, our main contributions are the following: a synthesized accelerator design for large scale cnns and dnns, the state of the art machinelearning algorithms. neural network hierarchy containing convolutional, pooling and classi er layers. the accelerator achieves high throughput in a small area, power and energy footprint. the accelerator design focuses on memory behavior, and measurements are not circumscribed to computational tasks, they factor in the performance and energy impact of memory transfers. in section #, we rst provide a primer on recent machine learning techniques and introduce the main layers composing cnns and dnns. in section #, we analyze and optimize the memory behavior of these layers, in preparation for both the baseline and the accelerator design. in section #, we explain why an asic implementation of large scale cnns or dnns cannot be the same as the straightforward asic implementation of small nns. we introduce our accelerator design in section #. the methodology is presented in section #, the experimental results in section #, related work in section #. many companies are deploying services, either for consumers or industry, which are largely based on machine learning algorithms for sophisticated processing of large amounts of data. the state of the art and most popular such machine learning algorithms are convolutional and deep neural networks, which are known to be both computationally and memory intensive. a number of neural network accelerators have been recently proposed which can offer high computational capacity area ratio, but which remain hampered by memory accesses. however, unlike the memory wall faced by processors on general purpose workloads, the cnns and dnns memory footprint, while large, is not beyond the capability of the on chip storage of a multi chip system. this property, combined with the cnn dnn algorithmic characteristics, can lead to high internal bandwidth and low external communications, which can in turn enable high degree parallelism at a reasonable area cost. in this article, we introduce a custom multi chip machine learning architecture along those lines. we show that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of over a gpu, and reduce the energy by on average for a chip system. we implement the node down to the place and route at nm, containing a combination of custom storage and computational units, with industry grade interconnects. as improvements in per transistor speed and energy efficiency diminish, radical departures from conventional approaches are needed to continue improvements in the performance and energy efficiency of general purpose processors. general purpose approximate computing explores a third dimension error and trades the accuracy of computation for gains in both energy and performance. techniques to harvest large savings from small errors have proven elusive. for a set of diverse applications, npu acceleration provides whole application speedup of and energy savings of on average with average quality loss of at most. one such departure is approximate computing, where error in computation is acceptable and the traditional robust digital abstraction of near perfect accuracy is relaxed. conventional techniques in energy efficient computing navigate a design space defined by the two dimensions of performance and energy, and traditionally trade one for the other. this paper describes a new approach that uses machine learning based transformations to accelerate approximation tolerant programs. the core idea is to train a learning model how an approximable region of code code that can produce imprecise but acceptable results behaves and replace the original code region with an efficient computation of the learned model. we use neural networks to learn code behavior and approximate it. we describe the parrot algorithmic transformation, which leverages a simple programmer annotation to transform a code region from a von neumann model to a neural model. after the learning phase, the compiler replaces the original code with an invocation of a low power accelerator called a neural processing unit. the npu is tightly coupled to the processor pipeline to permit profitable acceleration even when small regions of code are transformed. offloading approximable code regions to npus is faster and more energy efficient than executing the original code. npus form a new class of accelerators and show that significant gains in both performance and efficiency are achievable when the traditional abstraction of near perfect accuracy is relaxed in general purpose computing. it is widely understood that energy efficiency now fundamentally limits microprocessor performance gains. in this post dennard scaling era, solutions that improve performance and efficiency while retaining as much generality as possible are highly desirable; hence the exploding interest in gpgpus and fpgas. these applications are common in mobile, embedded, and server systems and fall into four broad categories: applications with analog inputs. cmos scaling is no longer providing gains in efficiency commensurate with transistor density increases, as a result, both the semiconductor industry and the research community are increasingly focusing on specialized accelerators, which can provide large gains in efficiency and performance by restricting the workloads that benefit. recent work has quantified three orders of magnitude of difference in efficiency between general purpose processors and asics the community is facing an iron triangle in this respect; we can choose any two of performance, energy efficiency, and generality at the expense of the third. before the traditional trend of transistor scaling dennard scaling broke down, we were able to improve all three on a consistent basis for decades. such programmable accelerators exploit some characteristic of an application domain to achieve efficiency gains at the cost of generality. fpgas, for example, exploit copious, fine grained, and irregular parallelism while gpus exploit many threads and data level simd style parallelism. whether an application can use an accelerator effectively depends on the degree to which it exhibits the acceleratorrequired characteristics. tolerance to approximation is one such program characteristic. a growing body of recent work, has focused on approximation as a strategy for improving efficiency. large classes of applications can tolerate small errors in their outputs with no discernible loss in their quality of result. this category includes image processing, sensor data processing, voice recognition, etc, that operate on noisy real world data. they are inherently resilient to some noise and can handle an extra noise resulting from approximation. the initial overheads are large: the cmp was less energy efficient than an application specific integrated circuit doing the same job. scaling the performance of a power limited processor requires decreasing the energy expended per instruction executed, since energy op op second is power. encoder running on a general purpose four processor cmp system. we explore methods to eliminate these overheads by transforming the cpu into a specialized system for. broadly applicable optimizations like single instruction, multiple data units improve cmp performance by and energy by, which is still worse than an asic. achieving asic like performance and effciency requires algorithm specifc optimizations. we create a large, specialized functional storage unit capable of executing hundreds of operations per instruction. to better understand what improvement in processor efficiency is possible, and what must be done to capture it, we quantify the sources of the performance and energy overheads of a hd. the problem is that the basic operation costs in. are so small that even with a simd unit doing over ops per cycle, of the energy is still overhead. this improves energy effciency by, and the final customized cmp reaches the same performance and within of an asic solution energy in comparable area. traditionally, chip designers were able to make increasingly complex designs both by increasing the system power, and by leveraging the energy gains from technology scaling. video encode, and transform the hardware it runs on from a generic multiprocessor to a custom multiprocessor with asic like specialized hardware units. the first has the advantage of being a programmable solution, while the second provides potentially greater efficiency. section # then presents our experimental methodology, describing our baseline, generic. since power is ops second energy op, we need to decrease the energy cost of each op if we want to continue to scale performance at constant power. historically each factor of in scaling made each gate evaluation take less energy however, technology scaling no longer provides the energy savings it once did, so designers must turn to other techniques to scale energy cost. most designs use processor based solutions because of their flexibility and low design costs, however, these are usually not the most energy efficient solutions. a shift to multi core systems has helped improve the efficiency of processor systems but that approach is also going to hit a limit pretty soon on the other hand, using hardware that has been customized for a specific application can be three orders of magnitude better than a processor in both energy op and ops area this paper compares asic solutions to processor based solutions, to try to understand the sources of inefficiency in general purpose processors. we hope this information will prove to be useful both for building more energy efficient processors and understanding why and where customization must be used for efficiency. the first extends the current trend of creating general data parallel engines on our processors. the second approach creates application specific data storage fused with functional units. communications of the acm area by and energy efficiency by. despite these customizations, the resulting solution is still less energy efficient than an asic. the next section provides the background needed to understand the rest of the paper. the performance and efficiency gains are described in section #, which also explores the causes of the overheads and different methods for addressing them. most computing systems today are power limited, whether it is the limit of a cell phone system on a chip, or the limit of a processor in a server. to build this understanding, we start with a single video compression application, hd. on this task, a general purpose software solution takes more energy per frame and more area than an asic to reach the same performance. because it demonstrates the large energy advantage of asic solutions and because there exist commercial asics that can serve as a benchmark. moreover, contains a variety of computational motifs, from highly data parallel algorithms to control intensive ones. to better understand the potential of producing generalpurpose chips with better efficiency, we consider two broad strategies for customized hardware. this approach mimics the addition of sse instructions, or the recent work in merging graphic processors on die to help with other applications. we claim these are similar to general functional units since they typically have some special instructions for important applications, but are still generally useful. in the limit this should be an asic like solution. starting from a energy penalty, adding relatively wide sse like parallel execution engines and rewriting the code to use them improves performance a previous version of this paper was published in proceedings of the th annual international symposium on computer architecture, acm, ny. an examination of the energy breakdown in the paper clearly demonstrates why. basic arithmetic operations are typically bits wide, and even when performing more than such operations per cycle, arithmetic unit energy comprises less than of the total. one must consider the energy cost of the desired operation compared with the energy cost of one processor cycle: for highly efficient machines, these energies should be similar. using the insight gained from our results, section # discusses the broader implications for efficient computing and supporting application driven design. in this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low dimensional space where the relevance of a document given a query is readily computed as the distance between them. to make our models applicable to large scale web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. the new models are evaluated on a web document ranking task using a real world data set. results show that our best model significantly outperforms other latent semantic models, which were considered state of the art in the performance prior to the work presented in this paper. latent semantic models, such as lsa, intend to map a query to its relevant documents at the semantic level where keyword based matching often fails. the proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. modern search engines retrieve web documents mainly by matching keywords in documents with those in search queries. these latent semantic models address the permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. xiaodong he, jianfeng gao, li deng, alex acero, larry heck microsoft research, redmond, wa usa microsoft com language discrepancy between web documents and search queries by grouping different terms that occur in a similar context into the same semantic cluster. extending from lsa, probabilistic topic models such as probabilistic lsa and latent dirichlet allocation have also been proposed for semantic matching. however, these models are often trained in an unsupervised manner using an objective function that is only loosely coupled with the evaluation metric for the retrieval task. these models are trained on clickthrough data using objectives that tailor to the document ranking task. however, the training of bltm, though using clickthrough data, is to maximize a log likelihood criterion which is sub optimal for the evaluation metric for document ranking. on the other hand, the training of dpm involves large scale matrix multiplications. they demonstrated that hierarchical semantic structure embedded in the query and the document can be extracted via deep learning. however, the deep learning approach they used still adopts an unsupervised learning method where the model parameters are optimized for the reconstruction of the documents rather than for differentiating the relevant documents from the irrelevant ones for a given query. as a result, the deep learning models do not, and similarity score of the corresponding concept vectors. more specifically, our best model uses a deep neural network to rank a set of documents for a given query as follows. different from the previous latent semantic models that are learned in an unsupervised fashion, our models are optimized directly for web document ranking, and thus give superior performance, as we will show shortly. the results show that our best model outperforms all the competing methods with a significant margin of in ndcg. in the rest of the paper, section # reviews related work. section # presents the experiments, and section # concludes the paper. significantly outperform the baseline retrieval models based on keyword matching. the neural network models are discriminatively trained using the clickthrough data such that the conditional likelihood of the clicked document given the query is maximized. in our experiments, we show that, by adding this extra layer of representation in semantic models, word hashing enables us to learn discriminatively the semantic models with large vocabularies, which are essential for web search. we evaluated the proposed dssms on a web document ranking task using a real world data set. however, lexical matching can be inaccurate due to the fact that a concept is often expressed using different vocabularies and language styles in documents and queries. latent semantic models such as latent semantic analysis are able to map a query to its relevant documents at the semantic level where lexical matching often fails. copyrights for components of this work owned by others than the author must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. thus, a query and a document, represented as two vectors in the lower dimensional semantic space, can still have a high similarity score even if they do not share any term. thus the performance of these models on web search tasks is not as good as originally expected. recently, two lines of research have been conducted to extend the aforementioned latent semantic models, which will be briefly reviewed below. first, clickthrough data, which consists of a list of queries and their clicked documents, is exploited for semantic modeling so as to bridge the language discrepancy between search queries and web documents. for example, gao et al propose the use of bi lingual topic models and linear discriminative projection models for query document matching at the semantic level. more specifically, bltm is a generative model which requires that a query and its clicked documents not only share the same distribution over topics, but also contain similar factions of words assigned to each topic. in contrast, the dpm is learned using thenet algorithm that follows the pairwise learning to rank paradigm outlined in. after projecting term vectors of queries and documents into concept vectors in a low dimensional semantic space, the concept vectors of the query and its clicked documents have a smaller distance than that of the query and its unclicked documents. gao et al report that both bltm and dpm outperform significantly the unsupervised latent semantic models, including lsa and plsa, in the document ranking task. the sizes of these matrices often grow quickly with the vocabulary size, which could be of an order of millions in web search tasks. in order to make the training time tolerable, the vocabulary was pruned aggressively. although a small vocabulary makes the models trainable, it leads to suboptimal performance. in the second line of research, salakhutdinov and hinton extended the semantic modeling using deep auto encoders. superior performance to the conventional lsa is reported. moreover, the semantic hashing model also faces the scalability challenge regarding large scale matrix multiplication. we will show in this paper that the capability of learning semantic models with large vocabularies is crucial to obtain good results in real world web search tasks. in this study, extending from both research lines discussed above, we propose a series of deep structured semantic models for web search. first, a non linear projection is performed to map the query and the documents to a common semantic space. then, the relevance of each document given the query is calculated as the cosine similarity between their vectors in that semantic space. furthermore, to deal with large vocabularies, we propose the so called word hashing method, through which the high dimensional term vectors of queries or documents are projected to low dimensional letter basedgram vectors with little information loss. section # describes our dssm for web search. the latter suggests that the proposed method performance can be further improved by using better sampling strategies and more sophisticated data augmentation techniques. in this paper we consider the problem of multi view face detection. while there has been significant research on this problem, current state of the art approaches for this task require annotation of facial landmarks, eg, tsm, or annotation of face poses. they also require training dozens of models to fully capture faces in all orientations, eg, models in headhunter method. in this paper we propose deep dense face detector, a method that does not require pose landmark annotation and is able to detect faces in a wide range of orientations using a single model based on deep convolutional neural networks. the proposed method has minimal complexity; unlike other recent deep learning object detection methods, it does not require additional components such as segmentation, bounding box regression, or svm classifiers. furthermore, we analyzed scores of the proposed face detector for faces in different orientations and found that the proposed method is able to detect faces from different angles and can handle occlusion to some extent, there seems to be a correlation between distribution of positive examples in the training set and scores of the proposed face detector. evaluations on popular face detection benchmark datasets show that our single model face detector algorithm has similar or better performance compared to the previous methods, which are more complex and require annotations of either different poses or facial landmarks. for the past two decades, face detection has always been an active research area in the vision community. moreover, in some cases dpm based models require annotation of facial landmarks for training, neural network based: there is a long history of using neural networks for the task of face detection. the rst network locates rough positions of faces and the second network veri es the detection and makes more accurate localization. trained a single multi layer network for face detection. the trained network is able to partially handle di erent poses and rotation angles. more recently, proposed to train a neural network jointly for face detection and pose estimation. this has made low complexity, rapid and accurate face detection an essential component for cloud based photo sharing storage platforms. moreover, detector cascade has been deployed in many commercial products such as smartphones and digital cameras. with the wide spread use of smartphones and fast mobile networks, millions of photos are uploaded everyday to the cloud storages such as dropbox or social networks such as facebook, twitter, instagram, google, and flicker. organizing and retrieving relevant information from these photos is very challenging and directly impact user experience on those platforms. for example, users commonly look for photos that were taken at a particular location, at a particular time, or with a particular friend. the former two queries are fairly straightforward, as almost all of todaycameras embed time and gps location into photos. the last query, ie, contextual query, is more challenging as there is no explicit signal about the identities of people in the photos. the key for this identi cation is the detection of human faces. the seminal work of viola and jones made it possible to rapidly detect up right faces in real time with very low computational complexity. their detector, called detector cascade, consists of a sequence of simple to complex face classi ers and has attracted extensive research. while cascade detectors can accurately nd visible up right faces, they often fail to detect faces from di erent angles, eg, side view or partially occluded faces. this failure can significantly impact the performance of photo organizing software applications since user generated content often contains faces from di erent angles or faces that are not fully visible; see for example figure #. this has motivated many works on the problem of multi view face detection over the past two decades. current solutions can be summarized into three categories: cascade based: these methods extend the viola and jones detector cascade. for example, proposed to train a detector cascade for each view of the face and combined their results at the test time. recently, combined this method with integral channel features and soft cascade, and showed that by using cascades, it is possible to obtain state of the art performance for multi view face detection. moreover its complexity in training and testing increases linearly with the number of models. to address the computational complexity issue, viola and jones proposed to rst estimate the face pose using a tree classi er and then run the cascade of corresponding face pose to verify the detection. while improving the detection speed, this method degrades the accuracy because mistakes of the initial tree classi er are irreversible. this method is further improved by where, instead of one detector cascade, several detectors are used after the initial classi er. finally, and combined detector cascade with multiclass boosting and proposed a method for multiclass multi view object detection. dpm based: these methods are based on the deformable part models technique where a face is de ned as a collection of its parts. the parts are de ned via unsupervised or supervised training, and a classi er, latent svm, is trained to nd those parts and their geometric relationship. these detectors are robust to partial occlusion because they can detect faces even when some of the parts are not present. these methods are, however, computationally intensive because they require solving a latent svm for each candidate location and multiple dpms have to be trained and combined to achieve the state of the art performance. in particular, trained a two stage system based on convolutional neural networks. in, the authors trained multiple face detection networks and combined their output to improve the performance. they showed that this joint learning scheme can signi cantly improve performance of both detection and pose estimation. our method follows the works in but constructs a deeper cnn for face detection. the key challenge in multi view face detection, as pointed out by viola and jones, is that learning algorithms such as boosting or svm and image features such as hog or haar wavelets are not strong enough to capture faces of different poses and thus the resulted classi ers are hopelessly inaccurate. however, with recent advances in deep learning and gpu computation, it is possible to utilize the high capacity of deep convolutional neural networks for feature extraction classi cation, and train a single model for the task of multi view face detection. deep convolutional neural network has recently demonstrated outstanding performance in a variety of vision tasks such as face recognition, object classi cation, and object detection. in particular trained an layered network, called alexnet, and showed that deep convolutional neural networks can signi cantly outperform other methods for the task of large scale image classi cation. for the task of object detection, proposedcnn method that uses an image segmentation technique, selective search, to nd candidate image regions and classify those candidates using a version of alexnet that is netuned for objects in the pascal voc dataset. more recently, improvedcnn by augmenting the selective search proposals with candidate regions from multibox approach, and replacing layered alexnet with a much deeper cnn model of googlenet. despite state of theart performance, these methods are computationally suboptimal because they require evaluating a cnn over more than, overlapping candidate regions independently. to address this issue, recently proposed to run the cnn model on the full image once and create a feature pyramid. the candidate regions, obtained by selective search, are then mapped into this feature pyramid space. then uses spatial pyramid pooling and svm on the mapped regions to classify candidate proposals. beyond region based methods, deep convolutional neural networks have also been used with sliding window approach, eg, overfeat and deformable part models for object detection and for human pose estimation. in general, for object detection these methods still have an inferior performance compared to region based methods such ascnn and. however, in our face detection experiments we found that the region based methods are often very slow and result in relatively weak performance. in this paper, we propose a method based on deep learning, called deep dense face detector, that does not require pose landmark annotation and is able to detect faces in a wide range of orientations using a single model. the proposed method has minimal complexity because unlike recent deep learning ob ject detection methods such as, it does not require additional components for segmentation, bounding box regression, or svm classi ers. compared to previous convolutional neural network based face detectors such as, our network is deeper and is trained on a signi cantly larger training set. in addition, by analyzing detection con dence scores, we show that there seems to be a correlation between the distribution of positive examples in the training set and the con dence scores of the proposed detector. this suggests that the performance of our method can be further improved by using better sampling strategies and more sophisticated data augmentation techniques. in our experiments, we compare the proposed method to a deep learning based method, cnn, and several cascade and dpm based methods. we show that ddfd can achieve similar or better performance even without using pose annotation or information about facial landmarks. recently, several learning algorithms relying on models with deep architectures have been proposed. though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. these models are compared with well established algorithms such as support vector machines and single hidden layer feed forward neural networks. linear model single layer kernel svm architecture neural network architecture architecture figure #. fortunately,ective optimization procedures using unsupervised learning have recently been proposed and have demonstrated impressive performance for deep architectures. more speci cally, they discuss prior knowledge about invariances such as translations, rotations etc. furthermore these invariances are highly problem speci c. several recent empirical and theoretical results have brought deep architectures to the attention of the machine learning community: they have been used, with good results, for dimensionality reduction, and classi cation of digits from the mnist data set. a core contribution of this body of work is the training strategy for a family of computational models that is similar or identical to traditional multilayer perceptrons with sigmoidal hidden units. ective when the gradient must be propagated across multiple nonlinearities. hinton gives empirical evidence that appearing in proceedings of the th international conference on machine learning, corvallis, or, year#. a sequential, greedy, optimization of the weights of each layer using the generative training criterion of a restricted boltzmann machine tends to initialize the weights such that global gradient based optimization can work. bengio et al showed that this procedure also worked using the autoassociator unsupervised training criterion and empirically studied the sequential, greedy layer wise strategy. however, to date, the only empirical comparison on classi cation problems between these deep training algorithms and the state of the art has been on mnist, on which many algorithms are relatively successful and in which the classes are known to be well separated in the input space. it remains to be seen whether the advantages seen in the mnist dataset are observed in other more challenging tasks. ultimately, we would like algorithms with the capacity to capture the complex structure found in language and vision tasks. these problems are characterized by many factors of variation that interact in nonlinear ways and make learning di cult. for example, the norb dataset introduced by lecun et al features toys in real scenes, in various lighting, orientation, clutter, and degrees of occlusion. in that work, they demonstrate that existing general algorithms perform poorly. in this work, we propose a suite of datasets that spans some of the territory between mnist and norb starting with mnist, and introducing multiple factors of variation such as rotation and background manipulations. these toy datasets allow us to test the limits of current state of the art algorithms, and explore the behavior of the newer deep architecture training procedures, with architectures not tailored to machine vision. in a very limited but signi cant way, we believe that these problems are closer to real world tasks, and can serve as milestones on the road to ai. on the other hand, deep architecture models are such that their output is the result of the composition of some number of computational units, commensurate with the amount of data one can possibly collect, ie, not exponential in the characteristics of the problem such as the number of factors of variation or the number of inputs. these units are generally organized in layers so that the many levels of computation can be composed. a function may appear complex from the point of view of a local non parametric learning algorithm such as a gaussian kernel machine, because it has many variations, such as the sine function. on the other hand, the kolmogorov complexity of that function could be small, and it could be representable. see bengio and le cun for more discussion on this sub ject, and pointers to the circuit complexity theory literature showing that shallow circuits can require exponentially more components than deeper circuits. it was believed until recently impractical to train deep neural networks, as iterative optimization procedures tended to get stuck near poor local minima. scaling to harder learning problems though there are benchmarks to evaluate generic learning algorithms many of these proposed learning problems do not possess the kind of complexity we address here. we are interested in problems for which the underlying data distribution can be thought as the product of factor distributions, which means that a sample corresponds to a combination of particular values for these factors. for example, in a digit recognition task, the factors might be the scaling, rotation angle, deviation from the center of the image and the background of the image. note how some of these factors may be very high dimensional. in natural language processing, factors which in uence the distribution over words in a document include topic, style and various characteristics of the author. in speech recognition, potential factors can be the gender of the speaker, the background noise and the amount of echo in the environment. in these important settings, it is not feasible to collect enough data to cover the input space. research in incorporating factors of variation into learning procedures has been abundant. a lot of the published results refer to learning invariance in the domain of digit recognition and most of these techniques are engineered for a speci. for instance, decoste and scholkopf present a thorough review that discusses the problem of incorporating prior knowledge into the training procedure of kernel based methods. three main methods are described: hand engineered kernel functions, arti cial generation of transformed examples, and a combination of the two: engineered kernels that generate arti cial examples. the main drawback of these methods, from our point of view, is that domain experts are required to explicitly identify the types of invariances that need to be modeled. while there are cases for which manually crafted invariant features are readily available, it is di cult in general to construct invariant features. we are interested in learning procedures and architectures that would automatically discover and represent such invariances. we believe that one good way of achieving such goals is to have procedures that learn high level features that build on lower level features. one of the main goals of this paper is thus to examine empirically the link between high level feature extraction and di erent types of invariances. we start by describing two architectures that are designed for extracting high level features. shallow and deep architectures we de ne a shallow model as a model with very few layers of composition, eg, linear models, one hiddenlayer neural networks and kernel svms. machine learning techniques are pervasive tools in various emerging commercial applications, but have to be accommodated by powerful computer systems to process very large data. although general purpose cpus and gpus have provided straightforward solutions, their energy efficiencies are limited due to their excessive supports for flexibility. hardware accelerators may achieve better energy efficiencies, but each accelerator often accommodates only a single ml technique. even if regardless of the learning accuracy, such accelerator can still become inapplicable simply because the concrete ml task is altered, or the user chooses another ml technique. benefited from our thorough analysis on computational primitives and locality properties of different ml techniques, pudiannao can perform up to year# gop in an area of mm, and consumes mw only. according to the famous no free lunch theorem in the ml domain, however, an ml technique performs well on a dataset may perform poorly on another dataset, which implies that such accelerator may sometimes lead to poor learning accuracy. in this study, we present an ml accelerator called pudiannao, which accommodates seven representative ml techniques, includingmeans, nearest neighbors, naive bayes, support vector machine, linear regression, classification tree, and deep neural network. compared with the nvidiagpu, pudiannao is faster, and can reduce the energy by. taobao com, the largest online retailer in china, generates tens of terabyte data every day. the increasing amount of data poses great challenges to ml techniques, as well as computer systems accommodating those techniques. the most straightforward way to accelerate large scale ml is to design more powerful general purpose cpus and gpus. however, such processors must consume a large fraction of transistors to exibly support diverse application domains, thus can often be inef cient for speci. in this context, there is a clear trend towards hardware accelerators that can execute speci. workloads with very high energy ef ciency or and performance. for ml techniques that have broad yet important applications in both cloud servers and mobile ends, of course, there have been some successful fpga asic accelerators, but each of which often targets at only a single ml technique or technique family. diannao is one such example, which effectively accelerates representative neural network algorithms to bene. while valid to support different ml scenarios at different scales, diannao still has disadvantages. although recent studies have reported the large success of neural network, they were often achieved at great computational efforts, and involve too many parameters to tune. even if regardless of the high computational complexity, neural network can still be worse than other ml techniques under certain scenarios. in application domains such as nancial quantitative trading, linear regression is more widely used than neural network due to the simplicity and interpretability of linear model. the insight here is that an ml accelerator should be able to support diverse ml techniques, in order to address needs under different scenarios. however, unlike neural networks which share similar computational patterns, there is signi cant diversity among existing ml techniques, making it hard to design a pervasive ml accelerator. for example, a large number of instances innn may be frequently reused to classify unseen instances, while each feature of an instance is not frequently reused in naive bayes. we conduct a thorough analysis of the ml techniques to extract critical computational primitives and locality optimizations that need to be supported by the accelerator. we present pudiannao, an architecture design at tsmc nm process. on critical phases of representative ml techniques, the accelerator, clocked at ghz, is faster and more energyef cient than the nvidiagpu on average. first, we thoroughly analyze several representative ml techniques to extract their key computational tasks and locality properties, which establishes a solid foundation for the design of ml accelerator. second, we design novel functional units to cover common computational primitives of different ml techniques, as well as on chip storage that comprehensively factors in locality properties of different ml techniques. third, we present pu diannao, an accelerator accommodating seven representative ml techniques and multiple ml scenarios. the rest of this paper proceeds as the following. section # conducts a thorough analysis on computational primitives and locality properties of seven ml techniques. section # presents the accelerator architecture of pudiannao. section # introduces the control module and code format of pudian nao. section # and provide the experimental methodology and experimental results, respectively. in the era of data explosion, machine learning techniques have become pervasive tools in emerging large scale commercial applications such as social network, recommendation system, computational advertising, and image recognition. facebook generates over petabyte log data per month. to be speci, when using diannao to solve an ml problem, one has no choice but to use a neural network. for example, in the classi cation of linearlyseparable data, complex neural networks can easily become over tting, and perform worse than even a linear classi er. the famous no free lunch theorem from the ml domain is a good summary of the above situation: any learning technique cannot perform universally better than another learning technique. first, different ml techniques may differ a lot in their computational primitives. here we take two classi cation algorithms, naive bayes andnearest neighbors, as examples. the most time consuming part of naive bayes is estimating the conditional probabilities, but that ofnn is calculating distances between instances. second, different ml techniques may differ a lot in their locality properties. in this case, one must identify and factor in computational primitives and locality properties of representative ml techniques before deciding the overall accelerator architecture. in this paper, we present an accelerator accommodating seven representative ml techniques, ie, means, nn, naive bayes, support vector machine, linear regression, classi cation tree, and deep neural network. a significant part of future microprocessor real estate will be dedicated to or caches. these on chip caches will heavily impact processor perfor mance, power dissipation, and thermal management strategies. there are a number of interconnect design considerations that influence power performance area characteristics of large caches, such as wire mod els, signaling strategy, router design, etc. yet, to date, there exists no analytical tool that takes all of these parameters into account to carry out a design space exploration for large caches and estimate an optimal organization. in this work, we implement two major extensions to the cacti cache modeling tool that focus on interconnect design for a large cache. second, we add the ability to model non uniform cache access. we not only adopt state of the art design space exploration strategies for nuca, we also enhance this exploration by considering on chip network contention and a wider spectrum of wiring and routing choices. we present a validation analysis of the new tool and present a case study to showcase how the tool can improve architecture research methodologies. keywords: cache models, non uniform cache archi tectures, memory hierarchies, on chip intercon nects. first, we add the ability to model different types of wires, such as rc based wires with different power delay characteristics and differential low swing buses. multi core processors will incorporate large and complex cache hierarchies. intel is already prototyping an core processor and there is speculation that entire dies in a package may be employed for large sram caches or dram main memory. therefore, it is expected that future processors will have to intelligently manage many megabytes of on chip cache. this work was supported in parts by nsf grant ccf and nsf career award ccf. cache into shared private domains, move data to improve locality and sharing, optimize the network parameters for ef cient communication between cores and cache banks. examples of ongoing research in these veins include. many cache evaluations employ the cacti cache access modeling tool to estimate delay, power, and area for a given cache size. the cacti estimates are invaluable in setting up baseline simulator parameters, computing temperatures of blocks neighboring the cache, evaluating the merits overheads of novel cache organizations, etc. while cacti produces reliable delay power area estimates for moderately sized caches, it does not model the requirements of large caches in suf cient detail. besides, the search space of the tool is limited and hence so is its application in power performance trade off studies. with much of future cache research focused on multi megabyte cache hierarchy design, this is a serious short coming. hence, in this work, we extend the cacti tool in many ways, with the primary goal of improving the delity of its large cache estimates. the tool can also aid in trade off analysis: for example, with a comprehensive design space exploration, cacti can identify cache con gurations that consume three times less power for about a delay penalty. the main enhancement provided in cacti is a very detailed modeling of the interconnect between cache banks. a large cache is typically partitioned into many smaller banks and an inter bank network is responsible for communicating addresses and data between banks and the cache controller. earlier versions of cacti have employed a simpletree network with global wires and have assumed uniform access times for every bank. recently, non uniform cache architectures have also been proposed that employ a packet switched network between banks and yield access times that are a function of where data blocks are found. we add support for such an architecture within cacti. whether we employ a packet switched ortree network, the delay and power of the network components dominate the overall cache access delay and power as the the rst four versions of cacti have been cited by more than year# papers and are also incorporated into other architectural simulators such as wattch. cache size tree delay percentagetree power percentage figure #. contribution oftree network to overall cache delay and power. figure # shows that thetree of the cacti model contributes an increasing percentage to the overall cache delay as the cache size is increased from to mb. its contribution to total cache power is also sizeable around. the inter bank network itself is sensitive to many parameters, especially the wire signaling strategy, wire parameters, topology, router con guration, etc. the new version of the tool carries out a design space exploration over these parameters to estimate a cache organization that optimizes a combination of power delay area metrics for uca and nuca architectures. network contention plays a non trivial role in determining the performance of an on chip network design. we also augment the design space exploration with empirical data on network contention. components of the tool are partially validated against detailed spice simulations. section # provides details on the new interconnect models and other enhancements integrated into cacti. a case study using cacti is discussed in section #. the intel montecito employs two mb private caches, one for each core. future research will likely explore architectural mechanisms to organize the or we thank the anonymous reviewers for their helpful suggestions. we also present an example case study to demonstrate how the tool can facilitate architectural evaluations. due to the evolution of technology constraints, especially energy constraints which may lead to heterogeneous multi cores, and the increasing number of defects, the design of defect tolerant accelerators for heterogeneous multi cores may become a major micro architecture research issue. and the emergence of high performance applications implementing recognition and mining tasks, for which competitive ann based algorithms exist, drastically expands the potential application scope of a hardware ann accelerator. however, while the error tolerance of ann algorithms is well documented, there are few in depth attempts at demonstrating that an actual hardware ann would be tolerant to faulty transistors. most fault models are abstract and cannot demonstrate that the error tolerance of ann algorithms can be translated into the defect tolerance of hardware ann accelerators. in this article, we introduce a hardware ann geared towards defect tolerance and energy efficiency, by spatially expanding the ann. most custom circuits are highly defect sensitive, a single transistor can wreck such circuits. on the contrary, artificial neural networks are inherently error tolerant algorithms. in order to precisely assess the defect tolerance capability of this hardware ann, we introduce defects at the level of transistors, and then assess the impact of such defects on the hardware ann functional behavior. we empirically show that the conceptual error tolerance of neural networks does translate into the defect tolerance of hardware neural networks, paving the way for their introduction in heterogeneous multi cores as intrinsically defect tolerant and energy efficient accelerators. transistor density continues to increase exponentially, but power dissipation per transistor is improving only slightly with each generation of moore law. given the constant chip level power budgets, this exponentially decreases the percentage of transistors that can switch at full frequency with each technology generation. hence, while the transistor budget continues to increase exponentially, the power budget has become the dominant limiting factor in processor design. in this regime, utilizing transistors to design specialized cores that optimize energy per computation becomes an effective approach to improve system performance. to trade transistors for energy efficiency in a scalable manner, we propose quasi specific cores, or qscores, specialized processors capable of executing multiple general purpose computations while providing an order of magnitude more energy efficiency than a general purpose processor. our approach exploits these similar code patterns to ensure that a small set of specialized cores support a large number of commonly used computations. our results show that qscores can provide better energy efficiency than general purpose processors while reducing the amount of specialized logic required to support the workload by up to. the qscores design flow is based on the insight that similar code patterns exist within and across applications. we evaluate qscores ability to target both a single application library as well as a diverse workload consisting of applications selected from different domains.