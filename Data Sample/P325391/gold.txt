due to the rapid development of integrated circuits and sensor technologies, the size and cost of a vision sensor quickly scales down, which offers a great opportunity to integrate higher resolution sensors in mobile ends and wearable devices. under emerging application scenarios such as image recognition search, however, end devices do not locally perform intensive visual processing on images captured by sensors, due to the limited computational capacity and power budget. instead, computation intensive visual processing algorithms like cnns are performed at the sever end, leading to considerable workloads to the server, which greatly limits the qos and, ultimately, the growth of end users. our study partially bridges this gap by shifting visual processing closer to sensors. neural networks were conventionally executed on cpus, and gpus. these platforms can exibly adapt to various workloads, but the exibility is achieved at a large fraction of transistors, signi cantly affecting the energy ef ciency of executing speci. rst wave of designs at the end of the last century, there have also been a few more modern application speci. accelerator architectures for various neural networks, with implementations on either fpgas or asics. for cnns, farabet et al proposed a systolic architecture called neuflow architecture, chakradhar et al designed a systolic like coprocessor. although effective to handle convolution in signal processing, systolic architectures do not provide suf cient exibility and ef ciency to support different settings of cnns, which is exempli ed by their strict restrictions on cnn parameters, as well as their high memory bandwidth requirements. there have been some neural network accelerators adopting simd like architectures. esmaeilzadeh et al proposed a neural network stream processing core with an array of pes, but the released version is still designed for multi layer perceptrons. peemen et al proposed to accelerate cnns with an fpga accelerator controlled by a host processor. although this accelerator is equipped with a memory subsystem customized for cnns, the requirement of a host processor limits the overall energy ef ciency. gokhale et al designed a mobile coprocessor for visual processing at mobile devices, which supports both cnns and dnns. the above studies did not treat main memory accesses as the rst order concern, or directly linked the computational block to the main memory via a dma. recently, some of us designed dedicated onchip sram buffers to reduce main memory accesses, and the proposed diannao accelerator cover a broad range of neural networks including cnns. however, in order to exibly support different neural networks, diannao does not implement specialized hardware to exploit data locality of feature maps in a cnn, but instead treats them as data vectors in common mlps. therefore, diannao still needs frequent memory accesses to execute a cnn, which is less energy ef cient than our design. recent members of the diannao family have been optimized for large scale neural networks and classic machine learning techniques respectively. however, they are not designed for embedded applications, and their architectures are signi cantly different from the shidiannao architecture. our design is substantially different from previous studies in two aspects. first, unlike previous designs requiring memory accesses to get data, our design does not access to the main memory when executing a cnn. second, unlike previous systolic designs supporting a single cnn with. xed parameter and layer setting, or a single convolutional layer with xed parameters, our design exibly accommodates different cnns with different parameter and layer settings. due to these attractive features, our design is more energy ef cient than previous designs on cnns, thus particularly suits visual recognition in embedded systems.