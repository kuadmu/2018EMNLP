in many applications, there are a variety of ways of referring to the same underlying entity. given a collection of references to entities, we would like to determine the set of true underlying entities and map the references to these entities. the references may be to entities of different types and more than one type of entity may need to be resolved at the same time. we propose similarity measures for clustering references taking into account the different relations that are observed among the typed references. we pose typed entity resolution in relational data as a clustering problem and present experimental results on real data showing improvements over attribute based models when relations are leveraged. many databases contain uncertain and imprecise references to real world entities. the absence of identifiers for the underlying entities often results in a database which contains multiple references to the same entity. entity resolution involves discovering the underlying entities and mapping each database reference to these entities. we evaluate our collective entity resolution algorithm on multiple real world databases. this can lead not only to data redundancy, but also inaccuracies in query processing and knowledge extraction. these problems can be alleviated through the use of entity resolution. traditionally, entities are resolved using pairwise similarity over the attributes of references. however, there is often additional relational information in the data. in these cases, collective entity resolution, in which entities for cooccurring references are determined jointly rather than independently, can improve entity resolution accuracy. we propose a novel relational clustering algorithm that uses both attribute and relational information for determining the underlying domain entities, and we give an efficient implementation. we investigate the impact that different relational similarity measures have on entity resolution quality. we show that it improves entity resolution performance over both attribute based baselines and over algorithms that consider relational information but do not resolve entities collectively. in addition, we perform detailed experiments on synthetically generated data to identify data characteristics that favor collective relational resolution over purely attribute based algorithms. for example in a census database, doe, jonathan doe, and jon doe may all refer to the same person. for example, the same database may also have records showing that jonathan doe is married to jeanette doe and has dependents james doe and jason doe, jon doe is married to jean doe, and. doe has dependents jim doe, jason doe and jackie doe. entity resolution is a common problem that comes in different guises in many computer science domains. attribute similarity measures and often learn such mapping functions from resolved data. then the references cannot be assigned to entities independently any more the entities for related references depend on one another. the collective entity resolution problem can be solved in an iterative fashion. here we propose a collective entity resolution approach based on a novel unsupervised relational clustering algorithm. in section #, we formalize the relational entity resolution problem. we explore and compare different approaches for entity resolution and formulate collective relational entity resolution as a clustering problem in section #. in section #, we describe experimental results using the different similarity measures on multiple real world datasets. in many applications, there are a variety of ways of referring to the same underlying real world entity. additionally, in many domains, references to different entities often cooccur in the data. such relationships between entity references are best represented as a graph, which we refer to as the reference graph, where the nodes are the entity references and edges in the graph indicate references which cooccur. the problem is, for any real world entity, there may be multiple references to it, and, accordingly, there is more than one node in the reference graph corresponding to that entity. thus an important rst step in any graph mining algorithm is transforming such a reference graph into an entity graph, where nodes are the entities themselves and edges are among entities. given a collection of references to entities, we would like to a determine the collection of true underlying entities andcorrectly map the entity references in the collection to these entities. figure # shows the reference graph from our census example and figure # shows the entity graph after the references in the reference graph have been resolved. even in this simple example, the entity graph is much smaller than the reference graph and consists of a single connected component. this provides a much more accurate picture of the underlying domain structure than the collection of disconnected subgraphs in the reference graph. examples include computer vision, where we need to gure out when regions in two different images refer to the same underlying object; natural language processing when we would like to determine which noun phrases refer to the same underlying entity; and databases, where, when merging two databases or cleaning a database, we would like to determine when two tuple records are referring to the same real world object. deduplication is important for both accurate analysis, for example, determining the number of customers, and for cost effectiveness, for example, removing duplicates from mailing lists. in information integration, determining approximate joins is important for consolidating information from multiple sources; most often there will not be a unique key that can be used to join tables in distributed databases, and we must infer when two records from different databases, possibly with different structures, refer to the same entity. in many of these examples, cooccurrence information in the input can be naturally represented as a graph. traditional approaches to entity resolution and deduplication use a variety of attribute similarity measures, often based on approximate string matching criteria. these work well for correcting typographical errors and other types of acm transactions on knowledge discovery from data, vol. collective entity resolution in relational data figure #. example of a reference graph for the simple example given in the text and the resolved entity graph. however, it is still dif cult to decide when identical references are in fact distinct. doe and living at the same address and of the same age may be brothers and not the same person. more recent approaches take structural similarity into account. one approach simply looks at the attributes of related references and incorporates them into the attribute similarity score. for example, if we are comparing two census records for jon doe and jonathan doe, we would be more likely to match them if they are married to jean doe and jeannette doe. the problem becomes even more interesting when we assume that the entity for a reference depends not on the attribute similarities of related references but instead on the entities to which they correspond. in our example, we would not consider jon doe and jonathan doe to be the same person simply because their wives names are similar since different people may have wives with similar names. but if we can determine that they are married to the same person, this would provide signi cant evidence that these references refer to the same entity. because the resolutions are no longer independent, the problem becomes one of collective entity resolution. determining that two references refer to the same entity may in turn allow us to make additional inferences about their related references. in our census example, if we are able to determine that the two jason does refer to the same person, that would provide further evidence that their fathers, jonathan doe and. doe, are also the same, and possibly that their siblings jim doe and james doe are the same person as well. using the relational evidence for collective entity resolution has the potential bene. that we may be able to produce more accurate results than using only attribute similarity measures. acm transactions on knowledge discovery from data, vol. comes at the cost of additional algorithmic complexity resulting from propagating the dependence between different resolution decisions. in this work, we study the trade off between the increased accuracy offered by collective resolution and the computational cost required to achieve this improvement. this article builds on our initial work on entity resolution in relational data described in a workshop paper and included in a survey book chapter. the contributions of this article are: a comprehensive description of the collective relational entity resolution algorithm, a thorough exploration of different types of neighborhood similarity measures, a comparison with a naive relational clustering approach, evaluations on signi cantly larger real datasets that have multivalued attributes, and a suite of synthetic data experiments which investigate the performance of collective entity resolution against varying structural characteristics of the data. the rest of this article is organized as follows. in section #, we present a more realistic motivating example for entity resolution using the relations between references. we propose novel relational similarity measures for collective relational clustering in section #. we discuss the clustering algorithm in further detail in section #. we also present detailed experiments on synthetically generated data to identify data characteristics that indicate when collective resolution should be favored over the more naive approaches. we review related work on entity resolution in section #, and nally conclude in section #. the problem of identifying approximately duplicate records in databases is an essential step for data cleaning and data integration processes. most existing approaches have relied on generic or manually tuned distance metrics for estimating the similarity of potential duplicates. in this paper, we present a framework for improving duplicate detection using trainable measures of textual similarity. we propose to employ learnable text distance functions for each database field, and show that such measures are capable of adapting to the specific notion of similarity that is appropriate for the field domain. we present two learnable text similarity measures suitable for this task: an extended variant of learnable string edit distance, and a novel vector space based measure that employs a support vector machine for training. experimental results on a range of datasets show that our framework can improve duplicate detection accuracy over traditional techniques. databases frequently contain eld values and records that refer to the same entity but are not syntactically identical. variations in representation can arise from typographical errors, misspellings, abbreviations, as well as integration of multiple data sources. variations are particularly pronounced in data that is automatically extracted from unstructured or semi structured documents or web pages. such approximate duplicates can have many deleterious effects, including preventing data mining algorithms from discovering important regularities. this problem is typically handled during a tedious manual data cleaning, or de duping, process. some previous work has addressed the problem of identifying duplicate records, where it was referred to as record linkage, the merge purge problem, duplicate detection, hardening soft databases, reference matching, and entityname clustering and matching. some more recent work has investigated the use of pairing functions that combine multiple standard metrics. because an estimate of similarity between strings can vary signi cantly depending on the domain and specield under consideration, traditional similarity measures may fail to estimate string similarity correctly. at the token level, certain words can be informative when comparing two strings for equivalence, while others are ignorable. thus, accurate similarity computations require adapting string similarity metrics for each eld of the database with respect to the particular data domain. rather than hand tuning a distance metric for each eld, we propose to use trainable similarity measures that can be learned from small corpora of labeled examples, and thus adapt to different domains. the rst one utilizes the expectation maximization algorithm for estimating the parameters of a generative model based on string edit distance with af ne gaps. the characterbased distance is best suited for shorter strings with minor variations, while the measure based on vector space representation is more appropriate for elds that contain longer strings with more global variations. our overall system, marlin, employs a two level learning approach. first, string similarity measures are trained for every database eld so that they can provide accurate estimates of string distance between values for that eld. next,nal predicate for detecting duplicate records is learned from similarity metrics applied to each of the individual elds. we again utilize support vector machines for this task, and show that they outperform decision trees, the classi er used in prior work. we evaluate our approach on several real world data sets containing duplicate records and show that marlin can lead to improved duplicate detection accuracy over traditional techniques. typically, standard string similarity metrics such as edit distance or vector space cosine similarity are used to determine whether two values or records are alike enough to be duplicates. for example, ignoring the substring street may be acceptable when comparing addresses, but not when comparing names of people or newspapers. at the character level, certain characters can be consistently replaced by others or omitted when syntactic variations are due to systematic typographical or ocr errors. the other string similarity measure employs a support vector machine to obtain a similarity estimate based on the vector space model of text. when integrating data from multiple web sources, objects can exist in different formats and structures, making it difficult to identify those that can be matched together. in this paper, we propose an identification approach to finding similar identities among objects from multiple web sources. in this approach, object identification works like the relational join operation where a similarity function takes the place of the equality condition. this similarity function is based on information retrieval techniques. our approach differs from others in the literature since it can be used to identify objects more complexly structured and not only objects with a flat structure such as relations. the effectiveness of our approach is demonstrated by experimental results with real web data sources from different domains, that reach precision levels above. with the popularization of the web and the widespreading of modern digital libraries, a huge amount of information has been made available to a large audience. this leads to a great necessity of new tools and techniques to retrieve, integrate, and manage information on the web. the need for such tools and techniques is further stressed by the fact that nding the desired information on the web permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. da silva department of computer science federal university of amazonas manaus am brazil alti dcc fua br is not a trivial task. information on the web, users generally are faced with the problem of having to access various distinct and independent web sites to obtain complementary pieces of information scattered among them. this occurs typically in situations where the required information cannot be found in a unique web source. for example, if someone wants to know which articles are derived from computer science theses and dissertations from a certain university, he she has to access at least two di erent web sources: one that contains the list of theses and dissertations of that university, and another one of computer science publications. however, just accessing these two web sources might not be enough to nd the desired information. a correct match of their data contents is required. thus, to facilitate users access to information spread across the web, it is necessary to integrate data from multiple sources. further, if the volume of data available in the target sources is large, automatic and scalable integration solutions must be applied. a fundamental problem to be solved in the integration process is how to identify similar objects from di erent web sources. when integrating data from multiple web sources, objects can exist in di erent formats and structure, making it di cult to identify those that can be joined together. information integration systems like tsimmis, information manifold, and webview, manage objects from multiple information sources, but they do not. er a general method to determine semantically similar objects. a possible approach to solving this problem in such systems is to use the so called skolem function that creates logical identi ers for the objects involved. in this paper, we propose a similarity based approach to identi ng similar identities among objects from multiple web sources. this approach works like the join operation in relational databases. in the traditional join operation, the equality condition identi es tuples that can be joined together. in our approach, a similarity function that is based on information retrieval techniques takes the place of the equality condition. other similarity based approaches for data integration have been proposed in the literature. whirl is a database management system that supports similarity joins among relations that have free text attribute values. it uses the vector space model to determine the similarity among text attributes. active atlas is an object identi cation system that also uses the vector space model to establish the mapping between objects of two sources. it learns to tailor mapping rules, through limited user input, to a speci. our approach also uses the vector space model to determine the similarity among objects from multiple sources, but unlike the previously discussed ones it can be used to integrate objects with complex structure and not only objects with. for example, to identify the articles written by a thesis or dissertation author we have to compare a speci. in this paper, we present four di erent strategies to de ne the similarity function using the vector space model and describe experimental results that show, for web sources of three di erent application domains, that our approach is quite. ective in nding objects with similar identities, achieving precision levels above. another problem of great interest in multiple data sources integration is schema matching. it consists in establishing a mapping between object attributes from di erent sources. a survey on automatic schema matching is presented in. our approach, however, does not address the matching schema problem. we assume that the mapping between object attributes from di erent sources is previously given. section # presents an overview of our approach. finally, section # presents our conclusions and describes future work. data mining practitioners frequently have to spend significant portion of their project time on data preprocessing before they can apply their algorithms on real world datasets. given the significance of the problem, numerous data cleaning techniques have been designed in the past to address the aforementioned problems with data in this paper, we address one of the data cleaning challenges, called object consolidation. this important challenge arises because objects in datasets are frequently represented via descriptions, which alone might not always uniquely identify the object. such a preprocessing is required because many real world datasets are not perfect, but rather they contain missing, erroneous, duplicate data and other data cleaning problems. it is a well established fact that, in general, if such problems with data are not corrected, applying data mining algorithm can lead to wrong results. the latter is known as the garbage in, garbage out principle. the goal of object consolidation is to correctly consolidate all the representations of the same object, for each object in the dataset. in contrast to traditional domain independent data cleaning techniques, our approach analyzes not only object features, but also additional semantic information: inter objects relationships, for the purpose of object consolidation. the approach views datasets as attributed relational graphs of object representations, connected via relationships. the approach then applies graph partitioning techniques to accurately cluster object representations. our empirical study over real datasets shows that analyzing relationships significantly improves the quality of the result. nowadays data mining techniques are widely used to analyze data for scienti. applications and business decision reldc project this work was supported by nsf grants, permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. many real world datasets however are not perfect, they frequently contain various data cleaning issues such as incomplete, erroneous and duplicate data, which need to be addressed before data mining techniques can be applied. in this paper, we address one common data cleaning challenge known as object consolidation. it arises most frequently when the dataset being processed is constructed by merging various data sources into a single uni ed database, such as by crawling the web. an ob ject might have multiple di erent representations in the dataset and also an object representation, in general, might match the description of multiple objects instead of one. for example, consider a database that contains information about two people: john. smith might have multiple representations throughout the dataset: eg, john smith, smith john smithx. consider the task of computing author impact in a citation network using a simple citation count statistic. this simple task might be more di cult than it seems due to the problem with representations identi ed above. because of such problems with representations, some of the papers written by john. the problem of object consolidation is related to the problem of record deduplication or record linkage that often arises when multiple tables are merged to create a single database. as a rule, each instance of the reference disambiguation problem can be formulated as an instance of the ob ject consolidation problem, while the reverse is not true. that is, the object consolidation problem is more general. in this paper, we study a domainindependent approach that utilizes not only features but also additional semantic information present in datasets: interob ject relationships. might be used to refer to an author in the context of a particular publication. this publication might also have more authors, which can be linked to their. liated organizations and so on, forming a web of entities inter connected via relationships. an empirical evaluation of the proposed technique, that establishes that analyzing relationships is important for ob ject consolidation. next, in section #, we present a motivational example and then, in section #, we formalize the problem and introduce the notation necessary to explain the approach. iqis year#, june, year#, baltimore, md, usa copyright year# acm. to build proper models and compute accurate results it is important that analyzed datasets are accurately represented and interpreted. as a result, data mining practitioners frequently spend signi cant. ort on preprocessing of data to address cleaning issues that exist in their datasets, to ensure high quality of the results. in many real world datasets ob jects entities are not represented by unique identi ers, instead an object is represented by a description, used in a certain context, which may lead to ambiguity. the goal of object consolidation is to correctly group all the representations that refer to the same object. smith, so one representation matches the descriptions of multiple entities. finally, the fact that there are only two john smithin the dataset might not be known in general. thus, for this example the goal is to determine that all john smith representations should be clustered into two groups and then assign them to groups such that all representations for john. smith are in one group and for john. sometimes it is possible to infer more attributes information from the context in which representations appear. context, it might be known that the mentioned. this context information can be potentially used to consolidate representations better. let us use an example to demonstrate the implication of applying data analysis techniques on datasets where the ob ject consolidation problem is not resolved correctly. that is, the task might be to compute the impact of john. smith by counting the number of citations of his publications. notice, even though the representations appear in some context, the information about the ob ject available from the context might be of a limited nature, which makes the object consolidation task challenging. for instance, the only direct information available about the authors, for some of the publications, might be only their rst initials and last names. smith might be wrongfully assigned to other authors and some of the papers written by other authors might be assigned to john. smith, computed on such a dataset, can be very di erent from the real one. while the object consolidation problem exists in di erent domains, in this paper we will often use citation networks, like in the example above, to illustrate our domainindependent approach. the causes of record linkage are similar, ie, di erences in representation of objects across di erent datasets, entry errors, etc. the di erence between the two problems is that while record linkage deals with records in a table, object consolidation deals with entities ob jects a semantic concept of a higher level. in record linkage it is often assumed that many attributes are available in each record, which are very. in object consolidation, however, very few attributes can be available, thus making the problem more challenging. another related problem is the problem of reference disambiguation. in the problem of reference disambiguation the goal is to match object representations with the list of possible objects which is known in advance and known to be clean. the requirement of having such a clean list of ob jects limits the applicability of reference disambiguation. most of the traditional domain indep endent data cleaning techniques belong to the class of feature based similarity metho ds to determine if two objects records are the same they employ a similarity function that compares values of object record attributes for the purpose of deduplication. the values of the attributes of an ob ject are typically derived from the object representation and the context in which it is used. smith and john smith, while not identical, are su ciently similar to suggest that one can be the other and fbs techniques can detect that. it can also be known from the context that the mentioned. smith works at mit and john smith works at mit, then fbs approaches can use this additional attribute and suggest that they are now more con dent that the two representations refer to the same person. the knowledge of relationships can be exploited alongside attribute based similarity resulting in improved accuracy of object consolidation. our approach is based on the following hypothesis, which is referred to as the context attraction principle: the cap hypothesis: if two representations refer to the same entity, there is a high likelihood that they are strongly connected to each other through multiple relationships, implicit in the database; if two representations refer to di erent entities, the connection between them via relationships is weak, compared with that of the representations that refer to the same entity. our approach views the underlying database as an attributed relational graph, where nodes correspond to object representations and edges correspond to relationships. our technique rst uses feature based similarity, to determine if two representations can refer to the same objects. if, based on the fbs similarity, two representations can refer to one ob ject, then the relationships between those representations are analyzed to measure the connection strength between them. graph partitioning techniques are then employed to consolidate the representations of objects based on the fbs similarity and connection strength among them. the primary contributions of this paper are: a novel object consolidation approach, which, unlike traditional techniques, employs not only attribute similarity, but also analyzes inter ob ject relationships to improve the quality of consolidation. novel metrics to analyze the quality of the outcome. in integrating databases found on the web or obtained by using information extraction methods, it is often possible to solve this problem by exploiting similarities in the textual names used for objects in different databases. in this paper we describe techniques for clustering and matching identifier names that are both scalable and adaptive, in the sense that they can be trained to obtain better performance in a particular domain. an experimental evaluation on a number of sample datasets shows that the adaptive method sometimes performs much better than either of two non adaptive baseline systems, and is nearly always competitive with the best baseline system. part of the process of data integration is determining which sets of identifiers refer to the same real world entities. a num ber of recent research papers have addressed this problem by exploiting similarities in the textual names used for ob jects in different databases. previous work in this area includes work in distance functions for matching and scalable matching and clustering al gorithms. we present techniques for entity name matching and clustering that are scalable and adaptive, in the sense that accuracy can be improved by training. data integration is the problem of combining information from multiple heterogeneous databases. one step of data integration is relating the primitive objects that appear in the different databases specifically, determining which sets of identifiers refer to the same real world entities. integration techniques based on textual similarity are especially useful for databases found on the web or obtained by extracting information from text, where descriptive names generally exist but permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. previous publications in using textual similarity for data integration have considered a number of related tasks. al though the terminology is not completely standardized, in this paper we define entity name matching as the task of taking two lists of entity names from two different sources and determining which pairs of names are co referent. matching is important in at tempting to join information across of pair of relations from different databases, and clustering is important in remov ing duplicates from a relation that has been drawn from the union of many different information sources. work in record linkage is similar but does not rely as heavily on textual similarities. in this paper we synthesize many of these ideas. we define entity name clustering as the task of taking a single list of entity names and assigning entity names to clusters such that all names in a cluster are co referent. reference reconciliation is the problem of identifying when different references in a dataset correspond to the same real world entity. most previous literature assumed references to a single class that had a fair number of attributes. we consider complex information spaces: our references belong to multiple related classes and each reference may have very few attribute values. a prime example of such a space is personal information management, where the goal is to provide a coherent view of all the information on one desktop our reconciliation algorithm has three principal features. first, we exploit the associations between references to design new methods for reference comparison. second, we propagate information between reconciliation decisions to accumulate positive and negative evidences. third, we gradually enrich references by merging attribute values. our experiments show that we considerably improve precision and recall over standard methods on a diverse set of personal information datasets, and there are advantages to using our algorithm even on a standard citation dataset benchmark. one of the ma jor impediments to integrating data from multiple sources, whether by warehousing, virtual integration or web services, is resolving references at the instance level. data sources have di erent ways of referring to the same real world entity. variations in representation arise for multiple reasons: mis spellings, use of abbreviations, di erent naming conventions, naming variations over time, and the presence of several values for particular attributes. to join data from multiple sources, and therefore, to do any kind of analysis, we must detect when di erent references refer to the same real world entity. reference reconciliation has received signi cant attention in the literature, and its variations have been referred to permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. as record linkage, merge purge, de duplication, hardening soft databases, reference matching, object identi cation and identity uncertainty. most of the previous work considered techniques for reconciling references to a single class. furthermore, typically the data contained many attributes with each instance. however, in practice, many data integration tasks need to tackle complex information spaces where instances of multiple classes and rich relationships between the instances exist, classes may have only few attributes, and references typically have unknown attribute values. the main motivation for our work comes from the application of personal information management, and speci cally, supporting higher level browsing of information on onedesktop. the need for better search tools for the desktop was highlighted by systems like sis and the google desktop search tool. however, these systems provide only keyword search to the desktopcontents. the vision of the personal memex and recent systems such as haystack and semex emphasize the ability to browse personal information by associations, representing important semantic relationships between objects. to support such browsing, a pim system examines data from a variety of sources on the desktop to extract instances of multiple classes: person, message, article, conference, etc. in addition, the system extracts associations between the instances, such as senderof, earlyversionof, authorof, and publishedin, which then provide the basis for browsing. however, since the data sources are heterogeneous and span several years, a real world object is typically referred to in several di erent ways. reconciliation of the above classes of references guarantees that they mesh together seamlessly, and so the pim system can provide palatable browsing and searching experiences. aside from pim, many reference reconciliation problems involve complex relationships between multiple entities. for example, most of the work on reconciling publications for portals such as citeseer and cora has focused solely on the publications. er a more powerful portal, we would also like to reconcile authors, institutions, publishers and conferences. in fact, reconciling the other entities can improve the accuracy of publication reconciliation. as another example, reconciling references in online product catalogs is a critical problem in electronic commerce. the reconciliation decision in this context involves the product entities, their manufacturers, suppliers, orders and related products. to date, most reference reconciliation approaches employ approximate string similarity measures and combine the similarities of multiple attributes. though we can apply these methods to each type of references in isolation, we miss the rich information carried in the relationships between the instances. furthermore, the previous techniques assume there are several attributes associated with every reference, and therefore a reasonable amount of information to consider in the reconciliation decision. for example, in pim, a person reference extracted from an email may contain only an email address, and in citeseer, a person reference extracted from a citation contains only a name, which may even be abbreviated. we describe a novel reference reconciliation algorithm that is well suited for complex information spaces and for cases where some of the references may lack information. the key ideas underlying our approach are the following. first, we make extensive use of context information to provide evidence for reconciliation decisions. for example, given two references to persons, we will consider their co authors and email contacts, to help decide whether to reconcile them. second, we propagate information between reconciliation decisions for di erent pairs of references. for example, when we decide to reconcile two papers, we obtain additional evidence for reconciling the person references to their authors. this, in turn, can further increase the con dence in reconciling other papers authored by the reconciled persons. third, we address the lack of information in each reference by reference enrichment. for example, when we reconcile two person references, we gather the di erent representations of the personname, collect her di erent email addresses, and enlarge her list of co authors and email contacts. this enriched reference can later be reconciled with other references where we previously lacked information for the reconciliation. our algorithm is based on propagating reference similarity decisions in a dependency graph whose nodes represent similarities between pairs of references, and whose edges represent the dependencies between the reconciliation decisions. our framework captures the dependence between di erent reference pair reconciliations and at the same time retains the exibility of using established reference comparison techniques. the result is a reconciliation algorithm that improves over the recall of previous techniques without sacri cing precision. we describe a novel reference reconciliation algorithm, based on a general framework for propagating information from one reconciliation decision to another. our algorithm uses context information, similarities computed on related entities, and enriched references. in addition, we show how our algorithm exploits knowledge that two references are guaranteed to be distinct. we evaluate our reconciliation algorithm on several personal information datasets and on the cora citation dataset. in the pim context, the experiments show that our approach signi cantly improves on standard reference reconciliation techniques. on the cora dataset, the results show that our approach is comparable to recent adaptive approaches for paper reconciliation, and at the same time produces accurate reconciliation on person and publisher instances. person article conference journal personal information schema article a a person conf raw references, reconciliation results figure #: the schema, references, and reconciliation results in example. section # de nes the reconciliation problem and illustrates our approach. section # describes the dependency graph framework, and section # describes the computation of similarities for each class. section # describes related work and section # concludes. matching is usually the first step of an integration process. this feature, as our experiments confirm, makes our approach capable of dealing with content as well as structural divergences. highly dynamic applications like the web and peer to peer systems require a great deal of effort in document management. documents from different sources may contain parts that, although having different structure or different contents, may be considered as representing the same conceptual information. one essential task in this scenario is the identification of complementary or overlapping documents that need to be integrated. in this paper, we deal specifically with documents represented in the xml format. xml document integration is an important process in highly dynamic applications, for the volume of data available in this format is constantly growing. xml integration is also a challenging task, due to the flexible nature of xml, which may lead to structure divergences and content conflicts between the documents. in this work, we present a novel approach to the matching problem, ie, the problem of defining which parts of two documents contain the same information. our approach is novel in the sense it combines similarity information from the content of the elements with information from the structure of the documents. in this paper, we are interested in identifying which portions of di erent documents represent what may be regarded as the same information at a conceptual level. one can easily infer that the data on both documents and represent the same conceptual information, despite the di erences in their content and structure. to nd out that john smith and smith, john represent the same name, a straight string comparison would not su ce. astring similarity function is required to discover that these two strings represent the same information. some well known similarity functions are edit distance, jaro winkler, monge elkan and softtf idf, among others. figure # also shows an example of structural divergence between documents and. this kind of divergence cannot be detected by a similarity function like the ones listed above. the use of semantic in figure #: two di erent ways of representing personal data in xml. in this context, the xml integration process can be treated as two separate problems: matching, which consists in the discovery of correspondences between elements of xml documents; merging, which is the process of determining, based on the correspondences found in the matching phase, how the nal document is going to look like. the remaining of this paper is organized as follows. document management in highly dynamic environments like the web and peer to peer systems is a hard task. in such environments, documents change very frequently, both in content and structure. additionally, the same information may be represented in documents from di erent sources, leading to overlap of documents. dealing with these overlaps and or duplications in a dynamic environment is challenging in many aspects. we focus on xml documents, since documents of this class mix structure and content, and are largely available. since each data source may publish data in its very own format, xml integration must deal with the problem of structure and content inconsistencies between documents. figure # shows an example of two di erent xml documents, represented in the form of trees. one example of content divergence occurs in the name element, which is spelled in document as john smith, while in document it is smith, john. such similarity functions get two strings as input and return a value indicating how similar they are. when such functions are used, we usually consider that two input strings match if their similarity value is above a certain threshold. the personaddress in document is represented as a single element, while in document it is represented as individual elements for street, number, etc. however, if we merge all these individual elements from into a single element, it will have the same content as the address element in. to correctly identify this case, the integration engine must be aware that one elementcontent is identical to the merged contents of sub elements of another element. formation available in the elementtags and attributes can also be valuable in this process. the identi cation and resolution of these kinds of structural and content divergences is the essence of the process known as matching. the matching process consists in determining which elements from two documents represent the same conceptual information. matching is the rst step in the process of integrating xml documents. this paper is restricted to the rst problem, ie, matching xml documents. in this context, we present a novel strategy to identify matchings between two xml documents. the main strength of our approach resides in the fact that it uses information from both content and structure of xml documents. in section #, we present and discuss related work. in section #, we describe our approach to xml matching. section # details the performed experiments and the results obtained with three document sets. in section #, we draw our conclusions and point out some future works direction. in this article, we address the problem of reference disambiguation. specifically, we consider a situation where entities in the database are referred to using descriptions. the objective of reference disambiguation is to identify the unique entity to which each description corresponds. the key difference between the approach we propose and the traditional techniques is that reldc analyzes not only object features but also inter object relationships to improve the disambiguation quality. our extensive experiments over two real data sets and over synthetic datasets show that analysis of relationships significantly improves quality of the result. recent surveys show that more than of researchers working on data mining projects spend more than of their project time on cleaning and preparation of data. the data cleaning problem often arises when information from heterogeneous sources is merged to create a single database. many distinct data cleaning challenges have been identi ed in the literature: dealing with missing data, handling erroneous data, record linkage, and so on. in this article, we address one such challenge called reference disambiguation, which is also known as fuzzy match and fuzzy lookup. the reference disambiguation problem arises when entities in a database contain references to other entities. if entities were referred to using unique identi ers, then disambiguating those references would be straightforward. instead, frequently, entities are represented using properties descriptions that may not uniquely identify them leading to ambiguity. for instance, a database may store information about two distinct individuals donald. white, both of whom are referred to as. references may also be ambiguous due to differences in the representations of the same entity and errors in data entries. the goal of reference disambiguation is for each reference to correctly identify the unique entity it refers to. the reference disambiguation problem is related to the problem of record deduplication or record linkage that often arise when multiple tables are merged to create a single table. the causes of record linkage and reference disambiguation problems are similar; viz, differences in representations of objects across different data sets, data entry errors, etc. the differences between the two can be intuitively viewed using the relational terminology as follows: while the record linkage problem consists of determining when two records are the same, reference disambiguation corresponds to ensuring that references in a database point to the correct entities. given the tight relationship between the two data cleaning tasks and the similarity of their causes, existing approaches to record linkage can be adapted for reference disambiguation. in particular, feature based similarity methods that analyze similarity of record attribute values can be used to determine if a particular reference corresponds to a given entity or not. this article argues that quality of disambiguation can be signi cantly improved by exploring additional semantic information. in particular, we observe that references occur within a context and de ne relationships connections between entities. for instance, white might be used to refer to an author in the context of a particular publication. this publication might also refer to different authors, which can be linked to their af liated organizations, etc, forming chains of relationships among entities. such knowledge can be exploited alongside attribute based similarity resulting in improved accuracy of disambiguation. in this article, we propose a domain independent data cleaning approach for reference disambiguation, referred to as relationship based data cleaning, which systematically exploits not only features but also relationships we are using the term foreign key loosely. usually, foreign key refers to a unique identi er of an entity in another table. instead, foreign key above means the set of properties that serve as a reference to an entity. reldc views the database as a graph of entities that are linked to each other via relationships. it rst utilizes a feature based method to identify a set of candidate entities for a reference to be disambiguated. graph theoretic techniques are then used to discover and analyze relationships that exist between the entity containing the reference and the set of candidates. the primary contributions of this article are: developing a systematic approach to exploiting both attributes as well as relationships among entities for reference disambiguation, developing a set of optimizations to achieve an ef cient and scalable implementation of the approach, establishing that exploiting relationships can signi cantly improve the quality of reference disambiguation by testing the developed approach over real world data sets as well as synthetic data sets. a preliminary version of this article appeared in kalashnikov et al; it presents an overview of the approach, without implementation and other details, required for implementing the approach in practice. the rest of this article is organized as follows: section # presents a motivational example. in section #, we precisely formulate the problem of reference disambiguation and introduce notation that will help explain the reldc approach. the empirical results of reldc are presented in section #. section # contains the related work, and section # concludes the article. fuzzy duplicate detection aims at identifying multiple representations of real world objects stored in a data source, and is a task of critical practical relevance in data cleaning, data mining, or data integration. it has a long history for relational data stored in a single table. algorithms for fuzzy duplicate detection in more complex structures, eg, hierarchies of a data warehouse, xml data, or graph data have only recently emerged. these algorithms use similarity measures that consider the duplicate status of their direct neighbors, eg, children in hierarchical data, to improve duplicate detection effectiveness. unlike previous approaches, it not only considers the duplicate status of children, but rather the probability of descendants being duplicates. probabilities are computed efficiently using a bayesian network. experiments show the proposed algorithm is able to maintain high precision and recall values, even when dealing with data containing a high amount of errors and missing information. in this paper, we propose a novel method for fuzzy duplicate detection in hierarchical and semi structured xml data. our proposal is also able to outperform a state of the art duplicate detection system on three different xml databases. deduplication is a key operation in integrating data from multiple sources. the main challenge in this task is designing a function that can resolve when a pair of records refer to the same entity in spite of various data inconsistencies. one way to overcome the tedium of hand coding is to train a classifier to distinguish between duplicates and non duplicates. the success of this method critically hinges on being able to provide a covering and challenging set of training pairs that bring out the subtlety of deduplication function. this is non trivial because it requires manually searching for various data inconsistencies between any two records spread apart in large lists we present our design of a learning based deduplication system that uses a novel method of interactively discovering challenging training pairs using active learning. our experiments on real life datasets show that active learning significantly reduces the number of instances needed to achieve high accuracy. we investigate various design issues that arise in building a system to provide interactive response, fast convergence, and interpretable output. in this paper, we describe a system incorporating an improved technique that detects the similarity of two xml documents based on content and structure similarity using keys. the technique consists of three major components: a subtree generator and validator, a key generator, and similarity components that compare content and structure of the xml documents. first, an xml document is stored in a relational database and extracted into small subtrees using leaf node parents. the leaf node parents are considered as a root of a subtree which is then recursively traversed bottom up for matching. second, a possible key is identified in order to match xml subtrees from two documents efficiently. key matchings help in reducing the number of comparisons dramatically. in addition, the number of subtrees to be processed is reduced in the subtree validation phase using instance statistics and taxonomic analyzer. the subtrees are matched by the key first and the remaining subtrees are matched by finding degrees of similarity in content and structure. to obtain improved similarity comparison results, xml element names are transformed according to their semantic similarity. the results show that the clustering points are selected appropriately and the overall execution time is reduced dramatically. duplicate detection is the problem of detecting different entries in a data source representing the same real world entity. while research abounds in the realm of duplicate detection in relational data, there is yet little work for duplicates in other, more complex data models, such as xml. we propose heuristics to determine which of these to choose, as well as a similarity measure specifically geared towards the xml data model. an evaluation of our algorithm using several heuristics validates our approach. entity resolution is the problem of identifying which records in a database refer to the same real world entity. however, most blocking techniques process blocks separately and do not exploit the results of other blocks. an exhaustive er process involves computing the similarities between pairs of records, which can be very expensive for large datasets. various blocking techniques can be used to enhance the performance of er by dividing the records into blocks in multiple ways and only comparing records within the same block. in this paper, we propose an iterative blocking framework where the er results of blocks are reflected to subsequently processed blocks. blocks are now iteratively processed until no block contains any more matching records. compared to simple blocking, iterative blocking may achieve higher accuracy because reflecting the er results of blocks to other blocks may generate additional record matches. iterative blocking may also be more efficient because processing a block now saves the processing time for other blocks. we implement a scalable iterative blocking system and demonstrate that iterative blocking can be more accurate and efficient than blocking for large datasets. er is a well known problem that arises in many applications. an exhaustive er process involves comparing all the pairs of records, which can be very expensive for large datasets. for example, we might partition a set of people records according to the zip codes in address elds. first, when two records match and merge in one block, their composite may match with records in other blocks. unlike previous blocking techniques, there is an additional stage where newly created records of a block are distributed to other blocks. record name address emailjohn doe jdoe yahoojohn doe. foe jdoe yahoobobbie brown year# bob google bobbie brown year# bob google figure #: customer records criterion partitions by, sc zip code, st sc char of last name, t, figure #: multiple blocking motivating example: consider the four people records shown in figure #, that are to be resolved. suppose that recordsandmatchwitheachother because their names are the same, but do not match withbecause the strings di er too much. in figure #, the matching recordsandcan be compared because, although they have di erent zip codes, they have the same last name. iterative blocking can nd this match by distributing the newly created to the other blocks. our example has illustrated the two potential advantages of iterative blocking. the rst is improved accuracy, as each iteration may nd additional record matches. the blocks are iteratively processed until no blocks contain any more matching records. blocker can accommodate any core er algorithm that resolves records within a single block. we presentblocker algorithms for two scenarios: lego: an in memory algorithm that. we experimentally evaluate lego and duplo using actual comparison shopping data from yahoo shopping and hotel information data from yahoo travel. entity resolution is the problem of matching records that represent the same real world entity and then merging permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. sigmod, june july, year#, providence, rhode island, usa. for example, mailing lists may contain multiple entries representing the same physical address, but each record may be slightly di erent, eg, containing di erent spellings or missing some information. as a second example, two companies that merge may want to combine their customer records: for a given customer that dealt with the two companies they create a composite record that combines the known information. various blocking techniques have been proposed to make er scalable. blocking divides the data into blocks and only compares records within the same block, assuming that records in di erent blocks are unlikely to match. we then only need to compare the records with the same zip code. since a single blocking criterion may miss matches, several blocking criteria are typically used to ensure that all the likely matching records are compared, improving the accuracy of the result. although the previous works above focus on nding the best blocking criteria, most of them assume that all the blocks are processed separately one at a time. in many cases, however, it is useful to exploit an er result of a previously processed block. second, an er result of a block can be used to reduce the time of processing another block. that is, the same pair of records may occur in multiple blocks, so once the pair is compared in one block, we can avoid comparing it in other blocks. to address these two points, we propose an iterative blocking framework where the er result of a block is immediately re ected to other blocks. since the propagation of er results can generate new record matches in other blocks, the entire operation becomes iterative in the sense that we are processing blocks until we arrive at a state where we cannot nd any more matching records. ectively processing the blocks given a blocking criteria. we would like to merge records that actually refer to the same person. however, onceandare merged into a new record, the combination of the address and email information ofandmay lead us to discover a new match with, therefore yielding an initially unforeseen merge. notice that, in order to nd this new merge, we need to compare the merged result ofandwith all the other records again. in reality, our dataset can be very large, and it may not be feasible to compare all pairs of the dataset. hence, we divide the customer records in figure # into blocks. we start by dividing the records by their zip codes. as a result, we only need to compare customers that are in the same geographical region. in figure #, the rst blocking criterion sc uses zip codes to divide the records. recordsandhave the same zip codeand areassigned to theblock and recordsandare assigned to, whileis assigned to, since we may miss matches for people who have moved to several places with di erent zip codes, say we also divide the customer records according to the rst characters of their last names. hence, even if two records referring to the same person have di erent zip codes, we will have a better chance of comparing them because their last names might be similar. after processing all the blocks, the nal result of blocking is. although the blocking of figure # reduces the number of records to compare, it misses the iterative match between and. assuming that contains the zip codes of bothand, is then assigned to both blocks of sc in, canthenbe compared with, generating the record. thus, the iterative blocking framework helps nd more record matches compared to simple blocking. for example, once the recordsandare merged in, they do not haveto becompared in, while the blocks in figure # are too small to show any improvements in runtime, we will later demonstrate in our experiments that iterative blocking can actually run faster than blocking for large datasets when the runtime savings exceed the overhead of iterative blocking. intuitively, the more work we do for each block, the runtime savings for other blocks become signi cant. the second potential advantage is improved runtime performance. by using the er result of a previous block, we can reduce the time to process other blocks, by avoiding comparisons that were already made. unlike blocking, the er results of blocks are now re ected to subsequent blocks. duplo: a scalable disk based algorithm that processes blocks from the disk. our results show that iterative blocking can improve over blocking both in accuracy and runtime. we evaluate our algorithms using two di erent core er algorithms, demonstrating the generality of our iterative blocking framework.