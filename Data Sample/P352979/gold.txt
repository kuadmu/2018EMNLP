duplicate detection, originally de ned by newcombe et al and formalized by fellegi and sunter, has been extensively studied, particularly for relational databases. the authors explore the hierarchies between dimensional tables to relate co occurrences in the database. their framework consists of three main steps: candidate de nition, duplicate de nition, and duplicate detection. only more recently, solutions directed to more structured data models have emerged. the work developed in presents an algorithm for eliminating duplicates in a data warehouse. with this strategy, ananthakrishna et al show that the quality of duplicate detection can be. ectively improved for entities stored across multiple tables, connected by a foreign key. in, the authors propose a solution to the problem of integrating tree structured data extracted from the web. two object representations, eg, two hierarchical representations of person elements, are compared by transforming each into a vector of terms and using a variation of the cosine measure to evaluate their similarity. the hierarchical structure of object representations is mostly ignored, and a linear combination of weighted similarities is used to account for the relative importance of the di erent elds within the vectors. the authors show that this simple strategy manages to achieve high precision values in a collection of scienti. nevertheless, and because of its more general nature, their approach does not take advantage of the useful features existing in web data, such as the element structure or tag semantics. solutions that directly address the problem of detecting duplicates in xml data have more recently been proposed. works like, explore the native characteristics of the xml data model. therefore, aspects like the object structure along with element contents and semantics are considered in the process. to determine the degree of similarity, the content and structure of the documents is taken into consideration, by combining node paths with text contents similarity. whereas the rst two provide the data necessary for duplicate detection, the third component includes the actual algorithm, an extension to xml data of the work of ananthakrishna et al. the xmldup system uses a bayesian network model for xml duplicate detection. the model is used to capture not only the textual information contained by data objects but also the information provided by their structure. and serves as basis to our structure optimization solution. milano et al propose a distance measure between two xml object representations that is de ned based on the concept of overlays. this measure is then used to perform a pairwise comparison between all candidates. if the distance measure determines that two xml candidates are closer than a given threshold, the pair is classi ed as a duplicate. like the original snm, the idea is to avoid performing useless comparisons between objects by grouping together those that are more likely to be similar it is worth noting that all the proposals described above use the object structure in its original form. here, we follow a di erent approach, trying to discover an object structure that best re ects the properties of data. in the authors provide strategies to match xml documents. in order to improve the similarity results, element names are relabeled according to their semantic similarity. finally, sxnm is a duplicate detection method that adapts the relational sorted neighborhood approach to xml data. this means that they assume the original database schema already provides su cient information on how to compare two object representations.