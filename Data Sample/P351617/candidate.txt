the best documents for this query are probably not company websites of disk storage manufacturers, which may be returned by a general purpose work partially done while the author was an intern at nec research institute during summer year#. search engine, but rather hardware tutorials or glossary pages with de nitions or descriptions of hard disks. intuitively, by requiring the phrase used to, we can bias most search engines towards retrieving this answer as one of the top ranked documents. a signi cant number of natural language questions are submitted to search engines on the web every day, and an increasing number of search services on the web specifically target natural language questions. for example, askjeeves uses databases of pre compiled information, metasearching, and other proprietary methods, while services such as askme facilitate interaction with human experts. web search engines such as altavista and google typically treat a natural language question as a list of terms and retrieve documents similar to the original query. consider the question what is a hard disk. a good response might contain an answer such as: hard disk: one or more rigid magnetic disks rotating about a central axle with associated read write heads and electronics, used to store data. this de nition can be retrieved by transforming the original question into a query. we present a new system, tritus, that automatically learns to transform natural language questions into queries containing terms and phrases expected to appear in documents containing answers to the questions. we evaluate tritus on a set of questions chosen randomly from the excite query logs, and compare the quality of the documents retrieved by tritus with documents retrieved by other state of the art systems in a blind evaluation. however, documents with the best answers may contain few of the terms from the original query and be ranked low by the search engine. these queries could be answered more precisely if a search engine recognized them as questions. although interactive query reformulation has been actively studied in the laboratory, little is known about the actual behavior of web searchers who are offered terminological feedback along with their search results. we analyze log sessions for two groups of users interacting with variants of the altavista search engine a baseline group given no terminological feedback and a feedback group to whom twelve refinement terms are offered along with the search results. we examine uptake, refinement effectiveness, conditions of use, and refinement type preferences. although our measure of overall session success shows no difference between outcomes for the two groups, we find evidence that a subset of those users presented with terminological feedback do make effective use of it on a continuing basis. one of the most widely studied is the use of interactive relevance feedback, in which term suggestions are permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. one workaround for this problem has been to generate search suggestions from the top ranked documents regardless of their actual relevance, using linguistic and other heuristics to select and order the terms displayed back to the user. space is already at a premium on the typical textually cluttered search results page. for example, a word cluster based refinement tool offered on the altavista web site several years ago received scant user attention and was eventually scrapped. the incorporation of such refinement tools into full scale web search engines provides ir researchers an opportunity to study user behavior both in the large and in its natural state, free of the potential confounding influences of a laboratory setting or artificial tasks. in order to assist the user in bridging the gap between an internal information need and an expression of that need in the language of the target documents, many ir researchers have proposed the use of terminological feedback mechanisms to offer search term suggestions. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. generated based on user relevance judgements of previously retrieved documents while this approach has been shown to be effective in improving the recall and precision of subsequent searches, it has been difficult to implement in practice because of the reluctance of users to make the prerequisite document relevance judgements. although a number of such systems have shown promise in the laboratory setting, their incorporation into large scale web search engines has been slow, and for good reason. furthermore, while the majority of end users are familiar with the way search engines currently work, they donreally understand how or why they work and may find additional interface features clumsy or confusing. recently, several web search engines have begun offering short lists of search refinement suggestions in order to encourage the interactive narrowing of query result sets. among these is the altavista prisma tool, which allows a user to augment or replace the current query expression by clicking on feedback terms derived dynamically from an analysis of the top ranked search results. in this paper, we will report on the results of several log based studies of user interaction with altavistaprisma assisted search tool. we were interested in the degree and nature of user uptake of the feature, as well as the conditions and effectiveness of its use within information seeking sessions. an experiment controlling the display of the interface allowed us to compare how behavior differed between users provided with the feature and those operating without it. we begin with a brief overview of the functionality provided within the prisma refinement interface. we then enumerate a range of issues which our study was intended to address, relating our investigation to previous work on each topic. next we lay out the methodology we used to conduct experiments and to extract session information from activity logs. finally, we offer our conclusions and suggestions for future research. we examine three approaches to topical categorization of general web queries: matching against a list of manually labeled queries, supervised learning of classifiers, and mining of selectional preference rules from large unlabeled query logs. accurate topical categorization of user queries allows for increased effectiveness, efficiency, and revenue potential in general purpose web search systems. such categorization becomes critical if the system is to return results not just from a general web collection but from topic specific databases as well. maintaining sufficient categorization recall is very difficult as web queries are typically short, yielding few features per query. each approach has its advantages in tackling the web query classification recall problem, and combining the three techniques allows us to classify a substantially larger proportion of queries than any of the individual techniques. we examine the performance of each approach on a real web query stream and show that our combined method accurately classifies of queries, outperforming the recall of the best single approach by nearly, with a improvement in overall effectiveness. understanding the meaning of user queries is a problem at the heart of web search. successfully mapping incoming general user queries to topical categories, particularly those for which the search engine has domain specific knowledge, can bring improvements in both the efficiency and the effectiveness of general web search. these improvements can be realized in applications such as query routing, topic specific query reformulation, and targeted advertising for commercial search services. to fully realize these gains, a classification system is required that can automatically classify a large portion of the general query stream with a reasonable degree of accuracy. in the case of large scale web search, this task is particularly challenging due to the sheer size and dynamic nature of the webcontent, users, and query traffic, as well as the difficulty of accurately determining a userdesired task and information need from short web queries. lewis, abdur chowdhury, aleksandr kolcz america online, inc. davelewis daviddlewis com aol com we examine three methods for classifying general web queries: exact match against a large set of manually classified queries, a weighted automatic classifier trained using supervised learning, and a rule based automatic classifier produced by mining selectional preferences from hundreds of millions of unlabeled queries. the key contribution of this work is the development of a query classification framework that leverages the strengths of all three individual approaches to achieve the most effective classification possible. our combined approach outperforms each individual method, particularly improves recall, and allows us to effectively classify a much larger proportion of an operational web query stream. we propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in real time with the query volume of a commercial web search engine. we use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query. motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic. empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported. we believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience. commercial search engines do a remarkably good job in interpreting these short strings, but they are not omniscient. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. the queries can go a long way in improving the search results and the user experience. given such classi cations, one can directly use them to provide better search results as well as more focused ads. we crawl the web pages pointed by these urls, and classify these pages. finally, we use these result page classi cations to classify the original query. we found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries. in its year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising. one thing, however, has remained constant: people use very short queries. various studies estimate the average length of a search query between and words, which by all accounts can carry only a small amount of information. therefore, using additional external knowledge to augment permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. at the same time, better understanding of query meaning has the potential of boosting the economic underpinning of web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results. for instance, knowing that the query sd is about cameras while nc is about laptops can obviously lead to more focused advertisements even if no advertiser has speci cally bidden on these particular queries. in this study we present a methodology for query classi cation, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately nodes. the problem of query classi cation is extremely di cult owing to the brevity of queries. observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it. of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world. for instance, in the example above, sd brings pages about canon cameras, while nc brings pages about compaq laptops, hence to a human the intent is quite clear. search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge. following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation. to this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query. certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query. for the purpose of this study we rst dispatch the given query to a general web search engine, and collect a number of the highest scoring urls. our empirical evaluation con rms that using web search results in this manner yields substantial improvements in the accuracy of query classi cation. note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre classi ed using the normal text processing and indexing pipeline. thus, at run time we only need to run the voting procedure, without doing any crawling or classi cation. this additional overhead is minimal, and therefore the use of search results to improve query classi cation is entirely feasible in run time. another important aspect of our work lies in the choice of queries. the volume of queries in todaysearch engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times. while individual queries in this long tail are rare, together they account for a considerable mass of all searches. furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on line advertising searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on. however, the tail queries simply do not have enough occurrences to allow statistical learning on a per query basis. therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters. a natural choice for such aggregation is to classify the queries into a topical taxonomy. knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries. consequently, in this work we focus on the classi cation of rare queries, whose correct classi cation is likely to be particularly bene cial. early studies in query interpretation focused on query augmentation through external dictionaries. more recent studies also attempted to gather some additional knowledge from the web. however, these studies had a number of shortcomings, which we overcome in this paper. speci cally, earlier works in the eld used very small query classi cation taxonomies of only a few dozens of nodes, which do not allow ample speci city for online advertising. they also used a separate ancillary taxonomy for web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies. the main contributions of this paper are as follows. first, we build the query classi er directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simpli es taxonomy maintenance and development. the taxonomy used in this work is two orders of magnitude larger than that used in prior studies. the empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported. since our taxonomy is considerably larger, the classi cation problem we face is much more di cult, making the improvements we achieve particularly notable. we also report the results of a thorough empirical study of di erent voting schemes and di erent depths of knowledge. this result is in contrast with prior ndings in query classi cation, but is supported by research in mainstream text classi cation. the efficiency of the evaluation process can be improved significantly using dynamic pruning techniques with very little cost in effectiveness. we present an efficient query evaluation method based on a two level approach: at the first level, our method iterates in parallel over query term postings and identifies candidate documents using an approximate evaluation taking into account only partial information on term occurrences and no query independent factors; at the second level, promising candidates are fully evaluated and their exact scores are computed. the amount of pruning can be controlled by the user as a function of time allocated for query evaluation. experimentally, using the trec web track data, we have determined that our algorithm significantly reduces the total number of full evaluations by more than, almost without any loss in precision or recall. at the heart of our approach there is an efficient implementation of a new boolean construct called wand or weak and that might be of independent interest. we propose a two level approach as a method for decreasing runtime latency: rst, we run a fast, approximate evaluation on candidate documents, and then we do a full, slower evaluation limited to promising candidates. if a slight loss is tolerable, the reduction is for short queries, and for long queries. such a two level evaluation approach is commonly used in other application areas to enhance performance: databases sometimes use bloom lters to construct a preliminary join followed by an exact evaluation; speech recognition systems use a fast but approximate acoustic match to decrease the number of extensions to be analyzed by a detailed match; and program committee often use. rst rough cut to reduce the number of papers to be fully discussed in the committee. taat strategies are more commonly used in traditional ir systems. both taat and daat strategies can be optimized signi cantly by compromising on the requirement that all document scores are complete and accurate. optimization strategies have been studied extensively in the information retrieval literature. an important optimization technique for daat strategies is termed max score by turtle and flood. given a recall parameter, it operates by keeping track of the topscoring documents seen so far. turtle and flood demonstrated experimentally that in an environment where it is not possible to store intermediate scores in main memory, optimized daat strategies outperform optimized taat strategies. such queries might contain boolean operators, proximity operators, and others. as mentioned in, most of the context sensitive queries can be evaluated much more. our approach in this paper we describe a novel algorithm suited for daat strategies that evaluates queries using two levels of granularity. for safe optimization, the two level strategy makes no false negative errors and thus it is guaranteed to return the top documents in the correct order and with accurate scores. for an approximate optimization, dynamic pruning techniques are used such that fewer documents pass the preliminary evaluation step, that is, we allow some falsenegative errors at the risk of missing some candidate documents whose accurate scores would have placed them in the returned document set. this measure has the advantage of being independent of both the software and the hardware environment, as well as the quality of the implementation. we also measure query search time and it shows very high correlation with the full number of evaluations. our main results, described in section #, demonstrate that our approach signi cantly reduces the total number of full evaluations while precision is only slightly decreased. compared to the naive approach of fully evaluating every document that contains at least one of the query terms, our method achieves on average a reduction in the number of full evaluations for short queries, and a reduction for long queries, without any loss in precision or recall. prior work turtle and flood classify evaluation strategies into two main classes: term at a time strategies process query terms one by one and accumulate partial document scores as the contribution of each query term is computed. for a comprehensive overview of optimization techniques see turtle and flood. in contrast, kaszkiel and zobbel showed that for long queries, the max score strategy is much more costly than optimized taat strategies, due to the required sorting of term postings at each stage of evaluation; a process that heavily depends on the number of query terms. furthermore, as in the standard daat approach, our algorithm iterates in parallel over query term postings but the nature of the preliminary evaluation is such that it is possible to skip quickly over large portions of the posting lists. if the result of this fast and rough evaluation is above a certain threshold, varied dynamically during the execution, then a full evaluation is performed and the exact score is computed. our approach allows both safe optimization and approximate optimization. fast and precise text search engines are widely used in both enterprise and web environments. while the amount of searchable data is constantly increasing, users have come to expect sub second response time and accurate search results regardless of the complexity of the query and the size permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. thus, system runtime performance is an increasingly important concern. document at a time strategies evaluate the contributions of every query term with respect to a single document before moving to the next document. for small corpus sizes, implementations which use taat strategies are elegant and perform well. for large corpora, daat strategies have two advantages: daat implementations require a smaller run time memory footprint because a per document intermediate score does not need to be maintained, and they exploit io parallelism more. ectively by traversing postings lists on di erent disk drives simultaneously. for taat strategies, the basic idea behind such optimization techniques is to process query terms in some order that lets the system identify the topscoring documents without processing all query terms. evaluation of a particular document is terminated as soon as it is clear that this document will not place in the top. the results mentioned above are based on context free queries, ie, queries that can be evaluated term by term independently, ignoring context sensitive queries for which the relation among terms is crucial. for example, identifying whether two query terms are found in proximity to each other in a given document can easily be determined by verifying the occurrence of both terms together in that document. in contrast, a taat strategy must keep the occurrences of the rst term within the document in the intermediate results, in order to verify whether the second term satis es the proximity constraint. web search engines which must handle context sensitive queries over very large collections indeed employ daat strategies; see, for instance, that describes altavista, and that describes google. the algorithm iterates in parallel over query term postings and identi es candidate documents using a preliminary evaluation taking into account only partial information on term occurrences and no query independent factors. once a candidate document is identi ed, it is fully evaluated and its exact score is computed. for large systems the full evaluation is an expensive task that depends on query dependent factors such as term occurrences within the document, as well as query independent properties such as document length, topological score based on link analysis, etc. some of these factors might have to be retrieved via an io operation but even if all these properties can be computed. ciently, there is still a substantial cost to combine them for. therefore the intention of the two level process is to minimize the number of full evaluations as much as possible. the amount of pruning can be controlled by the user as a function of time allocated for query evaluation. ciency of the evaluation process can be improved signi cantly using dynamic pruning with very little cost in. experimental results our experiments were done on the juru system, a java search engine developed at the ibm research lab in haifa. juru was used to index the wt collection of the trec webtrack and we measured precision and. precision was measured by precision at and mean average precision. ciency of our approach is counting the number of full evaluations required to return a certain number of top results. thus, we consider the number of full evaluations measure a good proxy for wall clock time. for approximate optimization strategies we measure two parameters: first we measure the performance gain, as before. second, we measure the change in recall and precision by looking at the distance between the set of original results and the set of results produced by dynamic pruning. query expansion has long been suggested as an effective way to resolve the short query and word mismatching problems. a number of query expansion methods have been proposed in traditional information retrieval. however, these previous methods do not take into account the specific characteristics of web searching; in particular, of the availability of large amount of user interaction information recorded in the web query logs. in this study, we propose a new method for query expansion based on query logs. the central idea is to extract probabilistic correlations between query terms and document terms by analyzing query logs. these correlations are then used to select high quality expansion terms for new queries. the experimental results show that our log based probabilistic query expansion method can greatly improve the search performance and has several advantages over other existing methods. with the explosive growth of information on the world wide web, there is an acute need for search engine technology to help users exploit such an extremely valuable resource. despite the fact that keywords are not always good descriptors of contents, most existing search engines still rely solely on the keywords contained in the queries to search and rank relevant documents. this is one of the key reasons that affect the precision of the search engines. in many cases, the answers returned by search engines are not relevant to the user information need, although they do contain the same keywords as the query. the web is not a well organized information source where innumerous authors created and are creating their websites independently. therefore, the vocabularies of the authors vary greatly. on the other hand, users usually tend not to use the same terms appearing in the documents as search terms. this raises a fundamental problem of term mismatch in information retrieval. moreover, most words in natural language have inherent ambiguity. these reasons make it a rather difficult task for the web users to formulate queries with appropriate words. it is also generally observed that web users typically submit very short queries to search engines and the average length of web queries is less than two words. short queries usually lack sufficient words to cover useful search terms and thus negatively affect the performance of web search in terms of both precision and. to overcome the above problems, researchers have focused on using query expansion techniques to help users formulate a better query. query expansion involves adding new words and phrases to the existing search terms to generate an expanded query. however, previous query expansion methods have been limited in extracting expansion terms from a subset of documents, but have not exploited the accumulated information on user interactions. we believe that this latter is extremely useful for adapting a search engine to the users. in particular, we will be able to find out what queries have been used to retrieve what documents, and from that, to extract strong relationships between query terms and document terms and to use them in query expansion. in this paper, we suggest a new query expansion method based on the analysis of user logs. by exploiting correlations among terms in documents and user queries mined from user logs, our query expansion method can achieve significant improvements in retrieval effectiveness compared to current query expansion techniques. information retrieval algorithms leverage various collection statistics to improve performance. because these statistics are often computed on a relatively small evaluation corpus, we believe using larger, non evaluation corpora should improve performance. specifically, we advocate incorporating external corpora based on language modeling. we refer to this process as external expansion. when compared to traditional pseudo relevance feedback techniques, external expansion is more stable across topics and up to more effective in terms of mean average precision. our results show that using a high quality corpus that is comparable to the evaluation corpus can be as, if not more, effective than using the web. our results also show that external expansion outperforms simulated relevance feedback. in addition, we propose a method for predicting the extent to which external expansion will improve retrieval performance. our new measure demonstrates positive correlation with improvements in mean average precision. most information retrieval algorithms leverage collection statistics to improve performance. these statistics can be permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. global, as in document frequency, or adaptive as in pseudorelevance feedback. other algorithms use a more complicated analysis such as clustering, latent semantic indexing, or probabilistic aspect models. since these techniques are inherently statistical, we hypothesize that access to more data should improve performance even further. one method of introducing additional data is to gather a larger corpus of documents. we refer to this large, potentiallyunrelated corpus as the external collection; we refer to the evaluation corpus as the target collection. increasing corpus size has improved performance in language tasks such as question answering, machine translation, cross lingual information retrieval, and ad hoc information retrieval. this can be seen more generally as the problem of using unlabeled data to improve machine learning algorithms. as a special case of pattern classi cation, information retrieval has not received a thorough exploration of using external data. we propose incorporating information from external corpora using a language model technique for pseudo relevance feedback. language modeling provides a theoretically wellmotivated framework for incorporating this information in a relevance model. using this relevance model as an expanded query on the target collection, we demonstrate consistent improvements in performance across a variety of target collections. furthermore, our results show that using a high quality corpus that is comparable to the target corpus can be as, if not more,ective than using the web. we begin by describing our model in section #. in section #, we evaluate our model on a variety of topic sets and external corpora. in section #, we analyze our results in order to explain precisely why and when an external corpus is helpful. we conclude in section # by placing our work in the context of related work in information retrieval. web pages can be characterized according to their geographical locality. for example, a web page with general information about wildflowers could be considered a global page, likely to be of interest to a geographically broad audience. in contrast, a web page with listings on houses for sale in a specific city could be regarded as a local page, likely to be of interest only to an audience in a relatively narrow region. similarly, some search engine queries target global pages, while other queries are after local pages. for example, the best results for query are probably global pages about wildflowers such as the one discussed above. however, local pages that are relevant to, say, san francisco are likely to be good matches for a query that was issued by a san francisco resident or by somebody moving to that city. unfortunately, search engines do not analyze the geographical locality of queries and users, and hence often produce sub optimal results. thus query might return pages that discuss wildflowers in specific. states, while query might return pages with real estate listings for locations other than that of interest to the person who issued the query. deciding whether an unseen query should produce mostly local or global pages without placing this burden on the search engine users is an important and challenging problem, because queries are often ambiguous or underspecify the information they are after. in this paper, we address this problem by first defining how to categorize queries according to their geographical locality. we then introduce several alternatives for automatically and efficiently categorizing queries in our scheme, using a variety of state of the art machine learning tools. we report a thorough evaluation of our classifiers using a large sample of queries from a real web search engine, and conclude by discussing how our query categorization approach can help improve query result quality. web pages can be characterized according to their geographical locality. for example, a web page with general information about wild owers could be considered a global page, likely to be of interest to a geographically broad audience. in contrast, a web page with listings on houses for sale in a speci. city could be regarded as a local page, likely to be of interest only to an audience in a relatively narrow region. earlier research has addressed the problem of automatically computing the geographical scope of web resources. often search engine queries target global web pages, while other queries are after local pages. for example, the best results for query are probably global pages about wild owers discussing what types of climates wild owers grow in, where wild owers can be purchased, or what types of wild ower species exist. in contrast, local pages that are relevant to, say, san francisco are likely to be good matches for a query that was issued by a san francisco resident, or by somebody moving to san francisco, even if san francisco is not mentioned in the query. the userintent when submitting a query may not be always easy to determine, but if underspeci ed queries such as can be detected, they can be subsequently modi ed by adding the most likely target geographical location or by getting further user input to customize the results. unfortunately, search engines do not analyze the geographical locality of queries and users, and hence often produce sub optimal results, even if these results are on topic and reasonably popular or authoritative. thus query might return pages that discuss wild owers in speci. in fact, as of the writing of this paper, the rst results that google provides for this query include pages each of which discusses wild owers in only one. similarly, the top results that google returns for query include real estate pages for tuscany, united kingdom, and new zealand. these pages are likely to be irrelevant to, say, somebody interested in san francisco real estate who types such an underspeci ed query. deciding whether a query posed by a regular search engine user should produce mostly local or global pages is an important and challenging problem, because queries are often ambiguous or underspecify the information they are after, as in the examples above. by identifying that, say, query is likely after global information, a search engine could rank the results for this query so that statespeci. pages do not appear among the top matches. by identifying that, say, query is likely after local information, a search engine could lter out pages whose geographical locality is not appropriate for the user who issued the query. note that deciding which location is of interest to a user who wrote an underspeci ed query such as is an orthogonal, important issue that we do not address in this paper. our focus is on identifying that such a query is after local pages in nature, and should therefore be treated di erently by a search engine than queries that are after global pages. by knowing that a user query is after local information, a search engine might choose to privilege pages whose geographical locality coincides with that of the useror, alternatively, attempt to obtain further input from the user on what location is of interest. in this paper, we rst de ne how to categorize user queries according to their geographical locality. we then introduce several alternatives for automatically and ef ciently classifying queries according to their locality, using a variety of state of the art machine learning tools. we report a thorough evaluation of our classi ers using a large sample of queries from a real web search engine query log. finally, we discuss how our query categorization approach can help improve query result quality. contributions of this paper are as follows: a discussion on how to categorize user queries according to their geographical locality, based on a careful analysis of a large query log from the excite web site. a feature representation for queries; we derive the feature representation of a query from the results produced for the query by a web search engine such as google. a variety of automatic query classi cation strategies that use our feature representation for queries. a large scale experimental evaluation of our strategies over real search engine queries. preliminary query reformulation and page re ranking strategies that exploit our query classi cation techniques to improve query result quality. in this paper, we present a general solution for the kdd cup year# problem. it uses the internet as source of knowledge and extends it to categorize very short documents with reasonable accuracy. our approach consists of three main parts: a central knowledge filter ii. our solution obtained creativity and precision runner up awards at the competition. the main idea of ferrety algorithm can be generalized for mapping one taxonomy to another if training documents are available. in an ad hoc retrieval task, the query is usually short and the user expects to find the relevant documents in the first several result pages. we explored the possibilities of using wikipedia articles as an external corpus to expand ad hoc queries. results show promising improvements over measures that emphasize on weak queries. in web retrieval tasks, the number of terms in a query is usually small. according to, if the terms cannot provide enough information of the userneed, the retrieval result may be poor. also, the relevant documents are likely to be scattered along the retrieval list. in this case, the user may give up after inspecting the rst one or two result pages without nding a relevant document. pseudo relevance feedback is a well known method for improving retrieval. however, it is based on the assumption that top retrieved documents are relevant, and thus may actually harm performance possibly when the initial retrievaltop ranked documents are irrelevant. some previous works have been done to address the issue of weak queries in ad hoc retrieval. web assistance and data fusion method probe a web search engine to form new queries, and then combine the corresponding retrieval lists. our experiments, however, use a local repository of wikipedia articles as external corpus. new queries are formed by analyzing wikipedia articles and a second retrieval on the target corpus is then performed. the task of the kdd cup year# competition was to classify, internet user search queries into predefined categories. this task is easy to understand, but the lack of straightforward training set, subjective user intents of queries, poor information in short queries, and high noise level make the task very challenge in this paper, we summarize the competition task, the evaluation method, and the results of the competition. here we only highlight some key techniques used in submitted solutions. at the end, we also share the results of a survey conducted with this year cup participants. to facilitate research in this area, the task description, data, answer set, and related information of this kdd cup are published at the kdd cup year# web site: http: www acm org sigs sigkdd kdd kddcup html. the kdd cup year# competition was held in conjunction with the eleventh acm sigkdd international conference on knowledge discovery and data mining. the technical details of the solutions from the three award winning teams are available in their papers separately in this issue of sigkdd explorations. given the exponential growth of informationavailability in electronic form, search becomes one of the most important and effective approaches to finding correct relevant information to serve our needs. a user can type in key words in a search engine to find out where to buy a product and whether a certain price is a good price. a user can also find travel attractions to fit his her interests. if a user is interested in certain medicine, he she can find plenty related information including its usage and potential side effects. researchers can easily find the latest development of a research topic. to plan a hiking trip for a coming weekend, one can find the weather forecast simply through a search. these are just a few examples of how search can help a userdaily life. although researchers and industry practitioners have achieved tremendous success in developing smart search engines, we are zijian zheng was at amazon com during the time of kdd cup year#. still facing many great challenges as current search engines are not very accurate. the difference is still quite big between what search engines can do and what we expect them to do. it is not uncommon that a search engine returns irrelevant, misleading or, incorrect results after you type in a query. in another time, the relevance results are returned but down to the bottom of a long result list. due to the nature that huge amount data is available from each search engine and many problems of search can be turned into learning or modeling problems, there is a great potential for data mining techniques to contribute to the success of search. since late, researchers and practitioners have been studying search query data, trying to find search patterns, understanding search user intents, and providing personalized search. a survey on search related research is available. the other side of search being a difficult problem is that the information contained inside the data is often incomplete, fuzzy, and indirect. all of these present big challenges to the data mining community. in kdd cup year#, we presented one challenge problem: search query categorization. manning and scht√ºze discusses general methodologies and applications of text categorization. most work in this area has been focused on categorizing web pages or longer text or corpus. however, search query classification is very different in the sense that queries are usually very short on the one hand, and with implicit and subjective user intents on the other hand. therefore, how to automatically understand user search intents given the search queries would be very interesting to ir and text mining researchers. in this report, we first describe the competition task presented to the participants in section #, including a discussion on why this task is challenging. in section #, we present the evaluation method. then, we highlight the interesting techniques from the submissions in section #, and analyze the overall results as a whole in section #. computer search including internet search has become a part of many peopledaily life and work. the intents of search engine users are highly subjective. the detailed presentations of techniques from the three winning teams are available as three separate papers in this issue of sigkdd explorations. text classification and categorization is a well known topic in information retrieval and text mining fields. many applications of supervised learning require good generalization from limited labeled data. in the bayesian setting, we can try to achieve this goal by using an informative prior over the parameters, one that encodes useful domain knowledge. focusing on logistic regression, we present an algorithm for automatically constructing a multivariate gaussian prior with a full covariance matrix for a given supervised learning task. this prior relaxes a commonly used but overly simplistic independence assumption, and allows parameters to be dependent. the algorithm uses other similar learning problems to estimate the covariance of pairs of individual parameters. we then use a semidefinite program to combine these estimates and learn a good prior for the current learning task. we apply our methods to binary text classification, and demonstrate a to test error reduction over a commonly used prior. classical supervised learning algorithms nd good classi ers for a given learning task using labeled inputoutput pairs. when labeled data is limited and expensive to obtain, an attractive alternative is to use other data sources to improve performance. for example, semi supervised learning uses unlabeled data for the given learning task. in this paper, we use transfer learning to improve performance on the learning task at hand. transfer learning utilizes labeled data from other similar learning tasks. it is inspired by the observation that humans appearing in proceedings of the rd international conference on machine learning, pittsburgh, pa, year#. do not receive tasks in isolation, but instead receive a sequence of learning tasks over their lifetimes. it appears intuitive that learning a sequence of related tasks should be easier than learning each of those tasks in isolation. for example, the visual system might nd it easier to recognize a guava if it already knows how to recognize apples and oranges; it might be easier to learn french if one already knows english and latin. humans can probably discover some underlying structure in each of these domains, and can thus learn new but similar tasks quickly and easily. with this inspiration, we present a transfer learning algorithm that constructs an informative bayesian prior for a given learning task. the prior enco des useful domain knowledge by capturing underlying dependencies between the parameters. the next section gives a brief overview of our approach. we assume that we have access to the text of the web page, the keywords declared by an advertiser, and a text associated with the advertiser business. further, a more sophisticated impedance coupling strategy, which expands the text of the web page to reduce vocabulary impedance with regard to an advertisement, can yield extra gains in average precision of. in this work, we study the problem of associating ads with a web page, referred to as content targeted advertising, from a computer science perspective. using no other information and operating in fully automatic fashion, we propose ten strategies for solving the problem and evaluate their effectiveness. our methods indicate that a matching strategy that takes into account the semantics of the problem can yield gains in average precision figures of compared to a trivial vector based strategy. they suggest that great accuracy in content targeted advertising can be attained with appropriate algorithms. the current boom of the web is associated with the revenues originated from on line advertising. while search based advertising is dominant, the association of ads with a web page is becoming increasingly important. ective strategies for on line advertising were required. as a consequence, many companies intensi ed the adoption of intrusive techniques for gathering information of users mostly without their consent. this raised privacy issues which stimulated the research for less invasive measures. more recently, internet information gatekeepers as, for example, search engines, recommender systems, and comparison shopping services, have employed what is called paid placement strategies. in such methods, an advertiser company is given prominent positioning in advertisement lists in return for a placement fee. amongst these methods, the most popular one is a non intrusive technique called keyword targeted marketing. in this technique, keywords extracted from the usersearch query are matched against keywords associated with ads provided by advertisers. the top ranked ads are displayed in the search result page together with the answers for the user query. the success of keyword targeted marketing has motivated information gatekeepers to. it is important to notice that paid placement advertising strategies imply some risks to information gatekeepers. for instance, there is the possibility of a negative impact on their credibility which, at long term, can demise their market share. this makes investments in the quality of ad recommendation systems even more important to minimize the possibility of exhibiting ads unrelated to the userinterests. by investing in their ad systems, information gatekeepers are investing in the maintenance of their credibility and in the reinforcement of a positive user attitude towards the advertisers and their ads. we propose new strategies for associating ads with a web page. five of these strategies are referred to as matching strategies. further, all our strategies rely on information that is already available to information gatekeepers that operate keyword targeted advertising systems. thus, no other data from the advertiser is required. our results indicate that a strategy that matches the ad plus its keywords to a web page, requiring the keywords to appear in the web page, provides improvements in average precision gures of roughly relative to a strategy that simply matches the ads to the web page. such strategy, which we call aak, is then taken as our baseline. they are based on the idea of expanding the ad and the web page with new terms to reduce the vocabulary impedance between their texts. our results indicate that it is possible to generate extra improvements in average precision gures of roughly relative to the aak strategy. in section #, we describe our experimental methodology and datasets and discuss our results. the emergence of the internet has opened up new marketing opportunities. for that, it was necessary to take into account short term and long term interests of the users related to their information needs. a ranking of the ads, which also takes into consideration the amount that each advertiser is willing to pay, is computed. for example, as shown in figure #, relevant ads could be shown to users directly in the pages of information portals. the motivation is to take advantage of the users immediate information interests at browsing time. in this case, instead of dealing with users keywords, we have to use the contents of a web page to decide which ads to display. at the bottom slice, we can see advertisements picked for this page by googlecontent based advertising system, adsense. further, that can translate into higher clickthrough rates that lead to an increase in revenues for information gatekeepers and advertisers, with gains to all parts. in this work, we focus on the problem of content targeted advertising. five other strategies, which we here introduce, are referred to as impedance coupling strategies. they are based on the idea of expanding the web page with new terms to facilitate the task of matching ads and web pages. this is motivated by the observation that there is frequently a mismatch between the vocabulary of a web page and the vocabulary of an advertisement. we say that there is a vocabulary impedance problem and that our technique provides a positive. ect of impedance coupling by reducing the vocabulary impedance. using a sample of a real case database with over, ads and web pages selected for testing, we evaluate our ad recommendation strategies. following we evaluate the ve impedance coupling strategies. in section #, we introduce ve matching strategies to solve content targeted advertising. in fact, a company has now the possibility of showing its advertisements to millions of people at a low cost. during the, many companies invested heavily on advertising in the internet with apparently no concerns about their investment return. this situation radically changed in the following decade when the failure of many web companies led to a dropping in supply of cheap venture capital and a considerable reduction in on line advertising investments. er their advertisement services in di erent contexts. the problem of matching ads to a web page that is browsed, which we also refer to as content targeted advertising, is di erent from that of keyword marketing. figure #: example of content based advertising in the page of a newspaper. the middle slice of the page shows the beginning of an article about the launch of a dvd movie. they are based on the idea of matching the text of the web page directly to the text of the ads and its associated keywords. they match ads to a web page using a standard vector model and provide what we may call trivial solutions. in section #, we present our impedance coupling strategies. determining the similarity of short text snippets, such as search queries, works poorly with traditional document similarity measures, since there are often few, if any, terms in common between two short text snippets. we address this problem by introducing a novel method for measuring the similarity between short text snippets by leveraging web search results to provide greater context for the short texts. in this paper, we define such a similarity kernel function, mathematically analyze some of its properties, and provide examples of its efficacy. we also show the use of this kernel function in a large scale system for suggesting related queries to search engine users. in analyzing text, there are many situations in which we wish to determine how similar two short text snippets are. for example, there may be di erent ways to describe some concept or individual, such as united nations secretary general and ko. annan, and we would like to determine that there is a high degree of semantic similarity between these two text snippets. similarly, the snippets ai and arti cial intelligence are very similar with regard to their meaning, even though they may not share any actual terms in common. directly applying traditional document similarity measures, such as the widely used cosine coe cient, to copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. such short text snippets often produces inadequate results, however. indeed, in both the examples given previously, applying the cosine would yield a similarity of since each given text pair contains no common terms. even in cases where two snippets may share terms, they may be using the term in di erent contexts. consider the snippets graphical models and graphical interface. the rst uses graphical in reference to graph structures whereas the second uses the term to refer to graphic displays. thus, while the cosine score between these two snippets would be due to the shared lexical term graphical, at a semantic level the use of this shared term is not truly an indication of similarity between the snippets. to address this problem, we would like to have a method for measuring the similarity between such short text snippets that captures more of the semantic context of the snippets rather than simply measuring their term wise similarity. to help us achieve this goal, we can leverage the large volume of documents on the web to determine greater context for a short text snippet. by examining documents that contain the text snippet terms we can discover other contextual terms that help to provide a greater context for the original snippet and potentially resolve ambiguity in the use of terms with multiple meanings. our approach to this problem is relatively simple, but surprisingly quite powerful. we simply treat each snippet as a query to a web search engine in order to nd a number of documents that contain the terms in the original snippets. we then use these returned documents to create a context vector for the original snippet, where such a context vector contains many words that tend to occur in context with the original snippet terms. such context vectors can now be much more robustly compared with a measure such as the cosine to determine the similarity between the original text snippets. furthermore, since the cosine is a valid kernel, using this function in conjunction with the generated context vectors makes this similarity function applicable in any kernel based machine learning algorithm where text data is being processed. while there are many cases where getting a robust measure of similarity between short texts is important, one particularly useful application in the context of search is to suggest related queries to a user. in such an application, a user who issues a query to a search engine may nd it helpful to be provided with a list of semantically related queries that he or she may consider to further explore the related information space. by employing our short text similarity kernel, we could match the userinitial query against a large repository of existing user queries to determine other similar queries to suggest to the user. thus, the results of the similarity function can be directly employed in an end user application. the approach we take in constructing our similarity function has relations to previous work in both the information retrieval and machine learning communities. we explore these relations and put our work in the context of previous research in section #. we then formally de ne our similarity function in section # and present initial examples of its use in section #. this is followed by a mathematical analysis of the similarity function in section #. section # presents a system for related query suggestion using our similarity function, and an empirical evaluation of this system is given in section #. finally, in section # we provide some conclusions and directions for future work. this survey discusses the main approaches to text categorization that fall within the machine learning paradigm. the automated categorization of texts into predefined categories has witnessed a booming interest in the last years, due to the increased availability of documents in digital form and the ensuing need to organize them. in the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. the advantages of this approach over the knowledge engineering approach are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. we will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation. to copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior speci. tc enjoys quite a rich literature now, but this is still fairly scattered although two international journals have devoted special issues to a fully searchable bibliography on tc created and maintained by this author is available at http: liinwww ira uka de bibliography ai automated text. in the last years content based document management tasks have gained a prominent status in the information systems eld, due to the increased availability of documents in digital form and the ensuing need to access them in exible ways. text categorization, the activity of labeling natural language texts with thematic categories from a prede ned set, is one such task. tc dates back to the early, but only in the early did it become a major sub eld of the information systems discipline, thanks to increased applicative interest and to the availability of more powerful hardware. tc is now being applied in many contexts, ranging from document indexing based on a controlled vocabulary, to document ltering, automated metadata generation, word sense disambiguation, population of authoraddress: istituto di elaborazione dell informazione, consiglio nazionale delle ricerche, via. moruzzi, pisa, italy; mail: fabrizio iei pi cnr it. permission to make digital hard copy of part or all of this work for personal or classroom use is granted without fee provided that the copies are not made or distributed for pro. or commercial advantage, the copyright notice, the title of the publication, and its date appear, and notice is given that copying is by permission of the acm, inc. permission and or a fee acm hierarchical catalogues of web resources, and in general any application requiring document organization or selective and adaptive document dispatching. until the late the most popular approach to tc, at least in the operational community, was a knowledge engineering one, consisting in manually de ning a set of rules encoding expert knowledge on how to classify documents under the given categories. in the this approach has increasingly lost popularity in favor of the machine learning paradigm, according to which a general inductive process automatically builds an automatic text classi er by learning, from a set of preclassi ed documents, the characteristics of the categories of interest. the advantages of this approach are an accuracy comparable to that achieved by human experts, and a considerable savings in terms of expert labor power, since no intervention from either knowledge engineers or domain experts is needed for the construction of the classi er or for its porting to a different set of categories. it is the ml approach to tc that this paper concentrates on. current day tc is thus a discipline at the crossroads of ml and ir, and as such it shares a number of characteristics with other tasks such as information knowledge extraction from texts and text mining. there is still considerable debate on where the exact border between these disciplines lies, and the terminology is still evolving. text mining is increasingly being used to denote all the tasks that, by analyzing large quantities of text and detecting usage patterns, try to extract probably useful information. according to this view, tc is an instance of text mining. this topic, there are no systematic treatments of the subject: there are neither textbooks nor journals entirely devoted to tc yet, and manning and sch utze is the only chapter length treatment of the subject. as a note, we should warn the reader that the term automatic text classi cation has sometimes been used in the literature to mean things quite different from the ones discussed here. in section # we formally de ne tc and its various subcases, and in section # we review its most important applications. section # describes the main ideas underlying the ml approach to classi cation. our discussion of text classi cation starts in section # by introducing text indexing, that is, the transformation of textual documents into a form that can be interpreted by a classi er building algorithm and by the classi er eventually built by it. section # tackles the inductive construction of a text classi er from a training set of preclassi ed documents. section # discusses the evaluation of text classi ers. section # concludes, discussing open issues and possible avenues of further research for tc. in this paper, we describe our ensemble search based approach, ust, for the query classification task for the kddcup year#. there are two aspects to the key difficulties of this problem: one is that the meaning of the queries and the semantics of the predefined categories are hard to determine. the other is that there are no training data for this classification problem. we apply a two phase framework to tackle the above difficulties. phasecorresponds to the training phase of machine learning research and phase ii corresponds to testing phase. in phase, two kinds of classifiers are developed as the base classifiers. one is synonym based and the other is statistics based. in the first stage, the queries are enriched such that for each query, its related web pages together with their category information are collected through the use of search engines. in the second stage, the enriched queries are classified through the base classifiers trained in phase. based on the classification results obtained by the base classifiers, two ensemble classifiers based on two different strategies are proposed. the experimental results on the validation dataset help confirm our conjectures on the performance of thec ust system. in addition, the evaluation results given by the kddcup year# organizer confirm the effectiveness of our proposed approaches. the best value of our two solutions is higher than the best of all other participants solutions. the average value of our two submitted solutions is higher than the average value from all other submitted solutions. historically, search engine technologies and automatic text classification techniques have progressed hand in hand. ever since the early papers by the pioneers, people have recognized the possibility of conducting web search through classification, and vice versa. the kddcup year# competition made this connection even stronger; the task being to automatically classify, of the queries on a web search engine into a set of predetermined categories provided by the organizers. this task deviates from the traditional machine learning and text classification formulation in the following aspects: there are no training data available. it is part of the task to collect the training data. in fact, the manually annotated web pages organized into topical directories could serve as the training data. however, different parts of the web may provide training data of different quality. in contrast, text classification in machine learning and information retrieval has always included the training data as part of the input. how to make the best use of the web, including its directory resources and search tools, provides a fresh perspective. the data are noisy and provide poor information. many words have multiple meanings and belong to several categories. for example, office can mean the working place as well as a kind of software. in contrast, several queries may in fact mean the same thing, such as mainboard and planar board. the target categories suffer a shortage of semantic meanings. therefore, simple exact matches between queries and category names would be certain to fail. despite these difficulties, we are also encouraged by the possibility of answering some exciting questions through completing this challenging task. one innovative aspect of our solution is the use of an ensemble of search engines to compose the classifiers, and an ensemble of classifiers to classify the massive input queries. the ensemble of search engines and the final integration of the search result showed that with moderate validation data, we can achieve a high level of performance when we appropriately combine the query categorization results. the performance of search engines crucially depends on their ability to capture the meaning of a query most likely intended by the user. we study the problem of mapping a search engine query to those nodes of a given subject taxonomy that characterize its most likely meanings. we describe the architecture of a classification system that uses a web directory to identify the subject context that the query terms are frequently used in. based on its performance on the classification of, example queries recorded from msn search, the system received the runner up award for query categorization performance of the kdd cup year#. techniques for automatic query expansion have been extensively studied in information research as a means of addressing the word mismatch between queries and documents. these techniques can be categorized as either global or local. while global techniques rely on analysis of a whole collection to discover word relationships, local techniques emphasize analysis of the top ranked documents retrieved for a query. while local techniques have shown to be more effective that global techniques in general, existing local techniques are not robust and can seriously hurt retrieved when few of the retrieval documents are relevant. we propose a new technique, called local context analysis, which selects expansion terms based on cooccurrence with the query terms within the top ranked documents. experiments on a number of collections, both english and non english, show that local context analysis offers more effective and consistent retrieval results. a fundamental problem in information retrieval is word mismatch, which refers to the phenomenon that the users of ir systems often use different words to describe the concepts in their queries than the authors use to describe the same concepts in their documents. word mismatch is a serious problem, as observed by furnas et al in a more general context. in their experiments, two people use the same term to describe an object less than of the time. the problem is more severe for short casual queries than for long elaborate queries because as queries get longer, there is more chance of some important words cooccurring in the query and the relevant documents. unfortunately, short queries are becoming increasingly common in retrieval applications, especially with the advent of the world wide web. addressing the word mismatch problem has become an increasingly important research topic in ir. in this article, we will discuss techniques that address the word mismatch problem through automatic query expansion. automatic query expansion techniques have a significant advantage over manual techniques such as relevance feedback and manual thesauri because they require no effort on the part of the user. existing techniques for automatic query expansion can be categorized as either global or local. a global technique requires some corpuswide statistics that take a considerable amount of computer resources to compute, such as the cooccurrence data about all possible pairs of terms in a corpus. the source of expansion terms is the whole corpus. a local technique processes a small number of top ranked documents retrieved for a query to expand that query. a local technique may use some global statistics such as the document frequency of a term, but such statistics must be cheap to obtain. the source of expansion terms is the set of top ranked documents. one of the earliest global techniques is term clustering, which groups words into clusters based on their cooccurrences and uses the clusters for query expansion. other well known global techniques include latent semantic indexing, similarity thesauri, and phrasefinder. generally, global techniques did not show consistent positive retrieval results until better strategies for term selection were introduced in recent years. global techniques typically need the cooccurrence information for every pair of terms. this is a computationally demanding task for large collections. local techniques expand a query based on the information in the set of top ranked documents retrieved for the query. the simplest local technique is local feedback, which assumes the top retrieved documents are relevant and uses standard relevance feedback procedures for query expansion. a similar and earlier technique was proposed in croft and harper, where information from the top ranked documents is used to reestimate the probabilities of query terms in the relevant set for a query. the terms chosen by local feedback for query expansion are typically the most frequent terms from the top ranked documents. recent trec results show that local feedback can significantly improve retrieval effectiveness. experiments have also shown that local feedback may not be a robust technique. it can seriously degrade retrieval performance if few of the top ranked documents retrieved for the original query are relevant, because in this case the expansion terms will be selected mostly from nonrelevant documents. in this article, we propose a new query expansion technique, called local context analysis. although local context analysis is a local technique, it employs cooccurrence analysis, a primary tool for global techniques, for query expansion. the expansion features, called concepts, are extracted from the top ranked documents. concepts can be as simple as single terms and pairs of terms. more sophisticated concepts are nouns and noun phrases. local context analysis ranks the concepts according to their cooccurrence within the top ranked documents with the query terms and uses the top ranked concepts for query expansion. experimental results show that local context analysis produces more effective and more robust query expansion than existing techniques. the remainder of the article is organized as follows: sections and review existing techniques and point out the problems. section # discusses local context analysis in detail. section # outlines the experimental methodology and describes the test collections. section # discusses other applications of local context analysis in ir. section # draws conclusions and points out future work. a large and growing number of web pages display contextual advertising based on keywords automatically extracted from the text of the page, and this is a substantial source of revenue supporting the web today. despite the importance of this area, little formal, published research exists. we describe a system that learns how to extract keywords from web pages for advertisement targeting. the system uses a number of features, such as term frequency of each potential keyword, inverse document frequency, presence in meta data, and how often the term occurs in search query logs. the system is trained with a set of example pages that have been hand labeled with relevant keywords. based on this training, it can then extract new keywords from previously unseen pages. accuracy is substantially better than several baseline systems. content targeted advertising systems, such as googleadsense program, and yahoocontextual match product, are becoming an increasingly important part of the funding for free web services. these programs automatically nd relevant keywords on a web page, and then display advertisements based on those keywords. in this paper, we systematically analyze techniques for determining which keywords are relevant. we demonstrate that a learning based technique using tf idf features, web page meta data, and, most surprisingly, information from query log les, substantially outperforms competing methods, and even approaches human levels of performance by at least one measure. typical content targeted advertising systems analyze a web page, such as a blog, a news page, or another source copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. of information, to nd prominent keywords on that page. these keywords are then sent to an advertising system, which matches the keywords against a database of ads. advertising appropriate to the keyword is displayed to the user. typically, if a user clicks on the ad, the advertiser is charged a fee, most of which is given to the web page owner, with a portion kept by the advertising service. picking appropriate keywords helps users in at least two ways. first, choosing appropriate keywords can lead to users seeing ads for products or services they would be interested in purchasing. second, the better targeted the advertising, the more revenue that is earned by the web page provider, and thus the more interesting the applications that can be supported. for instance, free blogging services and free email accounts with large amounts of storage are both enabled by good advertising systems from the perspective of the advertiser, it is even more important to pick good keywords. for most areas of research, such as speech recognition, a improvement leads to better products, but the increase in revenue is usually much smaller. for keyword selection, however, a improvement might actually lead to nearly a higher clickthrough rate, directly increasing potential revenue and pro t. in this paper, we systematically investigated several different aspects of keyword extraction. first, we compared looking at each occurrence of a word or phrase in a document separately, versus combining all of our information about the word or phrase. we also compared approaches that look at the word or phrase monolithically to approaches that decompose a phrase into separate words. second, we examined a wide variety of information sources, analyzing which sources were most helpful. these included various meta tags, title information, and even the words in the url of the page. one surprisingly useful source of information was query frequency information from msn search query logs. that is, knowing the overall query frequency of a particular word or phrase on a page was helpful in determining if that word or phrase was relevant to that page. we compared these approaches to several di erent baseline approaches, including a traditional tf idf model; a model using tf and idf features but with learned weights; and the kea system. kea is also a machine learning system, but with a simpler learning mechanism and fewer features. as we will show, our system is substantially better than any of these baseline systems. we also compared our system to the maximum achievable given the human labeling, and found that on one measure, our system was in the same range as human levels of performance.