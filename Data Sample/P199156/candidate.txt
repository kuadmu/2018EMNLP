in recent years there has been an explosive growth of digital content in the form of news feeds, videos, and original content, on online platforms such as blogs and social networks. we consider the problem of curating this vast catalogue of content such that aggregators or publishers can offer readers content that is of interest to them, with minimal spam. under a game theoretic model we obtain several results on the optimal content selection and on the efficiency of distributed curation. in recent years there has been an explosive growth of digital content in many forms, such as blogs, news feeds, original content and content propagated through online social networks. there is thus a need for recommendation and ltering of content, especially content that arrives in streams. online social networks such as google, twitter, and facebook are rapidly turning into social reading or social content sharing networks, whereby each user shares content such as news, videos, etc, with her friends or followers. by resharing content, each user acts to lter items she receives or produces such that it is of interest to her followers. indeed online aggregators serve to perform a similar function of content curation where content is chosen such that it matches with subscribers interest. content curation can be quite complex when users have di ering interests and a limited budget of attention. by the latter we mean that users have limited time to sift through contents in their stream. the curation function by the aggregator then must take this into account by choosing a relatively small set of contents that followers would nd most interesting. we address the scenario of content curation where a set of aggregators, or publishers, aims to optimize the set of content to publish such that the followers receive contents of interest to them. we consider a very general setting with a set of aggregators or publishers, a set of followers or readers, and a set of contents or items of interest to the followers. each reader has an intrinsic interest in a given item, represented through a quanti ed value. the problem then is: how should the publishers choose a limited set of items to publish such that readers receive contents of maximal value as we will show, the problem of centralized optimization of this set of items is np complete, but admits approximation algorithms. address the distributed content curation problem through a game theoretic model where publishers are strategic agents with the aim of maximizing their utility, expressed in terms of feedback or incentives they receive from readers. in our analysis we aim to answer the following questions: does distributed content curation based on sel sh behavior of publishers converge to a pure equilibrium how. cient is decentralized content curation we de ne. ciency in terms of the price of anarchy, the ratio of social welfare under centralized curation to the sum of utilities achieved through distributed means. what is the rate of convergence to equilibria how do the results change when readers are also strategic our contributions. to the best of our knowledge our model is the rst to address the problem of distributed content curation as a game. we show that with a certain form of feedback, or incentive, the distributed content curation game is. in particular, we show that the game has a pure nash equilibrium and that best response dynamics converge to a pure ne. further, we show that the price of anarchy is bounded by. we study the speed of convergence and show that in a polynomial number of best response moves, players converge to a solution with an approximation factor of. in the case of centralized content curation, we show that the problem is np complete. we further show that there exists a approximation algorithm for this problem. lastly we show that when readers are strategic in a certain sense, the price of anarchy is bounded by. in the next section we present our model of content curation. we then analyze distributed content curation as a game in section # and the centralized case in section #. we consider strategic readers in section # and conclude in section #. website traffic varies through time in consistent and predictable ways, with highest traffic in the middle of the day. when providing media content to visitors, it is important to present repeat visitors with new content so that they keep coming back. in this paper we present an algorithm to balance the need to keep a website fresh with new content with the desire to present the best content to the most visitors at times of peak traffic. we formulate this as the media scheduling problem, where we attempt to maximize total clicks, given the overall traffic pattern and the time varying clickthrough rates of available media content. we present an efficient algorithm to perform this scheduling under certain conditions and apply this algorithm to real data obtained from server logs, showing evidence of significant improvements in traffic from our algorithmic schedules. finally, we analyze the click data, presenting models for why and how the clickthrough rate for new content declines as it ages. many websites have featured items placed prominently on their web pages. news sites have featured news stories, content sharing sites have featured media, and on line stores have featured products. the exact business goals of these sites vary, but in general these items are placed in featured locations with the expectation that a large fraction of visitors will examine them. for a news site, the featured article is typically a story that many people will be interested in, and one metric of success that can be used by a news site when evaluating its selection of fea part of the work done while the author was visiting yahoo research. supported in part by nsf grants ccf, cns, bcs, and iis, and by funding from google, yahoo, and the john. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. tured articles is the total number of clicks. a news site that does a good job of delivering the news that is interesting to most people will have a large number of overall clicks. similarly, a contentsharing site would like to present high quality content in featured spots. an on line store might have slightly different goals in selecting featured products: they might select high margin rather than popular items, for instance. in all of these cases, the website operator sees some value in having the featured items appear prominently and receiving user attention. some utility is gained for each impression of the featured item. the visitor may think more highly of the website the featured item is placed on, or be inspired to follow a link to another page, or even buy a product. in this work, we will consider the value of an impression to be represented by the probability of a user clicking on that featured item, but other tasks, such as maximizing total sales or total pro. can be considered within our framework as well. there is thus an underlying content scheduling problem that is central to the choice of featured items on all such sites. however, this scheduling problem has remained essentially implicit and unexamined in the literature, despite its role in determining the clickthrough rates for content that is in some cases viewed by hundreds of millions of individuals. in this paper we provide a formulation of this problem in terms of a few underlying parameters that can be reliably inferred from a sitelog data; and by formulating the problem precisely, we are able to develop algorithms that improve signi cantly on current practice. while the problem in its most general form is intractable, we provide ef cient algorithms for a special case that closely approximates data from real user traf c. we then evaluate our algorithm on trace data from the front page of yahoo, showing how to improve the estimated clickthrough rate on featured articles by over relative to the human intensive strategies that are currently employed. we now give an overview of the problem, before formally specifying it in the next section. the operator of a website has a pool of potential items that can be featured on the front page over the course of a day. in the nal section we discuss some interesting directions for future research based on adapting our model to this setting. ulation viewing it will increasingly consist of repeat visitors who have seen it linked from the front page already. thus, as we show in section #, user interest in a featured item can be characterized by two parameters: a peak interest level, which is the probability of a user clicking on the item when it is rst presented; and a decay function, which is the item speci. rate over time at which this click probability decreases while the item is being featured. moreover, these parameters can be quickly estimated using bucket tests on a small subset of the full user population. in section # we show that the website operator can also reliably estimate. nal important parameter in the formulation of the problem: the user traf. per minute over the course of a day. we thus have the following media scheduling problem, which will be the focus of the paper: given the estimated time series of user traf. for the day, and a pool of potential items to be featured each with its own peak interest level and decay function determine an order in which to feature items, and a length of time that each should be featured. this formulation of the problem captures some of the fundamental trade offs at work in scheduling items for high volume sites. in particular if we always present the best item, repeat visitors will have no opportunity to see other items, and the value per impression will decrease as more and more of the impressions are from repeat visitors. if we change the items too often, we will not extract maximum value from the best items, as low quality items will often ll the featured spot. since the general version of the problem is np hard, we seek tractable cases that approximate real traf. this is challenging, since not only does user traf. vary considerably over a day and peak interest vary considerably from one item to another even the decay functions of different items can have quite different shapes. data from the yahoo front page, however, we are able to identify a crucial phenomenon in the data that leads to a tractable formulation: the decay functions for different items can all be approximately. we then show that for any instance of the media scheduling in which decay functions all form segments of a single curve, and in which traf. over the course of a day is unimodal, the media scheduling problem can be solved ef ciently. when these properties hold to within some approximation bound as they do in our case, with small error our algorithm provides the same approximation guarantee to the optimum schedule. we evaluate the algorithm in comparison both to simpler baselines and to the way content is actually scheduled on the yahoo home page, showing signi cant gains in total clicks. thus, this is a case where the analysis of a large volume of data, and the identi cation of regular structure in it, feeds directly into algorithm design for a large scale application. the problem formulation, as argued above, is general enough to apply to a wide range of high traf. we believe that our observations about the data will be useful in the context of many other sites as well, since they are aggregates of hundreds of millions of visitors and do not appear to depend on any idiosyncrasies of yahoocontent presentation. with this in mind, we explore the structure of the traf. data in detail, identifying principles about clickthroughs and decay rates, as well as a simple generative model that explains some of the fundamental patterns that we see. the remainder of the paper is organized as follows. in section # we will formally de ne the media scheduling problem. in section # we will examine some data from the yahoo front page, which gives us insights into the details of daily traf, declining clickthrough rates, and variable article quality. section # will present an exponential time algorithm which is optimal under all conditions, and a polynomial time algorithm which gives optimal results under the conditions discussed above, which are approximately met by our data. in section # we will look at the performance of these algorithms and compare them to the performance of the manual scheduling that actually occurred. finally, in section #, we will look in more depth at the user behavior which gives rise to the phenomena we observe, with particular focus on why the clickthrough rate declines in the way it does. several approaches to collaborative filtering have been studied but seldom have studies been reported for large and dynamic settings. in this paper we describe our approach to collaborative filtering for generating personalized recommendations for users of google news. we generate recommendations using three approaches: collaborative filtering using minhash clustering, probabilistic latent semantic indexing, and covisitation counts. we combine recommendations from different algorithms using a linear model. our approach is content agnostic and consequently domain independent, making it easily adaptable for other applications and languages with minimal effort. this paper will describe our algorithms and system setup in detail, and report results of running the recommendations engine on google news. the challenge is in nding the right content for yourself: something that will answer your current information needs or something that you would love to read, listen or watch. search engines help solve the former problem; particularly if you are looking for something speci. that can be formulated as a keyword query. however, in many cases, a user may not even know what to look for. often this is the case with things like news, movies etc, and users instead end up browsing sites like news google com, www net ix com etc, looking around for things that might interest them with the attitude: show copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. in such cases, we would like to present recommendations to a user based on her interests as demonstrated by her past activity on the relevant site. col laborative ltering is a technology that aims to learn user preferences and make recommendations based on user and community data. it is a complementary technology to content based ltering. probably the most well known use of collaborative ltering has been by amazon com where a userpast shopping history is used to make recommendations for new products. various approaches to collaborative ltering have been proposed in the past in research community. our aim was to build a scalable online recommendation engine that could be used for making personalized recommendations on a large web property like google news. quality of recommendations notwithstanding, the following requirements set us apart from most of the known recommender systems: scalability: google news, is visited by several million unique visitors over a period of few days. item churn: most systems assume that the underlying item set is either static or the amount of churn is minimal which in turn is handled by either approximately updating the models or by rebuilding the models ever so often to incorporate any new items. rebuilding, typically being an expensive task, is not done too frequently. however, for a property like google news, the underlying item set undergoes churn every few minutes and at any given time the stories of interest are the ones that appeared in last couple of hours. therefore any model older than a few hours may no longer be of interest and partial updates will not work. for the above reasons, we found the existing recommender systems unsuitable for our needs and embarked on a new approach with novel scalable algorithms. we believe that amazon also does recommendations at a similar scale. however, it is the second point that distinguishes us signi cantly from their system. this paper describes our approach and the underlying algorithms and system components involved. the rest of this paper is organized as follows: section # describes the problem setting. section # presents a brief summary of related work. section # describes our algorithms; namely, user clustering using minhash and plsi, and item item covisitation based recommendations. section # describes how such a system can be implemented. section # reports the results of comparative analysis with other collaborative ltering algorithms and quality evaluations on live tra c. we nish with some conclusions and open problems in section #. the number of items, news stories as identi ed by the cluster of news articles, is also of the order of several million. we model this market scenario as an extensive form game of complete information, where sites choose a set of content to aggregate and users associate with sites that are nearest to their interests. thus, our scenario is a location game in which sites choose to aggregate content at a certain point in user preference space, and our choice of distance metric, jacquard distance, induces a lattice structure on the game. we provide two variants of this scenario: one where users associate with the first site to enter amongst sites of equal distances, and a second where users choose uniformly between sites at equal distances. we show that subgame perfect nash equilibria exist for both games. while it appears to be computationally hard to compute equilibria in both games, we show a polynomial time satisficing strategy called frontier descent for the first game. a satisficing strategy is not a best response, but ensures that earlier sites will have positive profits, assuming all subsequent sites also have positive profits. by contrast, we show that the second game has no satisficing solution. we model the strategic decisions of web sites in content markets, where sites may reduce user search cost by aggregating content. example aggregations include political news, technology, and other niche topic websites. comparatively, directories allowed users to start at a very general area of interest and gradually re ne content until a suitable set was found. hence, the burden of search was left to the user, but they did not need to have a speci. to a topic, emerged as a middle ground be george varghese uc san diego la jolla, ca varghese cs ucsd edu tween search engines and directories. first, search engine optimization and spam make it hard to nd what one wants when searching for some topics, such as cameras. the illusion that one can satisfy oneself with a small set of portals is becoming harder to sustain; yet, user attention and clicks remain the primary, limited resource used to discern the gems among the garbage. there are subtle differences between the offerings: allthingsd appears to specialize more in technical gossip, whereas slashdot seems to specialize in technical ideas. what incentives do aggregators have why do aggregators enter some spaces and not others at what point does specialization lead to losses in this paper, we attempt to answer these questions from a game theoretic perspective. users have a simple strategy: they pick the closest aggregator. aggregators, on the other hand, seek to maximize the number of users that select them. we consider a sequential game in which aggregators enter serially, if they can make pro. and position themselves on nodes in the content lattice. after all entry decisions are made, each user selects an aggregator. the questions we ask include: when and where should an aggregator enter in a game with in nite number of potential entrants, is there a subgame perfect nash equilibrium is it in pure strategies under what conditions is there entry in equilibrium what is the maximum minimum number of equilibrium entrants are there reasonably computable strategies that an aggregator can use to decide when and where to enter. related work the phenomenon of specialization and strategic entry is certainly present in other markets. the seminal early papers on location games in one dimensional taste space are hotelling and salop. these models have been extended in many ways, see for example economides, caplin and nalebuff, ansari, economides and steckel among many others. while some of these extensions consider multiple product characteristics, there are two important differences from our model. first, the location space is very different to the best of our knowledge, in all these papers location is chosen on a subset of rn. by contrast, we model content aggregators as choosing subsets of the universe of available content, introducing a lattice structure on the location space and a different distance metric that is associated with it. second, we do not have prices because most existing aggregators offer free access to users. our model is also related to issues of product design. if there is a universe of all features that a product may have, then each producer designs its product expecting future competition with other producers. that problem is often modeled as a quality choice problem, but that makes it similar to the models of location choice. for simplicity we abstract away from competition in the amount of advertising. product design, it would be natural to allow for price competition and for the costs of the product to depend on the number of features. in contrast, it appears to us that the costs of aggregation of online content increase much less with the amount of aggregated content, hence we propose to assume these effects away. finally, another set of related literature is on bundling. these papers often focus on whether a particular producer should offer its products separately or to bundle them. such considerations may be relevant for publishers that introduce paywalls, as well as to questions of rms managing multiple aggregators. we leave the rst topic for future research and provide partial discussion of the case where one rm can enter with multiple aggregators. in summary, even though there is a vast literature in economics on entry decisions with differentiated products, we believe that our model of entry on a lattice has several advantages over the existing models: we believe euclidean distance is a poor model for content disparities between users and web sites. the process by which users nd and consume digital content, browsing for short, has transitioned through several phases since the advent of the internet. in early periods, two styles of browsing were prevalent: search engines and directories. in terms of user interaction, search engines offered low search cost, given that the user knew apriori the speci. portals, websites consisting of links or hosted content speci. the idea of a favorite portal took hold, and sites like aol and yahoo attempted to cover the set of user interests with a small set of portals each devoted to distinct, but high level, topics. conceptually, this middle ground would allow users to visit a small set of websites to consume their ll of content, with relatively low search costs. recently, strategic behavior of some providers and unabated growth of online content have started undermining the standard search fortopic and browse through favorite portal paradigms. second, along with inferior content proliferating, well crafted niche sites keep appearing for instance, starfall or macrumors. this qualitative argument may explain why web site aggregators have become such a powerful and rising phenomenon. rather than read thousands of individual technical sites or more general web sites such as yahoo and aol, many software professionals get their technical news from some combination of techcrunch, slashdot, ars technica, and allthingsd. there is further specialization: for example, appleinsider combs the web for apple news and mac software updates. tastes is widespread: for example, while many people consume news with broad sites such as yahoo news and cnn news, a large constituency prefers to consume news with a political slant, at say huf ngton post or drudge report. if one believes in the phenomena of the long tail that explains the success of amazon and net ix, then it seems reasonable to posit that increasingly specialized aggregators will keep appearing. we develop a model where aggregators and users strategies are modeled as subsets of a con this work was done when the author was a senior research sci copyright is held by the international world wide web conference com entist at yahoo research. distribution of these papers is limited to classroom use, this work was done when the author was visiting yahoo re and personal use by others. a user set models user preferences, describing the content that a user at a given point in time is interested in. an aggregator set models the content the aggregator hosts. the tension in the model is that specialization can decrease jacquard distance and cause some users to switch, but generalization can capture a larger set because a user that cannot nd the perfect aggregator prefers to go to an aggregator that hasadditional content pieces than one that is missingpieces. we are not aware of any models of product design that would treat the design as a selection of features and or consumer preferences to be characterized by jacquard distance, instead of the more usual euclidean distance. additionally, for our model to be applied to negative prices to attract users are impractical for obvious reasons, but as long as advertising is perceived by users as negative utility, aggregators could compete in the amount of advertising their sites have. yet, it is possible that at least for some small level of advertising users do not mind advertising and competition between aggregators could drive the advertising to that level. in some simple cases, a euclidean may be appropriate, but the nuances of taste such as techcrunch versus macscour, yahoo news versus huffington post seems harder to capture. we believe jacquard distance is a better model. scientific communities confer many forms of credit both implicit and explicit on their successful members, and it has long been argued that the motivation provided by these forms of credit helps to shape a community collective attention toward different lines of research. the allocation of scientific credit, however, has also been the focus of long documented pathologies: certain research questions are said to command too much credit, at the expense of other equally important questions; and certain researchers seem to receive a disproportionate share of the credit, even when the contributions of others are similar. here we show that the presence of each of these pathologies can in fact increase the collective productivity of a community. we consider a model for the allocation of credit, in which individuals can choose among projects of varying levels of importance and difficulty, and they compete to receive credit with others who choose the same project. under the most natural mechanism for allocating credit, in which it is divided among those who succeed at a project in proportion to the project importance, the resulting selection of projects by self interested, credit maximizing individuals will in general be socially sub optimal. however, we show that there exist ways of allocating credit out of proportion to the true importance of the projects, as well as mechanisms that assign credit out of proportion to the relative contributions of the individuals, that lead credit maximizing individuals to collectively achieve social optimality. these results therefore suggest how well known forms of misallocation of scientific credit can in fact serve to channel self interested behavior into socially optimal outcomes. tracking new topics, ideas, and memes across the web has been an issue of considerable interest. recent work has developed methods for tracking topic shifts over long time scales, as well as abrupt spikes in the appearance of particular named entities. however, these approaches are less well suited to the identification of content that spreads widely and then fades over time scales on the order of days the time scale at which we perceive news and events. we develop a framework for tracking short, distinctive phrases that travel relatively intact through on line text; developing scalable algorithms for clustering textual variants of such phrases, we identify a broad class of memes that exhibit wide spread and rich variation on a daily basis. as our principal domain of study, we show how such a meme tracking approach can provide a coherent representation of the news cycle the daily rhythms in the news media that have long been the subject of qualitative interpretation but have never been captured accurately enough to permit actual quantitative analysis. we tracked million mainstream media sites and blogs over a period of three months with the total of million articles and we find a set of novel and persistent temporal patterns in the news cycle. in particular, we observe a typical lag of hours between the peaks of attention to a phrase in the news media and in blogs respectively, with divergent behavior around the overall peak and a heartbeat like pattern in the handoff between news and blogs. we also develop and analyze a mathematical model for the kinds of temporal variation that the system exhibits. this is exactly the focus of our study here. to do this, we work with a massive set of million news and blog articles that we collected over the nal three months of the year#. a growing line of research has focused on the issues raised by the diffusion and evolution of highly dynamic on line information, particularly the problem of tracking topics, ideas, and memes as they evolve over time and spread across the web. prior work has identi ed two main approaches to this problem, which have been successful at two correspondingly different extremes of it. prob permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. abilistic term mixtures have been successful at identifying longrange trends in general topics over time. at the other extreme, identifying hyperlinks between blogs and extracting rare named entities has been used to track short information cascades through the blogosphere. however, between these two extremes lies much of the temporal and textual range over which propagation on the web and between people typically occurs, through the continuous interaction of news, blogs, and websites on a daily basis. intuitively, short units of text, short phrases, and memes that act as signatures of topics and events propagate and diffuse over the web, from mainstream media to blogs, and vice versa. moreover, it is at this intermediate temporal and textual granularity of memes and phrases that people experience news and current events. a succession of story lines that evolve and compete for attention within a relatively stable set of broader topics collectively produces an effect that commentators refer to as the news cycle. tracking dynamic information at this temporal and topical resolution has proved dif cult, since the continuous appearance, growth, and decay of new story lines takes place without signi cant shifts in the overall vocabulary; in general, this process can also not be closely aligned with the appearance and disappearance of speci. as a result, while the dynamics of the news cycle has been a subject of intense interest to researchers in media and the political process, the focus has been mainly qualitative, with a corresponding lack of techniques for undertaking quantitative analysis of the news cycle as a whole. our approach to meme tracking, with applications to the news cycle. here we develop a method for tracking units of information as they spread over the web. our approach is the rst to scalably identify short distinctive phrases that travel relatively intact through on line text as it evolves over time. thus, for the rst time at a large scale, we are able to automatically identify and actually see such textual elements and study them in a massive dataset providing essentially complete coverage of on line mainstream and blog media. working with phrases naturally interpolates between the two extremes of topic models on the one hand and named entities on the other. first, the set of distinctive phrases shows signi cant diversity over short periods of time, even as the broader vocabulary remains relatively stable. as a result, they can be used to dissect a general topic into a large collection of threads or memes that vary from day to day. second, such distinctive phrases are abundant, and therefore are rich enough to act as tracers for a large collection of memes; we therefore do not have to restrict attention to the much smaller collection of memes that happen to be associated with the appearance and disappearance of a single named entity. from an algorithmic point of view, we consider these distinctive phrases to act as the analogue of genetic signatures for different memes. and like genetic signatures, we nd that while they remain recognizable as they appear in text over time, they also undergo signi cant mutation. as a result, a central computational challenge in this approach is to nd robust ways of extracting and identifying all the mutational variants of each of these distinctive phrases, and to group them together. we develop scalable algorithms for this problem, so that memes end up corresponding to clusters containing all the mutational variants of a single phrase. as an application of our technique, we use it to produce some of the rst quantitative analysis of the global news cycle. in this context, the collection of distinctive phrases that will act as tracers for memes are the set of quoted phrases and sentences that we nd in articles that is, quotations attributed to individuals. this is natural for the domain of news: quotes are an integral part of journalistic practice, and even if a news story is not speci cally about a particular quote, quotes are deployed in essentially all articles, and they tend to follow iterations of a story as it evolves. however, each individual quote tends to exhibit very high levels of variation across its occurrence in many articles, and so the aspects of our approach based on clustering mutational variants will be crucial. thus, our analysis of the news cycle will consist of studying the most signi cant groups of mutational variants as they evolve over time. we perform this analysis both at a global level understanding the temporal variation as a whole and at a local level identifying recurring patterns in the growth and decay of a meme around its period of peak intensity. at a global level, we nd a structure in which individual memes compete with another over short time periods, producing daily and weekly patterns of variation. we also show how the temporal patterns we observe arise naturally from a simple mathematical model in which news sources imitate each otherdecisions about what to cover, but subject to recency effects penalizing older content. this combination of imitation and recency can produce synthetic temporal patterns resembling the real data; neither ingredient alone is able to do this. at a local level, we identify some of the ne grained dynamics governing how the intensity of a meme behaves. we nd a characteristic spike around the point of peak intensity; in both directions away from the peak the volume decreases exponentially with time, but in an hour window of time around the median, we nd that volumeas a function of timebehaves like a log. this function diverges at indicating an explosive amount of activity right at the peak period. further interesting dynamics emerge when one separates the websites under consideration into two distinct categories news media and blogs. we nd that the peak of news media attention of a phrase typically comes hours earlier than the peak attention of the blogosphere. moreover, if we look at the proportion of phrase mentions in blogs in a few hour window around the peak, it displays a characteristic heartbeat type shape as the meme bounces between mainstream media and blogs. we further break down the analysis to the level of individual blogs and news sources, characterizing the typical amount by which each source leads or lags the overall peak. among the fastest sources we nd a number of popular political blogs; this measure thus sug this is of course a period when news coverage was particularly high intensity, but it gives us a chance to study the news cycle over precisely the kind of interval in which peoplegeneral intuitions about it are formed and in which it is enormously consequential. in the latter regard, studying the effect of communication technology on elections is a research tradition that goes back at least to the work of lazarsfeld, berelson, and gaudet in the year#s. gests a way of identifying sites that are regularly far ahead of the bulk of media attention to a topic. in addition to the range of different approaches for tracking topics, ideas, and memes discussed above, there has been considerable work in computer science focused on news data in particular. two dominant themes in this work to date have been the use of algorithmic tools for organizing and ltering news; and the role of blogging and the production of news by individuals rather than professional media organizations. some of the key research issues here have been the identi cation of topics over time, the evolving practices of bloggers, the cascading adoption of stories, and the ideological divisions in the blogosphere. this has led to development of a number of interesting tools to help people better understand the news. outside of computer science, the interplay between technology, the news media, and the political process has been a focus of considerable research interest for much of the past century. this research tradition has included work by sociologists, communication scholars, and media theorists, usually at qualitative level exploring the political and economic contexts in which news is produced, its effect on public opinion, and its ability to facilitate either polarization or consensus. an important recent theme within this literature has been the increasing intensity of the news cycle, and the increasing role it plays in the political process. in their in uential book warp speed: america in the age of the mixed media, kovach and rosenstiel discuss how the excesses of the news cycle have become intertwined with the fragmentation of the news audience, writing, the classic function of journalism to sort out a true and reliable account of the dayevents is being undermined. it is being displaced by the continuous news cycle, the growing power of sources over reporters, varying standards of journalism, and a fascination with inexpensive, polarizing argument. the press is also increasingly xated on nding the big story that will temporarily reassemble the nowfragmented mass audience. in addition to illuminating their effect on the producers and consumers of news, researchers have also investigated the role these issues play in policy making by government. as jayson harsin observes, over time the news cycle has grown from being a dominant aspect of election campaign season to a constant feature of the political landscape more generally; not only, he writes, are campaign tactics normalized for governing but the communication tactics are themselves institutionally in uenced by the twenty four hour cable and internet news cycle. moving beyond qualitative analysis has proven dif cult here, and the intriguing assertions in the social science work on this topic form a signi cant part of the motivation for our current approach. speci cally, the discussions in this area had largely left open the question of whether the news cycle is primarily a metaphorical construct that describes our perceptions of the news, or whether it is something that one could actually observe and measure. we show that by tracking essentially all news stories at the right level of granularity, it is indeed possible to build structures that closely match our intuitive picture of the news cycle, making it possible to begin a more formal and quantitative study of its basic properties. traditionally, users have discovered information on the web by browsing or searching. recently, word of mouth has emerged as a popular way of discovering the web, particularly on social networking sites like facebook and twitter. on these sites, users discover web content by following urls posted by their friends. such word of mouth based content discovery has become a major driver of traffic to many web sites today. to better understand this popular phenomenon, in this paper we present a detailed analysis of word of mouth exchange of urls among twitter users. among our key findings, we show that twitter yields propagation trees that are wider than they are deep. our analysis on the geolocation of users indicates that users who are geographically close together are more likely to share the same url. recently online social networking sites like facebook and twitter have emerged as a popular way of discovering information on the world wide web. in contrast to traditional methods of content discovery such as browsing or searching, content sharing in social networking sites occurs through word of mouth, where content spreads via conversations between users. for instance, users share links to content on the web with personal recommendations like this is a must see video. while such word of mouth based content discovery existed long before in the form of emails and web forums, online social networks have made this phenomenon extremely popular and globally reaching. in fact, today social networking sites are known to be a major driver of tra. for certain web sites, facebook and twitter drive, respectively, and of the tra. these osns are sharing tens of millions of web links every day, and we expect that the amount of information exchanged by word of mouth in osns will grow over time. in this paper, we present a detailed analysis of the wordof mouth based content discovery on the web, by analyzing the web links shared on a popular social platform, twitter. we used the twitter data in, which comprises million user pro les, billion follow links, and all billion tweets posted by twitter users between march year# and september year#. twitter is an ideal medium to study wordof mouth based discovery of web content for several reasons. first, the core functionality provided by twitter, tweeting, is centered around the idea of spreading information by wordof mouth. second, twitter provides additional mechanisms like retweet, which enable users to propagate information across multiple hops in the network through word of mouth. third, thanks to url shortening services, sharing urls has become a common practice in twitter. in fact, nearly a quarter of all tweets in our data contained urls, and a total of million urls were shared during the three year period. in order to study how the million users in twitter collaboratively discovered and spread web links, we built an information propagation tree for every url that was shared during a random week in year#. we considered both explicit and implicit information ows. by accounting for implicit information ows, our methodology produces much denser trees than previous work that considers only explicit links the number of edges in a tree has increased by a factor of. based on the propagation trees of urls, we try to answer several key questions that are fundamental to understanding word of mouth based web discovery. the key ndings of our work can be summarized as follows: word of mouth can be used to spread a single url to a large portion of the user population, in some cases even to an audience of several million users. we study several longstanding questions in media communications research, in the context of the microblogging service twitter, regarding the production, flow, and consumption of information. to do so, we exploit a recently introduced feature of twitter known as lists to distinguish between elite users by which we mean celebrities, bloggers, and representatives of media outlets and other formal organizations and ordinary users. based on this classification, we find a striking concentration of attention on twitter, in that roughly of urls consumed are generated by just elite users, where the media produces the most information, but celebrities are the most followed. we also find significant homophily within categories: celebrities listen to celebrities, while bloggers listen to bloggers etc; however, bloggers in general rebroadcast more information than the other categories. next we re examine the classical two step flow theory of communications, finding considerable support for it on twitter. third, we find that urls broadcast by different categories of users or containing different types of content exhibit systematically different lifespans. and finally, we examine the attention paid by the different user categories to different news topics. a longstanding objective of media communications research is encapsulated by what is known as lasswellmaxim: part of this research was performed while the author was visiting yahoo research, new york. the author was also supported by nsf grant iis. copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. hofman yahoo research, ny, usa hofman yahoo inc com duncan. watts yahoo research, ny, usa djw yahoo inc com who says what to whom in what channel with what effect, so named for one of the pioneers of the eld, harold lasswell. although simple to state, laswellmaxim has proven di cult to answer in the more than years since he stated it, in part because it is generally di cult to observe information ows in large populations, and in part because di erent channels have very di erent attributes and. as a result, theories of communications have tended to focus either on mass communication, de ned as oneway message transmissions from one source to a large, relatively undi erentiated and anonymous audience, or on interpersonal communication, meaning a two way message exchange between two or more individuals. correspondingly, debates among communication theorists have tended to revolve around the relative importance of these two putative modes of communication. for example, whereas early theories such as the hypodermic needle model posited that mass media exerted direct and relatively strong. ects on public opinion, mid century researchers argued that the mass media in uenced the public only indirectly, via what they called a two step ow of communications, where the critical intermediate layer was occupied by a category of media savvy individuals called opinion leaders. ects paradigm was then subsequently challenged by a new generation of researchers, who claimed that the real importance of the mass media lay in its ability to set the agenda of public discourse. but in recent years rising public skepticism of mass media, along with changes in media and communication technology, have tilted conventional academic wisdom once more in favor of interpersonal communication, which some identify as a new era of minimal. recent changes in technology, however, have increasingly undermined the validity of the mass vs. on the one hand, over the past few decades mass communication has experienced a proliferation of new channels, including cable television, satellite radio, specialist book and magazine publishers, and of course an array of web based media such as sponsored blogs, online communities, and social news sites. correspondingly, the traditional mass audience once associated with, say, network television has fragmented into many smaller audiences, each of which increasingly selects the information to which it is exposed, and in some cases generates the information itself. meanwhile, in the opposite direction interpersonal communication has become increasingly ampli ed through personal blogs, email lists, and social networking sites to. together, these two trends have greatly obscured the historical distinction between mass and interpersonal communications, leading some scholars to refer instead to masspersonal communications. a striking illustration of this erosion of traditional media categories is provided by the micro blogging platform twitter. for example, the top ten most followed users on twitter are not corporations or media organizations, but individual people, mostly celebrities. moreover, these individuals communicate directly with their millions of followers via their tweets, often managed by themselves or publicists, thus bypassing the traditional intermediation of the mass media between celebrities and fans. next, in addition to conventional celebrities, a new class of semi public individuals like bloggers, authors, journalists, and subject matter experts has come to occupy an important niche on twitter, in some cases becoming more prominent than traditional public gures such as entertainers and elected. third, in spite of these shifts away from centralized media power, media organizations along with corporations, governments, and ngos all remain well represented among highly followed users, and are often extremely active. and nally, twitter is primarily made up of many millions of users who seem to be ordinary individuals communicating with their friends and acquaintances in a manner largely consistent with traditional notions of interpersonal communication. twitter, therefore, represents the full spectrum of communicationsfrompersonal andprivateto masspersonal totraditional mass media. consequently it provides an interesting context in which to address lasswellmaxim, especially as twitter unlike television, radio, and print media enables one to easily observe information ows among the members of its ecosystem. unfortunately, however, the kinds of effects that are of most interest to communications theorists, such as changes in behavior, attitudes, etc, remain di cult to measure on twitter. therefore in this paper we limit our focus to the who says what to whom part of laswellmaxim. to this end, our paper makes three main contributions: we introduce a method for classifying users using twitter lists into elite and ordinary users, further classifying elite users into one of four categories of interest media, celebrities, organizations, and bloggers. we investigate the ow of information among these categories, nding that although audience attention is highly concentrated on a minority of elite users, much of the information they produce reaches the masses indirectly via a large population of intermediaries. we nd that di erent categories of users emphasize different types of content, and that di erent content types exhibit dramatically di erent characteristic lifespans, ranging from less than a day to months. the remainder of the paper proceeds as follows. in the next section, we review related work. in section # we discuss our data and methods, including section # in which we describe how we use twitter lists to classify users, outline two di erent sampling methods, and show that they deliver qualitatively similar results. in section # we analyze the production of information on twitter, particularly who pays attention to whom. in section #, we revisit the theory of the two step ow arguably the dominant theory of communications for much of the past years nding considerable support for the theory. in section #, we consider who listens to what, examining rst who shares what kinds of media content, and second the lifespan of urls as a function of their origin and their content. finally, in section # we conclude with a brief discussion of future work. we develop thespectral centroid clustering algorithm that effectively finds cluster centroids with our similarity measure. we find thatsc outperforms themeans clustering algorithm in finding distinct shapes of time series. online content exhibits rich temporal dynamics, and diverse realtime user generated content further intensifies this process. however, temporal patterns by which online content grows and fades over time, and by which different pieces of content compete for attention remain largely unexplored. we study temporal patterns associated with online content and how the content popularity grows and fades over time. the attention that content receives on the web varies depending on many factors and occurs on very different time scales and at different resolutions. in order to uncover the temporal dynamics of online content we formulate a time series clustering problem using a similarity metric that is invariant to scaling and shifting. by applying an adaptive wavelet based incremental approach to clustering, we scalesc to large data sets. we demonstrate our approach on two massive datasets: a set of million tweets, and a set of million blog posts and news media articles. our analysis shows that there are six main temporal shapes of attention of online content. we also present a simple model that reliably predicts the shape of attention by using information about only a small number of participants. our analyses offer insight into common temporal patterns of the content on theweb and broaden the understanding of the dynamics of human attention. online information is becoming increasingly dynamic and the emergence of online social media and rich user generated content further intensi es this phenomena. popularity of various pieces of content on the web, like news articles, blog posts, videos, posts in online discussion forums and product reviews, vary on very different temporal scales. for example, content on micro blogging platforms, like twitter, is very volatile, and pieces of content become popular and fade away in a matter of hours. short quoted textual phrases rise and decay on a temporal scale of days, and represent the integral part of the news cycle. temporal variation of named entities and general themes exhibits variations at even larger temporal scale. however, uncovering patterns of temporal variation on the web is dif cult because human behavior behind the temporal variation is highly unpredictable. previous research on the timing of an individualactivity has reported that human actions range from random to highly correlated. although the aggregate dynamics of individual activities tends to create seasonal trends or simple patterns, sometimes collective actions of people and the effects of personal networks result in a deviation from trends. the overall picture of temporal activity on the web is even more complex due to the interactions between individuals, small groups, and corporations. bloggers and mainstream media are both producing and pushing new content into the system. the content then gets adopted through personal social networks and discussed as it diffuses through the web. despite extensive qualitative research, there has been little work about temporal patterns by which content grows and fades over time and by which different pieces of content compete for attention during this process. here we study what temporal patterns exist in the popularity of content in social media. the popularity of online content varies rapidly and exhibits many different temporal patterns. we aim to uncover and detect such temporal patterns of online textual content. more speci cally, we focus on the propagation of the hashtags on twitter, and the quotation of short textual phrases in the news articles and blog posts on the web. such content exhibits rich temporal dynamics and is a direct re ection of the attention that people pay to various topics. moreover, the online media space is occupied by a wide spectrum of very distinct participants. first of all, there are many personal blogs and twitter accounts, with a relatively small readership. secondly, there are professional bloggers and small community driven or professional online media sites that have specialized interests and respond quickly to events. finally, mainstream mass media, like tv stations, large newspapers and news agencies all produce content and push it to the other contributors mentioned above. we aim to understand what kinds of temporal variations are exhibited by online content, how different media sites shape the temporal dynamics, and what kinds of temporal patterns they produce and in uence. we analyze a set of more than million news articles and blog posts over a period of one year. in addition, we examine the adoption of twitter hashtags in a massive set of million twitter posts collected over a month period. we measure the attention given to various pieces of content by tracing the number of mentions over time. we formulate a time series clustering problem and use a time series shape similarity metric that is invariant to the total volume and the time of peak activity. to nd the common temporal patterns, we develop aspectral centroid clustering algorithm that allows the ef cient computation of cluster centroids under our distance metric. we nd thatsc is more useful in nding diverse temporal patterns than themeans clustering algorithm. we develop an incremental approach based on haar wavelets to improve the scalability ofsc for high dimensional time series. we nd that temporal variation of popularity of content in online social media can be accurately described by a small set of time series shapes. surprisingly, we nd that both of the adoption of hashtags in twitter and the propagation of quoted phrases on the web exhibit nearly identical temporal patterns. we nd that such patterns are governed by a particular type of online media. most press agency news exhibits a very rapid rise followed by a relatively slow decay. whereas, bloggers play a very important role in determining the longevity of news on the web. depending on when bloggers start participating in the online discourse the news story may experience one or more rebounds in its popularity. moreover, we present a simple predictive model which, based on timings of only few sites or twitter users, predicts with accuracy which of the temporal patterns the popularity time series will follow. we also observe complex interactions between different types of participants in the online discourse. more generally, our work develops scalable computational tools to further extend understanding of the roles of different participants play in the online media space. we nd that the collective behavior of various participants governs how we experience new content and react to it. our results have direct applications for predicting the overall popularity and temporal trends exhibited by the online content. moreover, our results can be used for better placing of content to maximize clickthrough rates and for nding in uential blogs and twitters. the diffusion of information on online social and information networks has been a popular topic of study in recent years, but attention has typically focused on speed of dissemination and recall. in this paper, we study the complementary notion of the precision of information diffusion. our model of information dissemination is broadcast based, ie, one where every message from a user goes to a fixed set of recipients, often called the user friends or followers, as in facebook and twitter. the precision of the diffusion process is then defined as the fraction of received messages that a user finds interesting. on first glance, it seems that broadcast based information diffusion is a blunt targeting mechanism, and must necessarily suffer from low precision. somewhat surprisingly, we present preliminary experimental and analytical evidence to the contrary: it is possible to simultaneously have high precision, high recall, and low diameter we start by presenting a set of conditions on the structure of user interests, and analytically show the necessity of each of these conditions for obtaining high precision. we also present preliminary experimental evidence from twitter verifying that these conditions are satisfied. we then prove that the kronecker graph based generative model of leskovec et al satisfies these conditions given an appropriate and natural definition of user interests. further, we show that this model also has high precision, high recall, and low diameter. we finally present preliminary experimental evidence showing twitter has high precision, validating our conclusion. this is perhaps a first step towards a formal understanding of the immense popularity of online social networks as an information dissemination mechanism. modern social and information networks such as facebook, linkedin and twitter are used by hundreds of millions of users every day. there are many hypotheses as to the source of their popularity, and one popular hypothesis relates to the. ectiveness of these networks as information dissemination mechanisms. ectiveness is one of personalization: given the large number of users, one would expect them to be interested in a diverse set of content, and the network must be an. ective information conduit, simultaneously, for all of them. given that information dissemination mechanism in these networks occurs via broadcast over the network topology, it is apriori unclear whether. for instance, wouldnusers receive a large amount of un interesting content via this mechanism and complementarily, wouldnusers miss a large amount of content they would have potentially been interested in the starting point of our study is this commonly stated belief, especially in the media, that online social and information networks mostly generate information that is irrelevant for most users. this claim is often based on inspecting a random tweet. however, such a claim ignores the interest based construction of social networks: as suggested earlier, users on any social or information network have diverse interests, and tend to follow other users who share some of their interests and post content that is interesting to them. thus, although a random tweet on twitter is uninteresting to a random user, it could be that for any given user, the tweets in their timeline are very relevant to them. the study of usefulness of content has been a primary theme in the information retrieval literature, but to the best of our knowledge it has not been directly studied for information di usion on social and information networks. we adapt the widely accepted de nition of relevance for networks by de ning the precision of information in a social network: the fraction of content received by a user that is relevant to them, where relevance is captured by a match between the content and some interest of the user. then we capture virality by de ning two quantities: the recall, which is the fraction of content relevant to a user that he does receive, and the dissemination time for content, ie, the number of hops in the graph taken for this content to spread to users who would be interested in this information. the users are students at stanford university who log in at least once a week on average, follow at least people, and receive at least new tweets a week in their timeline. the set of tweets is put together by choosing tweets from the usertimeline in the past days, and unique randomly selected tweets out of the set of all tweet impressions over the same days. the set of tweets is then rendered in a random order as per usual tweet rendering guidelines. the precision of each set of tweets is then the fraction of these tweets that the user marks as being relevant. the results of the experiment for each of the users is shown in figure #. the average precision of users for tweets drawn from their timeline is. on the other hand, the precision drops to around for the set of random tweets shown to the users even though this is too small a user study to draw a de nitive conclusion about the actual value of precision on twitter, the results lend some credence to the hypothesis that social networks such as twitter are much more precise than one would expect if users were seeing content at random. note that since we showed each user random tweets chosen from tweet impressions, and got a low relevance score for this control set, it does not appear that inspection paradox alone could be an adequate explanation of the high precision we see in this trial. necessary conditions for precision in this paper, we rst outline some necessary conditions for obtaining high precision. for each of these conditions, we state the hypothesis, validate it with data, and argue via modeling and analysis, why the hypothesis is necessary for obtaining high precision. our rst hypothesis is a natural one: users on social and information networks have interests, and link to other users who share some or all of these interests. this assumption is folklore in how these networks are generated several commonly used generative models of social networks indeed use this assumption. we de ne an ana we imposed two restrictions on the randomly selected tweets: the tweets must be in english, and the tweet must not be a reply. the inspection paradox is an analogue to the well known friendship paradox: high quality users have more followers and hence a random tweet impression is of higher quality than a random tweet. figure #: comparison of self reported precision between tweets from a usertimeline and tweets chosen at random. lytic model capturing the essence of these generative models: there are a set of usersand a set of interests. has a set of interests that he is interested in. each user connects to other users based on their interests, and this yields a graph on the users, which is the observed social network. this network could be directed, where some users follow others and information ows along directed edges, or undirected, where friendship is mutual, and information can ow in both directions along an edge. in order to analyze precision in this model, we need to de ne which users sharing an interest. let denote the set of users who act as producers, we show that if for all interests, which means any consumer can be a potential producer, then it is only possible to construct networks with good precision in the trivial scenario where all users have the same interests. this leads us to our second hypothesis: the production interests of a user are narrower than the consumption interests. in other words, we validate this assumption on twitter. we de ne production as either tweeting or retweeting a tweet, and consumption as clicks by the user on tweets that contain a url. for simplicity, we refer to this as a click on a tweet. we show that the set of interests captured by clicks has larger entropy than the set capturing tweets or retweets. we note that both restricting attention only to tweets containing urls, and requiring clicks as a measure of consumption interests are strict notions, which makes the empirical results stronger. we also show via analysis that separation of production from consumption is still insu cient to explain high precision. in particular, we show that if users choose their production and consumption interests at random from any distribution over interests, it is not possible to achieve even constant precision. our result is fairly robust to the empirically observed variability in the number of user interests, and the cardinality of the interests. in appendix, we show the same result when users themselves have varying number of interests, as in the. the above result makes a case for interests with structure: users do not choose interests randomly, but rather, choose them in a correlated fashion. in other words, interests have a correlation structure, and users are more likely to choose from among correlated interests than from among uncorrelated interests. we verify this assumption by measuring the correlation between interests on twitter de ned by the overlap between the sets of users having these interests. we show that the correlation is indeed much larger than what can be expected had users chosen interests at random. we then cluster the interests using these correlations, and show that these clusters have natural interpretations sports, art, technology, etc. it would therefore appear that users are de ned by their values on various attributes, and interests themselves are de ned either as these attributes or sets of attributes taking speci. we nally consider a generative model of social networks that is based on users having attributes: this is the kronecker graph model, where users connect with other users based on similarity in attribute values. we de ne interests using the attributes in this model, as well as producers and consumers of these interests in a natural way, so that producers are more aligned with an interest in terms of attribute similarity than consumers. we show that the resulting user user graph has perfect precision and recall, and constant dissemination diameter for any interest. finally, we present an empirical study to measure the precision of twitter, de ned as the fraction of the set of interests that a user receives from her friends that she is actually interested in consuming. as before, we use clicks on urls within tweets as a proxy for consumption interest. this implies on average, users are interested in one in topics their neighbors tweet about. while this is already a surprisingly good number, it is worth repeating that clicks on urls in tweets are a strict notion for capturing user interests, and it is conceivable that we are under estimating precision in our experiments. for all our experiments, we use a classi er trained within twitter to assign topics or interests to a tweet. in summary, we show, both by theoretical as well as empirical analysis, that it is indeed possible for a social network to have high precision and recall for broadcast information dissemination if users have interests, and connect with other users based on similarity in interests; the producers of an interest are a small subset of consumers; and users donchoose interests at random, but the interests have structure de ned by attributes, which also de ne the users. we consider this to be a surprising result since a priori, a low dissemination time seems to require a well connected network which seems to trade. we should emphasize that our results should be viewed as. rst step in the understanding of the theoretical and empirical underpinnings of precision in information dissemination. for instance, our empirical measure of precision is somewhat primitive and can be re ned. though we have made use of proprietary click data in our empirical analysis of precision, we believe our user study provides an empirically better and reproducible template for measuring precision across social networks, and as future work, we plan to replicate it on a larger scale in a more principled fashion. in summary, each of our hypotheses presented above is a valid area of research in itself, deserving a more in depth study with ne tuned metrics, experiments, and theory. we discuss future research directions in section #. related work the rise of the world wide web and online social networks has seen an explosion of interest in the structure of these networks. in particular, researchers have made many empirical observations about network structure and posited models that could explain this structure. a comprehensive survey of these is beyond the scope of this work, but we mention some relevant works. there is a long line of work studying the powerlaw degree distributions that arise in networks. among other structural properties that have been studied extensively are small diameter, navigability, densi cation and clustering coe cients. it is important to note that much of the above work not only identi es the relevant structural properties, but also proposes models of phenomenon that could give rise to those properties. among the desiderata for such models is mathematical tractability and statistical soundness in that itassumptions and predictions match well with empirical data. since the focus of this paper is the interplay between network structure and information dissemination, we focus our attention to modeling approaches that seek to explain properties related to information dissemination. the empirical study of information dissemination through social networks, and the role of network structure in this process, has a long history in sociology. there has been an explosion of work from the computer science community in this area since the in uential works of, and we refer the interested reader to a slightly old but excellent survey of work in this area. another line of work in network modeling is relevant to us, namely one that seeks to capture the role of user interests in the formation of the network. the works we are aware of are the kronecker graph model, the mag model, the. liation networks model, and a network model based on user behavior. among these, both the kronecker graph model and the mag model seek to be both mathematically tractable and statistically sound. liation networks model and the model based on user behavior are theoretical. we note that many of these models study the role of network structure in information propagation, but to the best of our knowledge none of them have studied the trade. recall in the broadcast model has been extensively studied in the context of rumor spreading, but the goal in that line of work has typically been to maximize speed of propagation. we are not aware of any work studying precision of information in social networks. finally, we mention that precision and recall are extremely well studied concepts in the information retrieval community. in particular, they are arguably the two most frequent and basic measures for information retrieval.