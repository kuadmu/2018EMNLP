sponsored search systems are tasked with matching queries to relevant advertisements. the current state of the art matching algorithms expand the user query using a variety of external resources, such as web search results. the approach builds an expanded query representation by leveraging offline processing done for related popular queries. our experimental results show that our approach significantly improves the effectiveness of advertising on rare queries with only a negligible increase in computational cost. while these expansion based algorithms are highly effective, they are largely inefficient and cannot be applied in real time. in practice, such algorithms are applied offline to popular queries, with the results of the expensive operations cached for fast access at query time. in this paper, we describe an efficient and effective approach for matching ads against rare queries that were not processed offline. search engines provide a gateway to the web for most internet users. the second search is over the corpus of advertisements provided to the search engine through an interface or a feed from advertisers. since copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. this typically includes matching the query against the ad text, target web site, or other information related to the user, ad, or advertiser. it is a well known fact that the volume distribution of web search queries follows the power law. the most frequent queries compose the head and torso of the curve, while the low volume, rarer queries make up the tail of the curve. for this reason, tail queries have signi cant potential for advertising revenue. the main reason for this is that tail queries are harder to interpret. we have shown in our previous work that such expanded queries can be. ectively be used to produce query rewrites for broad match. expanding them online with web results would require the sponsored search to wait for the web search to nish prior to performing ad selection, which in many cases would result in unacceptable latency. we then use the features of the topmost similar queries returned by this procedure to construct an enriched query, which is subsequently used to search over the ad space. indeed, the method can easily be applied to a variety of other search tasks that require expanding rare queries with external knowledge to improve textual matching by overcoming the vocabulary mismatch problem. other potential applications include web search, enterprise search, and social media search. second, we describe a novel approach for directly indexing query expansions for fast computation of query to query similarity. third, we propose a formalism for expanding queries with features derived from pre expanded related queries. finally, we describe a ranking and scoring method that adapts standard information retrieval techniques to the structure of the ads by modifying a unit of retrieval. next, section # explains the general architecture of our system and its various components. in section # we describe our online ad matching algorithm that leverages. they also support the web ecosystem by providing much needed tra. each query submitted to a commercial search engine results into two searches. the rst search is over the corpus of web pages crawled by the search engine. the web crawl performed by the search engine can be viewed as a pull mechanism used to obtain documents. we can view this as a search over pushed content. to commercial web sites that might otherwise not show up in the top web search results for the query. advertisers pay for the placement of their ads on the result page, the search of the ad space is commonly called sponsored search. the two main scenarios of sponsored search advertising are exact match, where advertisers specify the exact query for which the ad is to be shown, and broad match where queries are matched against ads using a broader criterion. while individually rare, tail queries make up a signi cant portion of the query volume. web search engines return results for most queries, including those in the tail of the curve. however, this is not the case for sponsored search. our evaluation of two major search engines has shown that only about of the query volume is covered by ad results. in most cases there are no ads that are explicitly associated with them by advertisers who speci cally bid on the query. furthermore, ad matching based on analyzing historical click data is also di cult, since due to the low volume it is harder to accumulate enough ad clicks to use statistical and explore exploit methods to identify good ads. search engines normally avoid displaying irrelevant ads in order not to degrade user experience, and so the current practice is not to advertise on most of the tail queries. in this paper we propose a method for online rewriting of tail queries for sponsored search. in our system, we preprocess a large number of head and torso queries. ine by expanding them with features extracted from web search results and store the results in a lookup table. at runtime, we look queries up in the table, and if the query is present, we use the expanded query to search the ad space. cient for head and torso queries, tail queries are too rare and cannot be expanded ahead of time. to solve this problem, we use the data of the pre processed queries in a di erent way. instead of an exact match lookup, we build an inverted index of the expanded query vectors, where each document represents a query and its features. at runtime, when the direct lookup into the query table fails, we use the inverted index to perform a similarity search between the userquery and the pre processed queries. although our primary focus is sponsored search, our proposed approach is rather general. the primary contributions of this paper are fourfold. cient online query expansion approach for tail queries. the remainder of this paper is laid out as follows. we begin with an introduction to sponsored search in section # and describe related work in section #. section # details our empirical evaluation over a large sponsored search data set. we propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in real time with the query volume of a commercial web search engine. we use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query. motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic. empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported. we believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience. commercial search engines do a remarkably good job in interpreting these short strings, but they are not omniscient. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. the queries can go a long way in improving the search results and the user experience. given such classi cations, one can directly use them to provide better search results as well as more focused ads. we crawl the web pages pointed by these urls, and classify these pages. finally, we use these result page classi cations to classify the original query. we found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries. in its year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising. one thing, however, has remained constant: people use very short queries. various studies estimate the average length of a search query between and words, which by all accounts can carry only a small amount of information. therefore, using additional external knowledge to augment permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. at the same time, better understanding of query meaning has the potential of boosting the economic underpinning of web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results. for instance, knowing that the query sd is about cameras while nc is about laptops can obviously lead to more focused advertisements even if no advertiser has speci cally bidden on these particular queries. in this study we present a methodology for query classi cation, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately nodes. the problem of query classi cation is extremely di cult owing to the brevity of queries. observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it. of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world. for instance, in the example above, sd brings pages about canon cameras, while nc brings pages about compaq laptops, hence to a human the intent is quite clear. search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge. following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation. to this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query. certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query. for the purpose of this study we rst dispatch the given query to a general web search engine, and collect a number of the highest scoring urls. our empirical evaluation con rms that using web search results in this manner yields substantial improvements in the accuracy of query classi cation. note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre classi ed using the normal text processing and indexing pipeline. thus, at run time we only need to run the voting procedure, without doing any crawling or classi cation. this additional overhead is minimal, and therefore the use of search results to improve query classi cation is entirely feasible in run time. another important aspect of our work lies in the choice of queries. the volume of queries in todaysearch engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times. while individual queries in this long tail are rare, together they account for a considerable mass of all searches. furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on line advertising searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on. however, the tail queries simply do not have enough occurrences to allow statistical learning on a per query basis. therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters. a natural choice for such aggregation is to classify the queries into a topical taxonomy. knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries. consequently, in this work we focus on the classi cation of rare queries, whose correct classi cation is likely to be particularly bene cial. early studies in query interpretation focused on query augmentation through external dictionaries. more recent studies also attempted to gather some additional knowledge from the web. however, these studies had a number of shortcomings, which we overcome in this paper. speci cally, earlier works in the eld used very small query classi cation taxonomies of only a few dozens of nodes, which do not allow ample speci city for online advertising. they also used a separate ancillary taxonomy for web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies. the main contributions of this paper are as follows. first, we build the query classi er directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simpli es taxonomy maintenance and development. the taxonomy used in this work is two orders of magnitude larger than that used in prior studies. the empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported. since our taxonomy is considerably larger, the classi cation problem we face is much more di cult, making the improvements we achieve particularly notable. we also report the results of a thorough empirical study of di erent voting schemes and di erent depths of knowledge. this result is in contrast with prior ndings in query classi cation, but is supported by research in mainstream text classi cation. a locality sensitive hashing scheme is a distribution on a family \f of hash functions operating on a collection of objects, such that for two objects, prf sim, where sim is some similarity function defined on the collection of objects. such a scheme leads to a compact representation of objects so that similarity of objects can be estimated from their compact sketches, and also leads to efficient algorithms for approximate nearest neighbor search and clustering. min wise independent permutations provide an elegant construction of such a locality sensitive hashing scheme for a collection of subsets with the set similarity measure sim \frac. we show that rounding algorithms for lps and sdps used in the context of approximation algorithms can be viewed as locality sensitive hashing schemes for several interesting collections of objects. based on this insight, we construct new locality sensitive hashing schemes for: ol a collection of vectors with the distance between \overand \overmeasured by, where is the angle between \over and \over. this yields a sketching scheme for estimating the cosine similarity measure between two vectors, as well as a simple alternative to minwise independent permutations for estimating set similarity. a collection of distributions onpoints in a metric space, with distance between distributions measured by the earth mover distance, our hash functions map distributions to points in the metric space such that, for distributionsand, emd h \f. the current information explosion has resulted in an increasing number of applications that need to deal with large volumes of data. while traditional algorithm analysis assumes that the data ts in main memory, it is unreasonable to make such assumptions when dealing with massive data sets such as data from phone calls collected by phone companies, multimedia data, web page repositories and so on. this new setting has resulted in an increased interest in algorithms that process the input data in restricted ways, including sampling a few data points, making only a few passes over the data, and constructing a succinct sketch of the input which can then be. there has been a lot of recent work on streaming algorithms, ie, algorithms that produce an output by making one pass over the data while using a limited amount of storage space and time. to cite a few examples, alon et al considered the problem of estimating frequency moments and guha et al considered the problem of clustering points in a streaming fashion. many of these streaming algorithms need to represent important aspects of the data they have seen so far in a small amount of space; in other words they maintain a compact sketch of the data that encapsulates the relevant properties of the data set. indeed, some of these techniques lead to sketching algorithms algorithms that produce a compact sketch of a data set so that various measurements on the original data set can be estimated by. building on the ideas of, alon et al give algorithms for estimating join sizes. gibbons and matias give sketching algorithms producing so called synopsis data structures for various problems including maintaining approximate histograms, hot lists and so on. gilbert et al give algorithms to compute sketches for data streams so as to estimate any linear projection of the data and use this to get individual point and range estimates. cient algorithms for the dynamic maintenance of histograms. their algorithm processes a stream of updates and maintains a small sketch of the data from which the optimal histogram representation can be approximated very quickly. in this work, we focus on sketching algorithms for estimating similarity, ie, the construction of functions that produce succinct sketches of objects in a collection, such that the similarity of objects can be estimated. here, similarity sim is a function that maps pairs of objects, to a number in, measuring the degree of similarity betweenand. sim corresponds to objects, that are identical while sim corresponds to objects that are very di erent. broder et al introduced the notion of min wise independent permutations, a technique for constructing such sketching functions for a collection of sets. the similarity measure considered there was ab sim. we note that this is exactly the jaccard coe cient of similarity used in information retrieval. the min wise independent permutation scheme allows the construction of a distribution on hash functions: u. heredenotes the family of hash functions operating on subsets of the universe. by choosing sayhash functions, ht from this family, a setcould be represented by the hash vector, ht. now, the similarity between two sets can be estimated by counting the number of matching coordinates in their corresponding hash vectors the work of broder et al was originally motivated by the application of eliminating near duplicate documents in the altavista index. representing documents as sets of features with similarity between sets determined as above, the hashing technique provided a simple method for estimating similarity of documents, thus allowing the original documents to be discarded and reducing the input size signi cantly. in fact, the minwise independent permutations hashing scheme is a particular instance of a locality sensitive hashing scheme introduced by indyk and motwani in their work on nearest neighbor search in high dimensions. a locality sensitive hashing scheme is a distribution on a familyof hash functions operating on a collection of objects, such that for two objects, prh. sim here sim is some similarity function de ned on the collection of objects. given a hash function familythat satis es, we will say thatis a locality sensitive hash function family corresponding to similarity function sim. indyk and motwani showed that such a hashing scheme facilitates the construction of. cient data structures for answering approximate nearest neighbor queries on the collection of objects. in particular, using the hashing scheme given by minwise independent permutations results in. cient data structures for set similarity queries and leads to. this was exploited later in several experimental papers: cohen et al for association rule mining, haveliwala et al for clustering web documents, chen et al for selectivity estimation of boolean queries, chen et al for twig queries, and gionis et al for indexing set value one question left open in was the issue of compact representation of hash functions in this family; this was settled by indyk, who gave a construction of a small family of minwise independent permutations. all of this work used the hashing technique for set similarity together with ideas from. we note that the de nition of locality sensitive hashing used by is slightly di erent, although in the same spirit as our de nition. a familyis said to be sensitive for a similarity measure sim if prh. despite the di erence in the precise de nition, we chose to retain the name locality sensitive hashing in this work since the two notions are essentially the same. hash functions with closely related properties were investigated earlier by linial and sasson and indyk et al. our results in this paper, we explore constructions of locality sensitive hash functions for various other interesting similarity functions. the utility of such hash function schemes crucially depends on the fact that the similarity estimation is based on a test of equality of the hash function values. we make an interesting connection between constructions of similarity preserving hash functions and rounding procedures used in the design of approximation algorithms. we show that procedures used for rounding fractional solutions from linear programs and vector solutions to semide nite programs can be used to derive similarity preserving hash functions for interesting classes of similarity functions. in section #, we prove some necessary conditions on similarity measures sim for the existence of locality sensitive hash functions satisfying. using this, we show that such locality sensitive hash functions do not exist for certain commonly used similarity measures in information retrieval, the dice coe cient and the overlap coe cient. in seminal work, goemans and williamson introduced semide nite programming relaxations as a tool for approximation algorithms. they used the random hyperplane rounding technique to round vector solutions for the max cut problem. we will see in section # that the random hyperplane technique naturally gives a family of hash functionsfor vectors such that. refers to the angle between vectors bu and bv. thus this similarity function is very closely related to the cosine similarity measure, commonly used in information retrieval. groups, each consisting of approximately the same weight. our approach, based on estimating the angle between vectors is more direct and is also more general since it applies to general vectors. we also note that the cosine between vectors can be estimated from known techniques based on random projections. however, the advantage of a locality sensitive hashing based scheme is that this directly yields techniques for nearest neighbor search for the cosine similarity measure. an attractive feature of the hash functions obtained from the random hyperplane method is that the output is a single bit; thus the output ofhash functions can be concatenated very easily to produce abit vector estimating similarity between vectors amounts to measuring the hamming distance between the correspondingbit hash vectors. we can represent sets by their characteristic vectors and use this locality sensitive hashing scheme for measuring similarity between sets. this yields a slightly di erent similarity measure for sets, one that is linearly proportional to the angle between their characteristic vectors. in section #, we present a locality sensitive hashing scheme for a certain metric on distributions on points, called the earth mover distance. we are given a set of points, with a distance function de ned on them. a probability distribution is aset of weights, pn on the points such that pi and pi. as simply, implicitly referring to an underlying setof points. the earth mover distance emd between two distributionsandis de ned to be the cost of the min cost matching that transforms one distribution to another. measures the minimum amount of work that must be done in transforming one distribution to the other. this is a popular metric for images and is used for image similarity, navigating image databases and so on. the idea is to represent an image as a distribution on features with an underlying distance metric on features. since the earth mover distance is expensive to compute, applications typically use an approximation of the earth mover distance. we construct a hash function family for estimating the earth mover distance. our family is based on rounding algorithms for lp relaxations for the problem of classi cation with pairwise relationships studied by kleinberg and tardos, and further studied by calinescu et al and chekuri et al. combining a new lp formulation described by chekuri et al together with a rounding technique of kleinberg and tardos, we show a construction of a hash function family which approximates the earth mover distance to a factor of. each hash function in this family maps a distribution on points to some point li in the set. for two distributionsand on the set of points, our family of hash functionssatis es the property that: emd eh. we also show an interesting fact about a rounding algorithm in kleinberg and tardos applying to the case where the underlying metric on points is a uniform metric. in this case, we show that their rounding algorithm can in section #, we will show that we can convert any locality sensitive hashing scheme to one that maps objects to with a slight change in similarity measure. however, the modi ed hash functions convey less information, eg, the collision probability for the modi ed hash function family is at least even for a pair of objects with original similarity. be viewed as a generalization of min wise independent permutations extended to a continuous setting. their rounding procedure yields a locality sensitive hash function for vectors whose coordinates are all non negative. given two vectors ba and bb, the similarity function is min i sim. max applications of locality sensitive hash functions to solving nearest neighbor queries typically reduce the problem to the hamming space. indyk and motwani give a data structure that solves the approximate nearest neighbor problem on the hamming space. their construction is a reduction to the so called pleb problem, followed by a hashing technique concatenating the values of several locality sensitive hash functions. we give a simple technique that achieves the same performance as the indyk motwani result in section #. the basic idea is as follows: given bit vectors consisting ofbits each, we choose a number of random permutations of the bits. for each random permutation, we maintain a sorted order of the bit vectors, in lexicographic order of the bits permuted by. to nd a nearest neighbor for a query bit vectorwe do the following: for each permutation, we perform a binary search on the sorted order corresponding toto locate the bit vectors closest to. further, we search in each of the sorted orders proceeding upwards and downwards from the location of, according to a certain rule. of all the bit vectors examined, we return the one that has the smallest hamming distance to the query vector. the performance bounds we can prove for this simple scheme are identical to that proved by indyk and motwani for their scheme. search engines can record which documents were clicked for which query, and use these query document pairs as soft relevance judgments. however, compared to the true judgments, click logs give noisy and sparse relevance information. we apply a markov random walk model to a large click log, producing a probabilistic ranking of documents for a given query. a key advantage of the model is its ability to retrieve relevant documents that have not yet been clicked for that query and rank those effectively. we conduct experiments on click logs from image search, comparing our random walk model to a different random walk, varying parameters such as walk length and self transition probability. the most effective combination is a long backward walk with high self transition probability. a search engine can track which of its search results were clicked for which query. for a popular system, these click records can amount to millions of query document pairs per day. each pair can be viewed as a weak indication of relevance: that the user decided to at least view the document, based on its description in the search results. although clicks are not real judgments, there is evidence that they are useful, for example as training data, as annotations, for query suggestion or directly as evidence for ranking. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. we can use the clicks of past users to improve the current search results. however, the clicked set of documents is likely to di er from the current userrelevant set. other di erences are due to presentation issues; for example, the user must decide whether to click based on a short summary and is in uenced by the ordering of results. for any given search, a large number of documents are never seen by the user, therefore not clicked. from the perspective of a user conducting a search, documents that are clicked but not relevant constitute noise in the click data. documents that are relevant but not clicked constitute sparsity in the click data. one class of approaches attempts to reduce noise in click data, by building a click model that may use additional information about the userbehaviour. these approaches can signi cantly reduce noise, by identifying some clicked documents as irrelevant. this paper focuses on the sparsity problem, although our model also has noise reduction properties. the model gives a probabilistic ranking of documents, which includes relevant documents that have not yet been clicked for the current query. the sparsity problem is evidenced by power law distributions observed in click logs. most queries in the click log have a small number of clicked documents. in such cases, it is useful to identify additional relevant documents. we rst describe the click information as a graph, and survey a range of click graph applications. then we detail our markov random walk model for nding relevant documents. the subsequent sections describe a real click dataset, and empirical evaluation of the new methods. some differences arise because we are aggregating clicks across users, who may simply disagree about which documents are relevant. for example, taking into account the userbrowsing patterns after clicking a document. we present a novel locality sensitive hashing scheme for the approximate nearest neighbor problem underp norm, based onstable distributions our scheme improves the running time of the earlier algorithm for the case of thep norm. it also yields the first known provably efficient approximate nn algorithm for the case. we also show that the algorithm finds the exact near neigbhor in time for data satisfying certain bounded growth condition unlike earlier schemes, our lsh scheme works directly on points in the euclidean space without embeddings. consequently, the resulting query time bound is free of large factors and is simple and easy to implement. our experiments show that the our data structure is up to times faster than kd tree. a similarity search problem involves a collection of objects that are characterized by a collection of relevant features and represented as points in a high dimensional attribute space; given queries in the form of points in this space, we this material is based upon work supported by the nsf career grant ccr. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. nicole immorlica laboratory for computer science, mit nickle theory lcs mit edu vahab. mirrokni laboratory for computer science, mit mirrokni theory lcs mit edu are required to nd the nearest object to the query. a particularly interesting and well studied instance isdimensional euclidean space. this problem is of major importance to a variety of applications; some examples are: data compression, databases and data mining, information retrieval, image and video databases, machine learning, pattern recognition, statistics and data analysis. typically, the features of the objects of interest are represented as points in dand a distance metric is used to measure similarity of objects. the basic problem then is to perform indexing or similarity searching for query objects. the number of features ranges anywhere from tens to thousands. the low dimensional case is well solved, so the main issue is that of dealing with a large number of dimensions, the so called curse of dimensionality. despite decades of intensive effort, the current solutions are not entirely satisfactory; in fact, for large enough, in theory or in practice, they often provide little improvement over a linear algorithm which compares a query to each point from the database. in particular, it was shown in that all current indexing techniques degrade to linear search for suf ciently high dimensions. in recent years, several researchers proposed to avoid the running time bottleneck by using approximation. this is due to the fact that, in many cases, approximate nearest neighbor is almost as good as the exact one; in particular, if the distance measure accurately captures the notion of user quality, then small differences in the distance should not matter. in fact, in situations when the quality of the approximate nearest neighbor is much worse than the quality of the actual nearest neighbor, then the nearest neighbor problem is unstable, and it is not clear if solving it is at all meaningful. in, the authors introduced an approximate high dimensional similarity search scheme with provably sublinear dependence on the data size. instead of using tree like space partitioning, it re lied on a new method called locality sensitive hashing. the key idea is to hash the points using several hash functions so as to ensure that, for each function, the probability of collision is much higher for objects which are close to each other than for those which are far apart. then, one can determine near neighbors by hashing the query point and retrieving elements stored in buckets containing that point. in the authors provided such locality sensitive hash functions for the case when the points live in binary hamming space. they showed experimentally that the data structure achieves large speedup over several tree based data structures when the data is stored on disk. in addition, since the lsh is a hashingbased scheme, it can be naturally extended to the dynamic setting, ie, when insertion and deletion operations also need to be supported. this avoids the complexity of dealing with tree structures when the data is dynamic. the lsh algorithm has been since used in numerous applied settings, eg, see. however, it suffers from a fundamental drawback: it is fast and simple only when the input points live in the hamming space. as mentioned in, it is possible to extend the algorithm to thenorm, by embeddingspace intospace, and thenspace into hamming space. however, it increases the query time and or error by a large factor and complicates the algorithm. in this paper we present a novel version of the lsh algorithm. as with the previous schemes, it works for the near neighbor problem, where the goal is to report a point within distance rfrom a query, if there is a point in the data set pwithin distance rfrom. unlike the earlier algorithm, our algorithm works directly on points in euclidean space without embeddings. as a consequence, it has the following advantages over the previous algorithm: logn, where for. thus, for large range of values of, the query time exponent is better than the one in. it is simple and quite easy to implement. it works for any lvnorm, as long as pe nn under ldvwhich usesspace, with query time, where where: max, to our knowledge, this is the only known provable algorithm for the high dimensional nearest neighbor problem for the case. similarity search under such fractional norms have recently attracted interest. our algorithm also inherits two very convenient properties of lsh schemes. the rst one is that it works well on data that is extremely high dimensional but sparse. speci cally, the running time bound remains unchanged if ddenotes the maximum number of non zero elements in vectors. to our knowledge, this property is not shared by other known spatial data structures. thanks to this property, we were able to use our new lsh scheme for fast color based image similarity search. in that context, each image was represented by a point in roughly year# dimensional space, but only about dimensions were non zero per point. the use of our lsh scheme enabled achieving order of magnitude speed up over the linear scan. the second property is that our algorithm provably reports the exact near neighbor very quickly, if the data satis es certain bounded growth property. speci cally, for a query point, and, letbe the number of approximate nearest neighbors of qin. ifgrows sub exponentially as a function of, then the lsh algorithm reports, the nearest neighbor, with constant probability within time, assuming it is given a constant factor approximation to the distance from qto its nearest neighbor. in particular, we show that if, then the running time is. ef cient nearest neighbor algorithms for data sets with polynomial growth properties in general metrics have been recently a focus of several papers. lsh solves an easier problem, while working under weaker assumptions about the growth function. it is also somewhat faster, due to the fact that the lognfactor in the query time of the earlier schemes is multiplied by a function of, while in our case this factor is additive. we complement our theoretical analysis with experimental evaluation of the algorithm on data with wide range of parameters. in particular, we compare our algorithm to an approximate version of thetree algorithm. we performed the experiments on synthetic data sets containing planted near neighbor; similar model was earlier used in. our experiments indicate that the new lsh scheme achieves query time of up to times better than the query time of thetree algorithm. notations and problem de nitions we use lvdto denote the space dunder the lvnorm. for any point ve dvllvvnorm of the vector, we denote by llvthe lv. the ball of radiuscentered at vis de ned as. in this paper we focus on the nn problem. observe that nn is simply a decision version of the approximate nearest neighbor problem. although in many applications solving the decision version is good enough, one can also reduce the approximate nn problem to approximate nn via binary search like approach. in particular, it is known that the approximate nn problem reduces toinstances of nn. then, the complexity of approximate nn is the same as that of the nn problem. in this paper, we present an information theoretic approach to learning a mahalanobis distance function. we formulate the problem as that of minimizing the differential relative entropy between two multivariate gaussians under constraints on the distance function. we express this problem as a particular bregman optimization problem that of minimizing the logdet divergence subject to linear constraints. our resulting algorithm has several advantages over existing methods. first, our method can handle a wide variety of constraints and can optionally incorporate a prior on the distance function. unlike most existing methods, no eigenvalue computations or semi definite programming are required. we also present an online version and derive regret bounds for the resulting algorithm. finally, we evaluate our method on a recent error reporting system for software called clarify, in the context of metric learning for nearest neighbor classification, as well as on standard data sets. selecting an appropriate distance measure is fundamental to many learning algorithms such asmeans, nearest neighbor searches, and others. however, choosing such a measure is highly problem speci. and ultimately dictates the success or failure of the learning algorithm. to this end, there have been several recent approaches that attempt to learn distance functions, eg, these methods work by exploiting distance information that is intrinsically available in many learning settings. for example, in the problem of semi supervised clustering, points are constrained to be ei appearing in proceedings of the th international conference on machine learning, corvallis, or, year#. in information retrieval settings, constraints between pairs of distances can be gathered from click through feedback. in fully supervised settings, constraints can be inferred so that points in the same class have smaller distances to each other than to points in different classes. while existing algorithms for metric learning have been shown to perform well across various learning tasks, each fails to satisfy some basic requirement. first, a metric learning algorithm should be suf ciently exible to support the variety of constraints realized across different learning paradigms. second, the algorithm must be able to learn a distance function that generalizes well to unseen test data. finally, the algorithm should be fast and scalable. in this paper, we propose a novel approach to learning a class of distance functions namely, mahalanobis distances that have been shown to possess good generalization performance. the mahalanobis distance generalizes the standard euclidean distance by admitting arbitrary linear scalings and rotations of the feature space. we model the problem in an information theoretic setting by leveraging the relationship between the multivariate gaussian distribution and the set of mahalanobis distances. we translate the problem of learning an optimal distance metric to that of learning the optimal gaussian with respect to an entropic objective. in fact, a special case of our formulation can be viewed as a maximum entropy objective: maximize the differential entropy of a multivariate gaussian subject to constraints on the associated mahalanobis distance. our formulation is quite general: we can accommodate a range of constraints, including similarity or dissimilarity constraints, and relations between pairs of distances. we can also incorporate prior information regarding the distance function itself. for some problems, standard euclidean distance may work well. in others, the mahalanobis distance using the inverse of the sample covariance may yield reasonable results. in such cases, our formulation nds a distance function that is closest to an initial distance function while also satisfying the given constraints. we show an interesting connection of our metric learning problem to a recently proposed low rank kernel learning problem. in the latter problem a lowrank kernelis learned that satis es a set of given distance constraints by minimizing the logdet divergence to a given initial kernel. this allows our metric learning algorithm to be kernelized, resulting in an optimization over a larger class of non linear distance functions. algorithmically, the connection also implies that the problem can be solved ef ciently: it was shown that the kernel learning problem can be optimized using an iterative optimization procedure with cost per iteration, whereis the number of distance constraints, andis the dimensionality of the data. in particular, this method does not require costly eigenvalue computations or semi de nite programming. we also present an online version of the algorithm and derive associated regret bounds. to demonstrate our algorithmability to learn a distance function that generalizes well to unseen points, we compare it to existing state of the art metric learning algorithms. we apply the algorithms to clarify, a recently developed system that classi es software errors using machine learning. in this domain, we show that our algorithm effectively learns a metric for the problem of nearest neighbor software support. furthermore, on standard uci datasets, we show that our algorithm consistently equals or outperforms the best existing methods when used to learn a distance function fornn classi cation. kernel learning plays an important role in many machine learning tasks. however, algorithms for learning a kernel matrix often scale poorly, with running times that are cubic in the number of data points. in this paper, we propose efficient algorithms for learning low rank kernel matrices; our algorithms scale linearly in the number of data points and quadratically in the rank of the kernel. we introduce and employ bregman matrix divergences for rank deficient matrices these divergences are natural for our problem since they preserve the rank as well as positive semi definiteness of the kernel matrix. special cases of our framework yield faster algorithms for various existing kernel learning problems. experimental results demonstrate the effectiveness of our algorithms in learning both low rank and full rank kernels. kernel methods have played a major role in many recent machine learning algorithms. however, scalability is often a concern: giveninput data points, many kernel based algorithms scale as. furthermore, the kernel matrix requires memory overhead, which may be prohibitive for large scale learning tasks. recently, research has been done on using low rank kernel representations to improve scalability. if the kernel matrix is assumed to be of rank, we need only store the decomposition of the kernel matrixggt, whereis. many kernel based learning algorithms can be reformulated in terms of, and lead to algorithms that scale linearly with. one of the main obstacles in using low rank kernel matrices lies in obtaining appearing in proceedings of the rd international conference on machine learning, pittsburgh, pa, year#. a matrix with such a property; most standard kernel functions do not produce low rank kernel matrices, in general. the focus in this paper is on learning low rank kernel matrices given distance and similarity constraints on the data. learning a kernel matrix has been a topic of signi cant research, but most existing algorithms are not very. moreover, there is not much literature on learning low rank kernel matrices. we propose to learn a low rank kernel by minimizing the divergence to an initial low rank kernel matrix while satisfying distance and similarity constraints as well as a low rank constraint. however, low rank constraints are non convex, and optimization problems involving such constraints are intractable in general. matrix divergences, we show how to naturally obtain convexity of the optimization problem, leading to algorithms that are substantially more. our main contributions in this paper are: we employ rank preserving bregman matrix divergences, dissimilarity measures over matrices which are natural for learning low rank kernel matrices, as they implicitly constrain the rank and maintain positive semi de niteness during the updates of our algorithms. we develop projection algorithms based on the burg matrix divergence and the von neumann divergence that scale linearly with the number of data points. a special case of our formulation leads to the definiteboost optimization problem of tsuda et al. our approach improves the running time of their algorithm by a factor of, from to per projection. additional special cases of our formulation lead to improved algorithms for nonlinear dimensionality reduction and the nearest correlation matrix problem. we experimentally demonstrate that our algorithms can be. ectively used in classi cation and clustering tasks. in this paper, aiming at providing semantically relevant queries for users, we develop a novel, effective and efficient two level query suggestion model by mining clickthrough data, in the form of two bipartite graphs extracted from the clickthrough data. based on this, we first propose a joint matrix factorization method which utilizes two bipartite graphs to learn the low rank query latent feature space, and then build a query similarity graph based on the features. due to the complexity of the web structure and the ambiguity of users inputs, most of the suggestion algorithms suffer from the problem of poor recommendation accuracy. for a given query raised by a specific user, the query suggestion technique aims to recommend relevant queries which potentially suit the information needs of that user. after that, we design an online ranking algorithm to propagate similarities on the query similarity graph, and finally recommend latent semantically relevant queries to users. experimental analysis on the clickthrough data of a commercial search engine shows the effectiveness and the efficiency of our method. or commercial advantage and that copies terface for web users to obtain any kind of information they may seek. queries containing ambiguous terms may confuse the search engine into retrieving web pages which do not satisfy the information needs of users. another consideration, as reported in, is that users tend to submit short queries consisting of only one or two terms under most circumstances, and short queries are more likely to be ambiguous. however, due to the commercial reasons, few public papers have been released to unveil the methods they adopt. in fact, clickthrough data is an ideal source for mining relevant queries. bear this notice and the full citation on the rst page. in this paper, by analyzing the clickthrough data, we develop a query suggestion framework using two level latent semantic analysis. then we build a query graph based on the representation of query space. we evaluate our model from di erent angles: first, it is assessed by a panel of three experts. ciency of our online query suggestion algorithm by measuring how much cpu time that it needs. the results show that our method is both. cient for improving the recommendation quality, as well as generating semantically related queries to users. the rst one is the ambiguity which commonly exists in the natural language. moreover, this noise is not easily removed by machine learning methods. in order to avoid these problems, some additional data sources are likely to be very helpful to improve the recommendation quality. however, most of these references extract only the query url bipartite graph of the clickthrough data for analysis, and ignore the information of users who issued the queries. we rst extract two bipartite graphs, which are user query and query url bipartite graphs. section # presents the similarity propagation model as well as the method for recommending queries. the rest of the paper is organized as follows. with the exponential growth of information on the world wide web, web search engines provide an indispensable in permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. although current commercial search engines have been proved to be successful for recommending the most relevant web pages to users, there are several outstanding issues that can potentially degrade the quality of search results, and these merit investigation. through the analysis of a commercial search enginequery logs recorded over three months in year#, we observe that of web queries are single term queries, and a further of web queries contain only two terms. thirdly, in most cases, the reason why users search is that they have little or even no knowledge about the topic they are searching for. in order to nd satisfactory answers, users have to rephrase their queries constantly. to overcome all of these problems, a valuable technique, query suggestion, has been employed by some famous commercial search engines, such as yahoo, live search, ask and google, to recommend relevant queries to users. typically, query suggestion is based on local and global document analysis, or anchor text analysis. however, these traditional methods have di culty summarizing the latent meaning of a web document due to the huge noise embedded in each web page. in the typical search scenario, a user initiates a query, and submits it to a search engine. the search engine returns a set of ranked related web pages or documents to this user. the user then clicks some pages of interest. some users even re ne their queries in order to nd the desired information. therefore, the collection of queries is likely to well re ect the relatedness of the target web pages. to copy otherwise, to http: www yahoo com republish, to post on servers or to redistribute to lists, requires prior speci. http: www live com permission and or a fee. http: www ask com cikm, october, year#, napa valley, california, usa. actually, users perform as the most important role in the clickthrough data, since all the queries are issued by the users, and which urls to click are also decided by the users. the connections between queries and urls are essentially bridged by di erent kinds of users. moreover, if two distinct users issued the similar set of queries, we can assume that these two users are very similar since they have similar information needs. from the above analysis, we cannot ignore the users in the clickthrough data. then we give solutions to the following two problems: how to learn the query latent feature space from these two bipartite graphs, and how to recommend semantically relevant queries to users as to the rst problem, we develop a joint matrix factorization method which fuse user query and query url bipartite graphs together to learn the low dimensional query latent feature space. in order to address the second problem, we develop a novel,ective, and. cient similarity propagation model, which not only suggests a list of queries relevant to the queries submitted by users, but also ranks the query list based on the similarity scores. we evaluate our model for query suggestion using clickthrough data of a commercial search engine. then, we evaluate it in terms of the ground truth extracted from the odp database. section # describes the construction of two bipartite graphs, and proposes a joint matrix factorization method of learning query latent feature space. in section #, we demonstrate the empirical analysis of our models and algo http: www dmoz org rithms. finally, conclusions and future work are given in section #. generating alternative queries, also known as query suggestion, has long been proved useful to help a user explore and express his information need. in many scenarios, such suggestions can be generated from a large scale graph of queries and other accessory information, such as the clickthrough. however, how to generate suggestions while ensuring their semantic consistency with the original query remains a challenging problem. in this work, we propose a novel query suggestion algorithm based on ranking queries with the hitting time on a large scale bipartite graph. without involvement of twisted heuristics or heavy tuning of parameters, this method clearly captures the semantic consistency between the suggested query and the original query. empirical experiments on a large scale query log of a commercial search engine and a scientific literature collection show that hitting time is effective to generate semantically consistent query suggestions. the proposed algorithm and its variations can successfully boost long tail queries, accommodating personalized query suggestion, as well as finding related authors in research. the explosive growth of web information has not only created a crucial challenge for search engine companies to handle large scale data, but also increased the di culty for a user to manage his information need. it has become increasingly di cult for a user to compose a succinct and precise query to present his search need. instead of pushing this burden to the users, it is common practice for a search engine to provide some types of query suggestions. this work was done when the rst author was on a summer internship at microsoft research. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. when a user types a query msg to the search engines, he will be provided with quite a few alternative potential queries. for example, he will be suggested msg chinese food, msg health, and other names for msg by google, and msg error, msg network, and msg seating chart by yahoo. there are also other query suggestion mechanisms which could automatically complete a query, and automatically correct spelling mistakes. such query suggestion mechanisms are usually developed based on morphological information of queries, or cooccurrence of one query word with other queries. although suchquery suggestions are proved useful in di erent ways, there is usually no guarantee that the suggested queries convey close semantic information with the original query. indeed, it is usually annoying for a researcher who searches for chris burges but is suggested with chris burgess or chris burge ministries. similarly, it is not very helpful to suggest kdd with kbb, kddi, ntt, and harry shum with harry potter. people searching for larry page maybe interested in sergey brin but not yellow page. a good query suggestion system should consider a handful of features, but in most cases it is important to ensure that the semantics of the suggested query do not drift too much from the original one. some users will issue the query msg to search for the sports center in newyork and others use it to search the food additive. msr could mean microsoft research, but also mountain safety research, or even mortgage servicing rights. without the constraint of semantics, a general suggestion to such ambiguous queries would easily be. another big challenge and opportunity for the current query suggestion systems lies in the suggestion of infrequent queries. it has been a well known theory in business that a company could sell less of more by boosting the long tail of the power law distribution. the same question lies in searchengine business, especially in advertising where customers bid for query terms. frequently clicked queries cost more and long tail queries cost less. if a well designed query suggestion system could route the traf all real examples are collected on feb. http: search yahoo com searchchris burges http: search live com results aspxchris burges http: search live com results aspxkdd http: search live com results aspxharry shum. and boost the clickthrough of long tail queries, there is a huge opportunity to maximize thebene ts forboth a search engine company and customers of its advertising system. is there a principled way to suggest semantically similar queries while also boosting long tail queries can such a method also provide a natural solution to personalization it is challenging because semantics is hard to de ne and both long tail queries andpersonalization usually su er from data sparsity. in this paper, we propose a uni ed approach to query suggestion, by computing the hitting time on a large scale bipartite graph of queries and clickthrough. despite its simplicity, this novel approach introduces quite a fewbene ts to query suggestion: the suggestions generated with the proposed algorithm are semantically similar to the original query; the suggestions generated do not have to occur with the original query; this approachboosts the long tail queries as suggestions; and this model provides a natural treatment for personalized query suggestion. empirical experiments on a large scale query log of a commercial search engine, as well as a public available scienti. bibliography dataset show that our proposed algorithm is. ective for semantically coherent query suggestion, which provides a potential new framework, or an important and novel feature for building a real query suggestion system. the approach of using hitting time is quite general, which could provide potential solutions to many other search related problems other than query suggestion. the rest of the paper is organized as follows. in section #, we formally introduce the concept of hitting time on a bipartite graph. in section #, we propose the algorithm of query suggestion using hitting time. we show our experiments and results in section #, introduce the related work in section #, and conclude in section #. net ix spends millions to look for an. ective way to suggest hard to nd movies. this paper proposes an efficient sparse metric learning algorithm in high dimensional space via an penalized log determinant regularization. compare to the most existing distance metric learning algorithms, the proposed algorithm exploits the sparsity nature underlying the intrinsic high dimensional feature space. this sparsity prior of learning distance metric serves to regularize the complexity of the distance model especially in the less example numberand high dimensionsetting. theoretically, by analogy to the covariance estimation problem, we find the proposed distance learning algorithm has a consistent result at rate to the target distance matrix with at mostnonzeros per row. moreover, from the implementation perspective, this penalized log determinant formulation can be efficiently optimized in a block coordinate descent fashion which is much faster than the standard semi definite programming which has been widely adopted in many other advanced distance learning algorithms. we compare this algorithm with other state of the art ones on various datasets and competitive results are obtained. determining the similarity of short text snippets, such as search queries, works poorly with traditional document similarity measures, since there are often few, if any, terms in common between two short text snippets. we address this problem by introducing a novel method for measuring the similarity between short text snippets by leveraging web search results to provide greater context for the short texts. in this paper, we define such a similarity kernel function, mathematically analyze some of its properties, and provide examples of its efficacy. we also show the use of this kernel function in a large scale system for suggesting related queries to search engine users. in analyzing text, there are many situations in which we wish to determine how similar two short text snippets are. for example, there may be di erent ways to describe some concept or individual, such as united nations secretary general and ko. annan, and we would like to determine that there is a high degree of semantic similarity between these two text snippets. similarly, the snippets ai and arti cial intelligence are very similar with regard to their meaning, even though they may not share any actual terms in common. directly applying traditional document similarity measures, such as the widely used cosine coe cient, to copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. such short text snippets often produces inadequate results, however. indeed, in both the examples given previously, applying the cosine would yield a similarity of since each given text pair contains no common terms. even in cases where two snippets may share terms, they may be using the term in di erent contexts. consider the snippets graphical models and graphical interface. the rst uses graphical in reference to graph structures whereas the second uses the term to refer to graphic displays. thus, while the cosine score between these two snippets would be due to the shared lexical term graphical, at a semantic level the use of this shared term is not truly an indication of similarity between the snippets. to address this problem, we would like to have a method for measuring the similarity between such short text snippets that captures more of the semantic context of the snippets rather than simply measuring their term wise similarity. to help us achieve this goal, we can leverage the large volume of documents on the web to determine greater context for a short text snippet. by examining documents that contain the text snippet terms we can discover other contextual terms that help to provide a greater context for the original snippet and potentially resolve ambiguity in the use of terms with multiple meanings. our approach to this problem is relatively simple, but surprisingly quite powerful. we simply treat each snippet as a query to a web search engine in order to nd a number of documents that contain the terms in the original snippets. we then use these returned documents to create a context vector for the original snippet, where such a context vector contains many words that tend to occur in context with the original snippet terms. such context vectors can now be much more robustly compared with a measure such as the cosine to determine the similarity between the original text snippets. furthermore, since the cosine is a valid kernel, using this function in conjunction with the generated context vectors makes this similarity function applicable in any kernel based machine learning algorithm where text data is being processed. while there are many cases where getting a robust measure of similarity between short texts is important, one particularly useful application in the context of search is to suggest related queries to a user. in such an application, a user who issues a query to a search engine may nd it helpful to be provided with a list of semantically related queries that he or she may consider to further explore the related information space. by employing our short text similarity kernel, we could match the userinitial query against a large repository of existing user queries to determine other similar queries to suggest to the user. thus, the results of the similarity function can be directly employed in an end user application. the approach we take in constructing our similarity function has relations to previous work in both the information retrieval and machine learning communities. we explore these relations and put our work in the context of previous research in section #. we then formally de ne our similarity function in section # and present initial examples of its use in section #. this is followed by a mathematical analysis of the similarity function in section #. section # presents a system for related query suggestion using our similarity function, and an empirical evaluation of this system is given in section #. finally, in section # we provide some conclusions and directions for future work. query suggestion has been an effective approach to help users narrow down to the information they need. however, most of existing studies focused on only popular head queries. in this paper, we propose an optimal rare query suggestion framework by leveraging implicit feedbacks from users in the query logs. our model resembles the principle of pseudo relevance feedback which assumes that top returned results by search engines are relevant. however, we argue that the clicked urls and skipped urls contain different levels of information and thus should be treated differently. unlike the rocchio algorithm, our learning process does not involve the content of the urls but simply leverages the click and skip counts in the query url bipartite graphs. experimental results on one month query logs from a large commercial search engine with over million rare queries demonstrate the superiority of our framework, with statistical significance, over the traditional random walk models and pseudo relevance feedback models. since rare queries possess much less information than popular queries in the query logs, it is much more difficult to efficiently suggest relevant queries to a rare query. hence, our framework optimally combines both the click and skip information from users and uses a random walk model to optimize the query correlation. our model specifically optimizes two parameters: the restarting rate of random walk, and the combination ratio of click and skip information. consequently, our model is capable of scaling up to the need of commercial search engines. web search engines have completely changed the way people acquire information during the last ten years. by providing a comprehensive portal between the internet users and the web, search engines are able to take a user query and return a ranked list of web pages according to the relevance between queries and the search engine index, which consists a subset of the entire web. the reason of failure is that the length of the queries is usually quite short, so that understanding user intents correctly has been a critical yet quite di cult task for search engines. among a variety of techniques, query suggestion related techniques have become an. among all query suggestion techniques, one of the most important and. speci cally, query logs are server end logs that record user activities in search engines. a typical query log entry contains timestamp, query, clicked urls as well as user personal information. in order to learn a query suggestion model, a commonly used approach is to leverage graph representation which forms query and url relationship into bipartite graphs. the edge between a queryand a urlindicates user clicks ofwhen issuing. as a matter of fact, a myriad of techniques have been proposed. while for rare queries that have only appeared a handful of times in the logs with very few clicks, click graph is unable to capture the underlying relationship between queries. for example, in figure #, and do not have commonly clicked urls, thus a random walk queries urls queries urls audipartstore com audipartstore com audiusa com audiusa com audi parts audi parts audirepair autorepairlocal com audirepair autorepairlocal com audi bodywork nwaaudidealers com audi bodywork nwaaudidealers com audi en wikipedia org wiki audi en wikipedia org wiki audi audi click graph skip graph figure #: an illustrative example of query url click graph and skip graph. while it is well known that in search engines, query frequencies follow a power law distribution where most queries are issued very few times by users, rare queries together constitute a great amount of search tra. ects the relevance and revenue of search engines signi cantly. motivation of our work figure # presents a motivation of our approach. ideally, audi parts should be a good suggested query for audi bodywork. however, after performing a random walk on the click graph, only the query audi can be suggested to audi parts because there is no commonly clicked urls between audi parts and audi bodywork so that their correlation is zero. however, if we leverage the top skipped urls for audi parts and audi bodywork as shown in figure #, it can be clearly observed that both queries skipped their top returned two urls: nwaaudidealers com and en wikipedia org wiki audi. however, for rare queries, many times the top skipped urls contain di erent levels of information than the clicked urls. because top returned urls are more likely to have high static rank scores which are representative of the highlevel topic that the query belongs to. eg, the url is a general entry about audi, while queries audi parts and audi bodywork address di erent aspects of user need of the speci. although users who issued these two queries clicked on more speci. ers a potential topic link between these queries. so if a user clicked the rd ranked url, then the st and nd urls are said to be skipped. fully analyzed query logs from a commercial search engine. figure # shows user session statistics in one of the data sets we use in the experiment which contains million unique queries. the gure compares the query frequency against the number of clicked and skipped urls. it can be observed that when the query frequency is low, more urls are skipped than clicked during the same user session. generally, users are tend to click more often on top returned results for popular queries, while for rare queries, the clicks are more random and thus have higher entropy scores. we further analyzed the quality of skipped urls for rare queries. figure # demonstrates the comparative ratings between skipped and clicked urls. overall, skipped urls indicate a little bit less relevance than clicked urls. this observation further supports our claim that skipped urls should be leveraged for rare queries in the context of relevance measurement. finally, combine two query correlation matrices to form the optimal query correlation matrix, which is used for query suggestion. url skipped url clicked urls clicked urls skipped of urls of urls year# year# query frequency query frequency average url rating figure #: number of urls clicked vs. number of urls skipped in the same user sessions from one week search log. there are more urls skipped than clicked for queries with lower frequencies. in pseudo relevance feedback models, this ratio is the same for both clicked and skipped urls, which is not optimal in practice for rare queries, as we shall see in the empirical analysis. recent study indicates that search is still quite di cult, approximately of times search engines fail to return relevant documents. ective way to interact between users and search engines, hence to improve the relevance of search results. a query url bipartite graph usually consists of two disjoint sets of nodes, corresponding to queries and urls respectively. an example of this bipartite representation has been shown in figure #, where the left hand set of nodes are queries and the righthand set are urls. the click graph possesses large amount of potential information that can be learnt for query suggestion, query clustering, query reformulation and so on. among them, random walk technique is one of the most. however, leveraging only the click information has a serious drawback. that is, the models learnt from click graph can only bene. popular queries which possess enough user click feedbacks. query audi parts and audi bodywork are not correlated if only performs random walk on the click graph, but will be highly correlation if random walk is performed on the skip graph. model which discovers query relationship according to their common clicks is unable to discover any correlation between and. ective proposals to deal with rare queries needs our immediate attentions. the left gure shows the click graph for three queries and ve urls that returned as top serp results. as a result, a random walk on the skip graph will assign a high correlation score to these two queries. our work is inspired by the principle of pseudo relevance feedback which assumes that the topk returned documents from search engines are always relevant to the queries, regardless of whether they are clicked or not. to further back up our argument regarding using both clicked and skipped urls for rare query suggestion, we care we de ne a url to be skipped if it was viewed by the user without being clicked. however, with the increase of query popularity, the click patterns become more stable. we selected, queries which have been issued less than times within a week and asked human judgers to judge the relevance on a scale. on average, clicked urls have a rating of while skipped urls have. contribution of this paper in this paper, we propose a novel graph combinationbased rare query suggestion framework. first, how to choose the optimal restarting rate for the random walk second, given two query url correlation matrices, how to optimally combine them the reason is that the restarting rate directly. ects the transition probability of random walk from nodes to nodes, which. ects the distribution of query relevance scores that is critical for determining the most relevant neighbor nodes. on the other hand, the combination rate decides the level of contributions from click and skip graphs respectively. to the best of our knowledge, we are among the rst to address the importance of the restarting rate of random walk, and optimize the parameter in a principled way. in other random walk like models, this rate is either pre xed, or empirically chosen without any support information. the rest of the paper is organized as follows: section # presents the literature in query suggestion, query clustering related research; section # introduces our framework for optimal rare query suggestions; section # provides empirical results on the performance of our model; nally, section # concludes our proposal with future work. existing work on mining such data has mostly attempted to discover knowledge at the level of queries. these two patterns can be used to address the mis specification and under specification problems of ineffective queries. experiment results on real search engine logs show that the mined context sensitive term substitutions can be used to effectively reword queries and improve their accuracy, while the mined context sensitive term addition patterns can be used to support query refinement in a more effective way. search engine logs are an emerging new type of data that offers interesting opportunities for data mining. in this paper, we propose to mine search engine logs for patterns at the level of terms through analyzing the relations of terms inside a query. we define two novel term association patterns and propose new methods for mining such patterns from search engine logs. such search engine logs contain a lot of valuable information such as patterns of query reformulation. thus the huge amount of search engine log data. indeed, mining search engine logs has recently attracted much attention. all these permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. studies have shown the promise of improving search accuracy through mining search engine logs. however, virtually all the previous work has treated a whole query as a unit for analysis; as a result, the discovered knowledge is mostly at the level of queries. existing query suggestion works such as and also consider a whole query as a unit and they further rely on other resources such as web snippets or human labeled training data to generate related queries. furthermore, most of the work only suggests related queries and does not consider the. a query is ine ective due to multiple reasons, but two of them are common: mis speci cation and under speci cation. the mis speci cation problem is caused by the fact that there may be multiple ways of expressing the same idea or describing the same thing, and a user may not know what exact terms have been used by the authors of the documents to be searched. for example, if a user wants to nd a place to wash his her vehicle, a good query would be car wash. this is because in most relevant web pages, the authors used car wash rather than vehicle wash or auto wash. this is an example of what we refer to as a contextsensitive term substitution pattern. the under speci cation problem in a query may be because the user does not know much about the content to be found or can not naturally think of additional speci. in such a case, it would be useful to suggest terms such as insurance and sale for a user to choose so as to make the query more discriminative. in order to do this, we need knowledge of the form insurance auto quotes and sale auto quotes. in this paper, we rst formally de ne the two novel term association patterns in search logs context sensitive term substitution and addition patterns. then we propose new probabilistic methods to discover these patterns through analyzing term co occurrences in query logs. such terms tend to co occur with the same or similar terms; for example, both auto and car often occur together with rental, pricing, etc. we propose to use probabilistic translation models for capturing quasi synonyms. for example, car and insurance often co occur in the queries and they can help each other to re ne a topic car insurance can be used to re ne both car and insurance. we propose to use probabilistic contextual models for capturing contextual terms. based on both translation models and contextual models, we cast our context sensitive term association pattern mining as probability estimation problems. patterns with high probabilities are with high con dence and then used for query reformulation. for example, car has a high probability in the translation model of auto and high probability to co occur with wash in contextual models, then the pattern auto car wash will have a high probability and thus is a pattern with high con dence. ectiveness of our proposed algorithms, we conduct experiments on a sample of search logs. these show that our proposed methods can discover useful knowledge based on the term relations inside queries. our methods are totally orthogonal to, and thus can be enhanced by, other techniques which use other information such as click through and user session data for query suggestions. the rest of the paper is organized as follows. we rst review the related work in section #. then we formally de ne our mining problem in section # and propose our models to discover term association patterns in section #. our search log data collection is described in section # and the experiments are presented in section #. finally we conclude this paper and discuss future work in section #. as search engines are being used, they naturally accumulate a lot of log data, including submitted queries, viewed search results, and clicked urls. in general, a web search engine answers millions of queries every day. for example, clustering search queries is studied in. the similarity of queries can be measured by the clicked documents or their temporal correlations. ectiveness of the suggested queries, which is very crucial for successful query suggestions. in this paper, we look into patterns at the level of terms through analyzing the relations of terms inside a query and use the discovered term association patterns for. our work is motivated from the following observations about what types of knowledge are useful to help a user formulate an. if the user uses a query such as auto wash or vehicle wash, the search results are generally not as good as those from using the query car wash even though all these queries have roughly the same meaning. in order to help a user in such a case, we need knowledge of the form auto car wash. for example, a query such as auto quotes can return mixed results with some about automobile insurance quotes and some about automobile sale prices. this is an example of what we refer to as a context sensitive term addition pattern. our basic idea is to analyze the co occurrences of terms within multi word queries in logs and obtain two kinds of term relations: quasi synonyms and contextual terms. quasi synonyms are words that are synonyms or that are syntactically substitutable. experimental results on the real search engine logs show that our proposed methods can. ectively mine term association patterns and all these patterns can be used for. second, we show how to reduce both training and testing times using metric ball trees; the speedups from ball trees are further magnified by learning low dimensional representations of the input space. for large data sets, the use of locally adaptive distance metrics leads to even lower error rates. in this paper we study how to improve nearest neighbor classification by learning a mahalanobis distance metric. we build on a recently proposed framework for distance metric learning known as large margin nearest neighbor classification. first, we describe a highly efficient solver for the particular instance of semidefinite programming that arises in lmnn classification; our solver can handle problems with billions of large margin constraints in a few hours. third, we show how to learn different mahalanobis distance metrics in different parts of the input space. many algorithms for pattern classi cation and machine learning depend on computing distances in a multidimensional input space. often, these distances are computed using a euclidean distance metric a choice whichhasboth the advantages of simplicity and generality. notwithstandingthese advantages, though, the euclideandistance metricis not very well adapted to most problems in pattern classi cation. viewing the euclidean distance metric as overly sim appearing in proceedings of the th international conference on machine learning, helsinki, finland, year#. plistic, many researchers have begun to ask how to learn or adapt the distance metric itself in order to achievebetter results. distance metric learning is an emerging area of statistical learning in which the goal is to induce a more powerful distance metric from labeled examples. a well chosen linear transformation can improve knn classi cation by decorrelating and reweighting elements of the feature vector. in fact, signi cant improvements have been observed within several di erent frameworks for this problem, including neighborhood components analysis, largemarginknn classi cation, andinformationtheoretic metriclearning. these studies have established the general utility of distance metric learning for knn classi cation. however, further work is required to explore its promise in more di cult regimes. they alsopresent the opportunity tolearn more adaptive and sophisticated distance metrics. in this paper, we study these issues as they arise in the recentlyproposedframework oflarge margin nearest neighbor classi cation. inthisframework, amahalanobisdistance metric is trained with the goal that thenearest neighbors of each example belong to the same class while examplesfromdi erent classes are separatedby alarge margin. the role of the margin in lmnn classi cation is inspiredby its rolein support vector machines. however, as described in section #, na ive implementations of this optimization do not scale well to larger data sets. addressing the challenges and opportunities raised by largerdatasets, thispapermakesthree contributions. first, we describe how to optimize the training procedure for lmnn classi cation so that it can readily handle data sets with tens of thousands of training examples. to our knowledge, problems of this size have yet to betackledby otherrecentlyproposed methods for learning mahalanobis distance metrics. as the second contribution of this paper, we explore the use of metricball trees forlmnn classi cation. ball trees are known to work best in input spaces of low to moderate dimensionality. mindful of thisregime, wealsoshowhowto modify theoptimizationinlmnn sothatitlearns alow rankmahalanobis distance metric. with this modi cation, the metric can be viewed as projecting the original inputs into a lower dimensional space, yielding further speedups. as the third contribution of this paper, we describe an important extension to the original framework for lmnn classi cation. speci cally, in section #, we showhow tolearndi erentmahalanobisdistance metricsfordi erentparts of theinput space. the promise of this approach is suggested by recent, related workin computer vision that has achieved state of the art results on image classi cation. ourparticular approach begins by partitioning the training data into disjoint clusters using class labels or unsupervised methods. we then learn a mahalanobis distance metric for each cluster. the globally coupled training of these metrics also distinguishes our approach from earlier work in adaptivedistance metricsforknn classi cation. thus, our results show that we can exploit large data sets to learn morepowerful and adaptivedistance metricsfor knn classi cation. the simplest instance of this problem arises in the context ofnearest neighbor classi cation using mahalanobisdistances. mahalanobisdistances are computedbylinearlytransforming the input space, then computing euclidean distances in the transformed space. in particular, larger data sets raise new and important challenges in scalability. simple in concept, useful in practice, the ideas behind lmnn classi cation have also inspired other related work in machine learning and computer vision. not surprisingly, given these roots, lmnn classi cation also inherits various strengths and weaknesses of svms. for example, asin svms, the training procedure in lmnn classi cation reduces to a convex optimization based on the hinge loss. in order to scale to this regime, we have implemented a special purpose solver for the particular instance of semide nite programming that arises in lmnn classi cation. in section #, we describe the details of this solver, which we have used to tackle problemsinvolvingbillions oflarge margin constraints. these data structures have been widely used to accelerate nearest neighbor search. in section #, we show how similar data structures can be used for faster training and testing in lmnn classi cation. the novelty of our approachliesinlearning a collection ofdi erent local metrics to maximize the margin of correct knn classi cation. while the training procedure couples the distance metricsindi erent clusters, the optimization remains a convexproblemin semide niteprogramming. to our knowledge, our approach yields the best knn test error rate on the extensively benchmarked mnist data set of handwritten digits thatdoes notincorporatedomain speci cpriorknowledge. the information explosion on the internet has placed high demands on search engines. yet people are far from being satisfied with the performance of the existing search engines, which often return thousands of documents in response to a user query. many of the returned documents are irrelevant to the user need. the precision of current search engines is well under people expectations. in order to find more precise answers to a query, a new generation of search engines or question answering systems have appeared on the web. unlike the traditional search engines that only use keywords to match documents, this new generation of systems tries to understand the user question, and suggest some similar questions that other people have often asked and for which the system has the correct answers. in fact, the correct answers have been prepared or checked by human editors in most cases. it is then guaranteed that, if one of the suggested questions is truly similar to that of the user, the answers provided by the system will be relevant. the assumption behind such a system is that many people are interested in the same questions the frequently asked copyright is held by the author owner. if the system can correctly answer these questions, then an important part of users questions will be answered precisely. however, the queries submitted by users are very different, both in form and in intention. how human editors can determine which queries are faqs is still an open issue. a closely related question is: how can a system judge if two questions are similar the classic approaches to information retrieval would suggest a similarity calculation according to their keywords. however, this approach has some known drawbacks due to the limitations of keywords. at the retrieval level, traditional approaches are also limited by the fact that they require a document to share some keywords with the query to be retrieved. in reality, it is known that users often use keywords or terms that are different from the documents. there are then two different term spaces, one for the users, and another for the documents. how to create relationships for the related terms between the two spaces is an important issue. this problem can also be viewed as the creation of a live online thesaurus. the creation of such relationships would allow the system to match queries with relevant documents, even though they contain different terms. again, with a large amount of user logs, this may be possible. in this paper, we propose a new approach to query clustering using user logs. if users clicked on the same documents for different queries, then the queries are similar. if a set of documents is often selected for a set of queries, then the terms in these documents are related to the terms of the queries to some extent. these principles are used in combination with the traditional approaches based on query contents. our preliminary results are very encouraging: many queries that we consider similar are actually clustered together using our approach. in addition, we notice that many similar questions would have been grouped into different clusters by traditional clustering approaches because they do not share any common keywords. this study demonstrates the usefulness of user logs for query clustering, and the feasibility of an automatic tool to detect faqs for a search engine. this paper presents a simple and intuitive method for mining search engine query logs to get fast query recommendations on a large scale industrial strength search engine. in order to get a more comprehensive solution, we combine two methods together. on the one hand, we study and model search engine users sequential search behavior, and interpret this consecutive search behavior as client side query refinement, that should form the basis for the search engine own query refinement process. on the other hand, we combine this method with a traditional content based similarity method to compensate for the high sparsity of real query log data, and more specifically, the shortness of most query sessions. to evaluate our method, we use one hundred day worth query logs from sina search engine to do off line mining. then we analyze three independent editors evaluations on a query test set. based on their judgement, our method was found to be effective for finding related queries, despite its simplicity. in addition to the subjective editors rating, we also perform tests based on actual anonymous user search sessions. providing related queries for search engine users can help them quickly nd the desired content. recently, some search engines started showing related search keywords in the bottom of the result page. their main purpose is to give search engine users a comprehensive recommendation when they search using a speci. recommending the most relevant search keywords set to users not only enhances the search enginehit rate, but also helps the user to nd the desired information more quickly. also, for some users who are not very familiar with a certain domain, we can use the queries that are used by previous similar searchers who may part of this work was done while the rst author was at search engined group, sina corporation, beijing, china have gradually re ned their query, hence turning into expert searchers, to help guide these novices in their search. that is, we can get query recommendations by mining the search engine query logs, which contain abundant information on past queries. search engine query log mining is a special type of web usage mining. in, a content ignorant approach and a graph based iterative clustering method was used to cluster both the urls and queries. later in, the authors presented a well rounded solution for query log clustering by combining content based clustering techniques and cross reference based clustering techniques. in, methods to get query recommendation by utilizing the click through data were presented.