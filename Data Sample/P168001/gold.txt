in, matrix factorization was employed to compute query similarity on both query url and query user bipartite graphs. search session data is also a widely used data source. most methods require to solve a semide nite programming problem, which is computationally very expensive and can hardly be applied to large scale or high dimensional problems. high dimensional indexing is also related to our work. locality sensitive hashing is a randomized approach to high dimensional indexing and does not su er from curse of dimensionality. for example, latent semantic analysis and supervised semantic indexing transform the original vector space of bag of words into a lowdimensional space of topics. in our work, we introduce a sparsity constraint to the highdimensional space and do not make dimension reduction. query processing is an important kind of tasks in web search, such as query suggestion, query reformulation, and query expansion, etc. recently, rare query processing has attracted much attention from ir community. most of previous work on query processing explicitly or implicitly calculate and utilize query similarity. our work di ers from the previous work in that it focuses on query similarity calculation, especially for rare queries. so far many methods have been proposed and various types of data have been utilized to calculate query similarity. our approach is unique because we make use of not only terms but also click through data, and automatically learn a similarity function which is applicable to any queries. by viewing queries as short documents, one can exploit the document similarity functions to measure query similarities. these include degree of term overlap between queries, edit distance between queries, tf idf weighted cosine similarity between queries and other measures. since queries are too short, these term based similarity functions are usually perform poorly. our work is unique in that we learn term dependencies from click through data to deal with the term mismatch problem. since queries are short and it is di cult to calculate their similarities, auxiliary information has been used as features to enrich query representations. search results from a search engine emerge as one type of auxiliary information. sahami et al proposed using search result snippets to compute tf idf weighted cosine similarity in query expansion. as an extension, yih took a learning approach to calculating term weights using human labeled training data. the search engine based methods heavily rely on the performance of search engine and are hard to apply into large scale and time sensitive scenarios. mei et al used the rst hitting time on the query url bipartite graph as a similarity measure. craswell szummer built two random walk processes to propagate query similarity along the graph and obtained better similarity scores between queries. besides, the overlap of clicked urls between two queries in a query url bipartite was also used as query similarity measure. zhang et al proposed using the number of co occurrence of query pairs in search sessions as a query similarity measure. these methods for calculating query similarities based on user behavior data perform well on common queries where the wisdom of crowds can be fully leveraged. our work di ers from the previous work in that it utilizes click through data to automatically generate training data, and then learns the query similarity function which is consistent with the training data. metric learning is a new area in machine learning, which aims to learn a metric space from supervision. so far, many methods for metric learning have been proposed. cient algorithms for metric learning have been devised. the basic idea of the algorithm is to introduce a prior with bregman divergence plus sparsity constraints. ciently nd similar queries from a large collection of existing queries is another core problem in our scenario. so far, various hash families have been devised for di erent metric functions, eg, euclidean distance and cosine similarity. bridging metric learning and locality sensitive hashing together turns out to be an interesting topic. some existing methods on term vector space can also be interpreted as methods of learning similarity function in the vector space. there are clear di erences between our approach and the existing approaches, however. in contrast, in the existing approaches the learning process is driven by dimension reduction. however, they are not applicable to rare or new queries. we employ metric learning method to solve the optimization problem in learning similarity function.