we develop and make publicly available an entity search test collection based on the dbpedia knowledge base. this includes a large number of queries and corresponding relevance judgments from previous benchmarking campaigns, covering a broad range of information needs, ranging from short keyword queries to natural language questions. further, we present baseline results for this collection with a set of retrieval models based on language modeling and bm. finally, we perform an initial analysis to shed light on certain characteristics that make this data set particularly challenging. modeling query concepts through term dependencies has been shown to have a significant positive effect on retrieval performance, especially for tasks such as web search, where relevance at high ranks is particularly critical. most previous work, however, treats all concepts as equally important, an assumption that often does not hold, especially for longer, more complex queries. in this paper, we show that one of the most effective existing term dependence models can be naturally extended by assigning weights to concepts. we demonstrate that the weighted dependence model can be trained using existing learning to rank techniques, even with a relatively small number of training queries. our study compares the effectiveness of both endogenous and exogenous features for determining concept importance. to test the weighted dependence model, we perform experiments on both publicly available trec corpora and a proprietary web corpus. our experimental results indicate that our model consistently and significantly outperforms both the standard bag of words model and the unweighted term dependence model, and that combining endogenous and exogenous features generally results in the best retrieval effectiveness. search queries come in many di erent avors, depending on the userinformation need and the particular search task. most modern search engines allow users to express their information needs as free text queries. although this is a convenient interface for users, it places a heavy burden on the search engine to properly interpret the userintent. for example, for the query american airlines reservations, the search engine should identify that the query is made up of the concepts american airlines and reservations and that the key focus of the query is reservations. this information, if properly identi ed and incorporated into the underlying retrieval model, can be used to retrieve highly relevant documents. however, most traditional information retrieval models, such as language modeling and bm, utilize very simple user query models. these models tend to treat query terms as independent and of uniform importance. simple heuristics, such as inverse document frequency, are integral parts of these models and can be thought of as a simple query term weighting model, but they are very rigid and are based on a single data source. furthermore, it is not clear if idf is an appropriate measure of importance for phrases and other generic concepts. recent research has shown that modeling query term dependencies and using non uniform query term weighting can signi cantly improve retrieval. ectiveness, especially on very large collections and for long, complex queries. to our knowledge, no work exists on simultaneously modeling query term dependencies and weighting generic query term concepts in a uni ed, trainable framework. it is precisely this problem that we tackle in this paper. our proposed model extends the markov random field model for information retrieval by automatically learning query concept weights. by making use of the mrf model, we go beyond the query term independence assumptions made by traditional retrieval models. furthermore, our proposed extension is a generic framework for learning the importance of query term concepts in a way that directly optimizes an underlying retrieval metric. it is important to note that this is quite di erent from query segmentation approaches. optimizing segmentation accuracy is not guaranteed to optimize retrieval. by implementing concept weighting directly into the underlying retrieval model we avoid the issue of metric divergence. as we will show, this strategy yields strong retrieval. concept weight civil war battle reenactments civil war war battle battle reenactments table #: concept weights generated for query civil war battle reenactments. as an illustration of such metric divergence, table # shows an actual example of unigram and bigram concept importances learned within our proposed model for the query civil war battle reenactments. if, instead, the weighting was done based on the output of a query segmenter, then it is likely that the phrase civil war and perhaps battle reenactments would be given large weights. however, our proposed model assigns high weights to the unigram reenacments and the bigram war battle, which happen to be the most discriminative concepts, not the most likely concepts in terms of query segmentation. there are three primary contributions of our work. first, we propose a straightforward,ective extension of the mrf model that dynamically weights query concepts. we will show that the model can be automatically trained using standard learning to rank approaches. ective weighting of query concepts can be derived using a combination of endogenous and exogenous query concept features. finally, we conduct an extensive evaluation on several publically available trec test collections and a real world web search test collection from a commercial search engine. our experiments show that our proposed approach is consistently and sign cantly better than the current publicly disclosed state of the art text matching model for web search across all test collections. the rest of this paper is laid out as follows. first, in section # we discuss previous work on term dependencies, concept weighting and learning to rank techniques. next, section # reviews the mrf model and describes our proposed extension. then, in section #, we present our experimental results. finally, section # concludes the paper and describes possible directions of future work. the amount of semantic data on the web has been growing rapidly in recent years. one of the key challenges triggered by this growth is the ad hoc querying, ie, the ability to retrieve answers from semantic resources using natural language queries. this facilitates interaction with semantic resources for the users so they can benefit from the knowledge covered by semantic data without the complexities of semantic query languages. in this paper, we focus on semantic queries, where the aim is to retrieve objects belonging to a set of semantically related entities. an example of such an ad hoc type query is apollo astronauts who walked on the moon. in order to address the task, we propose the semsets retrieval model that exploits and combines traditional document based information retrieval, link structure of the semantic data and entity membership in semantic sets, in order to provide the answers. the novelty of the approach lies in the utilization of semantic sets, ie, groups of semantically related entities. we propose two approaches to identify such semantic sets from the knowledge bases; the first one requires involvement of an expert user knowledgeable of the data set structure, the second one is fully automatic and provides results that are comparable with those delivered by the expert users. as demonstrated in the experimental evaluation, the proposed model has the state of the art performance on the semsearch data set, which has been designed especially for the semantic list search evaluation. this growth can be largely contributed to the emerging web of open linked data. in this work, we focus on semantic type queries. the recent boom in the amount of available semantic data increases the already high interest of the research community in the semantic technologies. more and more sources http: www linkeddata org copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. publish, along with the traditional human readable content, structured and linked metadata records in formats such as rdf, rdfa and microformats. the increasing amount of semantic data brings along important technical challenges. except the basic need for the ability to store and access those data, the prevailing challenge is the retrieval from semantic data. while there exist languages for querying semantic data, such as sparql for rdf data sets, they require a certain level of technical skills to formulate queries, as well as the knowledge of the data representation in the underlying knowledge bases. it is only natural that the research community targets the question whether the natural language queries could be used instead of traditional structured query languages to tap the knowledge stored in semantic data sources. the ad hoc semantic search targets the challenge of answering keyword queries from structured knowledge bases. while in information retrieval, the result for a keyword query is the ranked list of documents from a collection, in semantic search, the result comprise ranked list of entities, resources from the queried knowledge base. as shown recently by pound et al, already today search engine users submit many queries that would be suitable for a semantic search; for example search for speci. based on this observation, pound et al described ad hoc object retrieval for semantic search where a user formulates queries using keywords, much like in the web search, and they propose a classi cation of semantic ad hoc queries into ve categories: entity queries, type queries, attribute queries, relation queries and other keyword queries. the task of answering semantic type query is, given an unstructured keyword query in natural language with the intent of retrieving objects of a give type and a semantic graph, to nd objects entities of the desired type. for example, the following keyword queries are examples of semantic type queries: apollo astronauts who walked on the moon, or arab states of the persian gulf. to continue the example, the correct answer for the rst query, using dbpedia as a knowledge base, would comprise dbpedia: neil armstrong, dbpedia: buzz aldrin and ten more astronauts represented by dbpedia entities. in this paper, we propose a retrieval model for ad hoc type queries called the semsets model. the approach can be described as trying to mimic the behaviour of a human trying to answer the query using a web search engine. a human user would probably enter such a query to a web search engine and inspect several topresults. the user would search the text of the inspected documents to nd desired set of entities. then, the user could rank the entities based on the quality of the information in retrieved documents, the query target and the userknowledge and con dence. similarly, in our approach, we rst search the documents con http: microformats org structed for resources of a knowledge base and we use the spreading activation technique to identify additional relevant entities in the knowledge base. this corresponds to a user performing a web search and retrieving the topresults. then, we check the membership of candidate resources in semantic sets constructed from the knowledge base. this corresponds to the user inspection of the retrieved documents. finally, we evaluate the relevance of identi ed semantic sets to a given query and rank the members of semantic sets accordingly. the nal step mimics user evaluation of the results, based on his her knowledge. the proposed approach combines information retrieval techniques, activation spreading over the link structure of a knowledge base and information about entity membership in semantic sets, de ned by the knowledge base. the idea of combining the text information retrieval with activation spreading is well known. the main innovation of the proposed approach is the utilization of semantic sets in the process. we propose two approaches for construction of semantic sets, groups of semantically related entities. one approaches requires an expert user for the task, the second one is fully automatic. the challenge of answering ad hoc keyword type queries from knowledge bases is a new task, unexplored for the most part. the novelty of the presented work is in the retrieval model itself, which exploits information about entity membership in semantic sets and in proposed methods for construction of semantic sets from a given knowledge base. the main contributions of the paper are: retrieval model for the ad hoc semantic type queries, with the goal of answering keyword queries for semantic list search from semantic data. it combines information retrieval techniques, link evidence and information on the membership of entities in semantic sets to produce the results. as shown in the evaluation section, the use of the information on the membership in semantic sets brings signi cant increase in the precision for the proposed retrieval model. we show how an expert user, knowledgeable about the data set, can de ne such semantic sets; in addition we propose a fully automatic method for the construction of semantic sets. when used in the proposed semsets retrieval model, semantic sets constructed by both methods have very similar positive effects on the precision of the results. evaluation results provide evidence of the methodef ciency. moreover, we use publicly available data sets in the evaluation, which makes our results reproducible and allows direct, head to head, comparison with other approaches to the semantic type retrieval task. the organization of the rest of the paper is as follows. discuss related work in section #, and in section # we state the problem and describe the required preliminaries. we propose the semsets model in section # and approaches to the semsets construction in section #. in section # we describe our experimental setup and present the results of the evaluation. finally, we discuss future work in section # before we conclude in section #. understanding how to leverage these entity annotations of text to improve ad hoc document retrieval is an open research area. in this paper, we propose a new technique, called entity query feature expansion which enriches the query with features from entities and their links to knowledge bases, including structured attributes and text. we evaluate our technique on trec text collections automatically annotated with knowledge base entity links, including the google freebase annotations data. we find that entity based feature expansion results in significant improvements in retrieval effectiveness over state of the art text expansion approaches. recent advances in automatic entity linking and knowledge base construction have resulted in entity annotations for document and query collections. for example, annotations of entities from large general purpose knowledge bases, such as freebase and the google knowledge graph. query expansion is a commonly used technique to improve retrieval effectiveness. most previous query expansion approaches focus on text, mainly using unigram concepts. we experiment using both explicit query entity annotations and latent entities. todaycommercial web search engines are increasingly incorporating entity data from structured knowledge bases into search results. google uses data from their knowledge graph and google plus, yahoo has web of objects, bing incorporates facebook and satori entities, and facebook searches over entities with graph search. however, the majority of content created on the web remains unstructured text in the form of web pages, blogs, and microblog posts. for many search tasks, these documents will continue to be the main permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than the author must be honored. in this work, we address the task of ad hoc document retrieval leveraging entity links to knowledge bases in order to improve the understanding and representation of text documents and queries. we demonstrate that this gain in semantic understanding results in signi cant improvements in retrieval effectiveness. the task of entity linking to a knowledge base has received signi cant attention, with one major venue being the text analysis conference knowledge base population entity linking task. in this task traditional named entities are linked to a knowledge base derived from wikipedia. beyond tac, there is increasing interest in more general concept entities, with the task of wikifying documents by linking them to wikipedia. beyond information extraction, content owners are augmenting html markup with embedded structured data through standardized markup efforts such as schema org. a study from year# showed that of web documents contain embedded structured data in rdfa or microformats. google recently released the facc dataset for the trec clueweb and clueweb web collections. the dataset contains automatically extracted entity mentions from web documents that are linkable to the freebase knowledge base. freebase is a publicly available general purpose knowledge base with over million entities and over billion facts the facc dataset is the rst publicly available web scale collection of entity linked documents. in addition to annotated documents, the facc data also contains explicit manual annotations for the trec web track queries. we present one of the rst published experiments using this data for retrieval. for this work, we de ne an entity broadly to be a thing or concept that exists in the world or ction, such as a person, a battle,lm, or a color. we focus primarily on entities that are linked to two existing publicly available knowledge bases, wikipedia and freebase. we use a combination of both of these knowledge bases because they provide complementary information. freebase provides a signi cantly larger database of concepts, many of whom may not meet wikipediastandards for notability, with structured data in rdf, including categories and types. our work addresses two fundamental research areas using entity annotations for ad hoc retrieval. the rst is the representation of both queries and documents with linked entities. what features, if any, improve retrieval effectiveness the second is inferring latent as of january, year# according to freebase com entities for an information need. the facc annotations include entity annotations for queries. for example, a document on the topic of obamafamily history may not explicitly refer to a, but may refer to other related entities, such as a and. in addition, for many existing collections, no explicit entity annotations for queries exist. in both cases, it is important to infer related entities and expand the query representation. entities provide a wealth of rich features that can be used for representation. these include text as well as structured data. some of the important attributes that we highlight for these experiments include: ne grained type information, category classi cations, and associations to other entities. although we do not explore them in detail in this work we also observe that the knowledge base contains rich relational data with attributes and relations to other entities. these attributes include: gender, nationality, profession, geographical information, and temporal attributes, and many more depending on the type of entity. we hypothesize that the language in the document contexts of entity mentions differs from that found in wikipedia or in the knowledge base description. but, mentions of the entity are also contained in text documents across the entire corpus. to address this, we propose new query speci. entity context models extracted from snippets in the feedback documents surrounding the entityannotations. we further hypothesize that this context information will allow us to identify entities that are relevant to the query and use their presence as signals of document relevance. context models that incorporate evidence from uncertainty inherent in automatic information extraction performing the rst published experiments using the facc freebase annotations for ad hoc document retrieval analyzing the clueweb facc annotations for their use in retrieval applications providing new entity annotated query datasets for the trec web track queries that substantially improve entity recall the remainder of the paper is structured as follows. in section #, we introduce the new feature expansion approach and introduce the entity context feedback model. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. we bridge the gap between entities and text using automatic information extraction to identify entities and link them to a knowledge base. however, these annotations are limited to entities that are explicitly mentioned, where we hypothesize that many more latent entities are relevant to the users information need. there are also relevant latent entities such as, and others. one issue is that explicit entity mentions have the same fundamental problems of query document mismatch as words. to summarize, the main contributions of this work are: introducing new query expansion techniques with featurebased enrichment using entity links to a knowledge base demonstrating signi cant improvements in retrieval effectiveness when entity features are combined with existing text approaches proposing a new entity modeling technique for building queryspeci. we experimentally evaluate our approach in section # on standard trec test collections including: robust, clueweb, and clueweb b. connections to related work are discussed in section # before concluding. large knowledge bases consisting of entities and relationships between them have become vital sources of information for many applications. most of these knowledge bases adopt the semantic web data model rdf as a representation model. querying these knowledge bases is typically done using structured queries utilizing graph pattern languages such as sparql. however, such structured queries require some expertise from users which limits the accessibility to such data sources. to overcome this, keyword search must be supported. in this paper, we propose a retrieval model for keyword queries over rdf graphs. our model retrieves a set of subgraphs that match the query keywords, and ranks them based on statistical language models. we show that our retrieval model outperforms the state of the art ir and db models for keyword search over structured data using experiments over two real world datasets. the continuous growth of knowledge sharing communities like wikipedia and the advances in automated information extraction from web pages have made it possible to build large scale knowledge bases. examplesof such knowledge bases includeyago, dbpedia and freebase. these repositories contain entities such as people, movies, books, etc. and the relationships between them such as bornin, actedin, isauthorof and so on. such data is typically represented in the form of subject predicateobject triples of the semantic web data model rdf, where work performed while intern atyahoo research. this work is partially supported by the eu large scale integrated project livingknowledge. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copyotherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. an rdf collection conceptually forms a large graph, which we refer to as anrdf graph, with nodes corresponding to subjects and objects and edges denoting predicates. rdf data can be queried using a conjunction of triple patterns, where a triple pattern is a triple with variables and the same variable in different patterns denotes a join condition. for example, the information need of nding comedies that have won the academy award can be expressed using the following triple patterns: structured queries like the one above are very expressive, yet very restrictive. they require the users to be familiar with the underlying data and a structured query language like sparql. in addition, it enables adapting the state of the art ir searching and ranking techniques. in this paper, we develop a retrieval model that enables users to search rdf graphs using keywords. our model takes as an input a keyword query and returns a ranked list of rdf subgraphs. by retrieving subgraphs instead of just entities, we treat triples in a holistic manner and we explicitly take into account the relationships between the entities. this can be particularly bene cial for both result retrieval and result representation. one possible resultto sucha query retrieved by our model is the triple subgraph. tobe ableto processkeyword queriesoverrdf graphs, we associate each triple witha setofkeywords derived from the subject and object of the triple, as well as representative keywords for the predicate. we explain how we do this in section #. to retrieve all subgraphs that matchagivenkeyword query, we utilizea backtracking algorithm over graphs, which is described in section #. we try to identify the structured information need intended by the keyword query. for instance, consider the query comedy academy award. it is likely that the user is looking for movies of genre comedy that have won the academy award. thus, we need to rank the subgraphs that match this information need higher. our ranking model is based on statistical language models and it utilizes the distribution of terms in the whole knowledge base as a means of inferring the structured information need of a user keyword query. for instance, given that the term comedy appears often in the object of triples with predicate hasgenre and the terms academy and award appear often in the object of triples with predicate haswonprize, we can infer that the most likely structured triple pattern query intendedby thekeyword query comedy academy award is. we thus rank subgraphs that match this implicit structured query higher. to show the effectiveness of our retrieval model, we create a benchmark for keyword queries over two real world rdf datasets and use it to compare our ranking model to well known ir and db techniques forkeyword searchover structured data. empowering users to search rdf graphs usingkeywords only can increase the usability of such data sources. as an example, consider running the query comedy academyaward against therdf collectionintable. once candidate subgraphs have been retrieved, we need to rank them. therefore, ir style ranking models are crucially needed. the success of knowledge sharing communities like wikipedia and the advances in automatic information extraction from textual and web sources have made it possible to build large knowledge repositories such as dbpedia, freebase, and yago. these collections can be viewed as graphs of entities and relationships and can be represented as a set of subject property object triples in the semantic web data model rdf. queries can be expressed in thec endorsed sparql language or by similarly designed graph pattern search. in this paper, we propose a language model based approach to ranking the results of exact, relaxed and keyword augmented graph pattern queries over rdf graphs such as er graphs. our method estimates a query model and a set of result graph models and ranks results based on their kullback leibler divergence with respect to the query model. we demonstrate the effectiveness of our ranking model by a comprehensive user study. however, exact match query semantics often fall short of satisfying the users needs by returning too many or too few results. woody allen actedinresults woody allen directed hollywood ending. woody allen actedin hollywood ending woody allen directed stardust memories. woody allen actedin stardust memories woody allen directed manhattan. woody allen directedresults query woody allen actedin scoop. woody allen directed scoop woody allen wrote vicky cristina barcelona. woody allen directed vicky cristina barcelona woody allen wrote match point. these repositories typically contain entities such as people, locations, movies, companies, conferences, etc. our main contribution is a novel ranking model based on statistical language models forexact, relaxed, andkeyword augmented queries. they are based on generative probabilistic models for text features in documents. our ranking model, described in section #, constructs lms for the query and for each possible result graph, and ranks the results based on thekullback leibler divergence between the query and result graph lms. building entity relationship graphs has received considerable attention in the recent database, ir, and www literature. for example, information extraction techniques have been successfully applied to textual as well as semi structuredweb sources such as wikipedia, to build large scale knowledge repositories such as dbpedia, freebase, yago, and also communityspeci. and the relationships between them such as bornin, actedin, hasgenre, isceoof, ispcmemberof, and so on. such data conceptually forms a large graph with nodes corresponding to entities and edges denoting relationships, and it can be conveniently represented in the form of subject property object triples of the semantic web data model rdf. when triples are extracted from webpages, they can be associated with a variety of weights, including, extraction con dence, witness count, entity extraction con dence, etc. here rdf is again a convenient way of representing the wealth and diversity of data items such as user groups, friendships, annotations, ratings, etc. in contrast to traditional databases, there is not necessarily a prescriptive schema and the data providers humans strongly prefer a relaxed pay as you go approach. overall, rdf style er graphs nicely capture the entire spectrum from loose collections of data items to curated databases. searching these kinds of data sources enables capturing underlying semantics of the data; a task often dif cult to achieve with traditional web search. entities are enclosed in boxes and the relationships between them are indicated by directed edges. the same graph can beexpressedasasetofrdf triples, asshownintable we refer to an edge of the graph, together with its two nodes, as an er fact or, equivalently, as an spo triple. search on this kind of er rdf data is naturally expressed by means of structured graph pattern queries. based on a syntax similar to that of thec endorsed query language sparql, we can express the query as shown intable. this query consists of triple patterns with variable names starting with a question mark. patterns are combined by a logical conjunction, which is denoted by the dot. a result graph is a subgraph of the underlying knowledge graph which binds the variables and matches the constants in the query. solely using the expressive but boolean match sparql like languages is often too restrictive. users prefer seeing a ranked result list rather than a list of unranked matches. additionally, if the user is interested only in movies set in new york, the query could be augmented with keywords new york to promote, in the ranking, movies such as manhattan and demote movies such as hollywood ending. consider another example query asking for movies by woody allen in which both woody allen and scarlett johansson played roles. as shown in table #, an exact match would yield only one result for this query. but, there are many movies in which woody allen had multiple responsibilities and in which scarlett johansson had a role. for example, scarlett johansson acted in match point which woody allen wrote and directed. this result can be returned only when the original query is relaxed that is, an approximate match to the original query is allowed. these considerations suggest that it is desirable to have an irstyle ranking model for er graph languages like sparql. woody allen directed match point table #: example query and exact and approximate results shouldbe possibletoexpresskeyword conditions together withthe structured predicates of sparql patterns and approximate matches to the query should be allowed. this would be in analogy to prior work on xml ir which has enhanced xpath and xquery by various forms of text search and ranking capabilities. however, in contrast to xml trees, we now need to address the more dif cult setting of graphs and a potentially much higher structural and typing diversity. the latter also distinguishes our scope from prior work on keyword search over relational graphs. in this paper, we investigate triplepattern queriesandshowhowto augmentthemwithkeyword search. this can be seen as the sparql counterpart of xpath fulltext. lms are the state of the art foundation of modern ir and will be explained in section #. our new approach advances such models into the realm of rdf graphs and providesa seamless model forexact, relaxed andkeyword augmented graph structured queries consisting of triple patterns. we conducted a user study on two reallife datasets excerpts of imdb and librarything to show the quality of our search result rankings. the results of our user study are reported in section #. similar kinds of er graphs arise in web settings, for example, in social tagging communities such as librarything. for example, for the queryintable, well knownmovieslike hollywood ending or manhattan should precede less known movies like stardust memories. woody allen actedin manhattan table #: example query and unranked results woody allen actedin. a number of retrieval models incorporating term dependencies have recently been introduced. most of these modify existing bag of words retrieval models by including features based on the proximity of pairs of terms. although these term dependency models have been shown to be significantly more effective than the bag of words models, there have been no previous systematic comparisons between the different approaches that have been proposed. in this paper, we compare the effectiveness of recent bi term dependency models over a range of trec collections, for both short and long queries. to ensure the reproducibility of our study, all experiments are performed on widely available trec collections, and all tuned retrieval model parameters are made public. these comparisons show that the weighted sequential dependence model is at least as effective as, and often significantly better than, any other model across this range of collections and queries. we observe that dependency features are much more valuable in improving the performance of longer queries than for shorter queries. we then examine the effectiveness of dependence models that incorporate proximity features involving more than two terms. the results show that these features can improve effectiveness, but not consistently, over the available data sets. recent research has demonstrated that retrieval models incorporating term dependencies can consistently outperform benchmark bag of words models over a variety of collections. we de ne a dependency model here as any model that exploits potential relationships between two permission to make digital or hard copies of all or part of thiswork for personal or classroom use is granted without fee provided that copies arenot made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work ownedbyothers thanthe author must be honored. or more words to improve a document ranking. using a dependency model requires a query processing component and a scoring component. this can be done in a variety of ways, but the most common is to select words that satisfy some proximity relationship, such as being next to each other in the query. an alternative would be to use linguistic analysis to identify words that have speci. the scoring component of a dependency model modi es the scores of documents to take into account the presence of the selected query words in a speci ed relationship, such as satisfying a proximity or linguistic constraint. most of the best performing dependency models are based on proximity features and we focus on these models in this paper. although there have been a number of comparisons of dependency models to bag of words baselines, there has been surprisingly little comparison between these models. given the importance of dependency models, it is critical to provide comparisons and baselines that can be used to establish the effectivenessof new models, instead of showing an improvement compared to relatively weak bag of words baselines. in this study, we compare the effectiveness of recent retrieval models that use term dependencies and, in addition, study the impact of different proximity features. in the rst part of the paper, we describe a systematic comparison of state of the art dependency models that use features based on the proximity of pairs of terms. by using these collections, query sets, and open source software, our results can be easily reproduced and used as baselines or benchmarks in future studies. the parameters for each model are extensively tuned to maximize performance, and fold cross validation is used to avoid over tting some dependency models have been proposed recently that use proximity features involving more than two terms. we de ne a many term dependency as a set of three of more terms that are assumed to be dependent. the studies comparing manyterm dependency features to bi term features have been inconclusive, and providing more comprehensive evidence of the relative effectiveness of these proximity features is another goal ofthis paper. to do this, we compare the best bi term dependency models to both existing many term dependency models and models created by adding many term features to the best bi term dependency models. similar to the rst part of the paper, the parameters for each model are extensively tuned and fold cross validation is used. to ensure fair comparisons between the models, we restrict the process of selection or generation of term dependencies fromthe for space reasons, details of parameter settings and query folds are provided in a companion technical report. input query so that it does not rely on external information, or a pseudo relevance feedback algorithm. these restrictions ensure that each tested model has access to identical information, and computing resources, thus allowing the direct attribution of retrieval effectiveness improvements or degradations to differences in model formulation and the features used. further, these restrictions make these models widely applicable in many different information retrieval problems. results from our experiments show that the performance of all dependency models can be improved signi cantly through appropriate parameter tuning. this may not be a new or surprising conclusion, but the extent of the improvements possible is quitenoticeable, and there are many published results where this tuning does not appear to have been done. we also con rm the previous results showing that dependency models using bi termsconsistently improve effectiveness compared to bag of words models. the comparison between the bi term dependency models shows that the variant of the weighted sequential dependence model tested in this study exhibits consistently strong performance across all collections and query types. in regards to the comparisonbetween short and long queries, we observe that dependency features have more potential to improve longer queries than shorter queries. we also show that many term proximity features have the potential to improve retrieval effectiveness over the strongest bi term dependency models. however, more research, and probably more training data, is required to fully exploit these features. the major contributions of this study are: the rst systematic comparison of dependency models incorporating bi terms, acomparison of theeffectiveness of dependency models across short and long queries asystematiccomparison of many term dependency models, including new variations of bi term models tuned parameter settings for each tested retrieval model, for three standard information retrieval collections and querysets. each of the tested dependency models are discussed in section #. the experimental setup is described in section #. section # presents a detailed comparison of the retrieval models, and results and analysis from the investigation of manyterm dependency features. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. in query processing, groups of words that are potentially related are selected. for this comparison, we use a range of trec collections, including both short and long queries. query expansion is an important and commonly used technique for improving web search results. existing methods for query expansion have mostly relied on global or local analysis of document collection, click through data, or simple ontologies such as wordnet. in this paper, we present the results of a systematic study of the methods leveraging the conceptnet knowledge base, an emerging new web resource, for query expansion. specifically, we focus on the methods leveraging conceptnet to improve the search results for poorly performing queries. unlike other lexico semantic resources, such as wordnet and wikipedia, which have been extensively studied in the past, conceptnet features a graph based representation model of commonsense knowledge, in which the terms are conceptually related through rich relational ontology. such representation structure enables complex, multi step inferences between the concepts, which can be applied to query expansion. we first demonstrate through simulation experiments that expanding queries with the related concepts from conceptnet has great potential for improving the search results for difficult queries. we then propose and study several supervised and unsupervised methods for selecting the concepts from conceptnet for automatic query expansion. the experimental results on multiple data sets indicate that the proposed methods can effectively leverage conceptnet to improve the retrieval performance of difficult queries both when used in isolation as well as in combination with pseudo relevance feedback. the information needs of the searchers, concisely formulated as keyword queries, can greatly vary in complexity, which is determined by the number of concepts that constitute the need. for this reason, the quality of search results largely depends on how completely and accurately all the concepts constituting the information need are translated into the query terms. it is often the case that the users of web search systems tend to minimize their. ort by intentionally posing very short, under speci ed queries. as a result, many documents representing certain aspects of the informationneedwillbe missinginthesearch results. inadditiontothat, synonymygivesrise totheproblemof vocabulary mismatch, which occurs when the authors of relevant documents and the searchers use di erent terms to designate the same concepts. this problem arises particularly often when non professional users perform domain speci. searches and are not closely familiar with the vocabulary of the domain of the search problem. the most common examples of such domains are legal and medical. query expansion is a standard technique allowing to mitigate the problems of di ering vocabularies and partially speci ed information needs by selecting and adding the related terms and phrases to the initial query. ective application of automatic query expansion lies in correct identi cation of underrepresented aspects of the information need and selecting the right expansion terms with the right weights. typical sources of term associations for query expansion can be either static and exist at the time of query or dynamic, such as the topranked initially retrieveddocuments, from which the expansion terms can either be selected automatically by the system through pseudo relevance feedback or by asking the users to designate the relevant documents through explicit relevance feedback. all approaches using the dynamic sources of expansion terms rely on the assumption that the initially retrieved results include some relevant documents, which canbeusedasasourceof expansion terms. itisoften the case, however, that the top ranked search results for a query include very few or no relevant documents and neither the search systems nor the users can communicate the positive relevance signals back to the search system through feedback mechanisms. such queries are often referred to as di cult for aparticularsearch systemand underspeci ed queries as well as vocabularymismatch are some ofthe main reasons for search systems failures. in most cases, the users are unaware of the underlying problems of poorly performing queries, the search systems currently. er no support to them in tryingto improve the search results. althoughmany users are aware that the quality of search results can be improved by reformulating a query, nding the right query formulationcanbeafairlydi cultandtimeconsumingprocess. while some of the static sources of expansion terms, such as the query logs and statistical co occurrence thesauri constructed through the global collection analysis, allow to avoid the dependence on the quality of initial results, the coverage of these resources is limited and they may simply not contain. ective expansion terms, which are broadly or conceptually related to a particular query. in this work, we systematically and comprehensively explore the potential of concept feedback, a set of di erent strategies for leveraging conceptnet as a source of expansion terms for di cult queries. conceptnet is presently thelargest commonsenseknowledgebase, consisting of more than millionassertionsabouttheworld. similarto wikipedia, conceptnet re ects the wisdom of the crowds and was constructed by gathering a large number of sentencelike assertions about the real world from a large number of on line collaborators. conceptnet uses semantic network as a knowledge representation model. the nodes in its semantic network correspond to semi structured natural language fragments and represent the real world concepts. an edge between the two nodes represents a semantic relationship between thetwoconcepts. a fragmentof theconceptgraph of conceptnet is shown in figure #. as opposed to ontologies, such as wordnet, conceptnet is not limited to hyponym hypernymrelationsandfeaturesa morediverserelational ontology of twenty relationship types, such as causal, spatial and functional. as opposed to on line encyclopedias, such as wikipedia, the network structure of concept net does not require any additional analysis to establish the relations between the concepts. the network based structure of conceptnet in combination with its rich relational ontology opens up possibilities for making more complex, multi step inferences. for example, fromfigure it follows that theconcepts morning and stomach are related via the following chain of inferences: wake up in the morning. the key idea behind this work is that the network based structure of conceptnet can be leveraged to make similar complex inferences to identify the. ective expansion terms that are broadly related to the query, when the initially retrieved results are ofpoor quality and, consequently, cannot be used as a source of expansion terms. although other lexico semantic resources, such as wordnet, can be used http: conceptnet media mit edu figure #: fragment of the concept graph of conceptnet to help address the issue of vocabulary divergence between the queries and relevant documents,ective expansion terms may have much broader conceptual relations to the query terms than the tight semantic coherence of wordnet synsets may allow. table #: top expansion concepts from concept net for the trec query airportsecurity along with the average precision of the query expanded with each of the concepts, when the expanded queries are run on the aquaint dataset. figure # provides an example of the expansion concepts fromconceptnetidenti edusingthesimulation experiment presented in section #. as can be seen from this example, some. ective expansionconceptshaveverybroad and complex semantic relationships with the original query terms. establishing such relationships may require making several inference steps, for which the graph based knowledge representation modelofconceptnetisparticularlywell suited. on the other hand, the hierarchical structure of wordnet and vector space knowledge representation models for wikipedia presentcertaindi cultiesfor making complexand multi step inferences. in this work, we explore di erent methods of leveraging the semantic network of conceptnet to select a small set of highly. ective expansion concepts to improve theperformance of di cult queries. in particular, we address the following two research questions. the rst question is whether conceptnetcaninprinciplebeleveragedtoimprovethe retrieval results of di cult queries to answer this question, we conducted a simulation experiment, in which for each query we measured the retrieval performance of that query extended with each single concept within a certain distance from its terms. the retrieval results of the best performing expansion concept for each query were used to determine the overall upper bound for the potential retrieval. ectiveness of concept feedback on each data set. the results of this experiment are presented in section #. the second question is how to design the methods to automatically select a small number of. ective expansion concepts to answer thisquestion, insections and wepropose the heuristic and machine learning based methods for selecting expansionconceptsandpresenttheresultsof an experimental evaluation of those methods in section #. concept ap target threat machine explosive tighten afghanistan worldwide chain measure link the maincontributionofthepresentworkisinsystematic and comprehensive exploration of the heuristic and machine learning based methods for selecting the query expansion terms from conceptnet and comparing the. ectiveness of automatic query expansion based on pseudo relevance feedback. in this work, we study a new text mining problem of discovering named entities with temporally correlated bursts of mention counts in multiple multilingual web news streams. mining named entities with temporally correlated bursts of mention counts in multilingual text streams has many interesting and important applications, such as identification of the latent events, attracting the attention of on line media in different countries, and valuable linguistic knowledge in the form of transliterations. while mining bursty terms in a single text stream has been studied before, the problem of detecting terms with temporally correlated bursts in multilingual web streams raises two new challenges: correlated terms in multiple streams may have bursts that are of different orders of magnitude in their intensity and bursts of correlated terms may be separated by time gaps. we propose a two stage method for mining items with temporally correlated bursts from multiple data streams, which addresses both challenges. in the first stage of the method, the temporal behavior of different entities is normalized by modeling them with the markov modulated poisson process. in the second stage, a dynamic programming algorithm is used to discover correlated bursts of different items, that can be potentially separated by time gaps. we evaluated our method with the task of discovering transliterations of named entities from multilingual web news streams. experimental results indicate that our method can not only effectively discover named entities with correlated bursts in multilingual web news streams, but also outperforms two state of the art baseline methods for unsupervised discovery of transliterations in static text collections. the vast amounts and availability of textual data constantly generated on the web open up many possibilities for exploring new interesting data miningproblems. most existing research on text stream mining has focused on mining single text streams. web data, however, is naturally generated by a large number of streams, and hence there exist many unique webspeci cproblems, which require takingintoaccount complex interactions and dependencies in the behavior of multiple streams. one such problem is, given a collection of multilingual textual streams in the form of the on line news documents providedbytherssfeedsofnewsagenciesindi erent countries, discover named entities, which exhibit similar pattern in the form of temporally correlated bursts of their mentions in the documents produced by the streams. we de ne the burst of an entity as a sudden and sharp increase in the total numberof itsoccurrencesinthedocumentsgeneratedby allthe web news streams in the same language. temporally correlatedbursts are simultaneousbursts of terms in di erent text streams which consistently occur at similar points or intervals of time. in general, bursty terms in text streams are interesting to study, since they often signal changes in some latent variable. for example, a sudden increase of mentions of a particular named entity in the news streams is generally correlated with the happening of a particular event. speci cally, whena majorinternationaleventhappens, news streams tend to frequently mention certain named entities associated with it, leading to temporally correlated bursts of related entities. if we can discover named entities with correlated bursts from textual streams in di erent natural languages, we will be able to group semantically related named entities in di erent languages together and potentially discover transliteration relations ofproper names in an unsupervised way. since proper names grow in an open ended way, it is hard to manually create an exhaustive list of transliterations for all possible proper names. therefore, methods to automatically mine such transliterations can be very useful, particularly for cross language informationretrieval and machine translation. in addition to that, the vastness of the webdata ensuresthecoverageof transliterationsinallpossible domains, including the new and emerging ones, that no dictionary can guarantee. therefore, terms with temporallycorrelatedburstsof mentioncountsnot onlyconstitute valuable knowledge by themselves, but can also be used to identify latent events that cause the change in the behavior of multiple text streams. moreover, entities with temporally correlated bursts can also reveal how particular events are represented in di erent languages. for example, by knowing the entities that occur in the news wires of di erent countries much more frequently than usual during a particular time point orinterval, onecannot onlydiscoverthe major events happening at that moment, but also di erentiate between the local events, which are important only to one country, and global events, which attract the interest of the media from di erent countries. in addition to that, the social impact of a real life event can be estimated by the strength and duration of the bursts of certain named entities that it causes. example, the nomination of sarah palin as a republican vice presidential candidate during the year# presidential election campaign has caused intense and long term bursts of mention counts of the term palin in the. news streams, but in other countries this event was only brie. while mining bursty terms in a single text stream has been studied previously, mining terms with correlated bursts from multiple web text streams raises three interesting new challenges: di erence in burst magnitude: correlated entities in multiple multilingual streams may have bursts that are of di erent orders of magnitude in their intensity in streams, corresponding to di erent languages. for example, the bursts of terms related to a major. event are likely to be several orders of magnitude higher in the. news media than the bursts of the corresponding entities in, for example, the russian media. thus, using raw mention counts of terms will likely produce inaccurate results. this paper develops a general, formal framework for modeling term dependencies via markov random fields. the model allows for arbitrary text features to be incorporated as evidence. in particular, we make use of features based on occurrences of single terms, ordered phrases, and unordered phrases. we explore full independence, sequential dependence, and full dependence variants of the model. a novel approach is developed to train the model that directly maximizes the mean average precision rather than maximizing the likelihood of the training data. ad hoc retrieval experiments are presented on several newswire and web collections, including the gov collection used at the trec year# terabyte track. the results show significant improvements are possible by modeling dependencies, especially on the larger web collections. there is a rich history of statistical models for information retrieval, including the binary independence model, language modeling, inference network model, and the divergence from randomness model, amongst others. it is well known that dependencies exist between terms in a collection of text. for example, within a sigir permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. proceedings, occurrences of certain pairs of terms are correlated, such as information and retrieval. the fact that either one occurs provides strong evidence that the other is also likely to occur. unfortunately, estimating statistical models for general term dependencies is infeasible, due to data sparsity. for this reason, most retrieval models assume some form of independence exists between terms. some researchers even suggest modeling term dependencies is unnecessary as long as a good term weighting function is used. most work on modeling term dependencies in the past has focused on phrases proximity or term co occurrences. most of these models only consider dependencies between pairs of terms. in, fagan examines how to identify and use non syntactic phrases. he identi es phrases using factors such as the number of times the phrase occurs in the collection and the proximity of the phrase terms. his results suggest no single method of phrase identi cation consistently yields improvements in retrieval. ectiveness are achieved when phrases are de ned as any two terms within a query or document with unlimited proximity. that is, any two terms that co occurred within a query or document were considered a phrase. however, for other collections, this de nition proved to yield marginal or negative improvements. in on the cacm collection suggest similar results, where phrases formed with a probabilistic and operator slightly outperformed proximity phrases. term co occurrence information also plays an important role in the tree dependence model, which attempts to incorporate dependencies between terms in the bim. the model treats each term as a node in a graph and constructs a maximum spanning tree over the nodes, where the weight between a pair of terms is the expected mutual information measure between them. other models have been proposed to capture dependencies between more than two terms, such as the bahadur lazarsfeld expansion, which is an exact method of modeling dependencies between all terms, and the generalized dependence model that generalizes both the tree dependence model and the ble expansion. despite the more complex nature of these models, they have been shown to yield little or no improvements in. several recent studies have examined term dependence models for the language modeling framework. these models are inspired by the tree dependence model and again only consider dependencies among pairs of terms. al in showed consistent improvements over a baseline query likelihood system on a number of trec collections. unfortunately, the model requires computing a link structure for each query, which is not straightforward. recently, a paper by mishne and de rijke discussed the use of proximity information to improve web retrieval. our model, developed without knowledge of this work, shares many closely related insights. despite the high level similarity, the details of the models di er greatly, with our model allowing more general query dependencies and features to be considered in a more formally well grounded framework. are in stark contrast to the results reported historically for other term dependence models. it is our hypothesis that the poor and inconsistent results achieved in the past are caused by two factors. first, most dependence models are built upon the bim. therefore, term dependencies must be estimated in both the relevant and non relevant classes, where there is often a very small or non existent sample of relevant nonrelevant documents available to estimate the model parameters from. second, the document collections used in past experiments, such as cacm and inspec consist of a very small number of short documents. there is very little hope of accurately modeling term dependencies when most pairs of terms only occur a handful of times, if at all. essentially, the models themselves are not de cient. instead, they simply lacked su cient data for proper parameter estimation and evaluation. thus, we formulate the following two hypotheses: dependence models will be more. ective for larger collections than smaller collections, and incorporating several types of evidence into a dependence model will further improve. our rst hypothesis is based on the fact that larger collections are noisier despite the fact they contain more information. as a result, independent query terms will match many irrelevant documents. if matching is instead done against more speci. patterns of dependent query terms, then many of the noisy, irrelevant documents will be ltered out. in addition, we feel that considering various combinations of term features can yield further improvements over the previously researched methods, as it allows us to abstract the notions of dependence and co occurrence. the rest of this paper is presented as follows. in section # we describe a general markov random eld retrieval model that captures various kinds of term dependencies. we also show how arbitrary features of the text can be used to generalize the idea of co occurrence, how the model can be used to express three di erent dependence assumptions, and how to automatically set the parameters from training data. in section # we describe the results of ad hoc retrieval experiments done using our model on two newswire and two web collections. we show that dependence models can significantly improve. finally, in section # we summarize the results and propose future research directions. this paper investigates the pre conditions for successful combination of document representations formed from structural markup for the task of known item search. as this task is very similar to work in meta search and data fusion, we adapt several hypotheses from those research areas and investigate them in this context. to investigate these hypotheses, we present a mixture based language model and also examine many of the current meta search algorithms. we find that compatible output from systems is important for successful combination of document representations. we also demonstrate that combining low performing document representations can improve performance, but not consistently. we find that the techniques best suited for this task are robust to the inclusion of poorly performing document representations. we also explore the role of variance of results across systems and its impact on the performance of fusion, with the surprising result that the correct documents have higher variance across document representations than highly ranking incorrect documents. known item finding is an important information seeking activity that has recently gained some attention in the information retrieval community. in this task the user knows of a particular document, but does not know where it is. example document permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. types may be a web page, a report, or a normal text document. very often, these documents have structural markup, such as html. we believe that by forming a variety of document representations using this structural information and combining these representations during retrieval, systems can improve retrieval performance over using the single best document representation. one natural approach to combining document representations can be borrowed directly from the meta search problem. the goal in meta search is to combine the results from different search engines to produce a single ranked list that is better than the results of any single search engine. this is sometimes referred to as data fusion. combining document representations is similar to creating a search engine for each of the document representations and performing meta search to combine the result lists. an alternative to meta search techniques is to use the document representations to modify term weights directly within a single search engine. we present a technique for this using generative language modeling. rather than using the language model estimated from a single document representation, this approach estimates a mixture language model based on a combination of language models created from the various document representations. there has been extensive work on studying the effectiveness of meta search and the conditions for success. aslam and montague summarize this work by stating: the systems being combined should have compatible output, each produce accurate estimates of relevance, and be independent of each other. this paper investigates whether these hypotheses should also extend to the task of combining document representations for known item finding. the first hypothesis can be directly applied to known item finding. we investigate this hypothesis of score compatibility using okapi and language modeling retrieval systems across different document representations. croft describes score compatibility as systems having comparable output in that they are trying to make the same decision within the same framework. within okapi and language modeling systems, croftstatements hold true when combining document representations. however, it is difficult to recover the probability estimates from the ranking function used by okapi. this may introduce some incompatibilities across the document representations. to investigate this, we define a measure of compatibility that compares the shape of the normalized ranking functions across document representations. the second hypothesis does not directly apply, as the notion of relevance used in ad hoc retrieval is different from the goal of known item finding. in known item finding the goal is not to exhaustively find documents about a topic, but to find a single correct document. a more appropriate hypothesis in this setting would be that the document representations tend to give higher weight to the correct document than to incorrect documents. even so, we would like an approach that is robust to the inclusion of representations that only sometimes give high score to the correct document. for example, a document representation of image alternate text in html may do very well sometimes at finding the correct document, but will tend to do rather poorly in general. the ideal approach would be robust to errors in the poorly performing representations, but would also be able to leverage the representation when the correct answer is found. we investigate the quality of representation hypothesis and also investigate whether any of the approaches can gain from poorly performing document representations. the third hypothesis states that the systems should be independent of each other. that is, the systems should give high scores different sets of non relevant documents than each other, but there should be a higher correlation among the relevant documents. applied to known item finding, an appropriate hypothesis is that the representations would give widely varying scores for the incorrect documents, but tend to score the correct document highly, with lower variance. in this paper, we explore these hypotheses using both meta search algorithms and an approach motivated by language modeling. section # describes related work and the techniques used in this paper. section # describes our experimental methodology, evaluation, and system details. information retrieval approaches for semantic web search engines have become very populars in the last years. popularization of different ir libraries, like lucene, that allows ir implementations almost out of the box have make easier ir integration in semantic web search engines. however, one of the most important features of semantic web documents is the structure, since this structure allow us to represent semantic in a machine readable format. in this paper we analyze the specific problems of structured ir and how to adapt weighting schemas for semantic document retrieval. keyword based semantic web search engine development has become a major research area garnering much attention in the semantic web community over the last seven years. research initiatives target not only simple semantic document and ontologies retrieval but explore the development of gateways supporting the implement semantic web applications. semantic search engines represent the semantic web communitycontribution to semantic services and are important for moving forward semantic web research. current practice with semantic web search queries frequently involve sparql language expressions. however, exact match query semantics, rendered with these approaches, generally retrieve too many or too few results. the approach, therefore falls short and fails to satisfy the userneeds. one potential approach to addressing these shortcomings is to integrate the use of an ir style ranking model based on keywords. in other words, exact match similarity searching might be improved by integrating known statistical algorithms, although research exploring this method is extremely limited. information retrieval plays a very important role in the design and implementation of nearly any if not all digital information systems. however, most of the times, the ir techniques used are basic, out of the box and do not really improve the performance of semantic search engines. uned vfresno lsi uned es in other words the perceived needed algorithmic changes are mainly implemented to solve. as a result, little work has been conducted to adapt weighting schema that might improve the relevance of the results. current keyword based semantic search engine work in the semantic web domain needs to consider how to take advantage of rdf structure. document structure has been very useful for traditional web search engines like google or yahoo, since the relevance of the documents for a given a query will be di erent depending where query terms occur. information about document structure can be used to boost terms that occur in any speci. for example, documents terms occurring in eld title might be more important than terms occurring in, say, the third paragraph. these heuristics have been applied with success to optimize web search engines. the ir community has been working on structured documents from. in fact, this early work supports more recent developments like the initiative for evaluation of xml retrieval competition, where di erent xml collections have been used to evaluate the performance of ir systems when the structure of the document is available. the amount of rdf data has been growing steadily over the last ve years thanks of web of data proposed by, and a good corpus of rdf semantic encoding is, in fact, available via the web. at the same time, the general increase in web of data aggravates retrieval performance, and, in many ways, highlights a need to explore means for improving traditional web search engines. if researchers are going to advance semantic web search, it seems not only reasonable, but imperative that they consider how rdf and encoded document structure might be used to determine weighting and assist the information retrieval ranking functions. the research presented in this paper considers this problem, specifically to best way to ingrate rdf encoded document structure and information retrieval ranking algorithms to improve the current state of keyword based semantic web search engine development. the three research questions guiding this research are: can semantic structure improve quality results in terms of relevance what is the behavior of ir ranking functions when we use semantic information http: www inex otago ac nz what features have to have a corpus for semantic search evaluation we will try to answer these questions in the next sections of this paper. the remainder of this paper is organized as follows: section # brie. reviews the status of rdf and semantic web search, and presents methods to create an rdf inverted index using indexing practices based on links. section # reviews structured ir and ranking functions for structured documents; and present a method of addressing problems in lucene weighting schema. section # reviews some approaches to semantic web search evaluation. in this section we present also our corpus for semantic search evaluation combining dbpedia inex collection, and, nally we review the evaluation metrics used in this paper. section # presents the results and discussion, and section # includes the conclusion and next steps. semantic search refers to a loose set of concepts, challenges and techniques having to do with harnessing the information of the growing web of data for web search. here we propose a formal model of one specific semantic search task: ad hoc object retrieval. we show that this task provides a solid framework to study some of the semantic search problems currently tackled by commercial web search engines. we connect this task to the traditional ad hoc document retrieval and discuss appropriate evaluation metrics. finally, we carry out a realistic evaluation of this task in the context of a web search application. in recent years the eld of web search has diversi ed, bringing new challenges beyond the traditional text based search problem studied by information retrieval researchers. among these new paradigms is the eld of semantic search, in which knowledge bases are used as the primary information source for search, or as a complement to text retrieval. the last two years in particular has seen an increase both in the number of knowledge bases published as linked data and in the availability of metadata embedded inside web pages, thanks to the support for metadata standards by major commercial search engines. www year#, april, year#, raleigh, north carolina, usa. the increase of size in the data that is handled rst led to signi cant research on scalable indexing techniques with much input from the database community and at the same time put a focus on the ranking of results. ranking is also motivated by a growing number of end user application scenarios where queries are given by ordinary users as keywords, and not in formal query languages such as sparql. despite the recent surge in semantic search research, there has been little work focusing on principled evaluation techniques for assessing the. components of semantic search technologies, as well as compare one system to another, a common evaluation methodology is needed. to semantic search has also been identi ed by the participants of the semantic search workshop series as one of the major stumbling blocks to future development of this research area. most current approaches to evaluating semantic search systems are adaptations of document evaluation techniques from the information retrieval community. in this setting, semantic search systems ultimately perform document retrieval, and the quality of documents returned is used as a metric of the quality of the entire system. these results make it di cult to interpret how well a semantic search system functions internally. we de ne the ad hoc object retrieval task for evaluating semantic search systems on the web of data. the task is based on answering arbitrary information needs related to particular aspects of objects, expressed in unconstrained natural language and resolved using a collection of structured data. we also explain the problems in mapping the current methodology for ad hoc document retrieval to ad hoc object retrieval. we analyse a real world web query log from a semantic search point of view, and propose a semantic search query classi cation based on this analysis. we propose a methodology for ad hoc semantic search evaluation. along the way, we discuss many of the di cult decisions that need to be made when designing a semantic search evaluation framework and the http: googlewebmastercentral blogspot com year# introducing rich snippets html figure #: overview of the semantic search process. a keyword query is used to identify relevant web objects. relevant objects are ranked, and for each object a set of connected objects is selected as the result. impact those decisions can have on the quality of the evaluation and the reusability of human judgments. we deploy our proposed evaluation methodology on a real world data set and query workload. we show experimentally that our proposal is stable for some popular evaluation metrics, and that it can reliably distinguish among the. background it is common to call any information retrieval system a semantic search system if it performs the matching of queries and potential results at a conceptual level, ie, by processing information beyond the surface forms of symbols. unfortunately, this de nition is hard to operationalize when it comes to semantic search evaluation, because it leaves open the type of queries processed by the system, the format of the content and the internal knowledge representation paradigm applied within the search system. to arrive at a comparable subset of search systems we will have to restrict the above de nition to systems that retrieve data from a knowledge base containing rdf data. rdf is the core part of the semantic web stack and de nes the abstract data model for the semantic web in the form of triples that express the connection between web resources and provide property values describing resources. broadly speaking, the goal of retrieval in this context is to retrieve parts of the knowledge base that best match the query entered by the user. we will use each of these notations in our discussion depending on the context. given the structured nature of the data, semantic search engines also bring new query capabilities to search. in practice, not all features of any structured query language would be equally necessary to perform common tasks. further, we expect that users will be limited by the capabilities of the input interface; in particular, we expect keyword queries to remain the prevalent form of input. to establish a realistic evaluation that covers the most commonly occurring query types, we will perform a study of keyword queries in a web search query log from the perspective of the semantic query functionality required to resolve the query. this will lead us to a simpli ed taxonomy of web queries as well as a measure of the commonality of each type of query, allowing semantic search engines to be evaluated on realistic work loads. relevant objects are ranked based on their content and relationships to other objects. finally, each object is explained by expanding a set of connected objects to provide the nal result. organization the remainder of the paper is organized as follows. in section # we outline the ad hoc object retrieval task and discuss the challenges and choices made in adapting the ad hoc document retrieval task. section # reports on the results of an analysis of a web query log which is then used to conduct an evaluation of a base line object ranking algorithm using our proposed evaluation task. in section # we contrast our approach with related work. we conclude in section # and discuss future work. distribution of these papers is limited to classroom use, and personal use by others. conservative estimates put the amount of structured data on the web at over billion triples today. note that resources in rdf describe objects in a web of data. since the semantic web is built upon such data models, it is also equivalent to think of these objects as semantic web entities. figure # illustrates a generic architecture for semantic search systems that perform object retrieval for keyword queries. in this setting, a keyword query is used to identify relevant web objects occurring in the wod. this paper describes a simple way of adapting the bm ranking formula to deal with structured documents. in the past it has been common to compute scores for the individual fields independently and then combine these scores to arrive at a final score for the document. we highlight how this approach can lead to poor performance by breaking the carefully constructed non linear saturation of term frequency in the bm function. we propose a much more intuitive alternative which weights term frequenciesbeforethe non linear term frequency saturation function is applied. in this scheme, a structured document with a title weight of two is mapped to an unstructured document with the title content repeated twice. this more verbose unstructured document is then ranked in the usual way. we demonstrate the advantages of this method with experiments on reuters vol and the trec dotgov collection. textual data is most often found in a structured form; for example, documents are often subdivided into elds such as title, author, abstract, body, etc. practitioners have found it bene cial to exploit the documentinternal structure to improve retrieval performance. although there has been a number of ir frameworks proposed for this, their complexity and radical departure from standard ranking algorithms renders their application di cult. in practice, many systems exploit structure in an ad hoc manner, permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. by implementing a linear combination of the scores obtained from scoring every eld. this however can be quite dangerous for several reasons, the most important one being the nonlinear treatment of term frequencies by ranking functions. in this paper we discuss these dangers in detail and propose a very simple solution to extend standard ranking functions to multiple weighted elds. the idea is to compute a single score for a linear combination of term frequencies, instead of combining the scores. the approach is so simple that it may not merit a paper on the subject; however, we were encouraged to write it after seeing many papers following the more dangerous linear combination of the scores. the proposed approach has many advantages, from the point of view of simplicity, interpretation, speed of computation and reduction of the number of parameters; furthermore it yields higher performance for the tasks considered in this paper. in section # we discuss in some detail the dangers of using a linear combination of scores. in section #, we discuss the problem of repeatable elds. section # reports on some experiments with the two methods in two test collections. prior art much work on structured document retrieval deals with the combination of eld scores, decoupling the problem from the scoring function itself. much of the discussion about combining scores for the same document has taken place in the context of meta search systems, where the information available to the combining engine may not include any details about exactly which query components contributed to which score. in these circumstances, some form of post hoc combination of scores is required; this may be more complex than a simple linear combination. ogilvie and callan give a good overview of the issues, and propose and test various combinations. one of the earlier empirical studies of eld weighting is the work of wilkinson where he evaluates di erent ways to weight and combine the scores obtained on the di erent elds of a document. a few authors have developed more formal frameworks to combine information from structured documents in a principled manner. for example, lalmas exploits the theory of evidence as a framework for information aggregation. myaeng extended the inquery model to incorporate structural information. more recently piwowarski has proposed the use of bayesian networks for this purpose; his paper gives a good review of work on this area. however for the most part ad hoc retrieval systems apply standard ranking algorithms and tackle the problem of structure by combining in some way the scores obtained from the di erent elds, in a similar manner to wilkinson. as examples, we could cite many systems developed for the trec web track, systems dealing with dbms aspects of structured ir and systems for xml retrieval. in the case of the trec web track, each of the following papers includes at least one linear combination of scores obtained from different text elds: the aim of this paper is not to propose a comprehensive approach to structured document retrieval, but rather to point out an elegant and simple alternative to score combinations. much closer to the spirit of this paper, ogilvie and callan discuss combining document representations from di erent sources in the language model, comparing it to score combination with the probabilistic model. this is essentially what we do in this paper for a wider range of ranking algorithms, by concentrating on transforming the statistics of the document collection rather than re parametrising a speci. information retrieval researchers commonly use three tests of statistical significance: the student pairedtest, the wilcoxon signed rank test, and the sign test. other researchers have previously proposed using both the bootstrap and fisher randomization test as non parametric significance tests for ir but these tests have seen little use. for each of these five tests, we took the ad hoc retrieval runs submitted to trecs and, and for each pair of runs, we measured the statistical significance of the difference in their mean average precision. we discovered that there is little practical difference between the randomization, bootstrap, andtests. both the wilcoxon and sign test have a poor ability to detect significance and have the potential to lead to false detections of significance. the wilcoxon and sign tests are simplified variants of the randomization test and their use should be discontinued for measuring the significance of a difference between means. a chief goal of the information retrieval researcher is to make progress by nding better retrieval methods and avoid the promotion of worse methods. given two information retrieval systems, how can we determine which one permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. is better than the other a common batch style experiment is to select a collection of documents, write a set of topics, and create relevance judgments for each topic and then measure the. ectiveness of each system using a metric like the mean average precision. we know that there is inherent noise in an evaluation. the assessors hired to judge relevance of documents are human and thus open to variability in their behavior. and nally, the choice of document collection can. we want to promote retrieval methods that truly are better rather than methods that by chance performed better given the set of topics, judgments, and documents used in the evaluation. statistical signi cance tests play an important role in helping the researcher achieve this goal. a powerful test allows the researcher to detect signi cant improvements even when the improvements are small. an accurate test only reports signi cance when it exists. an important question then is: what statistical signi cance test should ir researchers use we take a pragmatic approach to answering this question. if two signi cance tests tend to produce nearly equivalent signi cance levels, then to the researcher there is little practical di erence between the tests. while the underlying fundamentals of the tests may be very di erent, if they report the same signi cance level, the fundamental di erences cease to be practical di erences. using the runs submitted to ve trec ad hoc retrieval evaluations, we computed the signi cance values for the studentpaired, wilcoxon signed rank, sign, shifted bootstrap, and randomization tests. comparing these signi cance values we found that: student, bootstrap, and randomization tests largely agree with each other. researchers using any of these three tests are likely to draw the same conclusions regarding statistical signi cance of their results. the wilcoxon and sign tests disagree with the other tests and each other. for a host of reasons that we explain, the wilcoxon and sign tests should no longer be used by ir researchers. we also came to the following conclusions as part of our study: a test should test the same statistic that a researcher reports. thus, thetest is only appropriate for testing system a system average precision average precision figure #: the distribution of average precision scores for two example ir systems submitted to trec. the mean average precision of sys tem a is and the map of systemis. both the randomization and bootstrap can use any statistic. based on the tests various fundamentals, we recommend the randomization test as the preferred test in all cases for which it is applicable. other researchers have studied the use of signi cance tests as part of ir evaluation, but we know of no other work that looks at all of these tests or takes our pragmatic, comparative approach. retrieving semi structured entities to answer keyword queries is an increasingly important feature of many modern web applications. the fast growing linked open data movement makes it possible to crawl and index very large amounts of structured data describing hundreds of millions of entities. however, entity retrieval approaches have yet to find efficient and effective ways of ranking and navigating through those large data sets. in this paper, we address the problem of ad hoc object retrieval over large scale lod data by proposing a hybrid approach that combines ir and structured search techniques. specifically, we propose an architecture that exploits an inverted index to answer keyword queries as well as a semi structured database to improve the search effectiveness by automatically generating queries over the lod graph. experimental results show that our ranking algorithms exploiting both ir and graph indices outperform state of the art entity retrieval techniques by up to over the bm baseline. many modern websites, such as web portals or news aggregators, are today including entity centric functionalities. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. common examples include the aggregation of all pieces of content related to a given person, or the extraction of the most important entities appearing in a given article. some companies, like the new york times, manually maintain a directory of entities and ask human experts to create links between their resources and the corresponding entities. increasingly, however, websites are turning to automated methods due to the sheer size of the resources they have to analyze, and the large number of entities they have to consider. recently, the linked open data movement started an. ort to make entity data openly available on the web. in this initiative, uniform resource identi ers are used to identify entities. each entity can be looked up online, where it is described and linked to further entities using the resource description framework. the fundamental di erence between lod and standard entity datasets like, for instance, wikipedia, lies in the inherent structure of the data. on the lod cloud, the data is provided by uncorrelated parties and is given as a giant graph of semi structured data. in the context of online entity search, the trec entity track has studied two related search tasks: related entity finding and entity list completion. the semsearch challenge focused on ad hoc object retrieval, that is, nding the entity identi er of a speci. this paper focuses on ad hoc object retrieval over semistructured data. we propose a novel search architecture that exploits both ir and graph data management techniques to. speci cally, we propose a hybrid solution that starts by retrieving an initial list of results from an inverted index using a ranking function, and then re ranks and extends the result list by exploiting the underlying graph representation of the data. our extended experimental evaluation performed over standard collections shows that our proposed solution signi cantly improves the. ectiveness while maintaining very low query execution times. we consider our results as especially promising since the lod test collections that are http: linkeddata org http: tools ietf org html rfc http: www org rdf http: semsearch yahoo com today available are noisy and incomplete, and since we expect both the quality and the coverage of lod datasets to rapidly improve in the future. in summary, the main contributions of this paper are: a comparison of entity search techniques for aor tasks including state of the art ir techniques such as query extension and pseudo relevance feedback. a new hybrid search architecture for aor that combines ir and graph data management techniques exploiting the graph structure of the underlying data. a new, fairer and continuous evaluation methodology for relevance assessment based on iterative crowsdsourcing. an extensive evaluation of our novel search system for aor using two standard test collections. the rest of the paper is structured as follows. we discuss related work, focussing on entity search and aor approaches, in section #. introduce the linked open data movement in section #. section # is devoted to a high level description of our hybrid system. we describe several approaches for aor based on inverted indices and nlp techniques in section #, and describe complementary techniques based on structured queries and graph data management techniques in section #. in section #, we experimentally compare the performance of our hybrid approach to a series of state of the art aor approaches on two standard test collections. finally, we present our conclusions in section #. knowledge bases and the web of linked data have become important assets for search, recommendation, and analytics. however, question answering technology does not work robustly in this setting as questions have to be translated into structured queries and users have to be careful in phrasing their questions. this paper advocates a new approach that allows questions to be partially translated into relaxed queries, covering the essential but not necessarily all aspects of the user input. to compensate for the omissions, we exploit textual sources associated with entities and relational facts. our system translates user questions into an extended form of structured sparql queries, with text predicates attached to triple patterns. our solution is based on a novel optimization model, cast into an integer linear program, for joint decomposition and disambiguation of the user question. we demonstrate the quality of our methods through experiments with the qald benchmark. natural language questions are a user friendly mode of tapping this wealth of knowledge and data. getting this translation right is all but an easy task. we still face the problem that the noun phrases and verbal phrases of the input are highly ambiguous and can be mapped to di erent semantic targets. motivation: with the success of ibmwatson system, natural language question answering is being revived permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than the author must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. cikm year# san francisco, california, usa acm year#. as a key technology towards coping with the deluge of digital contents. watson primarily tapped into textual contents, with limited use of structured background knowledge. however, there is a strongly growing wealth of structured data on the web: large knowledge bases like dbpedia org, freebase com, and yago knowledge org; billions of inter linked rdf triples from these and other sources, together forming the web of linked data; hundreds of millions of web tables; and a large amount of rdf style microdata embedded in html pages. ectively searching this wealth of structured web data, speci cally, the rdfbased world of linked data. although there are structured query languages like sparql that could, in principle, be used to this end, such an approach is impractical for several reasons: users are not familiar with the diverse vocabulary of the data, ii the data itself exhibits high heterogeneity so that even a power user would struggle with formulating the right queries, iii even a perfectly formulated query may fail to nd answers if the query vocabulary does not match the data vocabulary. to bridge the gap between users information needs and the underlying data and knowledge, it has recently been proposed to take natural language questions as user input and automatically translate these into structured queries, for example, in the sparql language. example: consider the question which music bands covered songs written by the rolling stones, or in conveniently short form: bands covering songs by the stones, or in even more telegraphic style: bands songs stones. an ideal qa system for rdf data would automatically translate this user input into a sparql query with the following triple patterns: type musicband. whereandare variables, and the binding foris the query answer. moreover, even this seemingly perfect query may not work if the underlying data di ers from the vocabulary and structure of the query. for example, the class song may not be su ciently populated; instead the data could make more frequent use of the type music. similarly, the predicate created may be used only for books, paintings, etc, and a di erent predicate composed could be prevalent for music. finally, it could be that only individual people are listed as composers, so that one would need additional alternative triple patterns: composedandmemberof rollingstones. problem: our goal in this paper is to improve qa over rdf data in terms of robustness, by devising a method for generating good queries that return answers even in such hard situations. the solution must not come at the expense of increasing the burden on the user. we would like to increase the systemrobustness also by allowing users to be informal if not sloppy in the way they express questions. for example, a variant of our example question could be: woman song writer covered love ballads by the stones. a solution must identify the right sub phrases like woman song writer, covered, love ballad, and the stones and must automatically map them onto semantic targets of the underlying data: types classes, relations, and entities. approach: this paper presents a solution based on generating a set of query variants that cover the user question to di erent extents. thus, we do not need a perfect translation, yet we can execute a query and return good answers. we address this issue by a joint optimization foridentifying the boundaries of the relevant phrases and ii jointly mapping all or a subset of these phrases onto classes, relations, and entities. the optimization is cast into an integer linear program with an appropriately designed objective function and constraints. sometimes a better query is generated by leaving out intractable parts of the user question. for example, love ballad may not map to any class and even woman song writer may be too hard so that simply mapping it to the class musician may be the best option. to compensate for such incomplete translations, we tap into textual descriptions that are often available with structured web data. these could be sources from which rdf triples have been gathered, texts about entities from their wikipedia articles or homepages, or the contents that surround a table or microdata in a web page. we extend sparql triple patterns into the notion of spox quad patterns that consist of the usual subjectpredicate object parts and a text component with keywords or text phrases that need to be matched by the textual descriptions associated with the spo triples in the data. for the example question, we could generate a query with the quad patterns: type singer. andultimately, improving the robustness of question to query translation without impeding user convenience. entity ranking has become increasingly important, both for retrieving structured entities and for use in general web search applications. the most common format for linked data, rdf graphs, provide extensive semantic structure via predicate links. while the semantic information is potentially valuable for effective search, the resulting adjacency matrices are often sparse, which introduces challenges for representation and ranking. in this paper, we propose a principled and scalable approach for integrating of latent semantic information into a learning to rank model, by combining compact representation of semantic similarity, achieved by using a modified algorithm for tensor factorization, with explicit entity information. our experiments show that the resulting ranking model scales well to the graphs with millions of entities, and outperforms the state of the art baseline on realistic yahoo semsearch challenge data sets. providing entity oriented search tools is one of the common trends of the past few years in the search engine industry. some examples of existing products are google knowledge graph, facebook graph search, bing snapshot, wolframalpha, and yandex islands. they aim to resolve entitycentric queries and show the search results in the user convenient form. the analysis of real query work done while visiting emory university permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. eugene agichtein maths and computer science department emory university atlanta, ga, usa eugene mathcs emory edu figure #: a semantic graph of entities that are relevant to a query richmond virginia logs justi es importance of the task. for example, a recent empirical study of the representative yahoo query log sample discovered that about queries have the clear user intent of soliciting information about certain entities, ie, goods, people, organizations, locations, events and others. in this paper, we address the problem of accurate modeling of the underlying query intent, which is key for enabling rich search experience. in entity search over rdf graphs, we consider an en tity description as an rdf subgraph that comprises rdf triples including the entity uri. the subgraph may contain literals along with pairs of predicates and uris of di erent entities from the graph. for example, figure # shows an entity graph for a city richmond, va. given a query richmond virginia, we may expect the following search result list: fb: richmond, virginia, dbr: richmond virginia, and fb: east end, richmond virginia. the rst two search results would be of excellent relevance level, and the last one is considered as of fair relevance. along with the growing commercial interest of the leading search companies, there is active research into methods in academia. most previous works adapt the standard ir approach to the task. given an entity, its literals and, possibly, literals of other entities associated with the given entity are folded into a pseudo document with multiple elds. then, one may apply bm or language model based ranking functions. in, string similarity scores and the ex plicit semantics of owl: sameas and dbpedia: redirect properties have been used for enhancing the search context and shown as remarkable improvement factors. the authors of have improved entity ranking consideringgram statis tics along with a pagerank adaptation. unlike previous works, our method takes advantage of the supervised learning to rank paradigm and rst integrates information about the semantic graph into a ranking model in a compact representation. it represents queries, entities and query entity pairs with a set of features that fall into two categories: term based features and structural features. the features of the rst category are derived from basic word statistics of queries and entities. the features of the second category capture the latent semantics of relations in the entity graph, inferring the distribution over latent factors for entities. to model this kind of semantics, our method relies on a tensor factorization based representation of initial relational data. finally, a machine learning algorithm discovers some patterns of feature values and optimizes those regularities with respect to the one of the standard evaluation measures. in summary, key contributions of our work include: a principled approach based on learning to rank to incorporate content and our novel structural features into the ranking model; a thorough evaluation of the proposed techniques by acquiring thousands of manual labels to augment the standard benchmark data set. in the next three sections, we describe the di erent components of our ranker: representing entity descriptions, representing link information, and com bining them together using a learning to rank approach.