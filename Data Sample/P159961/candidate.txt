programmers hoping to achieve performance improvements often use custom memory allocators. this in depth study examines eight applications that use custom allocators. surprisingly, for six of these applications, a state of the art general purpose allocator performs as well as or better than the custom allocators. the two exceptions use regions, which deliver higher performance. regions also reduce programmer burden and eliminate a source of memory leaks. however, we show that the inability of programmers to free individual objects within regions can lead to a substantial increase in memory consumption. worse, this limitation precludes the use of regions for common programming idioms, reducing their usefulness we present a generalization of general purpose and region based allocators that we call reaps. reaps are a combination of regions and heaps, providing a full range of region semantics with the addition of individual object deletion. we show that our implementation of reaps provides high performance, outperforming other allocators with region like semantics. we then use a case study to demonstrate the space advantages and software engineering benefits of reaps in practice. our results indicate that programmers needing fast regions should use reaps, and that most programmers considering custom allocators should instead use the lea allocator. programmers seeking to improve performance often incorporate custom memory allocators into their applications. custom allo this work is supported by nsf itr grant ccr and darpa grant year#. this work was done while emery berger was a research intern at microsoft research and a doctoral student at the university of texas. emery berger was also supported by a microsoft research fellowship. any opinions, ndings and conclusions or recommendations expressed in this material are the authors and do not necessarily re ect those of the sponsors. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. cators aim to take advantage of application speci. allocation patterns to manage memory more ef ciently than a general purpose memory allocator. for instance, parser runs over faster with its custom allocator than with the windows xp allocator. numerous books and articles recommend custom allocators as an optimization technique. the use of custom memory allocators is widespread, including the apache web server, the gcc compiler, three of the specint benchmarks, and thestandard template library, all of which we examine here. thelanguage itself provides language constructs that directly support custom memory allocation. the key contributions of this work are the following. we perform a comprehensive evaluation of custom allocation. we survey a variety of applications that use a wide range of custom allocators. we compare their performance and memory consumption to general purpose allocators. we were surprised to nd that, contrary to conventional wisdom, custom allocation generally does not improve performance, and in one case, actually leads to a performance degradation. a state of the art general purpose allocator yields performance equivalent to custom memory allocators for six of our eight benchmarks. these results suggest that most programmers seeking faster memory allocation should use the lea allocator rather than writing their own custom allocator. the custom allocators that do provide higher performance both use regions. regions provide high performance but force the programmer to retain all memory associated with a region until the last object in the region dies. we show that the performance gains of regions can come at the expense of excessive memory retention. more importantly, the inability to free individual objects within regions greatly complicates the programming of server applications like apache which rely on regions to avoid resource leaks. many programs cannot use regions because of their memory allocation patterns. if programs with intensive memory reuse, producer consumer allocation patterns, or dynamic arrays were to use regions, they could consume very large or even unbounded amounts of memory. we present a generalization of regions and heaps we call reaps. our implementation of reaps provides the performance and semantics of regions while allowing programmers to delete individual objects. we show that reaps nearly match the speed of regions when used in the same way, and provide additional semantics and generality. reaps provide a reusable library solution for region allocation with competitive performance, the potential for reduced memory consumption, and greater memory management exibility than regions. we demonstrate individual object deletion using reaps with a case study in which we add a new module to apache. the original version of this program uses malloc free. we show that by modifying only a few lines to use the reap interface, we can get region semantics with individual object deletion and thus reduce memory consumption signi cantly. the remainder of this paper is organized as follows. in section #, we analyze the structure of custom memory allocators used by our benchmark applications and explain why regions do not provide suf cient support for many applications, in particular server applications like apache. in section #, we describe reaps and present our implementation in detail. we describe our experimental infrastructure and methodology in section #. in section #, we present experimental results, including a comparison to previous allocators with region like semantics, and present our case study. we discuss our results in section #, explaining why we believe programmers used custom memory allocators despite the fact that these do not provide the performance they promise, and we conclude in section #. parallel, multithreadedandprograms such as web servers, database managers, news servers, and scientific applications are becoming increasingly prevalent. for these applications, the memory allocator is often a bottleneck that severely limits program performance and scalability on multiprocessor systems. previous allocators suffer from problems that include poor performance and scalability, and heap organizations that introduce false sharing. worse, many allocators exhibit a dramatic increase in memory consumption when confronted with a producer consumer pattern of object allocation and freeing. this increase in memory consumption can range from a factor of to unbounded memory consumption this paper introduces hoard, a fast, highly scalable allocator that largely avoids false sharing and is memory efficient. hoard is the first allocator to simultaneously solve the above problems. hoard combines one global heap and per processor heaps with a novel discipline that provably bounds memory consumption and has very low synchronization costs in the common case. our results on eleven programs demonstrate that hoard yields low average fragmentation and improves overall program performance over the standard solaris allocator by up to a factor of on processors, and up to a factor of over the next best allocator we tested. these applications include web servers, database managers, news servers, as well as more traditional parallel applications such as scienti. they are generally written inorto run ef ciently on modern shared memory multiprocessor this work is supported in part by the defense advanced research projects agency under grant from the. kathryn mckinley was supported by darpa grant year#, nsf grant eia, and nsf career award ccr. in addition, emery berger was supported by a novell corporation fellowship. multiprocessor computing facilities were provided through a generous donation by sun microsystems. wilson department of computer science university of massachusetts amherst, massachusetts mckinley cs umass edu servers. many of these applications make intensive use of dynamic memory allocation. unfortunately, the memory allocator is often a bottleneck that severely limits program scalability on multiprocessor systems. existing serial memory allocators do not scale well for multithreaded applications, and existing concurrent allocators do not provide one or more of the following features, all of which are needed in order to attain scalable and memory ef cient allocator performance: speed. a memory allocator should perform memory operations about as fast as a state of the art serial memory allocator. this feature guarantees good allocator performance even when a multithreaded program executes on a single processor. as the number of processors in the system grows, the performance of the allocator must scale linearly with the number of processors to ensure scalable application performance. the allocator should not introduce false sharing of cache lines in which threads on distinct processors inadvertently share data on the same cache line. we de ne fragmentation as the maximum amount of memory allocated from the operating system divided by the maximum amount of memory required by the application. excessive fragmentation can degrade performance by causing poor data locality, leading to paging. certain classes of memory allocators exhibit a special kind of fragmentation that we call blowup. intuitively, blowup is the increase in memory consumption caused when a concurrent allocator reclaims memory freed by the program but fails to use it to satisfy future memory requests. we de ne blowup as the maximum amount of memory allocated by a given allocator divided by the maximum amount of memory allocated by an ideal uniprocessor allocator. as we show in section #, the common producer consumer programming idiom can cause blowup. in many allocators, blowup ranges from a factor of to unbounded memory consumption. such a pathological increase in memory consumption can be catastrophic, resulting in premature application termination due to exhaustion of swap space. permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full cita the contribution of this paper is to introduce the hoard allocator tion on the rst page. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior speci. permis and show that it enables parallel multithreaded programs to achieve sionand orafee requestpermissionsfrompublicationsdept, acminc, fax, or permissions acm org. hoard achieves this result by simultaneously solving all of the above prob asplos year# cambridge, ma nov. in particular, hoard solves the blowup and false sharing problems, which, as far as we know, have never been addressed in the literature. as we demonstrate, hoard also achieves nearly zero synchronization costs in practice. hoard maintains per processor heaps and one global heap. when a per processor heapusage drops below a certain fraction, hoard transfers a large xed size chunk of its memory from the per processor heap to the global heap, where it is then available for reuse by another processor. we show that this algorithm bounds blowup and synchronization costs to a constant factor. this algorithm avoids false sharing by ensuring that the same processor almost always reuses from a given cache line. results on eleven programs demonstrate that hoard scales linearly as the number of processors grows and that its fragmentation costs are low. on processors, hoard improves performance over the standard solaris allocator by up to a factor of and a factor of over the next best allocator we tested. these features have led to its incorporation in a number of high performance commercial applications, including the twister, typhoon, breeze and cyclone chat and usenet servers and bemsolver, a high performance scienti. the rest of this paper is organized as follows. in section #, we explain in detail the issues of blowup and allocator induced false sharing. in section #, we motivate and describe in detail the algorithms used by hoard to simultaneously solve these problems. we sketch proofs of the bounds on blowup and contention in section #. we demonstrate hoardspeed, scalability, false sharing avoidance, and low fragmentation empirically in section #, including comparisons with serial and concurrent memory allocators. we also show that hoard is robust with respect to changes to its key parameter. we classify previous work into a taxonomy of memory allocators in section #, focusing on speed, scalability, false sharing and fragmentation problems described above. finally, we discuss future directions for this research in section #, and conclude in section #. hardware trends have produced an increasing disparity between processor speeds and memory access times. while a variety of techniques for tolerating or reducing memory latency have been proposed, these are rarely successful for pointer manipulating programs this paper explores a complementary approach that attacks the source of the problem rather than its manifestation. it demonstrates that careful data organization and layout provides an essential mechanism to improve the cache locality of pointer manipulating programs and consequently, their performance. it explores two placement techniques clustering and coloring that improve cache performance by increasing a pointer structure spatial and temporal locality, and by reducing cache conflicts to reduce the cost of applying these techniques, this paper discusses two strategies cache conscious reorganization and cache conscious allocation and describes two semi automatic tools ccmorph and ccmalloc that use these strategies to produce cache conscious pointer structure layouts. ccmorph is a transparent tree reorganizer that utilizes topology information to cluster and color the structure. ccmalloc is a cache conscious heap allocator that attempts to co locate contemporaneously accessed data elements in the same physical cache block. our evaluations, with microbenchmarks, several small benchmarks, and a couple of large real world applications, demonstrate that the cache conscious structure layouts produced by ccmorph and ccmalloc offer large performance benefits in most cases, significantly outperforming state of the art prefetching. in general purpose applications, most data is dynamically allocated. the memory manager therefore plays a crucial role in application performance by determining the spatial locality of heap objects. previous general purpose allocators have focused on reducing fragmentation, while most locality improving allocators have either focused on improving the locality of the allocator, or required programmer hints or profiling to guide object placement. we present a high performance memory allocator called vam that transparently improves both cache level and page level locality of the application while achieving low fragmentation. over a range of large footprint benchmarks, vam improves application performance by an average of versus the lea and freebsd allocators. when memory is scarce, vam improves application performance by up to compared to the freebsd allocator, and by over compared to the lea allocator. explicit memory managers have traditionally focused on reducing the number of discontiguous free chunks of memory, or fragmentation. reducing fragmentation improves space ef ciency and understandably has received considerable attention by memory manager designers. for example, the widely used lea allocator that forms the basis of the linux malloc was designed speci cally for high performance and low fragmentation. this material is based upon work supported by the national science foundation under award cns. any opinions, ndings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily re ect the views of the national science foundation. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. however, the widely acknowledged increasing latency gap between the cpu and the various levels of the memory hierarchy makes improving data locality. for many applications, this means improving the locality of the heap. while applications typically exhibit temporal locality, their spatial locality is dictated by the memory allocator, which determines where and how to lay out the applicationdynamic data. this allocator controlled locality can have a signi cant impact on the applicationoverall performance. we present a new general purpose memory allocator called vam that improves data locality while providing low fragmentation. vam increases page level locality by managing the heap in page sized chunks and aggressively giving up free pages to the virtual memory manager. by eliminating object headers, using a judicious selection of size classes, and by allocating objects using a reap based algorithm, vam improves cache level locality. we compare vam to the low fragmentation linux allocator and to the page level locality improving freebsd allocator, both of which we describe in detail. to our knowledge, phkmalloc has not been discussed previously in the memory management literature. we build on these algorithms, incorporating their best features while removing most of their disadvantages. our experiments on a suite of memory intensive benchmarks show that vam consistently achieves the best performance. vam performs on average faster than dlmalloc and faster than phkmalloc when there is suf cient physical memory to avoid paging. when physical memory is scarce, vam outperforms these allocators by over and up to, respectively. we show that part of this improvement is due to an unintended but fortunate synergy between vam and the way linux manages swap space, which holds evicted pages on disk. we call this phenomenon swap prefetchability and show that it leads to improved performance when paging. the allocation and disposal of memory is a ubiquitous operation in most programs. rarely do programmers concern themselves with details of memory allocators; most assume that memory allocators provided by the system perform well. this paper presents a performance evaluation of the reference locality of dynamic storage allocation algorithms based on trace driven simualtion of five large allocation intensiveprograms. in this paper, we show how the design of a memory allocator can significantly affect the reference locality for various applications. our measurements show that poor locality in sequential fit allocation algorithms reduces program performance, both by increasing paging and cache miss rates. while increased paging can be debilitating on any architecture, cache misses rates are also important for modern computer architectures. we show that algorithms attempting to be space efficient by coalescing adjacent free objects show poor reference locality, possibly negating the benefits of space efficiency. at the other extreme, algorithms can expend considerable effort to increase reference locality yet gain little in total execution performance. our measurements suggest an allocator design that is both very fast and has good locality of reference. while the high performance computing world is dominated by distributed memory computer systems, applications that require random access into large shared data structures continue to motivate development of ever larger shared memory parallel computers such as cray mta and sgi altix systems to support scalable application performance on such architectures, the memory allocator must be able to satisfy requests at a rate proportional to system size. for example, a processor cray mta can experience over concurrent requests, one from each of its streams per processor. cray eldorado, to be built upon the same network as sandia, processor red storm system, will sport thousands of multithreaded processors leading to hundreds of thousands of concurrent requests in this paper, we present mama, a scalable shared memory allocator designed to service any rate of concurrent requests. mama is distinguished from prior work on shared memory allocators in that it employs software combining to aggregate requests serviced by a single heap structure: hoard and mta malloc necessitate repetition of the underlying heap data structures in proportion to processor count. unlike hoard, mama does not exploit processor local data structures, limiting its applicability today to systems that sustain high utilization in the presence of global references such as cray mta systems. we believe mama relevance to other shared memory systems will grow as they become increasingly multithreaded and, consequently, more tolerant of references to non local memory we show not only that mama scales on cray mta systems, but also that it delivers absolute performance competitive with allocators employing heap repetition. in addition, we demonstrate that performance of repetition based allocators does not scale under heavy loads. we also argue more generally that methods using repetition alone to support concurrency are subject to an impractical tradeoff of scalability against space consumption: when scaled up to meet increasing concurrency demands, repetition based allocators necessarily house unused space quadratic in the number of processors. hierarchical structure may reduce this tolog, but in building large scale shared memory parallel computers, unused memory more than linear inis unacceptable. mama, in contrast, scales to arbitrarily large systems while consuming memory that increases only linearly with system and request size mama is of both theoretical interest for its use of novel algorithmic techniques and practical importance as the concurrency upon which shared memory performance depends continues to grow and multithreaded architectures emerge that are increasingly latency tolerant. while our work is a very recent contribution to memory allocation technology, mama already has been incorporated into production as the cornerstone for global memory allocation in cray multithreaded systems. when purchasing tickets for a concert from a single kiosk, customers typically stand in line and are serviced sequentially. when a kiosk runs out of tickets, the salesperson leaves the kiosk and runs back to a storage shed somewhere to retrieve a fresh roll of tickets. given enough such kiosks, the storage shed then becomes a bottleneck: to reduce frequency of re lls, each salesperson must retrieve a number of rolls of tickets proportional to the number of kiosks. consequently, the total number of tickets in kiosks rises quadratically in the rate of customers serviced. using repetition and hoarding to increase throughput is not practical. we view this process as analogous to parallel memory allocation: a ticket corresponds to a token, a piece of memory of restricted size, that is at least as large as the request size. the kiosk corresponds to a heap of tokens; the storage shed to the operating system; a ticket roll to the contiguous chunk of memory returned by the operating system; and the salesperson to the activity of the memory allocator. in our experience, memory allocation schemes that rely on repetition, even those that are simply arrays of lists, do suffer the bottleneck or space explosion described above as the memory request rate increases. in addition, where blocks are allocated independently to each heap, fragmentation increases with the number of heaps. returning to our analogy, imagine that customers encounter each other on the way to a single kiosk. the customers agree that one will wait while the other purchases two tickets; the customer purchasing two tickets repeats this should another purchaser be encountered. when the customer reaches the kiosk the request may be quite large, but the wait is brief: there is no line, and our assumption is that purchasing many tickets takes about the same time as purchasing one ticket. the purchaser then distributes the tickets, reversing the order in which requests were combined. the work of distributing is thus divided amongst those who wait. utilizing just one kiosk, any rate of requests can be accommodated simply by ensuring there is enough space in front of the kiosk for customers to encounter one another. xed rate of requests of some maximum size determined by the desired throughput. the number of tickets retrieved from the storage shed must be large enough to satisfy the requests that have accumulated during the salespersonabsence. that number is proportional to the throughput: thus, the number of tickets held by the salesperson is never more than linear in the rate of service. furthermore, affording space in front of the kiosk invites scalpers, whose purpose is to sell back a previously purchased ticket. we naively envision cooperative scalpers who behave similarly to customers: when two encounter each other, they combine their requests to return tickets, one waiting for their share of the money while the other takes responsibility for unloading their aggregate holdings. whenever one reaches the kiosk, the tickets are returned to the salesperson. if, before then, a customer encounters a scalper, an exchange takes place. this annihilation not only reduces contention at the kiosk, but reduces the wait time for customers and scalpers alike. mama exploits annihilation to accelerate memory management when threads repeatedly allocate and free same sized tokens of memory. the goal of this paper is to propose a scheme that provides comprehensive security protection for the heap. heap vulnerabilities are increasingly being exploited for attacks on computer programs. in most implementations, the heap management library keeps the heap meta data and the application heap data in an interleaved fashion and does not protect them against each other. such implementations are inherently unsafe: vulnerabilities in the application can cause the heap library to perform unintended actions to achieve control flow and non control attacks unfortunately, current heap protection techniques are limited in that they use too many assumptions on how the attacks will be performed, require new hardware support, or require too many changes to the software developers toolchain. we propose heap server, a new solution that does not have such drawbacks. through existing virtual memory and inter process protection mechanisms, heap server prevents the heap meta data from being illegally overwritten, and heap data from being meaningfully overwritten. we show that through aggressive optimizations and parallelism, heap server protects the heap with nearly negligible performance overheads even on heap intensive applications. we also verify the protection against several real world exploits and attack kernels. computer systems have evolved signi cantly in the last few decades, with machines that are increasingly inter connected and run increasingly complex software. unfortunately, such systems are also more exposed to security attacks which have not only grown in quantity, but also in variety. as a result, users are very concerned about protecting their systems from viruses, worms, trojan horses, and spyware. in order to attack a single application, attacks often rely on overwriting a set of locationswith a set of valueswith the purpose this research was supported by nsf early faculty career awards ccf and ccf, nsf award ccf, ibm faculty partnership award, north carolina state university and georgia institute of technology. much of this work is based on kharbutliphd thesis at ncsu. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. of altering the program behavior when it uses. attackers may use a variety of well known techniques to overwritewith, such as buffer over ows, integer over ows, and format strings, typically by supplying unexpected external input to the application. the desired program behavior alteration may include direct control ow modi cations in whichcontains control ow information such as function pointers, return addresses, and conditional branch target addresses. alternatively, attackers may choose to indirectly change program behavior by modifying critical data that determines program behavior. researchers have shown that the stack is vulnerable to overwrites, so many protection schemes have been proposed to protect it. in this paper, we will concentrate on protecting the heap against heap attacks, which are growing in number. examples include the slapper worm on the apache web server, gif image processing library heap over ow attack on the mozzila web browser, and heap corruption attack on null httpd. moreover, new vulnerabilities are continuously discovered for popular programs such as microsoft windows, microsoft internet explorer, cvs, and check point firewall. even the execution protection and sandboxing heap protection schemes used by the recent windows xp service pack have been shown to be vulnerable. the growth in heap attacks is mostly due to several main weaknesses in the way a programheap space is managed. first, in most current implementations, memory allocations and deallocations are performed by user level library code which keeps the heap metadata and the applicationheap data stored in an interleaved fashion in contiguous locations in the main memory. secondly, heap data and heap meta data are not protected from each other. such heap management implementations are inherently unsafe: they allow a vulnerability in how heap data is managed to be exploited to corrupt the heap meta data. for example, the lack of bound checking on a heap buffer can be exploited to over ow the buffer and corrupt the heap meta data. heap metadata is used by the heap management library to perform various functions such as allocating deallocating space for an object, consolidating free space, and reallocating a recently deallocated object. with corrupted meta data, the heap management library performs unintended actions such as overwriting an arbitrary memory location with an arbitrary value, where the attacker can choose both the target location and its new value. by overwriting a location that contains control ow information, the attacker can hijack control ow and execute malicious code. even if control ow hijacking is prevented, attackers can still do considerable damage, eg, by overwriting critical program data to alter the behavior of the program. a third weakness of current heap management implementations is the predictability of the heap layout. this allows attackers to gure out the exact location of various heap meta data structures and of critical heap data such as function pointers. despite the growing threat of heap attacks, current heap protection schemes either make too many assumptions on how heap attacks will be carried out, or are dif cult to implement due to high performance overheads and or demanding too many changes. it is hard for users to adopt solutions that require too many changes due to the need to retool their software development toolchain. it is useful to note that heap attacks are carried out in three basic steps: bug vulnerability exploitation stage: a bug or vulnerability in an application is exploited to corrupt certain structures, eg, by corrupting heap meta data through a buffer over ow, format string vulnerability, or other vulnerabilities. pre execution techniques have received much attention as aneffective way of prefetching cache blocks to tolerate the ever increasingmemory latency. a number of pre execution techniquesbased on hardware, compiler, or both have been proposed andstudied extensively by researchers. our studyshows a program behavior changes dynamically during execution in addition, the organizations of certain critical hardware structuresin the hyper threaded processors are either shared or partitionedin the multi threading mode and thus, the tradeoffs regardingresource contention can be intricate. they report promising resultson simulators that model a simultaneous multithreading processor. in this paper, we apply the helper threading idea ona real multithreaded machine, ie, intel pentium processor withhyper threading technology, and show that indeed it can providewall clock speedup on real silicon. to achieve further performanceimprovements via helper threads, we investigate threehelper threading scenarios that are driven by automated compilerinfrastructure, and identify several key challenges and opportunitiesfor novel hardware and software optimizations. therefore, it is essentialto judiciously invoke helper threads by adapting to the dynamicprogram behavior so that we can alleviate potential performancedegradation due to resource contention. moreover, since adaptingto the dynamic behavior requires frequent thread synchronization, having light weight thread synchronization mechanisms is important. as the speed gap between processor and memory system increases, a processor spends signi cant amount of time on memory stalls waiting for the arrival of cache blocks. to tolerate the large memory latency, there have been a plethora of proposals for data prefetching. recently, a novel thread based prefetching technique, called pre execution, has received much attention in the research community. compared to prediction based prefetching techniques, preexecution directly executes a subset of the original program instructions, called a slice, on separate threads alongside the main computation thread, in order to compute future memory accesses accurately. the prefetch threads run ahead of the main thread and trigger cache misses earlier on its behalf, thereby hiding the memory latency. kim and yeung proposed a compiler framework to generate helper threads at the program source level. we provide the insights gained from our experience and discuss the important ingredients to achieve speedup on such a physical system. in this study, we nd these unique aspects directly in uence the tradeoffs for applying helper threads. to improve the performance of an application program with helper threads, we observe that several key issues need to be addressed. first, the program behavior changes dynamically; for instance, even the same static load incurs different number of cache misses for different time phases. lastly, we observe that the dynamic program behavior changes at. therefore, it is important to have low overhead thread synchronization mechanisms. this property of helper threads provides interesting opportunities for applying various dynamic optimizations. in particular, since the dynamic program behaviors such as cache misses are generally micro architectural and not available at compile time, they can be most effectively captured and adapted to when monitored at run time. the rest of the paper is organized as follows. section # summarizes the key observations gained from the experiments. section # discusses the related works and section # concludes. to be effective, the pre execution techniques require construction of ef cient helper threads and processor level support to allow multiple threads to run concurrently. since manual construction of helper threads is cumbersome and error prone, it is desirable to automate this process. researchers have proposed a number of systems to generate helper threads automatically. collins et al proposed a hardware mechanism to construct helper threads at run time using a postretirement queue. liao et al developed a post pass binary adaptation tool to analyze an existing application binary, extract helper threads, and form an augmented new binary by attaching the helper threads to the original binary. all these studies have evaluated the helper threading idea on simulation based environments that model smt processors and demonstrated that it is a promising data prefetching technique. with the advent of intel pentium processor with hyper threading technology, a commercially available multithreaded processor supporting two logical processors simultaneously, it is possible to evaluate the helper threading idea on a physical smt machine. in this paper, we develop a new optimization module in the intel pre production compiler to construct helper threads automatically, and evaluate the helper threading idea on the intel pentium processor with hyper threading technology. to the best of our knowledge, there has been no published work that implements and experiments with helper threads on a real multithreaded processor. although supporting smt, the hyper threaded processor does have unique implementation aspects that are distinct from the proposed design of a research smt processor. therefore, a helper thread should be able to detect the dynamic program behavior at run time. second, since some of the hardware structures in the hyper threaded processors are shared or partitioned in the multi threading mode, resource contention can be an issue, and consequently, helper threads need to be invoked judiciously to avoid potential performance degradation due to resource contention. to adapt to the dynamic behavior, helper threads need to be activated and synchronized very frequently. compared to traditional multi threading where every thread should be executed and committed in pre de ned order to guarantee the correctness of program execution, helper threads only affect the performance of the application. therefore, a helper thread does not have to be always executed and can be deactivated whenever the helper thread does not improve the performance of the main computation thread. in this paper, we demonstrate that, in order to obtain signi cant bene. from helper threading, it is essential to employ run time mechanisms to throttle helper threads dynamically at. section # presents the software infrastructures that enable our experiments. section # describes our experimental framework and section # details the three helper threading scenarios. section # discusses the experimental results on an intel pentium processor with hyper threading technology. this paper describes automatic pool allocation, a transformation framework that segregates distinct instances of heap based data structures into seperate memory pools and allows heuristics to be used to partially control the internal layout of those data structures. the primary goal of this work is performance improvement, not automatic memory management, and the paper makes several new contributions. the key contribution is a new compiler algorithm for partitioning heap objects in imperative programs based on a context sensitive pointer analysis, including a novel strategy for correct handling of indirect function calls. second, the paper describes several optimizations that exploit data structure partitioning to further improve program performance. third, the paper evaluates how memory hierarchy behavior and overall program performance are impacted by the new transformations. using a number of benchmarks and a few applications, we find that compilation times are extremely low, and overall running times for heap intensive programs speed up by in many cases, about in two cases, and more than in two small benchmarks. overall, we believe this work provides a new framework for optimizing pointer intensive programs by segregating and controlling the layout of heap based data structures. the transformation does not require type safe programs and works for the full generality ofand. data prefetching via helper threading has been extensively investigated on simultaneous multi threading or virtual multi threading architectures. although reportedly large cache latency can be hidden by helper threads at runtime, most techniques rely on hardware support to reduce context switch overhead between the main thread and helper thread as well as rely on static profile feedback to construct the help thread code. this paper develops a new solution by exploiting helper threaded prefetching through dynamic optimization on the latest ultrasparc chip multiprocessing processor. our experiments show that by utilizing the otherwise idle processor core, a single user level helper thread is sufficient to improve the runtime performance of the main thread without triggering multiple thread slices. moreover, since the multiple cores are physically decoupled in the cmp, contention introduced by helper threading is minimal. this paper also discusses several key technical challenges of building a lightweight dynamic optimization software scouting system on the ultrasparc solaris platform. initial research on software helper threads developed the underlying run time compiler algorithms or evaluated them using simulation. however, on itanium, the large overhead in toggling between these modes limits the number of cycles available for actual helper code execution to a couple of hundred cycles. in our system, the main thread is bound to one core and the runtime performance monitoring code, the runtime optimizer and the dynamically generated helper code execute on the other core. the helper code generated for these regions is optimized to prefetch for delinquent loads. the normal caching mechanism maintains this mailbox in the cache and also in the cache of the helper thread core. section # discusses the helper thread model in our optimization framework, including code selection, helper thread dispatching, communication and synchronization. section # introduces the dynamic optimization framework on the ultrasparc system. modern processors spend a significant fraction of overall execution time waiting for the memory systems to deliver cache lines. this observation has motivated copious research on hardware and software data prefetching schemes. execution based prefetching is a promising approach that aims to provide high prefetching coverage and accuracy. these schemes exploit the abundant execution resources that are severely underutilized following an or cache miss on contemporary processors supporting simultaneous multi threading or virtual multi threading. in hardware preexecution or scouting, the processor checkpoints the architectural state and continues speculative execution that prefetches subsequent misses in the shadow of the initial triggering missing load. when the initial load arrives, the processor resumes execution from the checkpointed state. in software pre execution, a distilled version of the forward slice starting from the missing load is executed, minimizing the utilization of execution resources. helper threads utilizing run time compilation techniques may also be effectively deployed on processors that do not have the necessary hardware support for hardware scouting. with the advent of processors supporting smt and vmt, helper threading has been shown to be effective on the intel pentium smt processor and the itanium processor, in an smt processor such as pentium, many of the processor core resources such as caches, issue queues are either partitioned or shared. helper threads need to be constructed, deployed and monitored carefully so that the negative resource contention effects do not outweigh the gains due to prefetching. the vmt method used a novel combination of performance monitoring and debugging features of the itanium to toggle between the main thread and helper thread execution. only a few missing loads can be launched in this short time interval. almost all general purpose processor chips are moving to chip multi processors, including the gemini, niagara, panther chips from sun, the power and power chips from ibm and recent announcements from amd intel. the ibm and sun cmps have a single cache shared by all the cores and private caches for each of the cores. since the cores do not share any execution or resources, helper thread execution on one core has minimal negative impact on main thread execution on another core. however, since the cache is shared, the helper thread may prefetch data into the cache on behalf of the main thread. since such cmps are soon going to be almost universal in the generalpurpose arena and since many single thread applications are dominated by off chip stall cycles, helper thread prefetching on cmps is an attractive proposition that needs further investigation. the minimal sharing of resources between cores gives rise to unique issues that are not present in an smt or vmt implementation. first, how does the main thread initiate helper thread execution for a particular cache miss in an smt system, both threads are co located on the same core enabling fast synchronization. second, how does the main thread communicate register values to the helper thread in the itanium vmt system, the register file is effectively shared between the main and helper threads. we have devised innovative mechanisms to address these issues, implemented a complete dynamic optimization system for helper thread based prefetching, and measured actual speedups on an existing sun ultrasparc iv cmp chip. runtime performance monitoring selects program regions that have delinquent loads. the main thread uses a mailbox in shared memory to communicate and initiate helper thread execution. we address many other implementation issues in this first evaluation of helper thread prefetching on a physical chip multiprocessor and measure significant performance gains on several spec benchmarks and a real world application. the remainder of this paper is organized as follows. section # provides the background and related work. section # evaluates the performance of dynamic helper threading and section # draws the conclusion. recursive data structures do not have these properties, so new techniques must be developed. we intend to exploit this execution model using compiler analyses and automatic parallelization techniques. compiling for distributed memory machines has been a very active research area in recent years. much of this work has concentrated on programs that use arrays as their primary data structures. to date, little work has been done to address the problem of supporting programs that use pointer based dynamic data structures. the techniques developed for supporting spmd execution of array based programs rely on the fact that arrays are statically defined and directly addressable. in this article, we describe an execution model for supporting programs that use pointer based dynamic data structures. this model uses a simple mechanism for migrating a thread of control based on the layout of heap allocated data and introduces parallelism using a technique based on futures and lazy task creation. we have implemented a prototype system, which we call olden, that runs on the intel ipsc and the thinking machines cm. we discuss our implementation and report on experiments with five benchmarks. we present streamflow, a new multithreaded memory manager designed for low overhead, high performance memory allocation while transparently favoring locality. streamflow enables low over head simultaneous allocation by multiple threads and adapts to sequential allocation at speeds comparable to that of custom sequential allocators. it favors the transparent exploitation of temporal and spatial object access locality, and reduces allocator induced cache conflicts and false sharing, all using a unified design based on segregated heaps. streamflow introduces an innovative design which uses only synchronization free operations in the most common case of local allocations and deallocations, while requiring minimal, non blocking synchronization in the less common case of remote deallocations. spatial locality at the cache and page level is favoredby eliminating small objects headers, reducing allocator induced conflicts via contiguous allocation of page blocks in physical memory, reducing allocator induced false sharing by using segregated heaps and achieving better tlb performance and fewer page faults via the use of superpages. combining these locality optimizations with the drastic reduction of synchronization and latency overhead allows streamflow to perform comparably with optimized sequential allocators and outperform on a shared memory systemwith four two way smt processors four state of the art multi processor allocators by sizeable margins in our experiments. the allocation intensive sequential and parallel benchmarks used in our experiments represent a variety of behaviors, including mostly local object allocation deallocation patterns and producer consumer allocation deallocation patterns. ef cient dynamic memory allocation is essential for desktop, server and scienti. as more of these applications use thread level parallelism to exploit multiprocessors and emerging processors with multiple cores and threads, scalable multiprocessor memory allocation becomes of paramount importance. dynamic memory allocation can negatively affect performance by adding overhead during allocation and deallocation operations, and by exacerbating object access latency due to poor locality. therefore, effective memory allocators need to be optimized for both low allocation overhead and good object access locality. scalability and synchronization overhead reduction has been the central consideration in the context of thread safe memory allocators, while locality has been the focal point of the design of sequential memory allocators for more than a decade. multiprocessor allocators add synchronization overhead on the critical path of all allocations and deallocations. synchronization is needed because a thread may need to access another threadheap in order to remotely release an object to the owning thread. since such operations may be initiated concurrently by multiple threads, synchronization is used to arbitrate thread accesses to the data structures used for managing the heaps. therefore, local heaps need to be protected with locks or updated atomically with readmodify write operations such as cmp swap. the vast majority of thread safe allocators use object headers, which facilitate object deallocation in local heaps but pollute the cache in codes that allocate many small objects. locality conscious sequential allocators segregate objects of different sizes to different page blocks allocated from the operating system. objects are allocated by merely bumping a pointer and no additional information is stored with each object. in general, the allocation order of objects does not necessarily match their access pattern. however, contiguous allocation of small objects works well in practice because eliminating object headers helps avoid fragmentation and cache pollution. ef cient, thread safe memory allocators use local heaps to reduce contention between threads. the use of local heaps helps a multiprocessor allocator avoid false sharing, since threads tend to allocate and deallocate most of their objects locally. at a lower level, page block allocation and recycling policies in thread safe allocators are primarily concerned with fragmentation and blowup, without necessarily accounting for locality. the design space of thread safe allocators that achieve both good scalability and good data locality merits further investigation. it is natural to consider combining scalable synchronization mechanisms with localityconscious object allocation mechanisms. although the two design considerations of locality and scalability may seem orthogonal and complementary at rst glance, combining them in a uni ed design is not merely an engineering effort. several problems and trade offarise in an attempt to integrate scalable concurrent allocation mechanisms with cache and page conscious object allocation mechanisms in a uni ed design. addressing these problems is a central contribution of this paper. we show that both memory management overhead and locality exploitation in thread safe memory allocators can be improved, compared to what is currently offered by state of the art multiprocessor allocators. these design improvements and the associated performance bene ts are also a key contribution of this paper. we present stream ow, a thread safe allocator designed for both scalability and locality. stream owdesign is a direct result of eliminating synchronization operations in the common case, while at the same time avoiding the memory blowup when strictly thread local heaps are used in codes with producer consumer allocation freeing patterns. local operations in stream ow are synchronization free. not only do these operations proceed without thread contention due to locking shared data, but they also proceed without the latency imposed by uncontested locks and atomic instructions. the synchronization free design of local heaps enables stream ow to exploit established sequential allocation optimizations which are critical for locality, such as eliminating object headers for small objects and using bump pointer allocation in page blocks comprising thread local heaps. stream ow also includes an innovative remote object deallocation mechanism. remote deallocations namely deallocations of objects from threads different than the ones that initially allocated them are decoupled from local allocations and deallocations by forwarding remotely freed objects to per thread, nonblocking, lock free lists. stream owremote deallocation mechanism enables lazy object reclamation from the owning thread. as a result, most allocation and deallocation operations proceed without the cost of atomic instructions, and the infrequent operations that do require atomic instructions are non blocking, lock free and provably fast under various producer consumer object allocationfreeing patterns. beyond reducing memory management overhead and latency, decoupling local and remote operations promotes temporal locality by allowing threads to favor locally recycled objects in their private heaps. the use of thread local heaps reduces allocator induced false sharing. removing object headers improves spatial locality within cache lines and page blocks. the integration with a lower level custom page manager which utilizes superpages avoids allocatorinduced cache con icts via contiguous allocation of page blocks in physical memory, and reduces the activity of the os page manager, the number of page faults and the rate of tlb misses. combining these techniques produces a memory allocator that consistently outperforms other multithreaded allocators in experiments with up to threads on a processor system with hyperthreaded xeon processors. stream ow, by design, also adapts well to sequential codes and performs competitively with optimized sequential and application speci. this paper makes the following contributions: we present a new thread safe dynamic memory manager which bridges the design space between allocators focused on locality and allocators focused on scalability. to our knowledge, this is the rst time a memory allocator ef ciently uni es locality considerations with multiprocessor scalability. we present a new method for eliminating and minimizing synchronization overhead in multiprocessor memory allocators. our method decouples remote and local free lists and uses a new non blocking remote object deallocation mechanism. this technique preserves the desirable properties of a multiprocessor memory allocator, namely blowup avoidance and false sharing avoidance, without sacri cing the locality and low latency bene ts of bump pointer allocation. we present memory allocation and deallocation schemes that take into account cache conscious layout of heaps, page and tlb locality. to our knowledge, stream ow is the rst multiprocessor allocator designed with multilevel and multigrain locality considerations. we demonstrate the performance advantages of our design using realistic sequential and multithreaded applications as well as synthesized benchmarks. stream ow outperforms four widely used, state of the art multiprocessor allocators in allocation intensive parallel applications. it also performs comparably to optimized sequential allocators in allocation intensive sequential applications. stream ow exhibits solid performance improvements both in codes with mostly local object allocationfreeing patterns and codes with producer consumer object allocation freeing patterns. we have experimented with an smp with four two way smt processors. such smps are popular as commercial server platforms, affordable high performance computing platforms for scienti. problems, and building blocks for large scale supercomputers. since stream ow eliminates or signi cantly reduces synchronization, the key scalability limiting factor of multithreaded memory managers, we expect it to be scalable and ef cient on larger shared memory multiprocessors as well. the rest of this paper is organized as follows. section # presents the major design principles, mechanisms and policies of stream ow. section # presents our experimental evaluation of stream ow alongside other multiprocessor allocators and some optimized sequential allocators. in section # we discuss some implications of the design of stream ow and potential future improvements. processors execute the full dynamic instruction stream to arrive at the final output of a program, yet there exist shorter instruction streams that produce the same overall effect. we propose creating a shorter but otherwise equivalent version of the original program by removing ineffectual computation and computation related to highly predictable control flow. the shortened program is run concurrently with the full program on a chip multiprocessor simultaneous multithreaded processor, with two key advantages: improved single program performance. the shorter program speculatively runs ahead of the full program and supplies the full program with control and data flow outcomes. the full program executes efficiently due to the communicated outcomes, at the same time validating the speculative, shorter program. the two programs combined run faster than the original program alone. detailed simulations of an example implementation show an average improvement of for the spec integer benchmarks fault tolerance. the shorter program is a subset of the full program and this partial redundancy is transparently leveraged for detecting and recovering from transient hardware faults. a conventional processor executes the full dynamic instruction stream to arrive at the nal output of the program. the slipstream paradigm proposes that only a subset of the original dynamic instruction stream is needed to make full, correct, forward progress. unfortunately, we cannot know for certain what dynamic instructions can be validly skipped. creating a shorter, equivalent program is speculative ultimately, it must be checked against the full program to verify it produces the same overall effect. therefore, the operating system creates two redundant processes, ie, the user program is instantiated twice and each instance has its own context. the two redundant programs execute simultaneously on a single chip multiprocessor or on a simultaneous multithreaded processor. one of the programs always runs slightly ahead of the other. the leading program is called the advanced stream, or a stream, and the trailing program is called the redundant stream, orstream. hardware monitors the trailingstream and detects dynamic instructions that repeatedly and predictably have no observable effect and dynamic branches whose outcomes are consistently predicted correctly. future dynamic instances of the ineffectual instructions, branch instructions, and the computation chains leading up to them are speculatively bypassed in the leading a stream but only if there is high con dence correct forward progress can still be made, in spite of bypassing the instructions. the much reduced a stream is sped up because it fetches, executes, and retires fewer instructions than it would otherwise. also, all values and branch outcomes produced in the leading a stream are communicated to the trailingstream. although thestream is not reduced in terms of retired instructions, it has an accurate picture of the future and fetches executes more ef ciently. in summary, the a stream is sped up because it is shorter and thestream is sped up because it receives accurate predictions from the a stream. the two redundant programs combined run faster than either can alone. the a streamoutcomes are used only as predictions to speed up thestream. but ultimately, the same information is redundantly and independently computed by thestream. this is crucial because the a stream occasionally bypasses computation that should not have been bypassed, and it no longer makes correct forward progress. thestream can detect deviations because its redundantly computed outcomes differ from the a streamoutcomes. and the checks are already in place if the existing design implements conventional branch and value prediction. when the a stream deviates, the architectural state of thestream is used to selectively recover the corrupted architectural state of the a stream. permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior speci. permis sionand orafee requestpermissionsfrompublicationsdept, acminc, fax an analogy to the slipstream paradigm is slipstreaming in stock car racing. p, high air pressure forms at the front of a race car and a partial vacuum forms behind it. this creates drag and limits the cartop speed. a second car can position itself, or permissions acm org. year# this lls the vacuum behind the lead car, reducing its drag. and the trailing car now has less wind resistance in front. as a result, both cars speed up by several. p: the two combined go faster than either can alone. similarly, the a stream andstream mutually improve one anotherperformance. the a stream could not be accurately reduced without the trailingstream. and thestream is helped along in the slipstream of the a stream. the user perceives an overall speedup because both programs nish earlier. the amount of performance improvement depends on the nature and amount of reduction in the a stream. slipstreaming also relies on proper resource allocation between the two programs. in addition to potential performance improvements, slipstreaming provides fault tolerant capabilities. the trends of very high clock speeds and very small transistors may make the entire chip prone to transient faults, and there is renewed interest in fault tolerant architectures for commodity, high performance microprocessors. slipstream processors provide substantial but incomplete fault coverage, speci cally, faults that affect redundantly executed instructions are detectable and recoverable. not all instructions are redundantly executed because the a stream is a subset of thestream, and this opens up opportunities for dynamically and exibly trading performance and fault coverage. a transient fault, whether it affects the a stream, thestream, or both streams, is transparently detected as a misprediction by thestream because the communicated control and data ow outcomes from the a stream will differ from the corresponding outcomes in thestream. fault detection recovery is transparent because transient faults are indistinguishable from prediction induced deviations. in summary, this paper makes the following contributions. we suggest speculatively creating a shorter but otherwise equivalent version of the program, exploiting computation that repeatedly and predictably has no effect on the nal program output and computation that in uences highly predictable branches. the shortened program is run in parallel with the full program on a single chip multiprocessor or simultaneous multithreaded processor and, by communicating information from the shortened program to the full program, single program execution time is potentially improved and substantial transient fault coverage is achieved. this work is part of a larger effort using multiple on chip, architectural contexts in new ways. cmp smt processors are strategic because they effectively utilize billion transistor chips with relative ease, integrating parallelism that already exists at the system level onto a single die. our goal is threefold: provide more functionality in the same cmp smt processor not just throughput oriented parallelism, but also fault tolerance and improved single program performance, provide the new functions in a non intrusive way, by placing hardware around the existing components and leveraging, as much as possible, the existing design, and enable the user operating system to exibly and dynamically choose from among several modes of operation, eg, throughput mode, single program speedup mode, or reliable mode. dynamic memory management is an important part of a large class of computer programs and high performance algorithms for dynamic memory management have been, and will continue to be, of considerable interest. this paper presents empirical data from a collection of six allocation intensiveprograms. extensive statistics about the allocation behavior of the programs measured, including the distributions of object sizes, lifetimes, and interarrival times, are presented. this data is valuable for the following reasons: first, the data from these programs can be used to design high performance algorithms for dynamic memory management. second, these programs can be used as a benchmark test suite for evaluating and comparing the performance of different dynamic memory management algorithms. finally, the data presented gives readers greater insight into the storage allocation patterns of a broad range of programs. the data presented in this paper is an abbreviated version of more extensive statistics that are publically available on the internet.