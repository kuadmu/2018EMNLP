the quality of user generated content varies drastically from excellent to abuse and spam. as the availability of such content increases, the task of identifying high quality content sites based on user contributions social media sites becomes increasingly important. social media in general exhibit a rich variety of information sources: in addition to the content itself, there is a wide array of non content information available, such as links between items and explicit quality ratings from members of the community. in this paper we investigate methods for exploiting such community feedback to automatically identify high quality content. as a test case, we focus on yahoo answers, a large community question answering portal that is particularly rich in the amount and types of content and social interactions available in it. we introduce a general classification framework for combining the evidence from different sources of information, that can be tuned automatically for a given social media type and quality definition. in particular, for the community question answering domain, we show that our system is able to separate high quality items from the rest with an accuracy close to that of humans. from the early year#s, user generated content has become increasingly popular on the web: more and more users participate in content creation, rather than just consumption. ers a combination of all of these with an emphasis on the relationships among the users of the community. community driven question answering portals are a particular form of user generated content that is gaining a large audience in recent years. these portals, in which users answer questions posed by other users, provide an alternative channel for obtaining information on the web: rather than browsing results of search engines, users present detailed information needs and get direct responses authored by humans. in some markets, this information seeking behavior is dominating over traditional web search. an important di erence between user generated content and traditional content that is particularly signi cant for knowledge based media such as question answering portals is the variance in the quality of the content. the main challenge posed by content in social media sites is the fact that the distribution of quality has high variance: from very high quality items to low quality, sometimes abusive content. this makes the tasks of ltering and ranking in such systems more complex than in other domains. however, for information retrieval tasks, social media systems present inherent advantages over traditional collections of documents: their rich structure. in addition to document content and link structure, social media exhibit a wide variety of user to document relation types, and user to user interactions. in this paper we address the task of identifying highquality content in community driven question answering sites, exploring the bene ts of having additional sources of information in this domain. as a test case, we focus on yahoo answers, a large portal that is particularly rich in the amount and types of content and social interaction available in it. we focus on the following research questions: what are the elements of social media that can be used to facilitate automated discovery of high quality content in addition to the content itself, there is a wide array of non content information available, from links between items to explicit and implicit quality rating from members of the community. what is the utility of each source of information to the task of estimating quality. how are these di erent factors related is content alone enough for identifying high quality items. can community feedback approximate judgments of specialists to our knowledge, this is the rst large scale study of combining the analysis of the content with the user feedback in social media. in particular, we model all user interactions in a principled graph based framework, allowing us to. ectively combine the di erent sources of evidence in a classi cation formulation. furthermore, we investigate the utility of the di erent sources of feedback in a large scale, experimental setting over the market leading question answering portal. our experimental results show that these sources of evidence are complementary, and allow our system to exhibit high accuracy in the task of identifying content of high quality. we discuss our ndings and directions for future work in section #, which concludes this paper. recent years have seen a transformation in the type of content available on the web. during the rst decade of the webprominence from the early year#s onwards most online content resembled traditional published material: the majority of web users were consumers of content, created by a relatively small amount of publishers. popular usergenerated content domains include blogs and web forums, social bookmarking sites, photo and video sharing communities, as well as social networking platforms such as facebook and myspace, which. as anderson describes, in traditional publishing mediated by a publisher the typical range of quality is substantially narrower than in niche, unmediated markets. ers more available data than in other domains. expertise retrieval has been largely unexplored on data other than thec collection. at the same time, many intranets of universities and other knowledge intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas. we first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people. for our experimental evaluation, we introduce a new test set based on a crawl of a university site. using this test set, we conduct two series of experiments. the first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set. the second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set. expertise retrieval models are shown to be robust with respect to environments smaller than thec collection, and current techniques appear to be generalizable to other settings. the goal of expert nding is to identify a list of people who are knowledgeable about a given topic. an organizationintranet provides a means for exchanging information between employees and for facilitating employee collaborations. to ef ciently and effectively achieve this, it is necessary permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. box, le tilburg, the netherlandsantal vdnbosch uvt nl to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues. at the trec enterprise track the need to study and un derstand expertise retrieval has been recognized through the introduction of expert finding tasks. this task is usually addressed by uncovering associations between people and topics; commonly, a co occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise. an alternative task, which using the same idea of people topic associations, is expert pro ling, where the task is to return a list of topics that a person is knowledgeable about. the launch of the expert finding task at trec has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects. however, nearly all of the expert nding or pro ling work performed has been validated experimentally using thec collection from the enterprise track. while this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet. with only one test collection it is not possible to generalize conclusions to other realistic settings. in this paper we focus on expertise retrieval in a realistic setting that differs from thec setting one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge intensive organizations. typically, this setting features several additional types of structure: topical structure, organizational structure, as well as multiple types of documents. this setting is quite different from thec setting in ways that might impact upon the performance of expertise retrieval tasks. this allows us to formulate the expert nding and expert pro ling tasks in a uniform way, and has the added bene. of allowing us to understand the relations between the two tasks. for our experimental evaluation, we introduce a new data set which is representative of the type of intranet that we described above. our collection is based on publicly available data, crawled from the website of tilburg university. this type of data is particularly interesting, since it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; contains information on the organizational hierarchy; it is bilingual; and the list of expertise areas of an individual are provided by the employees themselves. using the uvt expert collection, we conduct two sets of experiments. the rst is aimed at determining the effectiveness of baseline expertise nding and pro ling methods in this new setting. a second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the uvt expert collection; speci cally, we propose and evaluate re ned expert nding and pro ling methods that incorporate topicality and organizational structure. apart from the research questions and data set that we contribute, our main contributions are as follows. the baseline models developed for expertise nding perform well on the new data set. while on thec setting the expert nding task appears to be more dif cult than pro ling, for the uvt data the opposite is the case. we nd that pro ling on the uvt data set is considerably more dif cult than on thec set, which we believe is due to the large number of topical areas that we used for pro ling: about, for the uvt set, versus in thec case. taking the similarity between topics into account can signi cantly improve retrieval performance. the best performing similarity measures are content based, therefore they can be applied on thec settings as well. finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving map scores for certain models by up to. the remainder of this paper is organized as follows. in the next section we review related work. then, in section # we provide de tailed descriptions of the expertise retrieval tasks that we address in this paper: expert nding and expert pro ling. in section # we present our baseline models, of which the performance is then assessed in section # using the uvt data set that we introduce in sec tion. features of our data are presented in section # and evaluated in section #. we present methods for finding experts usingmail messages. we locate messages on a topic, and then find the associated experts. our approach is unsupervised: both the list of potential experts and their personal details are obtained automatically frommail message headers and signatures, respectively. evaluation is done using themail lists in thec corpus. mail has become the primary means of communication in many organizations. it is a rich source of information that could be used to improve the functioning of an organization. hence, search and analysis ofmail messages has drawn signi cant interest from the research community. speci cally, mail messages can serve as a source for expertise identi cation, since they capture peopleactivities, interests, and goals in a natural way. while early approaches to expert nding employed manually maintained databases, there has been a move towards unsupervised methods that use expertise indicators from documents produced within an organization; the resulting evidence of expertise is then used to build an employeeexpertise pro le. our main aim in this paper is to study the use ofmail messages for mining expertise information. our main ndings are that the elded structure ofmail messages can be. ectively exploited to nd pieces of evidence of expertise, which can then be successfully combined in a language copyright is held by the author owner. modeling framework, and mail signatures are a reliable source of personal contact information. the rest of the paper is structured as follows. in section # we detail and assess our model of expert search. in section # we harvest contact details for candidates by miningmail signatures. the paper proposes identifying relevant information sources from the history of combined searching and browsing behavior of many web users. while it has been previously shown that user interactions with search engines can be employed to improve document ranking, browsing behavior that occurs beyond search result pages has been largely overlooked in prior work. the paper demonstrates that users post search browsing activity strongly reflects implicit endorsement of visited pages, which allows estimating topical relevance of web resources by mining large scale datasets of search trails. we present heuristic and probabilistic algorithms that rely on such datasets for suggesting authoritative websites for search queries. experimental evaluation shows that exploiting complete post search browsing trails outperforms alternatives in isolation, and yields accuracy improvements when employed as a feature in learning to rank for web search. a common method for finding information in an organization is to use social networks ask people, following referrals until someone with the right information is found. another way is to automatically mine documents to determine who knows what. email documents seem particularly well suited to this task of expertise location, as people routinely communicate what they know. moreover, because people explicitly direct email to one another, social networks are likely to be contained in the patterns of communication. can these patterns be used to discover experts on particular topics is this approach better than mining message content alone to find answers to these questions, two algorithms for determining expertise from email were compared: a content based approach that takes account only of email text, and a graph based ranking algorithm that takes account both of text and communication patterns. an evaluation was done using email and explicit expertise ratings from two different organizations. the rankings given by each algorithm were compared to the explicit rankings with the precision and recall measures commonly used in information retrieval, as well as themeasure commonly used in signal detection theory. results show that the graph based algorithm performs better than the content based algorithm at identifying experts in both cases, demonstrating that the graph based algorithm effectively extracts more information than is found in content alone. knowledge in an organization is contained in the skill, experience, and expertise of its people. yet the very problem permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. the online version of this article contains two figures mistakenly omitted from the printed version. of discovering who knows what is often challenging. social networks relationships among people in an organization provide the basis for nding experts or for nding answers to questions. people ask others they know to nd someone with a particular skill, experience, or expertise, following pointers until an appropriate person is found. this social networking process involves asking a co worker, manager, or senior employee, which leads to a chain of queries for either the answer to the question or the name of someone who could answer the question. many times, the chain leads to people who act as contact brokers or mediators, providing pointers to people or groups that might help. social networks naturally form in a way that turns large groups into small worlds. nevertheless, there are huge costs to following pointers to experts, such as. ort repeated by di erent people looking for the same answers, miscommunication that leads to the wrong expert, and time pressures that lead to taking the advice of not so expert experts who happen to be found quickly. how can we take advantage of social networks to nd experts more. ectively email is a valuable source of expertise. it provides an easy to mine repository of communication between people in the social network, and it contains actual demonstrations of expertise as well as knowledge of expertise. both the content of email and the pattern of communication contain information about who knows what in an organization. in this paper, we take seriously communication patterns in email. we describe a system that identi es expertise from email, along with an evaluation of how well two di erent algorithms for mining expertise compare with human judgments. how to find experts knowledge of expertise within organizations is often isolated to a speci. our goal is to mine knowledge of expertise, store that information, and make it available to the larger organization so that it can be leveraged for problem resolution. in essence, we want to manage the knowledge that people have about who knows what in an organization to facilitate expertise mapping and improve organizational functioning. because this knowledge of expertise is based on human judgments as coded in the email sent in an organization, our approach mines perceived expertise rather than true expertise. nevertheless, perceived expertise will be valuable, as it re ects judgments of people who are knowledgeable about particular topics. we de ne expertise mapping as the process of locating and identifying people who have knowledge of a particular topic. by locating an expert, we mean nding a person who seems knowledgeable on a topic. by identifying an expert, we mean deciding if that person is truly knowledgeable on a topic. because expertise mapping relies on informal social networks, it is cannot be considered a formal business process; rather, it is a tacit process that underlies real organizational functioning. such tacit processes are increasingly recognized as strategic elements of organizations, and so understanding them provides an opportunity to improve existing processes, develop information technology to enhance tacit processes, reduce risk in organizational change, and improve organizational adaptation. for instance, if one is not already knowledgeable on a particular topic, it might be hard to distinguish valuable experts from those who merely talk a lot about a particular topic. for some topics, everyone has an opinion, which might make it even more di cult to identify the real expert. in addition, although working in an organization for years can lead to the development of an. cient expertise map, being new to an organization can limit access to informal social networks. to automatically map expertise in an organization, we need a data source that captures how expertise is communicated in social networks. we believe the best record of this activity is email, which is a primary means of communication in many businesses. it contains precious information about the activities, interests and priorities of an individual or the organization, and because it ows continuously as a part of everyday operations, it naturally captures changing interests, projects, and goals. email shows who communicates with whom and what those communications are about. ectly providing a window onto informal social networks. email displays not only demonstrations of expertise but also knowledge of who knows what. the choice of who to send a question to is based on the senderknowledge of expertise, as well as knowledge of subject matter. it is reasonable to suppose, therefore, that appropriate analysis of an organizationaggregate email can identify individuals with a high level of expertise in topics of interest. related work schwartz and wood were rst to analyze email ows to identify groups of individuals with common interests. to preserve privacy, they used only email ow and not email content, requiring initial identi cation of a distinguished person to seed the search for others with knowledge of a particular topic. the result was a list of related people with no intrinsic ranking order. the contactfinder system used text and addresses of messages on bulletin boards to nd the right person to answer a certain question. xpert nder uses a pre existing hierarchy of subject areas, characterized by word frequencies, to identify experts in speci. areas by analyzing the word frequencies of email written by each individual. the expertfinder system uses number of self published documents containing topic keyword and frequency of person mentions near topic keyword in non self published documents to produce expertise scores and ranks. commercial systems for expert identi cation include autonomyidol server, which identi es employeeexpertise on the basis of the documents they access www autonomy com and submit on the intranet, and tacitknowledgemail and lotusdiscovery server, both of which build interest pro les by scanning email and matching these to document taxonomies. in contrast to all these, our approach uses text analysis and network analysis in an integrated system: the text of the messages is used to generate clusters of similar content, and the graph of message exchanges for each cluster is used to compute a ranked list of the individuals involved in the exchanges, sorted according to estimated expertise. it has long been recognized that capturing term relationships is an important aspect of information retrieval. even with large amounts of data, we usually only have significant evidence for a fraction of all potential term pairs. it is therefore important to consider whether multiple sources of evidence may be combined to predict term relations more accurately. this is particularly important when trying to predict the probability of relevance of a set of terms given a query, which may involve both lexical and semantic relations between the terms we describe a markov chain framework that combines multiple sources of knowledge on term associations. the stationary distribution of the model is used to obtain probability estimates that a potential expansion term reflects aspects of the original query. we use this model for query expansion and evaluate the effectiveness of the model by examining the accuracy and robustness of the expansion methods, and investigate the relative effectiveness of various sources of term evidence. statistically significant differences in accuracy were observed depending on the weighting of evidence in the random walk. for example, using co occurrence data later in the walk was generally better than using it early, suggesting further improvements in effectiveness may be possible by learning walk behaviors. negotiations where, andrepresent co occurrence, synonymy, and morphology relations respectively. we also do not use a pre defined network for all queries, but customize each network for each query. associative models consider relationships between terms in addition to the terms themselves. they have been extensively considered and studied for information retrieval, eg, by bush, stiles, van rijsbergen and salton buckley among many others. there are many lexical and semantic relations that may be considered for associating a pair of terms. for example: stemming, based on common morphology; synonymy, where aspects of meaning are shared; cooccurrence, in which both words tend to appear together; and general association, where a person is likely to give one word as a free association response to the other. each relation may be thought of as an inference step, in which a source wordhas some property, and a new wordcan be inferred to have the property value with probability pr, based on their shared relation. for example, ifis the word matrix and the property is relevancy to a query, then one possible way to calculate pr is based on co occurrence, so that, for example, the term row also has some measure of relevance. note that this is not symmetric: row having more senses and being more common, it is less likely to imply relevance of matrix, unless another term is also present for context, such as column. while lexical and semantic relations may be useful individually, it is important to consider how they may be used in combination. one reason for this is a common problem in language processing called sparsity: for cooccurrence relations for example, even with a huge corpus, we only have reliable co occurrence data for a fraction of all potential term pairs. external semantic resources such as wordnet or stemming dictionaries supply a broad set of terms but are limited in the depth and currency of their vocabulary. by combining multiple relations into chains of inference, we can help bridge the gaps that exist in the data. a second reason is that the various relations between words represent potentially complimentary sources of evidence that may help to distinguish and disambiguate terms. for example, if bank and merger are known to be relevant to a query, then the following inference brain figure #. the walk uses co occurrence relations in early steps, then shifts to stemming and association for later steps in the walk. values inside the nodes are example probabilities from the stationary distribution. chains would provide evidence that negotiations may also be relevant: bank. note that chains can emphasize different types of evidence at different walk stages. in the above example, co occurring terms are found first, followed by their synonyms or stems. in this paper we propose and evaluate a markov chainbased framework for modeling term relations that can perform such combination of behavior and apply this model to query expansion. given a small set of initial query terms, we construct a term network and use a random walk to estimate the likelihood of relevance for potential expansion terms. the features used by the random walk can come from a variety of sources, such as term co occurrence in an external corpus, cooccurrence in the top retrieved documents, synonym dictionaries, general word association scores, and so on. unlike many previous related models used for information retrieval, we define a much richer set of potential walk behaviors that support a variety of link types, where different combinations of evidence can be used at different stages of the walk. for example, cooccurrence may initially be given higher weight early in the walk, with synonyms weighted more highly in later steps. we apply our model to the problem of query expansion in the language modeling approach to information retrieval. by estimating the probability that the various aspects of the query can be inferred from a potential expansion term, we essentially perform a form of semantic smoothing of the query language model. the main hypothesis of this paper is that combining query specific term dependencies from multiple sources can lead to more accurate and or robust expansion algorithms. simplified example of a stationary distribution induced by a random walk starting at the node parkinsondisease. search engines can record which documents were clicked for which query, and use these query document pairs as soft relevance judgments. however, compared to the true judgments, click logs give noisy and sparse relevance information. we apply a markov random walk model to a large click log, producing a probabilistic ranking of documents for a given query. a key advantage of the model is its ability to retrieve relevant documents that have not yet been clicked for that query and rank those effectively. we conduct experiments on click logs from image search, comparing our random walk model to a different random walk, varying parameters such as walk length and self transition probability. the most effective combination is a long backward walk with high self transition probability. a search engine can track which of its search results were clicked for which query. for a popular system, these click records can amount to millions of query document pairs per day. each pair can be viewed as a weak indication of relevance: that the user decided to at least view the document, based on its description in the search results. although clicks are not real judgments, there is evidence that they are useful, for example as training data, as annotations, for query suggestion or directly as evidence for ranking. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. we can use the clicks of past users to improve the current search results. however, the clicked set of documents is likely to di er from the current userrelevant set. other di erences are due to presentation issues; for example, the user must decide whether to click based on a short summary and is in uenced by the ordering of results. for any given search, a large number of documents are never seen by the user, therefore not clicked. from the perspective of a user conducting a search, documents that are clicked but not relevant constitute noise in the click data. documents that are relevant but not clicked constitute sparsity in the click data. one class of approaches attempts to reduce noise in click data, by building a click model that may use additional information about the userbehaviour. these approaches can signi cantly reduce noise, by identifying some clicked documents as irrelevant. this paper focuses on the sparsity problem, although our model also has noise reduction properties. the model gives a probabilistic ranking of documents, which includes relevant documents that have not yet been clicked for the current query. the sparsity problem is evidenced by power law distributions observed in click logs. most queries in the click log have a small number of clicked documents. in such cases, it is useful to identify additional relevant documents. we rst describe the click information as a graph, and survey a range of click graph applications. then we detail our markov random walk model for nding relevant documents. the subsequent sections describe a real click dataset, and empirical evaluation of the new methods. some differences arise because we are aggregating clicks across users, who may simply disagree about which documents are relevant. for example, taking into account the userbrowsing patterns after clicking a document. we present a novel framework for answering complex questions that relies on question decomposition. complex questions are decomposed by a procedure that operates on a markov chain, by following a random walk on a bipartite graph of relations established between concepts related to the topic of a complex question and subquestions derived from topic relevant passages that manifest these relations. decomposed questions discovered during this random walk are then submitted to a state of the art question answering system in order to retrieve a set of passages that can later be merged into a comprehensive answer by a multi document summarization system. in our evaluations, we show that access to the decompositions generated using this method can significantly enhance the relevance and comprehensiveness of summary length answers to complex questions. complex questions cannot be answered using the same techniques that apply to factoid questions. complex questions refer to relations between entities or events; they refer to complex processes and model scenarios that involve deep knowledge of the topic under investigation. for example, a question like: what are the key activities in the research and development phase of creating new drugs looks for information on two distinct phases of creating drugs. typically, relevant information for these kinds permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. of questions can be found in multiple documents and needs to be fused together into. in the document understanding conferences, the answer to complex questions like is considered to be a multi sentence multiple document summary that meets the information need of the question. we introduce a new paradigm for processing complex questions that relies on a combination of question decompositions; factoid question answering techniques; and multi document summarization techniques. central to this process is a question decomposition model that enables the selection of the textual information aggregated in the nal answer. we present a novel question decomposition procedure that operates on a markov chain model inspired from the markov chains used for expanding language models introduced in. we propose that question decomposition depends on the successive recognition of the relations that exist between words and concepts extracted from topic relevant sentences. for example, if a topic relation between develop and drugs is recognized in question, we assume that this sentence will contain relevant information that can be used to decompose of furthermore, we expect that sentences containing topic relevant relations will also contain other relevant relations that should be leveraged in question decomposition. for example, if is identi ed in the sentence the challenge for glaxo was to develop a drug that was pleasant to swallow, we expect that a new relation between the concept company and develop should be extracted and used to identify still other sentences that could potentially provide relevant information. as new relations are discovered, we expect that sentences containing the most relevant relations can be used to generate questions that can represent possible decompositions of the original complex question. for example, given and in, a question like what companies develop new drugs can be created which could be used to obtain a set of answers which could represent a partial response to. relevant answers to each newly decomposed question can be used to discover more relevant relations, that in turn, prompt still more question decompositions. this process ends when either no new relations are discovered, or the random walk is stabilizing within a threshold. first, we compare them against decompositions produced by humans. second, we conduct several evaluations of the quality of the mds answers they enable. third, we use every sentence from the mds question answering system question answer answer complex question answeranswer answer answer summary summary summary figure #: the architecture of our framework for processing complex questions. answer and generate questions with the same procedure employed when creating question decompositions from relevant sentences. the questions that have answers in the summaries are evaluated against questions generated by human linguists. they are also used for measuring the similarity to the decomposed questions. our studies indicate that these comparisons correlate with the relevance of the answers. we claim that this is an important nding since current mds evaluation methods typically rely on human produced answers, or human judgments. the automatic scoring of the mds answers based on comparisons of decomposed questions allows a framework in which researchers can test multiplea techniques or multiple mds techniques that best operate for nding answers. the remainder of the paper is organized as follows. section # presents the framework we have designed for processing complex questions. section # describes the random walk models employed for decomposing questions. section # details the evaluation results while section # summarizes the conclusions. recent web search techniques augment traditional text matching with a global notion of importance based on the linkage structure of the web, such as in google pagerank algorithm. for more refined searches, this global notion of importance can be specialized to create personalized views of importance for example, importance scores can be biased according to a user specified set of initially interesting pages. computing and storing all possible personalized views in advance is impractical, as is computing personalized views at query time, since the computation of each view requires an iterative computation over the web graph. we present new graph theoretical results, and a new technique based on these results, that encode personalized views as partial vectors. partial vectors are shared across multiple personalized views, and their computation and storage costs scale well with the number of views. our approach enables incremental computation, so that the construction of personalized views from partial vectors is practical at query time. we present efficient dynamic programming algorithms for computing partial vectors, an algorithm for constructing personalized views from partial vectors, and experimental results demonstrating the effectiveness and scalability of our techniques. general web search is performed predominantly through text queries to search engines. because of the enormous size of the web, text alone is usually not selective enough to limit the number of query results to a manageable size. the pagerank algorithm, among others, has been proposed to exploit the linkage structure of the web to compute this work was supported by the national science foundation under grant iis. this is an abbreviated version of the full paper that omits appendices. the full version is available on the web at http: dbpubs stanford edu pub year# copyright is held by the author owner. jennifer widom stanford university widom db stanford edu global importance scores that can be used to in uence the ranking of search results. to encompass different notions of importance for different users and queries, the basic pagerank algorithm can be modi ed to create personalized views of the web, rede ning importance according to user preference. for example, a user may wish to specify his bookmarks as a set of preferred pages, so that any query results that are important with respect to his bookmarked pages would be ranked higher. while experimentation with the use of personalized pagerank has shown its utility and promise, the size of the web makes its practical realization extremely dif cult. to see why, let us review the intuition behind the pagerank algorithm and its extension for personalization. the fundamental motivation underlying pagerank is the recursive notion that important pages are those linked to by many important pages. a page with only two in links, for example, may seem unlikely to be an important page, but it may be important if the two referencing pages are yahoo and netscape, which themselves are important pages because they have numerous in links. one way to formalize this recursive notion is to use the random surfer model introduced in. imagine that trillions of random surfers are browsing the web: if at a certain time step a surfer is looking at page, at the next time step he looks at a random outneighbor of. as time goes on, the expected percentage of surfers at each pageconverges to a limit that is independent of the distribution of starting points. intuitively, this limit is the pagerank of, and is taken to be an importance score for, since it re ects the number of people expected to be looking atat any one time. the pagerank score re ects a democratic importance that has no preference for any particular pages. in reality, a user may have a setof preferred pages which he considers more interesting. we can account for preferred pages in the random surfer model by introducing a teleportation probability: at each step, a surfer jumps back to a random page inwith probability, and with probability continues forth along a hyperlink. the limit distribution of surfers in this model would favor pages in, pages linked to by, pages linked to in turn, etc. we represent this distribution as a personalized pagerank vector personalized on the set. informally, a ppv is a personalized view of the importance of pages on the web. rankings of a usertext based query results can be biased according to a ppv instead of the global importance distribution. each ppv is of length, whereis the number of pages on the web. xed point iteration requires multiple scans of the web graph, which makes it impossible to carry out online in response to a user query. on the other hand, ppvfor all preference sets, of which there are, is far too large to compute and store of ine. we present a method for encoding ppvas partially computed, shared vectors that are practical to compute and store of ine, and from which ppvcan be computed quickly at query time. in our approach we restrict preference setsto subsets of a set of hub pages, selected as those of greater interest for personalization. in practice, we expectto be a set of pages with high pagerank, pages in a human constructed directory such as yahoo or open directory, or pages important to a particular enterprise or application. the size ofcan be thought of as the available degree of personalization. we present algorithms that, unlike previous work, scale well with the size of. moreover, the same techniques we introduce can yield approximations on the much broader set of all ppv, allowing at least some level of personalization on arbitrary preference sets. the main contributions of this paper are as follows. a method, based on new graph theoretical results, of encoding ppvas partial quantities, enabling an ef cient, scalable computation that can be divided between precomputation time and query time, in a customized fashion according to available resources and application requirements. three main theorems: the linearity theorem allows every ppv to be represented as a linear combination of basis vectors, yielding a natural way to construct ppvfrom shared components. the hubs theorem allows basis vectors to be encoded as partial vectors and a hubs skeleton, enabling basis vectors themselves to be constructed from common components. the decomposition theorem establishes a linear relationship among basis vectors, which is exploited to minimize redundant computation. several algorithms for computing basis vectors, specializations of these algorithms for computing partial vectors and the hubs skeleton, and an algorithm for constructing ppvfrom partial vectors using the hubs skeleton. experimental results on real web data demonstrating the effectiveness and scalability of our techniques. in section # we introduce the notation used in this paper and formalize personalized pagerank mathematically. section # presents basis vectors, the rst step towards encoding ppvas shared components. the full encoding is presented in section #. section # discusses the computation of partial quantities. section # summarizes the contributions of this paper. due to space constraints, this paper omits proofs of the theorems and algorithms presented. these proofs are included as appendices in the full version of this paper. we also describe structural differences between question topics that correlate with the success of link analysis for authority discovery. question answer portals such as naver and yahoo answers are quickly becoming rich sources of knowledge on many topics which are not well served by general web search engines. unfortunately, the quality of the submitted answers is uneven, ranging from excellent detailed answers to snappy and insulting remarks or even advertisements for commercial content. furthermore, user feedback for many topics is sparse, and can be insufficient to reliably identify good answers from the bad ones. hence, estimating the authority of users is a crucial task for this emerging domain, with potential applications to answer ranking, spam detection, and incentive mechanism design. we present an analysis of the link structure of a general purpose question answering community to discover authoritative users, and promising experimental results over a dataset of more than million answers from a popular community qa site. in particular, we attempt to discover authoritative users for specific question categories by analyzing the link structure of the community. portals allowing users to answer questions posted by others are rapidly growing in popularity. the reason is that people can share their knowledge, and can find answers for both common and unique questions. some of these information needs are too specific to formulate as web search queries, or the content simply does not exist on the web. other users seek opinions of the community, or are not adept at searching the web and would prefer other people to help them find the relevant information. some popular qa portals include naver and yahoo answers. all non abusive answers later become available for search and retrieval. since going live relatively recently, yahoo answers attracted millions of users and over million answers for more than million questions. unfortunately, the quality of the submitted answers is uneven, ranging from excellent detailed answers to snappy and insulting permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. eugene agichtein department of mathematics and computer science emory university eugene mathcs emory edu remarks or even advertisements for commercial content. therefore, it is increasingly important to better understand the issues of authority and trust in such communities, which differ drastically from previously studied online communities both in types of interactions that are available to users, and the content of the sites. qa portals provide many mechanisms for community feedback. when a question author chooses a best answer, he or she can provide a quality rating. another measure of quality of answer are the thumbs up and thumbs down votes. such community feedback is extremely valuable, but requires some time to accumulate, and often remains sparse for obscure or unpopular topics. in a large sample of the yahoo answers portal that we analyzed, fewer than of all questions had any user votes cast for any of the answers. therefore, it becomes important to estimate the authority of users without exclusively relying on user feedback. we present a large scale study of the link structure of community question answering portal for discovering authorities in topical categories. in particular, we formulate a graph structure for the qa domain, and adapt a web link analysis algorithm for topical authority estimation. we describe an experimental evaluation over a dataset of more than million answers, demonstrating the viability of our approach. we summarize our findings in section #, which concludes the paper. the network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. we develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of context on the world wide web. the central issue we address within our framework is the distillation of broad search topics, through the discovery of authorative information sources on such topics. we propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of hub pages that join them together in the link structure. our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristrics for link based analysis. to copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and or a fee. of course, there are a number of potential pitfalls in the application of links for such a purpose. the network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. in this work, we develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of contexts on preliminary versions of this paper appeared in proceedings of the annual acm siam symposium on discrete algorithms. and as ibm research report rj year#, may year#. this work was performed in large part while. kleinberg was on leave at the ibm almaden research center, san jose, ca. sloan research fellowship, an onr young investigator award, and by nsf faculty early career development award clr. authoraddress: department of computer science, cornell university, ithaca, ny year#, mail: kleinber cs cornell edu. permission to make digital hard copy of part or all of this work for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication, and its date appear, and notice is given that copying is by permission of the association for computing machinery, inc. in particular, we focus on the use of links for analyzing the collection of pages relevant to a broad search topic, and for discovering the most authoritative pages on such topics. while our techniques are not specific to the www, we find the problems of search and structural analysis particularly compelling in the context of this domain. the www is a hypertext corpus of enormous complexity, and it continues to expand at a phenomenal rate. moreover, it can be viewed as an intricate form of populist hypermedia, in which millions of on line participants, with diverse and often conflicting goals, are continuously creating hyperlinked content. thus, while individuals can impose order at an extremely local level, its global organization is utterly unplanned high level structure can emerge only through a posteriori analysis. our work originates in the problem of searching on the www, which we could define roughly as the process of discovering pages that are relevant to a given query. the quality of a search method necessarily requires human evaluation, due to the subjectivity inherent in notions such as relevance. we begin from the observation that improving the quality of search methods on the www is, at the present time, a rich and interesting problem that is in many ways orthogonal to concerns of algorithmic efficiency and storage. in particular, consider that current search engines typically index a sizable portion of the www and respond on the order of seconds. although there would be considerable utility in a search tool with a longer response time, provided that the results were of significantly greater value to a user, it has typically been very hard to say what such a search tool should be computing with this extra time. clearly, we are lacking objective functions that are both concretely defined and correspond to human notions of quality. we view searching as beginning from a user supplied query. it seems best not to take too unified a view of the notion of a query; there is more than one type of query, and the handling of each may require different techniques. consider, for example, the following types of queries: specific queries. for example, does netscape support the jdk code signing api broad topic queries. for example, find information about the java program ming language. for example, find pages similar to java sun com. concentrating on just the first two types of queries for now, we see that they present very different sorts of obstacles. the difficulty in handling specific queries is centered, roughly, around what could be called the scarcity problem: there are very few pages that contain the required information, and it is often difficult to determine the identity of these pages. for broad topic queries, on the other hand, one expects to find many thousand relevant pages on the www; such a set of pages might be generated by variants of term matching do not use the term on their pages. this is a fundamental and recurring phenomenon as another example, there is no reason to expect the home pages of honda or toyota to contain the term automobile manufacturers. analyzing the hyperlink structure among www pages gives us a way to address many of the difficulties discussed above. hyperlinks encode a considerable amount of latent human judgment, and we claim that this type of judgment is precisely what is needed to formulate a notion of authority. specifically, the creation of a link on the www represents a concrete indication of the following type of judgment: the creator of page, by including a link to page, has in some measure conferred authority on. moreover, links afford us the opportunity to find potential authorities purely through the pages that point to them; this offers a way to circumvent the problem, discussed above, that many prominent pages are not sufficiently self descriptive. first of all, links are created for a wide variety of reasons, many of which have nothing to do with the conferral of authority. for example, a large number of links are created primarily for navigational purposes; others represent paid advertisements. another issue is the difficulty in finding an appropriate balance between the criteria of relevance and popularity, each of which contributes to our intuitive notion of authority. it is instructive to consider the serious problems inherent in the following simple heuristic for locating authoritative pages: of all pages containing the query string, return those with the greatest number of in links. we have already argued that for a great many queries, a number of the most authoritative pages do not contain the associated query string. conversely, this heuristic would consider a universally popular page such as www yahoo com or www. netscape com to be highly authoritative with respect to any query string that it contained. in this work, we propose a link based model for the conferral of authority, and show how it leads to a method that consistently identifies relevant, authoritative www pages for broad search topics. our model is based on the relationship that exists between the authorities for a topic and those pages that link to many related authorities we refer to pages of this latter type as hubs. we observe that a certain natural type of equilibrium exists between hubs and authorities in the graph defined by the link structure, and we exploit this to develop an algorithm that identifies both types of pages simultaneously. the algorithm operates on focused subgraphs of the www that we construct from the output of a text based www search engine; our technique for constructing such subgraphs is designed to produce small collections of pages likely to contain the most authoritative pages for a given topic. our approach to discovering authoritative www sources is meant to have a global nature: we wish to identify the most central pages for broad search topics in the context of the www as a whole. global approaches involve basic problems of representing and filtering large volumes of information, since the entire set of pages relevant to a broad topic query can have a size in the millions. this is in contrast to local approaches that seek to understand the interconnections among the set of www pages belonging to a single logical site or intranet; in such cases the amount of data is much smaller, and often a different set of considerations dominates. it is also important to note the sense in which our main concerns are fundamentally different from problems of clustering. clustering addresses the issue of dissecting a heterogeneous population into subpopulations that are in some way more cohesive; in the context of the www, this may involve distinguishing pages related to different meanings or senses of a query term. thus, clustering is intrinsically different from the issue of distilling broad topics via the discovery of authorities, although a subsequent section will indicate some connections. for even if we were able perfectly to dissect the multiple senses of an ambiguous query term, we would still be left with the same underlying problem of representing and filtering the vast number of pages that are relevant to each of the main senses of the query term. section # discusses the method by which we construct a focused subgraph of the www with respect to a broad search topic, producing a set of relevant pages rich in candidate authorities. sections and discuss our main algorithm for identifying hubs and authorities in such a subgraph, and some of the applications of this algorithm. section # discusses the connections with related work in the areas of www search, bibliometrics, and the study of social networks. section # describes how an extension of our basic algorithm produces multiple collections of hubs and authorities within a common link structure. finally, section # investigates the question of how broad a topic must be in order for our techniques to be effective, and section # surveys some work that has been done on the evaluation of the method presented here. today, when searching for information on the www, one usually performs a query through a term based search engine. these engines return, as the query result, a list of web pages whose contents matches the query. for broad topic queries, such searches often result in a huge set of retrieved documents, many of which are irrelevant to the user. however, much information is contained in the link structure of the www. information such as which pages are linked to others can be used to augment search algorithms. in this context, jon kleinberg introduced the notion of two distinct types of web pages: hubs and authorities. kleinberg argued that hubs and authorities exhibit a mutually reinforcing relationship: a good hub will point to many authorities, and a good authority will be pointed at by many hubs. in light of this, he dervised an algoirthm aimed at finding authoritative pages. we show that both salsa and kleinberg mutual reinforcement approach employ the same metaalgorithm. we then prove that salsa is quivalent to a weighted in degree analysis of the link sturcutre of www subgraphs, making it computationally more efficient than the mutual reinforcement approach. these comparisions reveal a topological phenomenon called the tkc effect which, in certain cases, prevents the mutual reinforcement approach from identifying meaningful authorities. we present salsa, a new stochastic approach for link structure analysis, which examines random walks on graphs derived from the link structure. we compare that results of applying salsa to the results derived through kleinberg approach. the lack of structure and the enormous volume of the www pose tremendous challenges on the www information retrieval systems called search engines. these search engines are presented with queries, and return a list of web pages which are deemed to pertain to the query. this distinction pertains to the presence which the querytopic has on the web: narrow topic queries are queries for which very few resources exist on the web, and which present a needle in the haystack challenge for search engines. an example of such a query is an attempt to locate the lyrics of a specific song, by quoting a line from it. the vast majority of users are not interested in retrieving the entire huge set of resources; most users will be quite satisfied with a few authoritative results: web pages which are highly relevant to the topic of the query, significantly more than most other pages. this work focuses on finding authoritative resources which pertain to broad topic queries. term based search engines term based search engines face both classical problems in information retrieval, as well as problems specific to the www setting, when handling broad topic queries. the classic problems include the following issues: synonymy: retrieving documents containing the term car when given the query automobile. polysemy ambiguity: when given the query jordan, should the engine retrieve pages pertaining to the hashemite kingdom of jordan, or pages pertaining to basketball legend michael jordan authorship styles: this is a generalization of the synonymy issue. two documents, which pertain to the same topic, can sometimes use very different vocabularies and figures of speech when written by different authors. from the song yellow submarine by the year#british pop group the beatles. in addition to the classical issues in information retrieval, there is a web specific obstacle which search engines must overcome, called search engine persuasion. there may be millions of sites pertaining in some manner to broad topic queries, but most users will only browse through the first results returned by their favorite search facility. with the growing economic impact of the www, and the growth ofcommerce, it is crucial for businesses to have their sites ranked high by the major search engines. there are quite a few companies who sell this kind of expertise: they design web sites which are tailored to rank high with specific queries on the major search engines. these companies research the ranking algorithms and heuristics of term based engines, and know how many keywords to place in a web page so as to improve the pageranking. a less sophisticated technique, used by some site creators, is called keyword spamming. here, the authors repeat certain terms, in order to lure search engines into ranking them highly for many queries. informative link sructure the answer the www is a hyperlinked collection. in addition to the textual content of the individual pages, the link structure of such collections contains information which can, and should, be tapped when searching for authoritative sources. with such a linksuggests, or even recommends, that surfers visitingfollow the link and visit. note that informative links provide a positive critical assessment ofs contents which originates from outside the control of the author of. this makes the information extracted from informative links less vulnerable to manipulative techniques such as spamming. there are many kinds of links which confer little or no authority, such as intradomain links and advertisements sponsorship links. related work on link structures prior to the introduction of hypertext, link structures were studied in the area of bibliometrics, which studies the citation structure of written documents. when hypertext was introduced, it was widely used to present highly structured information in a flexible computer format which supported browsing. some works have studied the weblink structure, in addition to the textual content of the pages, as means to visualize areas thought to contain good resources. other works used link structures for categorizing pages and clustering them. this is done by considering the potential hyperinformation contained in each web page: the information that can be found when following hyperlinks which originate in the page. this work is motivated by the approach introduced by jon kleinberg. in an attempt to impose some structure on the chaotic www, kleinberg distinguished between two types of web pages which pertain to a certain topic. hubs are primarily resource lists, linking to many authorities on the topic possibly without directly containing the authoritative information. according to this model, hubs and authorities exhibit a mutually reinforcing relationship: good hubs point to many good authorities, and good authorities are pointed at by for example, llc canyontrace new media marketing. link site with strategic link development services by canyontrace. http: www canyontrace com strategic link development htm; also grantastic designs, search engine optimization services from grantastic designs. http: www grantasticdesigns com seo html; as well as internet marketing for internet business. the most prominent community in a www subgraph is called the principal community of the collection. researchers from ibmalmaden research center have implemented kleinbergalgorithm in various projects. the first was hits, which is described in gibson et al, and offers some enlightening practical remarks. the arc system, described in chakrabarti et al, augments kleinberglink structure analysis by considering also the anchor text, the text which surrounds the hyperlink in the pointing page. the reasoning behind this is that many times the pointing page describes the destination pagecontents around the hyperlink, and thus the authority conferred by the links can be better assessed. these projects were extended by the clever project researchers from outside ibm, such as henzinger and bharat, have also studied kleinbergapproach and have proposed improvements to it. anchor text was also used by brin and page. see section # for more details on pagerank. law et al use the links surrounding a small set of same topic sites to assemble a larger collection of neighboring pages which should contain many authoritative resources on the initial topic. the textual content of the collection is then analyzed in ranking the relevancy of its individual pages. this work while preserving the theme that web pages pertaining to a given topic should be split to hubs and authorities, we replace kleinbergmutual reinforcement approach by a new stochastic approach, in which the coupling between hubs and authorities is less tight. thus, we will attempt to identify these pages by examining certain random walks in, under the proviso that such random walks will tend to visit these highly visible pages more frequently than other, less connected pages. we show, that in finding the principal communities of hubs and authorities, both kleinbergmutual reinforcement approach and our ibm corporation almaden research center. stochastic approach employ the same metaalgorithm on different representations of the input graph. through these comparisons, we isolate a particular topological phenomenon which we call the tightly knit community effect. in certain scenarios, this effect hampers the ability of the mutual reinforcement approach to identify meaningful authorities. we demonstrate that salsa is less vulnerable to the tkc effect, and can find meaningful authorities in collections where the mutual reinforcement approach fails to do so. after demonstrating some results achieved by means of salsa, we prove that the ranking of pages in the stochastic approach may be calculated by examining the weighted in out degrees of the pages in. this result yields that salsa is computationally lighter than the mutual reinforcement approach. the rest of the paper is organized as follows. then, in section # we prove the connection between salsa and weighted in out degree rankings of pages. our conclusions and ideas for future work are brought in section #. search engines encounter a recall challenge when handling such queries: finding the few resources which pertain to the query. on the other hand, broad topic queries pertain to topics for which there is an abundance of information on the web, sometimes as many as millions of relevant resources. the challenge which search engines face here is one of precision: retrieving only the most relevant resources to the query. such a link, called an informative link, iss way to confer authority on. another kind of noninformative links are those which result from link exchange agreements. these are bidirectional links between two web pages, whose purpose is to increase the visibility and link popularity of both pages. botafogo et al provided authors of such hypertexts with tools and metrics to analyze the hierarchical structure of their documents during the authoring phase. frisse proposed a new information retrieval scheme for tree hypertext structures, in which the relevancy of each hypertext node to a given query depends upon the nodetextual contents as well as on the relevancy of its descendants. marchiori uses the link structure of the web to enhance search results of term based search engines. the first are authoritative pages in the sense described previously. in light of the mutually reinforcing relationship, hubs and authorities should form communities, which can be pictured as dense bipartite portions of the web, where the hubs link densely to the authorities. kleinberg suggested an algorithm to identify these communities, which is described in detail in section #. another major feature of their work on the google search engine is a link structure based ranking approach called pagerank, which can be interpreted as a stochastic analysis of some random walk behavior through the entire www. the intuition behind our approach is the following: consider a bipartite graph, whose two parts correspond to hubs and authorities, where an edge between huband authoritymeans that there is an informative link fromto. then, authorities and hubs pertaining to the dominant topic of the pages inshould be highly visible from many pages in. we then compare the results of applying salsa to the results derived by kleinbergapproach. we also discuss the reason for our success with analyzing weighted in out degrees of pages, which previous work has claimed to be unsatisfactory for identifying authoritative pages. in section # we view kleinbergapproach from a higher level, and define a metaalgorithm for link structure analysis. in section # we compare the two approaches by considering their outputs on the www and on artificial topologies. the paper uses basic results from the theories of nonnegative matrices and of stochastic processes. the required mathematical background, as well as the proofs of the propositions which appear in section #, can be found in lempel and moran. searching the www the challenge the www is a rapidly expanding hyperlinked collection of unstructured information. when considering the difficulties which www search engines face, we distinguish between narrow topic queries and broad topic queries. this may reflect the fact that pagesandshare a common topic of interest, and that the author ofthinks highly ofs contents. many works in this area were aimed at finding high impact papers published in scientific journals, and at clustering related documents. the advent of the world wide web presented many new research directions involving link structure analysis. http: www almaden ibm com cs clever html. in an expert search task, the users need is to identify people who have relevant expertise to a topic of interest. an expert search system predicts and ranks the expertise of a set of candidate persons with respect to the users query. in this paper, we propose a novel approach for predicting and ranking candidate expertise with respect to a query. we see the problem of ranking experts as a voting problem, which we model by adapting eleven data fusion techniques we investigate the effectiveness of the voting approach and the associated data fusion techniques across a range of document weighting models, in the context of the trec year# enterprise track. the evaluation results show that the voting paradigm is very effective, without using any collection specific heuristics. moreover, we show that improving the quality of the underlying document representation can significantly improve the retrieval performance of the data fusion techniques on an expert search task. in particular, we demonstrate that applying field based weighting models improves the ranking of candidates. finally, we demonstrate that the relative performance of the adapted data fusion techniques for the proposed approach is stable regardless of the used weighting models. with the advent of the vast pools of information and documents in large enterprise organisations, collaborative users regularly have the need to nd not only documents, but also people with whom they share common interests, or who have speci. mertzum pejtersen found that engineers in productdevelopment organisations often intertwine looking for informative documents with looking for informed people. people are a critical source of information because they can explain and provide arguments about why speci. yimam seid kobsa identi ed ve scenarios when people may seek an expert as a source of information to complement other sources: access to non documented information eg, in an organisation where not all relevant information is documented. the kleinberg hits and the google pagerank algorithms are eigenvector methods for identifying authoritative or influential articles, given hyperlink or citation information. that such algorithms should give reliable or consistent answers is surely a desideratum, and in \cite, we analyzed when they can be expected to give stable rankings under small perturbations to the linkage patterns. in this paper, we extend the analysis and show how it gives insight into ways of designing stable link analysis methods. this in turn motivates two new algorithms, whose performance we study empirically using citation data and web hyperlink data. one aspect in which retrieving named entities is different from retrieving documents is that the items to be retrieved persons, locations, organizations are only indirectly described by documents throughout the collection. much work has been dedicated to finding references to named entities, in particular to the problems of named entity extraction and disambiguation. however, just as important for retrieval performance is how these snippets of text are combined to build named entity representations. we focus on the trec expert search task where the goal is to identify people who are knowledgeable on a specific topic. existing language modeling techniques for expert finding assume that terms and person entities are conditionally independent given a document. we present theoretical and experimental evidence that this simplifying assumption ignores information on how named entities relate to document content. to address this issue, we propose a new document representation which emphasizes text in proximity to entities and thus incorporates sequential information implicit in text. our experiments demonstrate that the proposed model significantly improves retrieval performance. the main contribution of this work is an effective formal method for explicitly modeling the dependency between the named entities and terms which appear in a document. a named entity is a semantic category, a pointer to a real world entity such as a city, an organization, a movie, a book, or a historical event. named entities are complex language features that are much richer in semantic content permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. bruce croft department of computer science university of massachusetts amherst amherst, ma croft cs umass edu than most vocabulary words; they occur often in documents, particularly in news. named entities have been shown useful for improving retrieval performance in a variety of tasks such as new event detection and cross language retrieval. named entities of particular interest are those referring to individuals person entities. recent work in retrieving people was motivated both by its practical importance and by the introduction of the enterprise track in the text retrieval conference. the track provides a common platform for empirically comparing entity modeling techniques on expert search, ie, the task of nding people who have skills and experience on a given topic. for example, given the query privacy on the web a relevant entity is someone who works on policies or technologies related to internet privacy. more speci cally, the task involves nding evidence to support the claim of expertise as well as combining the evidence to estimate the probability of a person being expert given the text. automatic methods to rank person entities according to expertise can be applied for a variety of tasks: to help the user identify the most quali ed person to contact, or nd prospective collaborators for a project. promising applications include an expert recommender system for enterprise search, automatically assigning conference submissions to reviewers according to their research interests and matching job applications to potential employers. we present a generative language modeling approach for entity nding that is based on estimating the joint distribution of words and entities. in particular, we focus on the dependency between the two variables. such dependency naturally exists in text because the words directly or indirectly describe the entities. our contributions include kernel based document representation to incorporate positional information by tting a multinomial density around named entity occurrences in a document. investigation of several aspects of named entity retrieval, including the assumption of query term independence, the. we evaluate the proposed model on the task of nding experts but the model is completely unstructured and domainindependent in that the knowledge, interests and expertise of candidate experts are not explicitly represented. instead we estimate a probability distribution over vocabulary terms to describe the context in which a named entity appears. in this paper we address the task of automatically finding an expert within the organization, known as the expert search problem. we present the theoretically based probabilistic algorithm which models retrieved documents as mixtures of expert candidate language models. experiments show that our approach outperforms existing theoretically sound solutions. we introduce a novel approach to expert finding based on multi step relevance propagation from documents to related candidates. relevance propagation is modeled with an absorbing random walk. the evaluation on the two official enterprise trec data sets demonstrates the advantage of our method over the state of the art method based on one step propagation. expert nding is a new rapidly growing information retrieval research area and a popular application domain. the propagation is modeled with an absorbing random walk. an expert nding system ranks people within an organization by their ability to share the knowledge described by a short user query. the analysis of personal expertise is usually based on measuring the degree of co occurrence of personal identi ers with query terms in the organizational documents. the most popular and successful methods following this principle consider the weighted sum of retrieval scores of all documents related to a candidate as a measure of candidateexpertise. in other words, they rely on one step relevance probability propagation from documents to candidates. the presented method, which is the main contribution of this paper, allows multi step relevance probability propagation from documents even to those candidates that are not explicitly mentioned in these documents. a major challenge in developing models for hypertext retrieval is to effectively combine content information with the link structure available in hypertext collections. although several link based ranking methods have been developed to improve retrieval results, none of them can fully exploit the discrimination power of contents as well as fully exploit all useful link structures. in this paper, we propose a general relevance propagation framework for combining content and link information. the framework gives a probabilistic score to each document defined based on a probabilistic surfing model. two main characteristics of our framework are our probabilistic view on the relevance propagation model and propagation through multiple sets of neighbors. we compare eight different models derived from the probabilistic relevance propagation framework on two standard trec web test collections. our results show that all the eight relevance propagation models can outperform the baseline content only ranking method for a wide range of parameter values, indicating that the relevance propagation framework provides a general, effective and robust way of exploiting link information. our experiments also show that using multiple neighbor sets outperforms using just one type of neighbors significantly and taking a probabilistic view of propagation provides guidance on setting propagation parameters. they have used different citation methods in this direction, namely direct citation, bibliographic coupling the sharing of one or more references by two documents and co citation. but intuitively, all neighbors can be potentially exploited; for example, both out links and in links may be useful as we will show in our experiments. in some sense, our work resembles previous work on spreading activation as both involve propagating values through a network graph. using probabilities to control the effect of different groups of neighbors helps. hypertext retrieval, the task of searching for information in a hypertext collection, has been around for a while. a key characteristic that distinguishes the search task in a hypertext collection from a traditional retrieval task is the existence of link information in the former one. although the primary goal of creating links is to guide a user to other parts of the collection, the link information can also be exploited to improve the search accuracy. the existence of this extra information makes it inappropriate to use traditional information retrieval methods, which do the retrieval task based on the content only, to do the search task. the early works on the hypertext retrieval task were more on the literature side. some researchers have used bibliographic citation methods to determine relationships among documents in scienti. modha and spangler have proposed a clustering algorithm that clusters hypertext documents using words, out links and in links, chakrabrti et al have developed a technique called spectral ltering for discovering high quality topical resources in hyperlinked corpora and ray larson has applied co citation analysis methods to the world wide web to produce clusterings of the www sites that have topical similarities. currently with the fast growth and popularity of the world wide web, the search task on this huge collection of hypertext data has gained much attention. the problem of hypertext retrieval on the web has been studied extensively. several link based ranking methods have been developed to improve retrieval results. although these algorithms have been shown to improve the performance over some baseline approaches, it remains a challenging research question what is the best way to exploit the content information and the link information to maximize search accuracy. these works appear to have adopted ve strategies for combining content and link information: using the query as. lter to select documents and rank them according to link based scores; computing a weighted combination of topic speci. pagerank scores, where the weights are determined by the query; using the query to compute the relevance value of each document and regulating the in uence of nodes in hits using these value; using the query to compute the relevance value of each document and propagate these values through links. unfortunately, none of these combination methods can fully exploit the discrimination power of contents as well as fully exploit all useful link structures. despite the importance of link information, the contents of documents are clearly the most direct evidence regarding whether a document is relevant to a userinterest. thus presumably, contents of the documents should be the main basis for ranking them. in this sense, among the ve strategies, only the last two are close to fully exploiting the content information to improve ranking. however, the intelligent surfer only considers the in links of a document, the relevance propagation method only considers direct in links or out links and the term propagation method only considers parent child links in a sitemap. each of these methods only considers one type of explicit neighbors and none of them fully take advantage of all the available link information. besides, for the propagation methods, there exist no principled framework to do the propagation. for example, the content scores can be transformed using any monotonic function without affecting the ranking, but such transformation would presumably affect the propagation. how should we transform the scores to achieve the best propagation results in this paper, we propose a general probabilistic relevance propagation framework for combining content and link information, which can fully take advantage of content information and the link structure in a principled way and can unify most existing link based ranking algorithms. the basic idea of probabilistic relevance propagation is to rst compute a content based relevance probability score for each document using the query, and then propagate the probabilities through different groups of neighbors. we exploit the content information as a basis for nding the probability of the relevance of a document to a query and use the link structure to de ne different groups of neighbors to propagate the probabilities through. after propagation, unlike, our model gives us a probabilistic score for each document de ned based on a probabilistic surfing model. moreover, our model supports using multiple types of neighbors, which is shown to outperform the results of using a single type of neighbor. on the other hand, the probabilistic interpretation of the model suggests that we should transfer the contentbased retrieval scores to probabilities of relevance, which is shown to be bene cial in our experiments. the probabilistic interpretation also provides guidance on how to set various parameters in the propagation model. the main difference, however, is that in these spreading activation methods, the number of steps for propagating the weights is prede ned and is a small value in most of the cases, while our framework is an iterative process which iterates until the ranks converge to a limit. we derive several special instances of the general probabilistic relevance propagation framework and show that probabilistic relevance propagation is a very general mechanism that allows us to recover most of the major existing algorithms as special cases. moreover, it also naturally suggests several new algorithms that can combine content and link information. in our experiments, we evaluated several propagation algorithms and the experiment results show that: using relevance propagation to combine link information and content information for scoring can improve retrieval accuracy over using only content for scoring. using multiple sets of neighbors for propagation outperforms using a single neighbor set. using probabilities to control the in uence of each document in a neighbor set helps. the rest of the paper is organized as follows: we present our relevance propagation framework and derive several special cases in section #. we discuss the experiment results in section # and conclude in section # nally. many nlp tasks rely on accurately estimating word dependency probabilities, where the words and have a particular relationship. because of the sparseness of counts of such dependencies, smoothing and the ability to use multiple sources of knowledge are important challenges. for example, if the probability of nounbeing the subject of verbis high, andtakes similar objects to, andis synonymous to, then we want to conclude that should also be reasonably high even when those words did not cooccur in the training data to capture these higher order relationships, we propose a markov chain model, whose stationary distribution is used to give word probability estimates. unlike the manually defined random walks used in some link analysis algorithms, we show how to automatically learn a rich set of parameters for the markov chain transition probabilities. we apply this model to the task of prepositional phrase attachment, obtaining an accuracy of. word dependency or co occurrence probabilities are needed in many natural language tasks. this includes lexicalized parsing, building language models, word sense disambiguation, and information retrieval. however, it is di cult to estimate these probabilities because of the extreme sparseness of data for individual words, and even more so for word pairs, triples, and so on. for instance, bikel shows that the parser of collins is able to use bi lexical word appearing in proceedings of the st international conference on machine learning, ban, canada, year#. dependency probabilities to guide parsing decisions only of the time; the rest of the time, it backs. to condition one word on just phrasal and part ofspeech categories. if a system could be built with reasonably accurate knowledge about dependency probabilities between all words, one would expect the performance gains on many tasks to be substantial. and interpolation methods have been developed for language modeling. dagan et al showed that performance on zero count events can be greatly improved if the model includes estimates based on distributional similarity. other kinds of similarity among words have also been used to reduce sparseness. for instance, stemming words is a very traditional way of somewhat lessening sparseness, and resources like wordnet have been used in many natural language models. all of these ways of using associations and similarities between words to predict the likelihood of unseen events have their advantages. symbolic knowledge bases, such as wordnet, have the advantage of being based on abundant world knowledge and human intuition, but have the disadvantages of having incomplete coverage and being non probabilistic. using stemming or lemmatized words has been helpful for reducing sparseness in some problems, and slightly harmful in others. here, we propose a method for combining these information sources that induces a distribution over words by learning a markov chain model, where the states correspond to words, such that its stationary distribution is a good model for a speci. the idea of constructing markov chains whose stationary distributions are informative has been seen in several other applications, such as the google pagerankalgorithm, some hits like link analysis algorithms, bi lexical probabilities include two words, one in the conditioning context and one in the future, in addition to possibly other variables, for example, and for query expansion in ir. our workis distinguished from these approaches in that rather than using a carefully hand picked markov chain, we will automatically learn the parameters for the random walk. this allows us to construct markov chains with many more parameters, that are much richer in structure and of signi cantly greater complexity than seen in other applications. in doing so, we can also allow our model to learn to exploit diverse knowledge sources such as wordnet, morphology, and various features of words derived from dependency relations; all of these simply become additional features made available to the random walklearning algorithm. the proposed techniques are general and can be applied to other problem domains, such as the web, citation, and clickstream data. in this paper, we choose deciding the attachment site of prepositional phrases as a touchstone problem, and show how random walkmethods can be applied to this problem. pp attachment decisions are a central component problem in parsing and one of the major sources of ambiguity in practice. for example, in the sentence: he broke the window with a hammer, the prepositional phrase with a hammer could either modify the verb broke, and thus mean that the hammer was the instrument of the breaking event, or it could modify the noun window and thus mean that the window perhaps had a stained glass rendition of a hammer in it. people immediately recognize the more plausible meaning using their world knowledge, but this knowledge is not readily available to parsers. previous research has shown that by using statistics of lexical co occurrences, much higher accuracy can be achieved in comparison to approaches that only look at structure. we discuss the problem of ranking very many entities of different types. in particular we deal with a heterogeneous set of types, some being very generic and some very specific. we discuss two approaches for this problem: exploiting the entity containment graph and ii using a web search engine to compute entity relevance. we evaluate these approaches on the real task of ranking wikipedia entities typed with a state of the art named entity tagger. results show that both approaches can greatly increase the performance of methods based only on passage retrieval. web based communities have become important places for people to seek and share expertise. we find that networks in these communities typically differ in their topology from other online networks such as the world wide web. systems targeted to augment web based communities by automatically identifying users with expertise, for example, need to adapt to the underlying interaction dynamics. we then use simulations to identify a small number of simple simulation rules governing the question answer dynamic in the network. these simple rules not only replicate the structural characteristics and algorithm performance on the empirically observed java forum, but also allow us to evaluate how other algorithms may perform in communities with different characteristics. we believe this approach will be fruitful for practical algorithm design and implementation for online expertise sharing communities. in this study, we analyze the java forum, a large online help seeking community, using social network analysis methods. we test a set of network based ranking algorithms, including pagerank and hits, on this large size social network in order to identify users with high expertise. steve is a java programmer who just started working on a project using java speech on a new mobile platform. but he cannot run his first java speech program on the new platform and needs some help. steve is unable to tell whether the problem has arisen because he does not understand how to use the java speech package, or because java speech does not support the mobile platform well. copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. instead, he may prefer to find and ask someone who has related expertise or experience, and online communities have emerged as one of the most important places for people to seek advice or help. the topics range from advice on medical treatment, programming, software, building a computer from scratch to repairing the kitchen sink. these communities are usually bound by shared professions, interests, or products among their participants. for instance, the sun java forum has thousands of java developers coming to the site to ask and answer questions related to java programming every day. the microsoft technet newsgroup is a major place for programmers to seek help for programming questions relating to microsoft products. even though users in these online communities usually do not know each other and are identified using pseudonyms, they are willing to help each other for various reasons, such as altruism, reputation enhancement benefits, expected reciprocity, and direct learning benefits. this work seeks to enhance online communities with expertise finders. expertise finders, or expertise location engines, are systems that help find others with the appropriate expertise to answer a question. these systems have been explored in a series of studies, including streeter and lochbaum, krulwich and burkey, and mcdonald and ackerman as well as the studies in ackerman et al. newer systems, which use a social network to help find people, have also been explored, most notably in yenta, referralweb, and most recently commercial systems from tacit and microsoft. aside from relying on social networks, another interesting characteristic of these systems is that they tend to blur the dichotomy between experts and seekers. in reality, relatively few people will claim themselves as an expert, but many people agree that they have some measure of expertise in some area. for these expertise finder systems to be of significant assistance, they must effectively identify people who have expertise in the area desired by the asker. a personexpertise is usually described as a term vector and is used later for matching expertise queries using standard ir techniques. the result usually is a list of related people with no intrinsic ranking order or ranks derived from term frequencies. it may reflect whether a person knows about a topic, but it is difficult to distinguish that personrelative expertise levels. this work, done at ibm research, applied several graph based algorithms, including pagerank and hits, to both a synthetic network set and a small email network to rank correspondents according to their degree of expertise on subjects of interest. they found that using a graph based algorithm effectively extracts more information than is found in content alone. however, there is a weakness in these studies. the size of their networks is very small and does not reflect the characteristics of realistic social networks. in this study, we analyze a large online help seeking community, the java forum, using social network analysis methods. we find a small number of structural characteristics in the social networks that we believe lead to differences in the algorithms performance for online communities. we expect that not only will these characteristics be fruitful for practical algorithm design and implementation, but that they will offer new research insights for others to explore. in section #, we introduce the community expertise network and briefly review related work. in section #, we present an evaluation comparing the rankings produced by human raters and by the algorithms. in section #, we then explore the network characteristics that affect the performance of these algorithms using a simulation study. it can be difficult to get a satisfactory answer to steveproblem by searching google directly. these systems attempt to leverage the social network within an organization or community to help find the appropriate others. these systems allow everyone to contribute as they can. most current systems use modern information retrieval techniques to discover expertise from implicit or secondary electronic resources. relying on word and document frequencies has proven to be limited. to ameliorate this, campbell et al and dom et al used graph based ranking algorithms in addition to content analysis to rank users expertise levels. as a result, we wished to revisit the possibilities of using graphbased algorithms on social networks of users in online communities. we then test a set of network based algorithms, including pagerank and hits, on this large size social network. using a set of simulations, we explore how various network structures affect the performance of these algorithms. in section #, we describe the network characteristics of our test online community, the java forum. in section #, we describe some expertise ranking algorithms. and finally, we summarize our findings in section #.