over the past couple of years, netflix has significantly expanded its online streaming offerings, which now encompass multiple delivery platforms and thousands of titles available for instant view. this paper documents the design and development of an outage detection system for the online services provided by netflix. unlike other internal quality control measures used at netflix, this system uses only publicly available information: the tweets, or twitter posts, that mention the word netflix, and has been developed and deployed externally, on servers independent of the netflix infrastructure. this paper discussed the system and provides assessment of the accuracy of its real time detection and alert mechanisms. net ix customers experiencing a servicedisruptionwill makephonecalls tonet ixcustomer service to complain. section discusses anomalousevent and outagedetection methodsimplemented inspoons section discusses theiraccuracy inpredicting serviceoutagesbased thepastyearofdata. futureresearch and development directions are discussed in section #. with more than million members worldwide, net ix, inc. is the worldleading internet subscription service for enjoying movies and tv shows. it streams videos to over internet connected devices. for net ix customers, streaming service availability is key to copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. alex dekhtyar calpoly state university san luis obispo, ca department of computer science dekhtyar calpoly edu matttognetti calpoly state university san luis obispo, ca department of computer science mtognett calpoly edu a satisfying experience. the ability to detect and react to service disruptions quickly is a critical business need and worthy of signi cant research and development. net ix has traditionally used three means of detecting service disruptions. each is described below with their accompanying defects. net ixhas internal systems deployed to monitor their streaming service. these systems are designed to monitor key basic health metrics such as server cpu utilization and disk utilization as well as top levelservice metricssuch asstreamstartspersecond. theproblem with the system is that it shares a common infrastructure with the streaming service itself. as a result, therehavebeen streaming servicedisruptions thathave also disrupted the internal monitoringe orts. ers no value in restoring streaming service functionality. net ix contracts the use of rd party synthetic transaction monitoring systems to monitor streaming service availability from outside of the net ix streaming service infrastructure. while this service avoids the shared infrastructure weakness of the internal monitoring system, it does have a key defect of its own: it only monitors the transactions that net ix designates it to monitor, and its impact is limited to only a handful of transactions deemed important enough to be repeatedly tested through synthetic monitoring. since net ix services are constantly and rapidly evolving, transactions comeandgoonaweeklybasis. however, the external mechanism is not evolving at the same pace. as a result, there are thousands of possible transactions for net ix customers that are not monitored by this external mechanism. this is apoor means of alerting a technical response team because it is too slow. the time it takes for a customer to become frustrated enough to pick up the phone, plus the time it takes for the customer service team to determine there is a real service disruption, can range from to minutes. whilenet ixcustomers maybeslow topick up thephone and contact customer service about service interruptions, a certain vocal percentage of them have found another outlet for their complaints: social streaming media. some net ix customers tend to post to their facebook walls or re. tweets, twitter posts, describing the movies they are currently streaming via a net ix service. more importantly, these customers also write about their negative experiences with the services, such as their inability to stream movies due to a service outage. in response to the weaknesses of the three existing monitoring systems at net ix, the technical teams have been manually using the search mechanism of twitter to search for terms like netflix down to extendtheir abilityto monitortheserviceaswellasdetermineifreparative. ortsduring aservicedisruptionwerehaving an impact twitterdoes not su er from the weaknesses of the existing mechanisms because it is external to the net ix infrastructure. customersalsoprovidefast and continuous feedback; customers will tweet about any and all types of failed transactions, and customers will tweet quickly when a disruption begins, and tweets will continue until the disruption is ended. net ix was curious to see if automation applied to this manual twitter monitoring. ort and engaged the research team fromcaliforniapolytechnicstateuniversity todesign, develop, and deploy such a system. this paper describes a service outage detection system spoons that relies on timely, publicly available information, resides outside net ix controlled server space, and produces realtime outage warnings. this system complements the internal qa solutions and provides an independent assessment of thestateofnet ixstreaming services, and, perhaps more importantly, the mood of net ix customers. the rest of the paper is organized as follows: section # describes research on the use of online streams for information mining and sentiment analysis; section # describes the requirements for and the architecture of spoons and discusses its main components; section # describes an evaluationof the feasibility of thisproject. under contribution is a problem for many online communities. social psychology theories of social loafing and goal setting can provide mid level design principles to address this problem. we tested the design principles in two field experiments. in one, members of an online movie recommender community were reminded of the uniqueness of their contributions and the benefits that follow from them. in the second, they were given a range of individual or group goals for contribution. as predicted by theory, individuals contributed when they were reminded of their uniqueness and when they were given specific and challenging goals, but other predictions were not borne out. the paper ends with suggestions and challenges for mining social science theories as well as implications for design. a significant number of security breaches result from employees failure to comply with security policies. drawing on previous research on usable security and economics of security, we propose a new approach to managing employee security behaviour. we conducted interviews with employees from two major commercial organizations, asking why they do or dont comply with security policies. our results show that key factors in the compliance decision are the actual and anticipated cost and benefits of compliance to the individual employee, and perceived cost and benefits to the organization. we present a new paradigm the compliance budget as a means of understanding how individuals perceive the costs and benefits of compliance with organisational security goals, and identify a range of approaches that security managers can use to influence employee perceptions. the compliance budget should be understood and managed in the same way as any financial budget, as compliance directly affects, and can place a cap on, effectiveness of organisational security measures. many organizations have tried to change or influence security behaviour, but found it a major challenge. with widespread broadband adoption, more households report experiencing sub optimal speeds. not only are slow speeds frustrating, they may indicate consumers are not receiving the services they are paying for from their internet service providers. yet, determining the speed and source of slow downs is difficult because few tools exist for broadband management. we report on results of a field trial with households using a visual network probe designed to address these problems. we describe the results of the study and provide design implications for future tools. more importantly, we argue that tools like this can educate and empower consumers by making broadband speeds and sources of slow downs more visible. does crowdsourcing work for web security while the herculean task of evaluating hundreds of millions of websites can certainly benefit from the wisdom of crowds, skeptics question the coverage and reliability of inputs from ordinary users for assessing web security. we analyze the contribution patterns of serious and casual users in web of trust, a community based system for website reputation and security. we find that the serious contributors are responsible for reporting and attending to a large percentage of bad sites, while a large fraction of attention on the goodness of sites come from the casual contributors. this complementarity enables wot to provide warnings about malicious sites while differentiating the good sites from the unknowns. this in turn helps steer users away from the numerous bad sites created daily. we also find that serious contributors are more reliable in evaluating bad sites, but no better than casual contributors in evaluating good sites. we discuss design implications for wot and for community based systems more generally. as interest in usable security spreads, the use of visual approaches in which the functioning of a distributed system is made visually available to end users is an approach that a number of researchers have examined. in this paper, we discuss the use of the social navigation paradigm as a way of organizing visual displays of system action. drawing on a previous study of security in the kazaa peer to peer system, we present some examples of the ways in which social navigation can be incorporated in support of usable security. symposium on usable privacy and security year#, july, year#, pittsburgh, pa, usa. security has always been a critical concern for information systems, but the rapid rise of the internet as a site for everyday activity has made it a particularly pressing concern lately. the internet is a major means for consumer commerce, for individual banking, and for participation in civic life as the daily use of the internet has increased, so has its attractiveness to attackers. bruce schneier has observed that security measures that arenunderstood and agreed to by everyone donwork. security copyright is held by the author owner. permission to make copyright is held by the author owner. permission to make digital or hard copies of all or part of this work for personal or classroom use is digital or hard copies of all or part of this work for personal granted without fee. symposium on usable privacy and security year#, july, year#, pittsburgh, pa, usa or classroom use is granted without fee. as a concern for end users, then, has become an increasingly important topic of research interest. a number of perspectives in this work have emerged. one research approach has focused on the critical examination of the usability of security mechanisms available in current networked systems. whitten and tygar study of the usability of pgp for secure electronic mail is perhaps the quintessential example of this approach, applying traditional usability analysis techniques to the technologies of security. analyses of these sorts have uncovered a range of problems with the ways in which security technologies have been grafted on to applications and infrastructures, and also demonstrated the considerable knowledge of security technologies that they seem to require on the parts of their users. a second research approach has been to create new mechanisms designed to replace existing security facilities while providing greater usability for example, the use of passfaces rather than passwords is designed to allow authentication mechanisms that are less likely to be forgotten and less susceptible to attack. a third approach has been to step back from the specific problems of current security mechanisms and to examine security as a facet of interaction more broadly. this is the approach that we wish to examine here. empirical work has looked at security as a practical concern and examined the ways in which people go about working securely, while design activities have examined new approaches in which security is understood not simply as a set of features to be included in a software system, but rather as a pervasive aspect of its design. the central concern here is that neither usability nor security can be added on to systems after the primary design work is done; rather, both need to be central aspects of the design effort. in our own work, we have been especially concerned with the use of visualization based approaches to security. in particular, we have argued that the central problem of security for most users is to match the settings within which they find themselves to an immediate set of needs and practical concerns. quite what secure means at any given moment is a determination that only an end user can make. attempts to make systems inherently secure, then, are problematic because they presuppose what secure might be, taking that decision out of the users hands; and attempts to incorporate transparent security into a system are equally problematic because they make it impossible for users to determine whether and how a system is secure. our approach, then, has been to explore the use of dynamically coupled visualizations of system activity that can make aspects of a systeminternal operation visible and examinable. we read system broadly here; our concern is not simply with a particular computer, but with the collective functioning of a range of components that together make up the system at any given moment. security is an end to end phenomenon, and so too must be our visualization strategy. beyond this broad concern for visualization as an approach to security, little has been said about particular approaches or design techniques. in this paper, we want to explore one particular paradigm for visual security interfaces, that of social navigation. security solutions fail not only because of technological or usability limitations, but also due to economic constraints and lack of coordinated adoption. existing research conceptualizes security as a public good suffering from underinvestment, or as a private good with externalities, ie, consequences that are not part of the price. we leverage canonical economic theory of club goods and common pool resources to encourage security through collective action and peer production. we operationalize these by providing examples of security solutions redesigned as club or pool goods. investigating the paradigm of cooperation through community informs novel solutions that impinge on real world security and we advocate further research to enable this shift. it is also difficult to distinguish high and low quality security products, thus where there is incentive the resulting investments may be misdirected. we argue for a new paradigm of security solutions designed for communities rather than individuals. current security solutions target individual end users, often identi ed as the weakest link. yet, individuals do not capture much of their investment in security, both because of the externalities that impinge on others as well as the public good nature of some investments. for example, the prevention of a ddos attack on the department permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. thus, the economics of investment, without cooperation, would lead to underinvestment, ie, without su cient incentives, the investment in security is suboptimal. in this paper we argue for a paradigm shift by proposing solutions for communities and encouraging security through collective action and peer production, ie, a collaborative approach to creating goods and services. security solutions targeting individual endusers are similarly limited. for example, the misalignment of cost and liability implies that while the cost of security investment is borne by individuals, the liability of security incidences, such as ddos attacks, is experienced by others. proposed solutions include legislative measures, such as graduated response, which, though well meaning, may be counterproductive due not only to the potential for abuse but also due to the lack of security education. due to the information asymmetry between security software vendors and consumers, the end users may not be able to distinguish between systems that are more secure from those that provide only basic protection, thereby leading to a market for lemons as we see in used car markets. even if individuals are capable of knowing the quality of security software with help from a clear indicator, the free rider problem remains. that is, individuals take advantage of increased network security without personal investments in security. ective, as in many cases this requires pure altruism and in other cases it requires over investment in a public good. without adequate incentives security solutions are subject to the free rider problem. when the reliability of a system is dependent on the best. ort of a group, only the individuals with the highest cost bene. ratio contributes, while others are rationally inclined to withhold investment. however, information systems are inherently value laden and can be restructured. the individual disincentive to invest and the individual incentive to free ride can be overcome by the peer production of a security club or common secure computational resources, eg, secure common bandwidth. we argue for a paradigm shift, focusing the design of security solutions for communities and not just individuals. the success of peer production through collective action is well documented in other domains. a classic example in software is that of the linux kernel. similarly, in security camp developed the net trust system, where individual website whitelists could be uploaded and shared among groups of friends without additional sharing of histories or comments. from an economic perspective our innovation lies in considering security as a club good or common pool resource. both of these economic notions suggest that security could be implemented in a community setting, but carry a slight di erence; as a common pool resource, every individual could participate as long as the resource constraint is satis ed, but as a club good people are invited to join a community and may be excluded from participation. section # introduces the economic paradigm we leverage such that security investment could approach the optimal level. section # elaborates the economic background of implementing security as a club good. section # discusses an instantiation for security as a club good. section # illustrates an alternative situation in which security can be considered as a common pool resource. section # provides an example of common pool resource implementation in security. section # elaborates on additional possible peer based implementations. we propose the design of security as a club good and show how this might be done to encourage communities to invest in patching and engage in cooperative subversion detection. ected the bystanders who unwillingly participated in anonymouslow orbit ion canon botnet. dshield, bugtraq and the con cker cabal are other examples of collaborative security. we also introduce two motivating examples for this innovation: cooperative subversion detection and community patching. section # summarizes our contributions and concludes the paper. individually rational decisions often lead to suboptimal group outcomes, as illustrated by over shing and global warming. thus, security solutions can be engineered as clubs to prevent the potential for hidden action. many commerce websites post privacy policies to address internet shoppers privacy concerns. iconic privacy indicators may make privacy policies more accessible and easier for users to understand: in this paper, we examine whether the timing and placement of online privacy indicators impact internet users browsing and purchasing decisions. we conducted a laboratory study where we controlled the placement of privacy information, the timing of its appearance, the privacy level of each website, and the price and items being purchased. we found that the timing of privacy information had a significant impact on how much of a premium users were willing to pay for privacy. we also found that timing had less impact when users were willing to examine multiple websites. finally, we found that users paid more attention to privacy indicators when purchasing privacy sensitive items than when purchasing items that raised minimal privacy concerns. many popular web browsers are now including active phishing warnings after previous research has shown that passive warnings are often ignored. in this laboratory study we examine the effectiveness of these warnings and examine if, how, and why they fail users. we simulated a spear phishing attack to expose users to browser warnings. we found that of our sixty participants fell for at least one of the phishing messages that we sent them. however, we also found that when presented with the active warnings, of participants heeded them, which was not the case for the passive warning that we tested where only one participant heeded the warnings. using a model from the warning sciences we analyzed how users perceive warning messages and offer suggestions for creating more effective warning messages within the phishing context. social navigation is a promising approach for supporting privacy and security management. by aggregating and presenting the choices made by others, social navigation systems can provide users with easily understandable guidance on security and privacy decisions, rather than requiring that they understand low level technical details in order to make informed decisions. we have developed two prototype systems to explore how social navigation can help users manage their privacy and security. the acumen system employs social navigation to address a common privacy activity, managing internet cookies, and the bonfire system uses social navigation to help users manage their personal firewall. our experiences with acumen and bonfire suggest that, despite the promise of social navigation, there are significant challenges in applying these techniques to the domains of end user privacy and security management. due to features of these domains, individuals may misuse community data when making decisions, leading to incorrect individual decisions, inaccurate community data, and herding behavior that is an example of what economists term an informational cascade. by understanding this phenomenon in these terms, we develop and present two general approaches for mitigating herding in social navigation systems that support end user security and privacy management, mitigation via algorithms and mitigation via user interaction. mitigation via user interaction is a novel and promising approach to mitigating cascades in social navigation systems. a social navigation system is a collaborative computing system copyright is held by the author owner. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee. that collects and aggregates behaviors, decisions, or opinions from a user community and displays this information to individuals to guide their behavior or inform their decision making. there has been substantial research in social navigation systems, and researchers have applied social navigation systems to many diverse domains. social navigation systems, however, are not just of academic interest; social navigation systems are an integral component of numerous businesses, pointing users toward highly rated posts on discussion forums, frequently downloaded recipes in an online cookbook, and recommendations for songs that a user may be interested in purchasing from an online store. many highly popular websites use social navigation systems either as a primary or complementary component of their site, including the online store amazon, the technology news and discussion website slashdot, and the websites for the news organizations bbc and the new york times. social navigation is a promising approach for supporting end user privacy and security management. since many users are unmotivated to manage their privacy and security and do not understand the technical issues associated with privacy and security management, social navigation systems can provide a new, simpler approach to informed decision making. for example, since prior research has shown that users often prefer to delegate privacy and security management to others, social navigation can provide for such delegation: a user that is unsure about how to manage his privacy or security can simply choose to follow the communitymajority decision. we have developed two prototype systems to explore how social navigation can help users manage their privacy and security. the acumen system employs social navigation for privacy management; acumen helps individuals manage their internet cookies both manually and automatically based on the behavior of others. the bonfire system uses social navigation for security management; bonfire is a personal firewall that uses multiple types of social navigation data to help users make firewall management decisions. our experiences with acumen and bonfire suggest that, despite the promise of social navigation in security and privacy applications, there are significant challenges in applying the technique in these domains. in particular, due to the types of decisions and general lack of expertise among users in these domains, individuals may make incorrect inferences from a social navigation systemcommunity data and misuse community data when making decisions. these incorrect inferences and misuse of community data can lead to incorrect individual decisions, symposium on usable privacy and security year#, july, year#, mountain view, ca, usa. inaccurate community data, and herding behavior in which a community consensus builds for an incorrect decision. these challenges serve as the motivation for this paper. we argue that these challenges are due to informational cascades that can arise in these systems. informational cascades are an economic concept and the subject of considerable research; cascades occur when individuals, faced with a decision, ignore their own information and choose to go with the majority decision, thereby creating a herd or cascade. an analysis of acumen and bonfire indicates that mitigating informational cascades is necessary if social navigation systems are to be useful for privacy and security management. we discuss two general approaches for mitigating cascades in social navigation systems, mitigation via algorithms and mitigation via user interaction. given the weaknesses of using an algorithmic approach to mitigating cascades, we propose that employing user interaction techniques is a promising approach for mitigating cascades. both approaches have merit but require tradeoffs and are dependent on features of the sociotechnical system surrounding the privacy or security management activity and its supporting social navigation system. first, we describe two systems that apply social navigation to support common privacy and security management activities, as well as our experiences with these systems and the challenges of using social navigation in these contexts. second, we analyze these challenges in the context of informational cascades research and argue that mitigating cascades can improve the utility of social navigation systems for privacy and security management. third, we describe two general approaches for mitigating cascades in social navigation systems targeted at end user privacy and security management: mitigation through algorithmic strategies and mitigation through user interaction techniques. in this paper, we explore novel techniques for combating the phishing problem using computational techniques to improve human effort. using tasks posted to the amazon mechanical turk human effort market, we measure the accuracy of minimally trained humans in identifying potential phish, and consider methods for best taking advantage of individual contributions. furthermore, we present our experiments using clustering techniques and vote weighting to improve the results of human effort in fighting phishing. we found that these techniques could increase coverage over and were significantly faster than existing blacklists used today. phishing is an ongoing kind of semantic attack that tricks victims into inadvertently sharing sensitive information. many problems still require human intelligence to solve. some require human intelligence as an intrinsic part of the process, such as in a democratic election. others have no known technical solutions which match human performance, such as image labeling. in the case of certain kinds of computer security tasks, it has been suggested that it is too risky to take the human entirely out of the loop. no matter what the problem, it is important to consider how, when, and how much human effort is necessary to determine an appropriate and sufficient solution. note copyright is held by the author owner. that this number does not include loss of productivity, cost of maintaining a helpdesk to field calls, recovery costs, or damage to an organizationreputation. however, these methods are prone to false positives as well as false negatives. sheng et al have observed that industry has been slow to adopt heuristics primarily from concerns over liability due to false positives. however, human verification is inherently more labor intensive and can be much slower in detecting attacks. submissions require a minimum of votes before labeling, with at least agreement. of particular interest to us here is the blacklist maintained by phishtank, which uses a wisdom of crowds approach. most of these urls represent wasted votes which did not reach the required number of votes for verification. lastly, hours represents the median, with past work suggesting that there is a power law distribution in identifying and taking down phish. a hybrid approach could also help with forensic analysis, as well as help reduce the labor in maintaining the many databases that store data about current and past phishing attacks. in this paper, we present the results of a study that we conducted with aquarium, an experimental system we developed on top of amazonmechanical turk system for gathering human verified labels on potential phishing sites. from a broad perspective, this paper looks at how to apply crowdsourcing techniques to a security task, and how to use computational techniques to improve the performance of a crowd. one particularly difficult to automate problem that currently requires human intelligence is identifying phishing scams. the most common form of phishing is where attackers build convincing imitations of legitimate websites and lure unsuspecting victims to divulge sensitive personal information. moore and clayton estimated that the minimum loss to consumers was million annually. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee. symposium on usable privacy and security year#, july, year#, pittsburgh, pa, usa. many feature based algorithms have been developed to automatically detect phishing sites, for example. the advantage of heuristics and machine learning approaches is that they can rapidly identify attacks with no human involvement. an alternative that has been widely adopted by industry is humanverified blacklists. these blacklists contain urls of sites that have been manually verified as phish. three well known phishing blacklists are operated by microsoft, google, and phishtank. the main advantage of blacklists is that there are very few, if any, false positives, thus reducing the liability risk of incorrectly labeling a legitimate site as a phishing attack. another advantage is the ability to detect new kinds of phishing attacks without explicit retraining. finally, human verification can also be overwhelmed by simply generating more phishing sites and or urls for phishing sites, as has been done with automated phishing attack toolkits and fast flux techniques that hide a phishing site behind a large number of compromised hosts to make detection more difficult. january year# january year# submissions, total votes, valid phish, invalid phish median time hrs min hrs min table #. volunteers submit potential phish and also vote on submitted urls, identifying them as phish or legitimate. according to phishtankown statistics, out of url submissions from volunteers, there were votes, resulting in about identified phish between october year# and february year#. phishtank has improved in performance as shown in table #. from jan year# to jan year#, the median time to identify a phish has dropped from hours to about hours. the percentage of valid phish identified has also increased, going from, out of, in january year# to, out of, we have two observations. first, for january year#, there are still, urls not identified as phish or legitimate. optimally, with votes required to identify a phish, votes could have identified a maximum of, labels rather than the, phish and legitimate sites actually identified. second, a median delay of hours still represents a significant gap in protection, as most victims of a phishing scam fall for it within hours of the start of the attack. furthermore, hours only represents the delay from when the url was first submitted to phishtank, meaning that the phish was in the wild longer. we believe a promising solution is to improve the wisdom of crowds by combining manually verified blacklists with computational techniques, to keep false positives extremely low while also reducing the time to verify attacks. such an approach would benefit not only sites like phishtank, but also other manually verified blacklists such as google and microsoft. more specifically, this paper makes the following research contributions: we present the design of aquarium, a novel phish detection approach that makes use of two points in this design space, namely clustering similar phish together and having minimally trained participants vote on clusters rather than individual phish, and developing a vote weighting mechanism based on a participanthistorical performance. the act of customizing software is generally viewed as a solitary activity that allows users to express individual preferences. in this study, users at two different research sites, working with two different kinds of customizable software, were found to actively share their customization files with each other. this sharing allowed the members of each organization to establish and perpetuate informally defined norms of behavior. a small percentage of people within the organization were responsible for most of the sharing. one group of these were highly skilled software engineers, who were usually the first to try new software. they used customization as a way to experiment with and learn about the software and made their files available to others through various broadcast mechanisms. this group did not try to determine whether their customizations were useful to other users. the second group were less skilled technically but much more interested in interpreting the needs of their colleagues and creating customization files tailored to those needs. they acted as translators between the highly technical group and the rest of the organization. the spontaneous sharing of customization files within an organization has implications for both organizations and for software designers. managers should recognize and support the role of translators, recognize that not all sharing is beneficial, and provide opportunities for the exchange of customization files and innovations among members of the organization. software designers should provide tools that allow users to evaluate the effectiveness of their customizations through reflective software, provide well tested examples of customization files with the first release of the software, explicitly support sharing of customizations, and provide tools to support the activities of translators. such normative conflicts between security policies and social norms are therefore undesirable from a security perspective. depending on the situation, this could mean forcing the user to make an ethical choice, by designing out conflicts. where normative conflicts either cannot be avoided or emerge later, the organisational processes are used to engage with subcultures to encourage communally mediated control. security weaknesses often stem from users trying to comply with social expectations rather than following security procedures. it has been argued that system developers have a meta task responsibility, meaning that they have a moral obligation to enable the users of the system they design to cope adequately with their responsibilities. in this paper, we ask the question to what extent it is possible to detect such potential normative conflicts in the design phase of security sensitive systems, using qualitative research in combination with so called system models. we then envision how security design might proactively reduce conflict by designing out conflict where possible in the development of policies and systems, and responding to residual and emergent conflict through organisational processes. the approach proposed in this paper is a so called subcultural approach, where security policies are designed to be culturally sympathetic. he would need an entrance card to do so. attackers know this, and exploit the resulting weaknesses by means of social engineering. one night, the rst author needed to go back to the of ce. he arrived at the same time as someone else. these drivers include the organisational ideology, beliefs, rituals and myths. for example, dressing up as santa claus seems to work particularly well for accessing restricted areas, since people do not regard santa claus as a malevolent symbol, are socialised with the ritual of granting access to benign visitors and do not associate santa claus with myths of malicious behaviour. in this paper, we focus on normative con icts in the security context, ie, situations in which agents face contradictory expectations on their behaviour, as expressed in organisational or social norms. if that is not the case, there is a con ict in theory, but no con ict in practice. we therefore aim at providing tools for removing certain con icts and identifying where others are inevitable. finally, we discuss how design changes can support the avoidance of policy con icts. if he opened the door rst, the second person might try to tailgate behind him. when the door is open, this becomes very hard to prevent, as he will basically need to block the entrance, which goes against social norms and could lead to him being considered rude and antisocial. instead, he chose to wait until the second person opened the door. this looked very suspicious to the second person, such that the latter did not want to open the door rst either. in many cases, security rules con ict with cultural practices that are prevalent in the workplace. the philosophy of security policy design is often an adversarial one: one needs to treat the other as a potential enemy, as a means rather than an end, and this goes against our social and ethical predispositions. social and ethical predispositions are coded into organisational culture, and a family of culturerelated drivers also motivate and shape the organisational practices. we evaluate how possible it is to design out normative con icts when designing security policies and systems. we do this from the perspective of security policy alignment. security policy alignment deals with consistency of security policies, and completeness of the re nement of security policies. normative con icts can be seen as a special case of policy inconsistency, namely one where non security related normative constraints have been explicitly included in the policy model, and con icts occur in the policies assigned to humans. not all policies that con ict in theory will actually con ict in practice. for example, there may be a policy that forces me to give my manager access to the sales data, and a policy that forbids sharing sales data with family. these would con ict if my manager were my wife. in organisations, there will be many potential con icts, but not all of them will actually manifest themselves in real life situations. models of the organisation can support such reasoning on actual con icts, and the interpretation of the system models can be compared across di erent organisational subcultures. rather than preventing attacks directly, or discovering problems in real time, our analysis aims at identifying normative policy con icts in the security policy design phase. having identi ed these con icts, our approach mitigates them by ensuring that security responsibilities are encoded into the design, in terms of the policy itself, security technology, and the organisational processes which implement and maintain the policy. this approach is motivated by the assumption that, even if policy con icts do not directly cause security threats, they lead to uncertainty with the users, which may in the end weaken their con dence in maintaining security procedures. in this sense, our analysis does not assume a priori that solutions for noncompliance should lie in educating the user; rather, we look for solutions in changing the design of the socio technical system surrounding the user, which is a composition of technology and the socio structural system. whereas technology has implicit constraints on how it can be used, the socio structural system can be thought of as containing the policies, procedures, organisational processes and controls. both need to be aligned to allow users to cope adequately with their security responsibilities. in order for this approach to analysis to succeed, we have to somehow reduce the complexity of a real life scenario. our approach is to take the complex cultural systems within an organisation and reduce the values and rituals to a set of organisational norms, acknowledging that some of the organisational norms will vary from subculture to subculture. it is understood that in reducing cultural systems in this way, some of the complex cultural interaction is abstracted away. as con icts emerge from interaction between users and the system, and as not all interactions can be predicted, not all con icts can be prevented. as a result, such an approach will not completely eliminate normative con ict, but will rather reduce it. organisational processes for implementing and maintaining security policies are used to respond to residual and emergent con ict. one can adjust the management processes to compensate, although these are generally weaker in. based on the novel idea of integrating security policies and social norms in a single analysis, our contributions in this paper are the following: an investigation of the fundamental challenges posed by normative con icts in security; an extension of reasoning on security policies to include social norms; and. an architecture for a method to reduce normative con icts in system design. our subcultural approach to security policy design and implementation is based on the following strategy. firstly, we study in more detail the ideas about embedding responsibility in design, and how these apply to security. secondly, we identify security policies, ethical norms and personal predispositions, formalise these, and analyse them for potential con icts. thirdly, in order to study whether such con icts can actually occur in the dynamic behaviour of the system, we extend system models with support for both detecting policy con icts and analysing their contribution to attacks. we investigate regrets associated with users posts on a popular social networking site. our findings are based on a series of interviews, user diaries, and online surveys involving american facebook users. we discuss methodological considerations in studying negative experiences associated with social networking posts, as well as ways of helping users of social networking sites avoid such regrets. their regrets revolved around sensitive topics, content with strong sentiment, lies, and secrets. some reported incidents had serious repercussions, such as breaking up relationships or job losses. over million households in the united states have a home computer and an internet connection. the vast majority of these are administered by people who have little computer security knowledge or training, and many users try to avoid making security decisions because they feel they dont have the knowledge and skills to maintain proper security. nevertheless, home computer users still make security related decisions on a regular basis for example, whether or not to click on a shady link in an email message without even knowing that what they are doing. their decisions are guided by how they think about computer security, or their mental models, which do not have to be technically correct to lead to desirable security behaviors. in other words, sometimes even wrong mental models produce good security decisions. by eliminating the constraint that nontechnical users must become more like computer security experts to properly protect themselves, we believe that we can create more effective ways of helping home computer users make good security decisions. to that end, we propose a research agenda that will help us learn how to shape the mental models of regular non technical computer users. there are over million computers located in households in the united states. a large proportion of these home computers frequently fall victim to various security threats, including contracting viruses, becoming botnet zombies, and being compromised by phishing scams. threats to home computer security comprise a major epidemic right now; this insecurity is causing thousands of hours and millions of dollars in lost time and energy, and is dramatically increasing the di culty of having a computer in the home. and, due to recent innovations by hackers, these security problems are now spilling over and causing problems for many other sectors of the economy. just recently, a hacker group calling itself anonymous used ddos attacks against paypal and amazon in retaliation for dropping support for wikileaks. it would not be surprising if many of the bots used in these attacks were compromised home computers. di erent security researchers provide di erent answers to why this problem persists. software engineers proclaim the di culty of writing bug free code. some usable security researchers believe that the software is simply too complex to operate securely. as social scientists, we have a di erent take on this problem. we donbelieve that the majority of people are stupid, or computer illiterate. rather, we believe that users are intentionally choosing actions that leave them insecure. this is not because they are being tricked by social engineering, but rather because people honestly believe that they are doing what is necessary to protect their computers. in a previous study, rick wash found that home computer users have a variety of di erent mental models of security threats. mental models describe how a user thinks about a problem; it is the model in the personmind of how things work. people use these models to make decisions about the. for example, some believed that hackers are mischievous teenagers showing. others believed that hackers are criminals out to steal nancial and identity information. all of the respondents he interviewed were motivated to take positive security actions, but only for the threats they believed existed. users who believed that hackers are teenaged troublemakers were likely to install rewall and other security software to keep them out, while the users who saw hackers as criminals frequently believed that they were not rich or important enough to be a target, and therefore didnneed to secure their computers. neither model is correct, though both are used to make decisions. these users were trying to do something to protect against the threats as they understood them. one of the most interesting results from this study was that even users who had wrong or incorrect mental models sometimes made good security choices. even though most hackers today are not teenagers trying to impress friends, people who had that mental model worked hard to ensure their computer was protected. this suggests a promising approach that may help home computer users better secure their systems: induce mental models of security threats that lead to good security behaviors. even if the mental models are wrong, they can still lead to good security behaviors and more secure computers. we should help people develop an understanding of computer security that leads to good security behaviors, even if those understandings might not be technically correct. to change peoplemental models, we need to do two things: identify how people form these mental models, and how we can in uence them; and identify which models are associated with what security behaviors, so we know which models we want home computer users to possess. this approach does not require informing non technical users about the details of computer security and how computers work, which most people are not interested in learning. nor does it require making decisions for the users, which is a di cult solution to implement because it is technically challenging to come up with defaults that work for everyone, and frustrating for users who feel like they have little choice in the matter. instead, we hypothesize that it is possible to empower home computer users to make their own choices, but in a way that leads to positive security for all. the interviews conducted by wash suggested that most people formed their mental models of security threats based on reasoning about information provided by stories recounted by their friends and colleagues. this process we described has much in common with existing research about folk models or lay theories that form through stories shared among people in a community, and through personal experience. like folk models, the models identi ed by wash are simple enough that many home computer users can understand them, but powerful enough to. therefore, we believe that the rst step is to better understand how people learn about and form mental models of security threats, so that we can develop ways to in uence these models and inspire better security behaviors. we suspect that most of the information comes from stories from friends and colleagues and other people like me. this knowl edge will also allow us to be more. ective at teaching nontechnical home computer users about security by giving us a way to talk to them that they can understand and incorporate into their behavior. one of the challenges to this approach is that most of the work in this area, including the previous interview study, provides only self reported data about security behaviors. unfortunately, people often do not accurately report their security behaviors, because they want to be seen as more security conscious than they actually are, or because they are mistaken or unaware. for example, the year# national cyber security alliance norton symantec online safety study found that of people answered yes to indicate that they had a full software security suite, but only actually had one installed. to make widespread progress, we need to measure the connection between mental models and actual security behaviors. not all mental models lead to positive security behaviors; we want to identify the ones that do. and we want to understand how prevalent di erent models are, which may help us and other security researchers to understand why certain types of vulnerability are more common than others. finally, to actually make a di erence in home computer security, we need to nd a way to distribute this information to actually help people form the mental models that lead to good security behaviors. traditional training methods, such as having an expert teach a group of home computer users, will not work here, both because these methods are intractable and expensive, and because previous work suggests that mental models are best transmitted through stories from friends and other people like me. ective method of improving home security therefore is to get home computer users to train each other and spread the good mental models amongst themselves. inspiring example: home thermostats as an inspirational example, consider some work done by willet kempton in the mid year#s. kempton is an anthropologist interested in energy conservation, and was studying how people make decisions about home heating. one of his papers focused on the question of thermostat setting: how do people set their thermostats in their homes to keep the house warm in the winter, and does this pattern signi cantly. ect the amount of energy they use kempton discovered that most people had one of two mental models of how thermostats work, and those models played a signi cant role in how these people made thermostat setting decisions. in the valve model, people believed the thermostat operated like a valve on a faucet; when you turn it higher, more heat comes out. people who operated based on this model showed fairly erratic thermostat settings, frequently turning the thermostat up to heat the house faster, then turning it way down once the house was warm. kempton con rmed this with data from the energy company. alternatively, people who operate with the feedback model believe that the thermostat turns the furnace on and. based on room temperature, but the furnace runs at a single constant output when on. people with this model frequently set it once and allow the thermostat to keep the temperature approximately the same throughout the day. as kempton puts it, heating engineers are fairly comfortable with the theory described here; they consider it simpli ed but essentially correct. the feedback theory is closer to the correct model of how thermostats work than the valve theory. however, upon interviewing users and looking at energy use data, kempton concluded that the valve theory actually works better than the feedback theory. kempton claimed that thermostat use consistent with the valve theory leads to more comfortable houses. but, more importantly, he found that it also leads to less overall energy use. he traced this to one important decision: should a user turn the thermostat down overnight users with the feedback model believe that it takes more energy to raise the temperature of the house from to than simply maintaining a steady temperature of. users operating with the valve theory correctly predict that more fuel is consumed at higher settings than at lower settings, and thus turning the thermostat down overnight saves energy. the valve model prediction is correct, even though the reasoning is wrong; hotter houses lose heat faster to the outside than colder houses. thus, valve model users are more likely to turn down the thermostat overnight, and thus generally use less energy than feedback model users even though their mental model is less correct. from this example we draw a number of interesting lessons. first, users make everyday decisions using simpli ed mental models. the people in kemptonstudy made multiple decisions every day based on simple mental models that do not represent a full understanding of home heating and energy use. neither model includes an understanding of how the house interacts with the outside air, which is an important part of expert reasoning about home heating. however, both models serve the goals of the user in helping to guide their decision making in setting the thermostats in their homes. second, none of the users in kemptonstudy possess a complete and accurate model of home heating. no one he interviewed included interactions with outside air in their mental model. we suspect that very few people include these interactions in their mental model of thermostats even today. the more correct model is complicated, and di cult to use for decision making. having to reason about a wide variety of factors to make a simple thermostat setting decision is not worth it, and people prefer to use simpler models that make the decisions easier and yield suf ciently good outcomes. in a word, most people satis ce when choosing a mental model. third, just because a model is closer to the correct model does not necessarily mean that it leads to better decisions. valve model users make decisions that both lead to greater comfort and lower energy use. all models that lay people use are simpli cations and will get some decisions wrong; it is important to gure out which models lead to better decisions, not which models are closer to correct. kempton argues that technical experts will evaluate folk theory from this perspective not by asking whether it ful lls the needs of the folk. but it is the latter criterion on which sound public policy must be based. experts often use correctness as a shortcut, assuming that more correct models lead to more correct decisions. finally, simpli ed mental models that non experts use often lead to behavior that seems erratic. valve model users frequently change the setting on their thermostat, micromanaging the heating of the house. but it is important to look at the big picture; that erratic micromanaging uses extra energy, but that energy is made up for by the lowering of the temperature overnight. it is extremely di cult to nd a mental model that is both simple enough that people use it, and always leads to a correct decision. but sometimes changing to a di erent model can lead to a better overall outcome even though there exist individual decisions that are problematic. kemptonstudy of home thermostats is inspiring, but it is important to recognize some of the important di erences between home thermostat control and information security. of strong feedback; if they really mess up the setting of the home thermostat they either freeze or roast in their house. this bounds how badly the mental model they use can perform; a model that regularly causes the person to be freezing is unlikely to be held for long. even if a person holds a model that leads to very poor choices, they may never realize how their actions are causing security problems. also, home thermostat feedback is fairly timely, and fairly directly connected to user actions. if a person sets their thermostat poorly, they usually nd out in a matter of hours. on the other hand, a person who makes poor home computer security decisions may not nd out that they have been phished for weeks, and then will have trouble associating that outcome with the speci. this lack of timely feedback means that mental models for home computer security are more di cult, and more variable, than mental models of home thermostats.