we show that incorporating user behavior data can significantly improve ordering of top results in real web search setting. we examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features. we report results of a large scale evaluation over, queries and million user interactions with a popular web search engine. we show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as relative to the original performance. implicit relevance feedback for ranking and personalization has become an active area of research. recent work by joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process. millions of users interact with search engines daily. they issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions. these interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments. our motivation for this work is to understand how implicit feedback can be used in a large scale operational environment to permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. how does it compare to and compliment evidence from page content, anchor text, or link based features such as inlinks or pagerank while it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies. our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy and a web search engine already uses hundreds of features and is heavily tuned. to this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine. the specific contributions of this paper include: analysis of alternatives for incorporating user behavior into web search ranking. an application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine. a large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback. we summarize our findings and discuss extensions to the current work in section #, which concludes the paper. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. furthermore, many web documents such as local news stories, lottery results, and sports team fan pages may not correspond to physical addresses, but the location of the user still plays an important role in document relevance. in this paper, we show how to infer a more general location relevance which uses not only physical location but a more general notion of locations of interest for web pages. our results show that a substantial fraction of web search queries can be significantly improved by incorporating location based features. personalization of search results offers the potential for significant improvements in web search. among the many observable user attributes, approximate user location is particularly simple for search engines to obtain and allows personalization even for a first time web search user. however, acting on user location information is difficult, since few web documents include an address that can be interpreted as constraining the locations where the document is relevant. we compute this information using implicit user behavioral data, characterize the most location centric pages, and show how location information can be incorporated into web search ranking. user behavior provides many cues to improve the relevance of search results through personalization. one aspect of user behavior that provides especially strong signals for delivering better relevance is an individual history of queries and clicked documents. our findings have implications for the design of search systems that leverage user behavior to personalize the search experience. we also characterize how the relative contribution of each model changes throughout the duration of a session. previous studies have explored how short term behavior or long term behavior can be predictive of relevance. ours is the first study to assess how short term behavior and long term behavior interact, and how each may be used in isolation or in combination to optimally contribute to gains in relevance through search personalization. our key findings include: historic behavior provides substantial benefits at the start of a search session; short term session behavior contributes the majority of gains in an extended search session; and the combination of session and historic behavior out performs using either alone. search personalization improves retrieval effectiveness by tailor ing the ranking of results for individual users based on models of their interests. to construct the profiles necessary for search personalization, evidence of a userinterests can be mined from observed past behaviors. an important determinant of the success of personalization is the behavioral information that is used to construct user profiles. alt hough there has been some work examining the effect of different contextual sources for modeling user interests, another critical aspect of personalization is the timespan of the behavioral information used for profile construction. short term profiles capture recent interactions but lack users long term interests. a principled investigation of the im pact of short and long term behavior on search personalization is lacking and we address that shortcoming with the research pre sented here. in this paper, we investigate how users long term search activity history interacts with their short term search session behavior. we characterize these interactions using a framework for modeling behavior from different timespans and predicting search rele vance. we explore the effectiveness of user profiles developed based on different temporal views. we make the following unique contributions with this research: propose a novel unified modeling framework that provides an integrative view of different parameters of personalization and controls key aspects such as the features generated from behavior and decay factors employed. section # describes our unified framework for combining a user historical behavior with their session activity and outlines the features and model training. section # describes the experiment, including the data and methodology. long term profiles represent long term interests but may not ade quately represent searcher needs for the current task. study dynamics in the relative contribution to personaliza tion of short and long term models over the course of a ses sion. we present findings in section #, discuss them and their implications in section #, and conclude in section #. this behavior can be sourced from the short term or the long term. these studies have shown that personalization is important but often care must be taken in how it is applied, eg, we may only want to personal ize queries which have high click entropy. earlier at tempts to address this challenge leveraged different representa tions for each source or made ad hoc decisions around how to weight distant actions. although each model makes use of sets of search activity gathered over different durations, the same feature set is used for each time span to remove that source of variation, and decay factors are studied in a principled manner. we evaluate the success of these models via a large search log containing queries, results, and clicks, enabling us to compare the performance of each personalized ranker rela tive to that of a high quality commercial search engine in a man ner similar to previous personalization research. as part of our analysis, we confirm intuitions that long term behavior is useful at the start of a session and that short term models yield benefit as the session proceeds. provide new findings on search personalization, such as the special properties of the first query in the session, and the strong performance of models that learn to combine short and long term features for each query, rather than simply ag gregating all features; suggesting that individual queries dif ferentially benefit from short and long term personalization. the remainder of this paper is structured as follows. section # presents related work on model based user behavior analysis for search personalization. we empirically evaluate client side keyword profiles for keyword advertising on a large scale dataset from a major search engine. additionally, we show that advertisers can potentially increase their return on investment significantly by utilizing bid increments for keyword profiles in their ad campaigns. personalization is ubiquitous in modern online applications as it provides significant improvements in user experience by adapting it to inferred user preferences. however, there are increasing concerns related to issues of privacy and control of the user data that is aggregated by online systems to power personalized experiences. these concerns are particularly significant for user profile aggregation in online advertising. this paper describes a practical, learning driven client side personalization approach for keyword advertising platforms, an emerging application previously not addressed in literature. our approach relies on storing user specific information entirely within the user control, thus allowing the user to view, edit or purge it at any time. we develop a principled, utility based formulation for the problem of iteratively updating user profiles stored client side, which relies on calibrated prediction of future user activity. while optimal profile construction is np hard for pay per click advertising with bid increments, it can be efficiently solved via a greedy approximation algorithm guaranteed to provide a near optimal solution due to the fact that keyword profile utility is submodular: it exhibits the property of diminishing returns with increasing profile size. experiments demonstrate that predictive client side personalization allows ad platforms to retain almost all of the revenue gains from personalization even if they give users the freedom to opt out of behavior tracking backed by server side storage. examples of such representations include advertiser defined categories for behavioral targeting in display advertising and lowdimensional latent topics in collaborative filtering methods based on matrix decomposition. server side aggregation is being increasingly challenged by consumer and privacy advocates due to the fact that it limits users ability to view and control data associated with their behavior, raising the need for privacy enhanced personalization methods. based on keyword bid increments that allow differentiating between users with casual and long term topical interests, the approach naturally integrates with existing ad delivery platforms, campaigns, and bid optimization frameworks, allowing advertisers to experiment with highly granular ad personalization without significant infrastructure investments. then, every user query is represented as an observed is the most relevant ad keyword. for the query, and while highly practical, the described approach for keyword profile construction is derived from a principled utility based framework. personalization is a core component of many web applications, where its uses vary from re ranking search engine results to recommending items in domains such as news or online shopping. traditional uses of personalization center on customizing the output of an information system for a given user based on attributes composing their profile. profile attributes may be explicitly or implicitly obtained, where explicit attributes are provided by the user or computed deterministically. implicit user attributes are inferred based on the logs of the userprior behavior, eg, past searching, browsing, reviews or shopping transactions. a wide variety of personalization approaches have been proposed in recent years; notable examples include algorithms that leverage preference correlations across users, and methods that use past behavior to assign users to one or more pre defined categories. raw behavior logs used to infer implicit user attributes are typically stored in the online servicedatacenter, where they are processed to compute each user profile in a compact representation chosen for the application at hand. the resulting profiles are used in subsequent interactions with the user to adjust the output of the application to user preferences. in the context of personalized search, methods have been proposed for constructing category based profiles on a usermachine, where they are employed to re rank search results. in online advertising, several alternative ad delivery architectures have been proposed based on client side category based profiles used to perform ad selection locally. the previous approaches reliance on broad categories for representing user interests is a significant barrier for their use in search and contextual advertising, where campaigns target highly specific intents via bids on individual keywords. furthermore, these approaches require significant architectural changes to ad delivery pipelines and installation of additional components client side both present significant barriers to adoption. in this paper, we describe a novel approach to keyword based personalization for search advertising that is practical, principled, and privacy friendly. where, context isa vector of features that may be based. we demonstrate that bid increment based keyword profile utility is a submodular function: it has the intuitive property of on various attributes of the query and keyword, eg, timestamp, user location, query or keyword cate and. be the domain of all sequences of observed events corresponding to user behavior history, and. be the domain of profile maps: function profile constructiona a sequence of contexts observed over a time period up to, diminishing returns with increasing profile size. submodularity allows employing a simple yet highly effective approximate algo representations. this enables real time personaliza tion based on client side profile storage, avoiding server side aggregation of user data, a major area of concern for consumer. the economic trade offs of privacy friendly policies are a key representation can be easily extended to include explicit profile attributes, eg, demographic data. the objective of profile construction is to maximize a utility func issue for the industry. for the proposed approach, we compare the performance of online, client side profiling to full history server side profiling. experiments on real world large scale datasets demonstrate that client side profiling retains almost all revenue gains that server side personalization yields, while allowing users to opt out of server side logging and gain full control of their behavioral history. following are the paperkey contributions: tion that captures the increase in performance for the task at hand. can be computed post hoc by evaluating performance of the profile constructed during a preceding time interval, is one that. the rest of the paper is organized as follows. section # describes a formal specification of the optimal profile construction problem in general, and its specialization to the client side setting. section # introduces keyword based profiles for search advertising and validates their utility on a large real world behavioral dataset. section # describes the profile construction algorithm, while section # explains the machine learning approach behind optimal profile construction. experimental evaluation of the approach on real world large scale dataset from a major search engine is described in section #. discussion of future work is provided section #. section # provides an overview of related work and finishes with concluding remarks. the paper proposes identifying relevant information sources from the history of combined searching and browsing behavior of many web users. while it has been previously shown that user interactions with search engines can be employed to improve document ranking, browsing behavior that occurs beyond search result pages has been largely overlooked in prior work. the paper demonstrates that users post search browsing activity strongly reflects implicit endorsement of visited pages, which allows estimating topical relevance of web resources by mining large scale datasets of search trails. we present heuristic and probabilistic algorithms that rely on such datasets for suggesting authoritative websites for search queries. experimental evaluation shows that exploiting complete post search browsing trails outperforms alternatives in isolation, and yields accuracy improvements when employed as a feature in learning to rank for web search. understanding usersearch intent expressed through their search queries is crucial to web search and online advertisement. web query classification has been widely studied for this purpose. most previous qc algorithms classify individual queries without considering their context information. however, as exemplified by the well known example on query jaguar, many web queries are short and ambiguous, whose real meanings are uncertain without the context information. in this paper, we incorporate context information into the problem of query classification by using conditional random field models. in our approach, we use neighboring queries and their corresponding clicked urls in search sessions as the context information. we perform extensive experiments on real world search logs and validate the effectiveness and effciency of our approach. we show that we can improve the score by as compared to other state of the art baselines. search engines have become one of the most popular tools for web users to nd their desired information. as a result, understanding the search intent behind the queries issued the work was done when huanhuan cao and derek hao hu were interns at microsoft research asia. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. by web users has become an important research problem. query classi cation, denoted as qc, has been studied for this purpose by classifying user queries into a ranked list of prede ned target categories. such category information can be used to trigger the most appropriate vertical searches corresponding to a query, improve web page ranking, and help nd the relevant online advertisements. query classi cation is dramatically di erent from traditional text classi cation because of two issues. as reported in, most queries contain only terms. second, many queries are ambiguous, and it is common that a query belongs to multiple categories. for example, manually labels randomly sampled queries from the public data set from acm kdd cup, and queries have multiple category labels. to address the above challenges, a variety of query classi cation approaches have been proposed in the literature. in general, these approaches can be divided into three categories. the rst category tries to augment the queries with extra data, including the search results returned for a certain query, the information from an existing corpus, or an intermediate taxonomy. the second category leverages unlabeled data to help improve the accuracy of supervised learning. finally, the third category of approaches expands the training data by automatically labeling some queries in some click through data via a self training like approach. although the existing methods may be successful in some cases, most of them are not context aware; that is, they treat each query individually without considering the user behavior history. suppose that a user issues a query michael jordan. it is not clear whether the user is interested in the famous basketball player or the machine learning researcher at uc berkeley. without understanding the usersearch intent, many existing methods may classify the query into both categories sports and computer science. however, if we nd that the user has issued a query nba right before michael jordan, it is likely that the user is interested in the category of sports. conversely, if the user issues some queries related to machine learning before the query michael jordan, it may suggest the user is interested in the topics related to computer science. acm kdd cup is an open contest conducted in conjunction with the acm kdd conference, which gives a qc task on, randomly selected web queries. intuitively, using search context information, such as the adjacent queries in the same session as well as the clicked urls of these queries, can help better understand users search intent and thus improve the classi cation accuracy. as shown in previous studies, adjacent queries raised by the same user are usually semantically related. moreover, compared with search queries, which are often short and ambiguous, the urls that are selectively clicked by a user after issuing the queries may better reveal the search intent of the user. rst attempt to leverage context information in query classi cation, in this paper, we intend to answer the following questions: how do we model context information. ectively and incorporate it into the problem of query classi cation how much improvement can we achieve by using context information in query classi cation would incorporating context information add too much computational burden and would it be possible to extend the idea for real world commercial search engines to answer these questions, we propose to use the conditional random field model to help incorporate the search context information. we have several motivations for using this model. first, crf is a sequential learning model which is particularly suitable for capturing the context information of queries. second, the crf model does not need any prior knowledge for the type of conditional distribution. finally, compared with hidden markov models, the crf model is more exible to incorporate richer features, such as the knowledge of an external web directory. in this paper, we show how crf can be used for modeling the context information for query classi cation. we conduct extensive experiments on real world search logs to empirically evaluate our proposed model. our experiments show that the crf approach improves the score by as much as as compared to the state of the art baselines. ine, the online inference stage is very fast, which makes our approach feasible to use in real world search engine systems. the rest of this paper is organized as follows. in section #, we discuss the related work of qc. in section #, we describe the problem of context aware query classi cation. review the crf model in section # and present the features in crf in section #. the experimental results are reported in section #. finally, we conclude our paper and point out some future research directions in section #. as with any application of machine learning, web search ranking requires labeled data. the labels usually come in the form of relevance assessments made by editors. click logs can also provide an important source of implicit feedback and can be used as a cheap proxy for editorial labels. the main difficulty however comes from the so called position bias urls appearing in lower positions are less likely to be clicked even if they are relevant. in this paper, we propose a dynamic bayesian network which aims at providing us with unbiased estimation of the relevance from the click logs. experiments show that the proposed click model outperforms other existing click models in predicting both click through rate and relevance. web page ranking has been traditionally based on hand designed ranking functions such as bm. with the inclusion of thousands of features for ranking, hand tuning of ranking function becomes intractable. several machine learning algorithms have been applied to automatically optimize ranking functions. machine learned ranking requires a large number of training examples, with relevance labels indicating the degree of relevance for each querydocument pair. the cost of the editorial labeling is usually quite expensive. moreover, the relevance labels of the training examples could change over time. for example, if the query is time sensitive or recurrent, a search engine is expected to return the copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. most up to date documents sites to the users. however, it would be prohibitive to keep all the relevance labels up to date. click logs embed important information about user satisfaction with a search engine and can provide a highly valuable source of relevance information. compare to editorial labels, clicks are much cheaper to obtain and always re ect current relevance. clicks have been used in multiple ways by a search engine: to tune search parameters, to evaluate di erent ranking functions, or as signals to directly in uence ranking. however, clicks are known to be biased, by the presentation order, the appearance of the documents, and the reputation of individual sites. many studies have attempted to account the position bias of click. carterette and jones proposed to model the relationship between clicks and relevance so that clicks can be used to unbiasedly evaluate search engine when lack of editorial relevance judgment. other research attempted to model user click behavior during search so that future clicks may be accurately predicted based on observations of past clicks. two di erent types of the click models are position models and the cascade model. a position model assumes that a click depends on both relevance and examination. each rank has a certain probability of being examined, which decays by rank and depends only on rank. a click on a url indicates that the url is examined and considered relevant by the user. however this model treats the individual urls in a search result page independently and fails to capture the interaction among urls in the examination probability. take for example two equally relevant urls for a query: a user may only click on the top one, feel satis ed, and then leave the search result page. in this case, the positional bias cannot fully explain the lack of clicks for the second url. the cascade model assumes that users examine the results sequentially and stop as soon as a relevant document is clicked. here, the probability of examination is indirectly determined by two factors: the rank of the url and the relevance of all previous urls. the cascade model makes a strong assumption that there is only one click per search and hence it could not explain the abandoned search or search with more than one clicks. even though the cascade model is quite restrictive, the authors of that paper showed that we refer to url as a shorthand for the entire display block consisting of the title, abstract and url of the corresponding result. it can predict click through rates more accurately than the position models described above. none of the above models distinguish perceived relevance and actual relevance. because users cannot examine the content of a document until they click on the url, the decision to click is made based on perceived relevance. while there is a strong correlation between perceived relevance and actual relevance, there are also many cases where they di er. in this paper, a dynamic bayesian network model is proposed to model the users browsing behavior. as in the position model, we assume that a click occurs if and only if the user has examined the url and deemed it relevant. similar to the cascade model, our model assumes that users make a linear transversal through the results and decide whether to click based on the perceived relevance of the document. the user chooses to examine the next url if he she is unsatis ed with the clicked url. our model di ers from the cascade model in two aspects: because a click does not necessarily mean that the user is satis ed with the clicked document, we attempt to distinguish the perceived relevance and actual relevance. the open directory project is clearly one of the largest collaborative efforts to manually annotate web pages. this effort involves over, editors and resulted in metadata specifying topic and importance for more than million web pages. still, given that this number is just about percent of the web pages indexed by google, is this effort enough to make a difference in this paper we discuss how these metadata can be exploited to achieve high quality personalized web search. first, we address this by introducing an additional criterion for web page ranking, namely the distance between a user profile defined using odp topics and the sets of odp topics covered by each url returned in regular web search. we empirically show that this enhancement yields better results than current web search using google. then, in the second part of the paper, we investigate the boundaries of biasing pagerank on subtopics of the odp in order to automatically extend these metadata to the whole web. probably almost everybody is equally convinced that we will not be able to manually annotate all web pages. this is one of the largest efforts to manually annotate web pages, exporting all this metadata information in rdf format. everyone working in the context of the semantic web is convinced of the utility of metadata describing the content and various other interesting properties of web pages and relationships between them. but do we really need to this paper focuses on manually entered metadata expressing topical categorizations of web pages, as well as on the importance of these pages. this kind of metadata was one of the rst metadata available on the web in signi cant quantities, because it is useful to provide hierarchically structured access to high quality content on the web, starting with efforts like the yahoo directory, collected and put together by a group of human editors. by inserting a web page into one or more categories, basically a content classi cation is annotated to the document. most notable is the annotation categorization done in the context of the open directory project. over, editors are busy keeping the directory reasonably up to date, and the odp now provides access to over million web pages in the odp catalog. still, given the fact that google now indexes more than billion pages, the odp effort still only covers about percent of the web pages indexed by google. so does search using these metadata stand any chance against google one good use these metadata can be put to is to personalize search, ie, returning search results which are both relevant to the user pro le, as well as of good quality. this paper investigates the possibilities we have for building such a personalized search engine based on odp or similar directory metadata and investigates the quality and effectiveness of such personalization. speci cally, this paper investigates two ways to personalize search and makes the following contributions: first, using odp entries directly, we show how to generalize personalized search in catalogs such as odp and google directory beyond the currently available search restricted to speci. the precision of this personalized search signi cantly surpassed the precision offered by google in a set of experiments on topic related searches. second, extending the manual odp classi cations from its cur rent million entries to a billion web in an automated way is fea pagerank and personalized pagerank sible, based on an analysis of how topic classi cations for a small but important subset of a large page collection can be extended to this large collection via topic sensitive biasing of pagerank values. this generalizes earlier approaches which already investigated topic sensitive page ranks, but relied on very simple classi cations using only topics. the paper is organized as follows: in section # we will give a short overview of the open directory project, as well as of page rank and personalized pagerank as relevant algorithms for this paper. in section # we discuss how we can directly use odp and google directory entries to implement personalized search based on user pro les corresponding to topic vectors from the odp hierarchy, and discuss a user study comparing google and odp search with these personalized versions. section # builds on the idea that sets of odp or other directory entries can be used to bias pagerank appropriately, and thus to implicitly extend such annotations to the rest of the web. we speci cally investigate when biasing on such a set actually makes a difference to non biased pagerank, presenting experiments with various kinds of biasing sets. we then use these results to analyze biasing sets from the odp year# crawl used in and show that all biasing sets we investigated can be successfully used for biasing. our method uses known item searching; comparing the relative ranks of the items in the search engines rankings. users of the world wide web are not only confronted by an immense overabundance of information, but also by a plethora of tools for searching for the web pages that suit their information needs. web search engines differ widely in interface, features, coverage of the web, ranking methods, delivery of advertising, and more. in this paper, we present a method for comparing search engines automatically based on how they rank known item search results. because the engines perform their search on overlapping subsets of the web collected at different points in time, evaluation of search engines poses significant challenges to the traditional information retrieval methodology. our approach automatically constructs known item queries using query log analysis and automatically constructs the result via analysis of editor comments from the odp. additionally, we present our comparison on five well known search services and find that some services perform known item searches better than others, but the majority are statistically equivalent. navigational queries were examined in the trec year# web track so a standard corpus could be created with relevance judgments and reproducible results. most recently, spink classified the types of data that people are looking for but did not examine the effectiveness of their searching. given the dynamic nature of the web, the difficulty of creating a large representative test collection and the resources needed for traditional evaluation methodologies, we present a technique that is able to automatically compare search services at regular, short intervals. web search evaluation poses a considerable number of challenges to traditional ir evaluation methods. first, the collection is constantly changing, ie, any evaluation is not reproducible in the future. since the collection is so large, it is not possible to manually judge enough queries to a sufficient result depth to be able to measure recall in any reasonable way. other researchers have also enumerated these fundamental challenges of web evaluations, however they have focused on the ad hoc search task. while other researchers have examined the effectiveness of various search services, their primary evaluation technique has been some precision variant evaluation of informational queries. singhal et al evaluated the search tasks of web users and proposed that navigational queries were more significant to web search than traditional trec ad hoc information gathering. in addition, singhal compared the effectiveness of trec systems for navigation queries in comparison to web search engines and copyright is held by the author owner. ian soboroff national institute of standards and technology ian soboroff nist gov concluded that the search services were performing known item search better than traditional ad hoc approaches. since our technique uses a large number of search queries that are automatically assessed, the problems of having to devote large numbers of assessors to determine the effectiveness of the various systems is removed. in addition, since this is an automatic task it can be repeated giving general rankings of effectiveness. in the next section, we present our evaluation method and results from the known item search task. many large internet websites are accessed by users anonymously, without requiring registration or logging in. however, to provide personalized service these sites build anonymous, yet persistent, user models based on repeated user visits. cookies, issued when a web browser first visits a site, are typically employed to anonymously associate a website visit with a distinct user. however, users may reset cookies, making such association short lived and noisy. in this paper we propose a solution to the cookie churn problem: a novel algorithm for grouping similar cookies into clusters that are more persistent than individual cookies. such clustering could potentially allow more robust estimation of the number of unique visitors of the site over a certain long time period, and also better user modeling which is key to plenty of web applications such as advertising and recommender systems. we present a novel method to cluster browser cookies into groups that are likely to belong to the same browser based on a statistical model of browser visitation patterns. we address each step of the clustering as a binary classification problem estimating the probability that two different subsets of cookies belong to the same browser. we observe that our clustering problem is a generalized interval graph coloring problem, and propose a greedy heuristic algorithm for solving it. the scalability of this method allows us to cluster hundreds of millions of browser cookies and provides significant improvements over baselines such as constrainedmeans. the world wide web has been growing extremely fast in recent years and large websites such as yahoo, google and permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. msn attract hundreds of millions of unique visitors every month. these user visits are logged anonymously by the websites, with an aim to measure user engagement and to improve user experience. when a web browser visits a website for the rst time, the website creates and assigns the browser a unique identi er, which is sent to the website on consecutive visits until it expires or is deleted by user through the browser interface or otherwise. cookies help websites to better estimate various user engagement metrics eg, the number of unique visitors, the average engagement time of an user, and the number of repeat visitors to the site. these metrics are important both for assessing website popularity and for measuring the effectiveness of display advertisements, which is a large source of revenue for internet companies. maintaining anonymous user pro les associated with each cookie allows a website to improve user experience, eg, to personalize the stories displayed on a web page based on the userpast history, even to the users that have not explicitly registered or otherwise identi ed themselves to the website. however, cookies do not last forever: some browsers are set up to clear cookies when the browser is closed; cookies can be deleted by user request; cookies expire after some prede ned period. a study released by comscorein year# indicates that around percent of. internet users clear their rst party cookies within a month. the conclusion drawn in the study is that cookie based unique visitor counting systems, which are widely implemented by many websites, usually overestimate the number of unique visitors by large factors. we refer to problem of frequent cookie deletion as cookie churn in this paper. figure # shows that a signi cant fraction of yahoo toolbar users clears cookies at least once a day, and even more do so weekly and monthly. figure #: fraction of yahoo toolbar users that clear cookies. in addition to causing overestimation of the number of unique visitors, the cookie churn problem also impacts the effectiveness of user targeting models and applications. in user behavioral modeling, recently created cookies have little history to learn from, which negatively affects model performance. in online advertising, advertisers are interested to know the joint distribution of how many users have been exposed to ads from a campaign, and how many times individual users have been exposed to the campaign. under cookie churn, estimating these metrics is hard. in fact, neither of the above two problems user modeling or reach frequency curve estimation can be solved by only estimating a scaling parameter; we do need some approximate grouping of the cookies in order to solve either. although the cookie churn problem is important for many applications, there is little research done on this problem, other than by companies eg, comscoreand nielsen, who are engaged in estimating user metrics. a popular approach used by these companies is to recruit panelists to estimate cookie visitor ratios, and then inferring the number of unique visitors to a site based on the number of unique cookies and these ratios. a similar work by tries to nd the cookie visitor or ip visitor ratios and uses these to normalize the number of cookies. as we mentioned, such ratio based methods are applicable only to coarse grained user counting and not to the user behavioral modeling or the reach and frequency problems. another interesting related work is a public web tool built by the electronic frontier foundation that de nes the concept of browser ngerprints using a number of browser features such as user agent, http accept headers, browser plugins, screen resolution, and third party cookies. the authors claim that among a sample of around browsers they have collected, of them have unique ngerprints. thus, such ngerprint could serve as a potential identi er of a browser. however, their estimation of uniqueness is built on a miscalculation of entropy that assumes the browser features to be independent of each other in general this is not true. also, for large sites with hundreds of millions of visitors every month, the collision rate is expected to be high. lastly, the ngerprint are based on browser properties that are not accessible to most websites unless explicitly authorized by the users and the instrumentation to collect them exists, and thus cannot be used in practice. we aim to construct similar browser ngerprints, but based on features that are much more benign and less discriminative. one of the contributions of this work is to show that even using a seemingly benign set of features enables us to identify browsers with high accuracy. our broad approach in solving the cookie churn problem in the settings of user modeling and reach frequency estimation from ad campaign is to create clusters of cookies based on a behavioral pro le each cluster ideally will represent all and only the cookies associated with a single browser. to solve the cookie clustering problem, one important observation is that the lifetimes of cookies in a cluster can not overlap, ie, a browser has to clear the previous cookie to generate a new one. this induces a set of hard cannot link constraints among cookies. additional cannot link constraints are derived from the fact that the operating system or the browsertypes of cookies in a cluster have to be the same. there are several major challenges we face when tackling this problem in a clustering framework. first, the solution has to scale well to be able to process hundreds of millions of cookies created every month at major websites. most clustering algorithms do not scale to this level. secondly, since each cluster represents a browser, the number of clusters is unknown in fact, one of the outputs of our method is an estimate of the number of underlying browsers. this is a major issue for most clustering algorithms often solved using model selection approaches, or simply by trying out different values for the number of clusters. moreover, in our setting the number of clusters is of the same order of magnitude as the number of cookies, an atypical setting for many clustering algorithms likemeans or spectral clustering. third, in large websites that have millions of simultaneous visitors, the number of cannot link constraints is quadratic in the number of cookies, stressing scalability even more. the hard cannot link constraints are known to be dif cult to satisfy if the number of clusters is pre speci ed. fourth, due to the short lifetime of the majority of cookies, the amount of useful information in their pro les is low, making it hard to extract meaningful cookie pro les and to design accurate similarity measure among these pro les. all of the above challenges make off the shelf clustering algorithms ineffective, necessitating a specialized solution. we propose a novel and scalable method which overcomes all the above challenges and is able to cluster hundreds of millions of browser cookies, essentially predicting typical browser identi ers spanning multiple cookie sessions. we observe that while in general clustering with cannot link constraints is a hard problem, the non overlap constraints induce an interval graph, where each cookie is an interval de ned by its lifetime. interval graphs have an elegant structure admitting an optimal greedy graph coloring algorithm that essentially partitions the graph into the provably minimal number of groups of cookies where the lifetimes of all cookies in a group do not overlap. however, the coloring algorithm does not use cookie pro le information, which as we show reduces clustering accuracy. to embed cookie pro le similarity into clustering, we need to solve an augmented coloring problem using binary classi cation model that predicts the likelihood of a cookie to belong to one of the several candidate clusters. hence we propose a greedy heuristic algorithm that is inspired by the interval graph coloring algorithm. we de ne a set of cookie pro le features such as cookie lifetime, number of page views, os type, distribution of ip addresses, categories of visited webpages, etc. we propose two pro le similarity measures, one based on distance between pro le vectors, and the other by further embedding bayes factors into the similarity measure. both measure are learned through logistic regression using some training data. we evaluated our algorithm on a dataset extracted from one months of web logs of yahoo websites, containing cookies. for training the similarity measure as well as evaluating our clustering algorithm, we use the yahoo toolbar web logs where we use toolbar id is used as the ground truth browser identi er. although the toolbar data likely carries some bias and does not accurately represent user population, the bias problem is orthogonal to the techniques we develop in this work, and can be dealt with independently using existing bias removal approaches, eg, our contributions. in summary, our major contributions in this work are: we formalize the problem of dealing with cookie churn by creating clusters of cookies using a constrained clustering framework. we de ne a simple browser model for cookie lifetimes and also uses real world data to validate our assumption. this model is also used to derive theoretical intuitions behind our clustering algorithm. we de ne a set of cookie pro le features and propose an ef cient and scalable clustering algorithm based on interval graph coloring and cookie pro le similarities; we also show how to learn the parameters of such similarity measures from the training data. we evaluate our algorithms on the real world yahoo data and show that it outperforms various baseline methods. todayinternet users are aware of their privacy when online, which explains the ndings by comscore and us that many users clear cookies often. we reiterate that cookie pro les that we use are completely anonymized and cannot be tracked to individual users. in particular, they are not linked to the richer user pro les of registered users. furthermore, given the increasing importance of personalization as a business strategy, research in exposing the tradeoff between privacy and value addition is inevitable and important. by showing the feasibility of browser identi cation using a weak set of features, we also see our work as a step towards exposing this tradeoff, demonstrating the need for improved anonymization techniques if privacy is desired. although personalized search has been proposed for many years and many personalization strategies have been investigated, it is still unclear whether personalization is consistently effective on different queries for different users, and under different search contexts. we present a large scale evaluation framework for personalized search based on query logs, and then evaluate five personalized search strategies using day msn query logs. by analyzing the results, we reveal that personalized search has significant improvement over common web search on some queries but it also has little effect on other queries. it even harms search accuracy under some situations. furthermore, we show that straightforward click based personalization strategies perform consistently and considerably well, while profile based ones are unstable in our experiments. in this paper, we study this problem and get some preliminary conclusions. we also reveal that both long term and short term contexts are very important in improving search performance for profile based personalized search strategies. we describe results from web search log studies aimed at elucidating user behaviors associated with queries and destination urls that appear with different frequencies. we note the diversity of information goals that searchers have and the differing ways that goals are specified. we examine rare and common information goals that are specified using rare or common queries. we identify several significant differences in user behavior depending on the rarity of the query and the destination url. we find that searchers are more likely to be successful when the frequencies of the query and destination url are similar. we also establish that the behavioral differences observed for queries and goals of varying rarity persist even after accounting for potential confounding variables, including query length, search engine ranking, session duration, and task difficulty. finally, using an information theoretic measure of search difficulty, we show that the benefits obtained by search and navigation actions depend on the frequency of the information goal. when searching the web, users typically issue a query to a search engine, are presented with a list of results, and then may click on one or more results in an attempt to satisfy an information goal. along the way, the searcher may modify their initial query in various ways. people have mixed experiences with this permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. process: search engines do a great job of returning relevant results for some queries and a poor job for others. although there has been a good deal of work characterizing the heavytailed zipf distribution of both queries and target urls, there has been much less work on understanding searchers experiences and behaviors associated with these characteristics. do search engines and searchers behave differently on rare queries than on common ones do rare queries represent rare informational goals, or simply atypical means of specifying common goals how can answers to such questions guide research toward enhancing web search experiences search queries are the articulation of a personinformation goals. people employ a mixture of search and navigation strategies to satisfy these goals. in laboratory studies, participants can be given known search goals or probed about their own information goals. in largescale log studies, information goals must be inferred from patterns of user interactions. one approach to inferring searchers information goals is to consider the patterns of pages viewed in sessions as well as the dwell times on pages as implicit indicators of interest. for example, a researcher may start a session by searching for a another researcher by name, select a link to that colleaguehome page from the results, navigate from the home page to a list of publications, and finally click a link to the paper which satisfies the intent of the search. in this paper, we shall take the last url visited in a search session as a candidate proxy for a searcherunderlying information goal. using such a proxy, for users goals, we can identify relationships between queries and goals that are consistent across many users and diverse search tasks. we seek to understand the relationship between the articulation of a goal and the actual information goal. we investigate how search behavior varies across rare and common queries, in the face of rare and common information goals. we present results from a largescale log study of search sessions from users of major web search engines. after a discussion of previous work, we describe the data collection and session extraction methodology in detail. we then present experimental results describing user behaviors for sessions consisting of queries and target urls of different frequencies. in some cases, we provide empirical confirmation of relationships one might expect to hold between goals and queries; in other cases, the findings are more surprising. our key results include: user behavior following a query varies significantly with the rarity of the userquery or destination url. web search engines are less effective as queries or target urls become rarer; searchers are less likely to click results, and are more likely to reformulate queries. a userquery is often much more specific or general than their underlying information goal. this has important implications for search engine effectiveness: search success is more likely when the relative frequency of the query matches that of the need. the decreased efficacy of search engines on rare needs can be characterized by increases in session length. the average number of queries the user must execute to satisfy rare goals is higher than for common goals. after establishing the results above, we proceed to examine potential confounding variables in the analysis. a number of different variables are known to correlate with user behavior as well as with the rarity of queries and information goals. we find that goal rarity appears to be the paramount influence in our findings, based on an investigation of a variety of variables including query length, search engine ranking, session length, and task difficulty. lastly, we discuss results showing that users often compensate successfully for decreased search engine effectiveness on rare queries by issuing more general queries, and then following hyperlinks to satisfy their need. we conclude with a discussion of the implications of our results for efforts toward enhancing web search. the performance of web search engines may often deteriorate due to the diversity and noisy information contained within web pages. user click through data can be used to introduce more accurate description for web pages, and to improve the search performance. however, noise and incompleteness, sparseness, and the volatility of web pages and queries are three major challenges for research work on user click through log mining. in this paper, we propose a novel iterative reinforced algorithm to utilize the user click through data to improve search performance. the algorithm fully explores the interrelations between queries and web pages, and effectively finds virtual queries for web pages and overcomes the challenges discussed above. experiment results on a large set of msn click through log data show a significant improvement on search performance over the naive query log mining algorithm as well as the baseline search engine. this method works well when users queries are clear and specific. these will very likely lead to the deteriorating of the performance of web search engines, due to the gap between query space and document permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. this problem can be partially solved by using external evidence to enrich the content of existing web pages the so called surrogate document approach. one of such examples is to use anchor texts as additional description of target web pages. previous research show that this method yields better search result than searching on web page content alone. this is because anchor texts represent the view of a web page by other web editors rather its own author. another solution is to introduce additional description by using click through data, which has not been extensively studied. user click through data can be extracted from a large amount of search logs accumulated by web search engines. these logs typically contain user submitted search queries, followed by the url of web pages which are clicked by users in the corresponding search result page. although these clicks donreflect the exact relevancy, they provide valuable indications to the users intention by associating a set of query terms with a set of web pages. if a user clicks on a web page, it is likely that the web page is relevant to the query, or at least related to some extent. many valuable applications have been proposed along this direction, such as term suggestion, query expansion, and query clustering. in this paper we try to use user click through data as the additional metadata to bridge the gap between users information need and the content of the web pages. the query log based web page metadata generation method has three important properties. first, click through data can be regarded as web searchers view of web pages, and they are more valuable than anchor texts because the performance of web search engines are evaluated by web users not editors of web pages. second, since such metadata can be combined with the content and other representations of web pages, we reduce the risk of losing relevant web pages in a pure reranking algorithm. third, the correlations between web pages and queries may evolve with the accumulation of clickthrough data. this process can reflect and update users view of web pages as time goes by. a naive method of applying user click through data is to associate the queries with the clicked web page as the metadata of the web pages. furthermore, associated queries can be found by analyzing co visited relationship of web pages, which we denoted as covisited based method. the basic assumption of the co visited method is that two web pages are similar if they are co visited by users with similar queries, and the associated queries of the two web pages can be taken as the metadata for each other. this work was conducted while the author was doing internship at microsoft research asia. however, several issues are not solved in these methods. they are: the clicks through data may be very noisy and incomplete, and introduce inaccurate metadata to associated web pages. the co visited method only considers the similarity of queries by content; it does not take into account that two queries are similar if they lead to the visit of similar web pages. such kind of similarity can be propagated between the web pages and the queries, and the effect of noisy information in clickthrough data can be constrained. existing web search engines often calculate the relevancy of web pages for a given query by counting the search keywords contained in the web pages. however, in real world, web search queries are often short and ambiguous, and web pages contain a lot diverse and noisy information. the task of the kdd cup year# competition was to classify, internet user search queries into predefined categories. this task is easy to understand, but the lack of straightforward training set, subjective user intents of queries, poor information in short queries, and high noise level make the task very challenge in this paper, we summarize the competition task, the evaluation method, and the results of the competition. here we only highlight some key techniques used in submitted solutions. at the end, we also share the results of a survey conducted with this year cup participants. to facilitate research in this area, the task description, data, answer set, and related information of this kdd cup are published at the kdd cup year# web site: http: www acm org sigs sigkdd kdd kddcup html. the kdd cup year# competition was held in conjunction with the eleventh acm sigkdd international conference on knowledge discovery and data mining. the technical details of the solutions from the three award winning teams are available in their papers separately in this issue of sigkdd explorations. given the exponential growth of informationavailability in electronic form, search becomes one of the most important and effective approaches to finding correct relevant information to serve our needs. a user can type in key words in a search engine to find out where to buy a product and whether a certain price is a good price. a user can also find travel attractions to fit his her interests. if a user is interested in certain medicine, he she can find plenty related information including its usage and potential side effects. researchers can easily find the latest development of a research topic. to plan a hiking trip for a coming weekend, one can find the weather forecast simply through a search. these are just a few examples of how search can help a userdaily life. although researchers and industry practitioners have achieved tremendous success in developing smart search engines, we are zijian zheng was at amazon com during the time of kdd cup year#. still facing many great challenges as current search engines are not very accurate. the difference is still quite big between what search engines can do and what we expect them to do. it is not uncommon that a search engine returns irrelevant, misleading or, incorrect results after you type in a query. in another time, the relevance results are returned but down to the bottom of a long result list. due to the nature that huge amount data is available from each search engine and many problems of search can be turned into learning or modeling problems, there is a great potential for data mining techniques to contribute to the success of search. since late, researchers and practitioners have been studying search query data, trying to find search patterns, understanding search user intents, and providing personalized search. a survey on search related research is available. the other side of search being a difficult problem is that the information contained inside the data is often incomplete, fuzzy, and indirect. all of these present big challenges to the data mining community. in kdd cup year#, we presented one challenge problem: search query categorization. manning and schtze discusses general methodologies and applications of text categorization. most work in this area has been focused on categorizing web pages or longer text or corpus. however, search query classification is very different in the sense that queries are usually very short on the one hand, and with implicit and subjective user intents on the other hand. therefore, how to automatically understand user search intents given the search queries would be very interesting to ir and text mining researchers. in this report, we first describe the competition task presented to the participants in section #, including a discussion on why this task is challenging. in section #, we present the evaluation method. then, we highlight the interesting techniques from the submissions in section #, and analyze the overall results as a whole in section #. computer search including internet search has become a part of many peopledaily life and work. the intents of search engine users are highly subjective. the detailed presentations of techniques from the three winning teams are available as three separate papers in this issue of sigkdd explorations. text classification and categorization is a well known topic in information retrieval and text mining fields. we present a personalization approach that builds a user interest profile using users complete browsing behavior, then uses this model to rerank web results. personalizing web search results has long been recognized as an avenue to greatly improve the search experience. we show that using a combination of content and previously visited websites provides effective personalization. we extend previous work by proposing a number of techniques for filtering previously viewed content that greatly improve the user model used for personalization. our approaches are compared to previous work in offline experiments and are evaluated against unpersonalized web search in large scale online tests. additionally, previous research has noted that the vast majority of search queries are short and ambiguous. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. second, we improve upon the evaluation methodology, by performing the rst large online comparative evaluation of personalization strategies. second, most previous work on search personalization has involved an evaluation using either a small number of users evaluating the relevance of documents for a small set of search queries not representative of a real workload, the trec query and document collection, and simulating a personalized search setting, or an after the fact log based analysis. in this work, we start by using document judgments obtained from a small number of users for queries to assess potential approaches. we then describe our evaluation approach in detail, with results from our. ine evaluation in section #, and online evaluation in section #. clearly, di erent users would prefer di erent results. often, di erent users consider the same query to mean di erent things. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. two alternatives are: ask users to label documents as more personally relevant or not, and infer personal relevance automatically. this data was collected using a firefox add on created for this purpose. the key di erence from previous work in the pro les we construct is that we parse web page structure, using term extraction and part of speech tagging to extract noun phrases to re ne the user model. we show that this yields signi cant retrieval improvements over web search and other personalization methods, without requiring any. ort on the userpart, and without changing the usersearch environment. improvements found using these methods do not necessarily translate to actual improvements in user search experience on a real query workload. we use an interleaving evaluation approach, that has been shown to accurately re ect di erences in ranking relevance. this can be illustrated with the search query ajax. ort from users, we opt for the latter. although web search has become an essential part of our lives, there is still room for improvement. in particular, a major de ciency of current retrieval systems is that they are not adaptive enough to users individual needs and interests. this query will return results about ajax based web development, about the dutch football team ajax amsterdam, and websites about the cleaning product ajax. personalized search is a potential solution to all these problems. personalizing web search has received a lot of attention by the research community. we improve upon this work in two key ways: first, we build an improved user pro le for personalizing web search results. to successfully personalize search results, it is essential to be able to identify what types of results are relevant to users. in particular, the content of all the web pages visited by users, along with the users particular behavior on web search results, is used to build a user model. the pro le constructed is then used to rerank the top search results returned by a non personalized web search engine. we then select three methods for complete online evaluation, with our personalized search system being used by users for two months to issue thousands of queries as part of their day to day web search activities. after reviewing related work next, we give an overview of the user pro le generation and re ranking strategies investigated in section #. web search engines consistently collect information about users interaction with the system: they record the query they issued, the url of presented and selected documents along with their ranking. query logs have the potential to partially alleviate the search engines from thousand of searches by providing a way to predict answers for a subset of queries and users without knowing the content of a document. even if the predicted result is at rank one, this analysis might be of interest: if there is enough confidence on a user click, we might redirect the user directly to the page whose link would be clicked. in this paper, we present three different models for predicting user clicks, ranging from most specific ones to very general ones. the former model has a very high precision at low recall values, while the latter can achieve high recalls. we show that it is possible to combine the different models to predict with high accuracy a high subset of query sessions. this information is very valuable: it is a poll over millions of users on the most various topics and it has been used in many ways to mine users interests and preferences. social search is quickly gaining acceptance and is increasingly seen as a promising way of harnessing the common knowledge of million of users to help each other and search more ef ciently. users are increasingly understood as the driving force of the internet and many initiatives are aimed at empowering them. in particular, search engines keep records of their interaction with users in click through logs, which record some sort of user id, the queries issued by the user, the results returned by the engine and the resulting user clicks. user clicks on hyperlinks are a soft source of endorsement, since users tend to click on documents they are interested in. for this reason click through logs are the source of growing attention in the search community. in this work we will analyse query logs from a novel perspective, concentrating on their click predicting ability. in particular we are interested in predicting which document a user will click on immediately after the issuing a query. we are interested in models that can accurately predict these clicks, but more importantly we are interested in models that can predict the con dence of the prediction. this is important because we wish to be conservative: we are happy not to make a prediction, but if we make a prediction we want to be con dent that this prediction is correct. one can imagine a number of applications for these models, if suf cient accuracy can be achieved. we do not pretend that it is possible to predict with high accuracy the target page of a user for most queries. if a user types italian cooking into a search engine, it is hard to tell if the user wants a restaurant, a book, a history page, the page on cooking the user saw last week and really liked, etc. however, we believe that we can achieve high accuracy for some queries. in particular, there are two types of queries that we hope to predict with high accuracy. the rst type of prediction we are interested arises when users re nd information. users commonly follow known paths when searching for information they already found. more speci cally, they will tend to re issue a query when searching for a document they found thanks to a search engine. our work was motivated by the research on click through logs carried out by teevan et. they studied click through logs and found that of all queries lead to a click on a result formerly clicked by the same user during a previous session. furthermore, of the queries leading to repeated clicks by a user were identical, and of identical query re writes led to repeated clicks. only of the queries leading to a repeated click also led to a click on a new document. furthermore, on average of documents clicked by a user received more than one click by that user that year. on the other hand, only of clicks were clicked by multiple users. clearly showed the important role that re nding plays in web search today. they went on to build models which predict whether a user query is a re nd or not. in this paper, we go one step forward and we build models which predict, not only if a re nd is taking place, but also what is the usertarget page, and which is the con dence of our prediction. we refer to such type of models as user centric or user models. furthermore, we generalise the concept of re nding to take into account not only queries issued by the user in the past, but also queries issued by similar users; we refer to these as user group models. finally, we can extend our models to deal with simple navigational queries that many other users have issued in the past and agreed on. we note here the difference between this type of predictive models and personalisation models. personalisation models attempt to produce better result rankings; this is done by building a model of the user, and then biasing the general ranking function with the user model to improve relevance. in fact, personalisation is a complementary problem to this one, and indeed personalisation models could be used simultaneously to the prediction models described here. this paper contains a number of novel ideas: it sets up the task of click prediction, de nes it formally and describes a number of evaluation measures that can be used to test systems, it proposes a new framework for click prediction using standard bayesian inference models, it proposes three alternative models of different characteristics: user centric, global and group based, as well as a combination method. furthermore it evaluates the different models as well as a baseline in real click through data from a commercial search engine. our results show that predictive models can be used effectively to greatly improve the search experience on the web. besides explicit user provided information, there exists very large sources of implicit user information in the internet logs that record user acts. for example, in a result page we could highlight the page that we believe is the one the user will choose, or directly take the user to that page. in our work we are trying to achieve something else: predicting with high con dence the target page of a user involved in a re nding activity. we present and evaluate methods for diversifying search results to improve personalized web search. a common personalization approach involves reranking the topsearch results such that documents likely to be preferred by the user are presented higher. the usefulness of reranking is limited in part by the number and diversity of results considered. we propose three methods to increase the diversity of the top results and evaluate the effectiveness of these methods. personalizing search results for individual users is increasingly being recognized as an important future direction for web search. to individual users is particularly important because di erent users expect di erent information even given the same query. one proposed approach involves providing a user pro le to the search engine, which can then use it to bias search results toward the userinterests. however, this requires the search engine to perform the personalization at additional computational expense, and requires that the user trusts the search engine with her pro le. in this work, we focus on an alternative approach that runs entirely client side, where the client requests a larger number of search results and reranks them such that documents more likely to interest the user are presented higher. the primary limitation of client side reranking is that the system can only rerank the topresults. ective personalization when web pages of particular interest to the user are present, it cannot be effective if all topresults are similar. anagnostopoulos et al recently proposed a method to sample search results to avoid homogeneity. we propose an alternative of using query query reformulations to understand the variety of user intents and improve the. by observing how large numbers of users modify their search queries, we see which kinds of results tend to be missing from the top of search results, from the usersusan dumais microsoft research redmond, wa, usa sdumais microsoft com perspective. for example, by looking at logs from a large web search engine, we observed that the query windows is often followed by specializations such as windows xp or clari cations such as house windows. this suggests that if we want to personalize results for a user who issued the query windows, we may also want to consider results for both of these reformulations. we believe that analyzing query query reformulations adds interesting diversity within the result set by focusing on user intents that are not well represented in the original results. we also believe that such a method could be used to diversify general web search results, although we do not address this question here. we rst present our general strategy for diversifying search results using query query reformulations, then propose how to select the reformulations to consider. next, we describe a method to measure result diversity. this paper presents a novel approach for using clickthrough data to learn ranked retrieval functions for web search results. we observe that users searching the web often perform a sequence, or chain, of queries with a similar information need. using query chains, we generate new types of preference judgments from search engine logs, thus taking advantage of user intelligence in reformulating queries. to validate our method we perform a controlled user study comparing generated preference judgments to explicit relevance judgments. we also implemented a real world search engine to test our approach, using a modified ranking svm to learn an improved ranking function from preference data. our results demonstrate significant improvements in the ranking given by the search engine. the learned rankings outperform both a static ranking function, as well as one trained without considering query chains. many researchers have noted that web search queries are often ambiguous or unclear. we present an approach for identifying the popular meanings of queries using web search logs and user click behavior. we show our approach to produce more complete and user centric intents than expert judges by evaluating on trec queries. this approach was also used by the trec year# web track judges to obtain more representative topic descriptions from real queries. search engine advertising has become a significant element of the web browsing experience. choosing the right ads for the query and the order in which they are displayed greatly affects the probability that a user will see and click on each ad. this ranking has a strong impact on the revenue the search engine receives from the ads. further, showing the user an ad that they prefer to click on improves user satisfaction. for these reasons, it is important to be able to accurately estimate the click through rate of ads in the system. for ads that have been displayed repeatedly, this is empirically measurable, but for new ads, other means must be used. we show that we can use features of ads, terms, and advertisers to learn a model that accurately predicts the click though rate for new ads. we also show that using our model improves the convergence and performance of an advertising system. as a result, our model increases both revenue and user satisfaction. most major search engines today are funded through textual advertising placed next to their search results. the market for these search advertisements has exploded in the last decade to billion, and is expected to double again by year#. the most notable example is google, which earned billion in revenue for the third quarter of year# from search advertising alone. though there are many forms of online advertising, in this paper we will restrict ourselves to the most common model: pay per copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. performance with a cost per click billing, which means the search engine is paid every time the ad is clicked by a user. google, yahoo, and microsoft all primarily use this model. to maximize revenue and user satisfaction, pay per performance systems must predict the expected user behavior for each displayed advertisement and must maximize the expectation that a user will act on it. the search system can make expected user behavior predictions based on historical click through performance of the ad. for example, if an ad has been displayed times in the past, and has received clicks, then the system could estimate its click through rate to be. this estimate, however, has very high variance, and may only reasonably be applied to ads that have been shown many times. this poses a particular problem when a new ad enters the system. a new ad has no historical information, so its expected click through rate is completely unknown. in this paper, we address the problem of estimating the probability that an ad will be clicked on, for newly created ads and advertising accounts. we show that we can use information about the ad itself, the page the ad points to, and statistics of related ads, to build a model that reasonably predicts the future ctr of that ad. we have developed a method for recommending items that combines content and collaborative data under a single probabilistic framework. we benchmark our algorithm against a na ve bayes classifier on the cold start problem, where we wish to recommend items that no one in the community has yet rated. we systematically explore three testing methodologies using a publicly available data set, and explain how these methods apply to specific real world applications. we advocate heuristic recommenders when benchmarking to give competent baseline performance. we introduce a new performance metric, the croc curve, and demonstrate empirically that the various components of our testing strategy combine to obtain deeper understanding of the performance characteristics of recommender systems. though the emphasis of our testing is on cold start recommending, our methods for recommending and evaluation are general. recommender systems suggest items of interest to users based on their explicit and implicit preferences, the preferences of other users, and user and item attributes. for example, a movie recommender might combine explicit ratings data, implicit data, user demographic information, and movie content information to make recommendations to speci. pure collaborative ltering methods base their recommendations on community preferences, ignoring user and item attributes. on the other hand, pure content based ltering or information ltering methods typically match query words or other user data with item attribute information, ignoring data from other users. though content usually refers to descriptive words associated with an item, we use the term more generally to refer to any form of item attribute information including, for example, the list of actors in a movie. one di cult, though common, problem for a recommender system is the cold start problem, where recommendations are required for items that no one has yet rated pure collaborative ltering cannot help in a coldstart setting, since no user preference information is available to form any basis for recommendations. however, content information can help bridge the gap from existing items to new items, by inferring similarities among them. thus we can make recommendations for new items that appear similar to other recommended items. in this paper, we evaluate the performance of two machine learning algorithms on cold start prediction. we present our own probabilistic model that combines content and collaborative information the phrase cold start has also been used to describe the situation when almost nothing is known about customer preferences. the problem of making recommendations for new users can also be thought of as a cold start problem. we concentrate on the new item problem, although the new user problem is symmetric when we have access to user attribute data. the new user problem without attribute data essentially falls within the framework of pure information ltering or information retrieval. we perform benchmarking on movie ratings data and compare against a na ive bayes method that has also been proposed for this task. some key questions in evaluating recommender systems on testbed data are: what to predict, how to grade performance and what baseline to compare with. we identify three useful components to predict on our data set, and show where past work has focussed. in deciding what metric to use in evaluating performance, we have borrowed heavily from the literature in addition to developing our own tool: the croc curve. for baseline measures of performance we advocate the use of heuristic recommenders: algorithms that are trivial to implement yet give performance that is well above random. we nd that heuristic recommenders do surprisingly well: in some cases outperforming more sophisticated methods. our testing goal is to uncover the most informative characterization of performance for our method and the na ive bayes algorithm. we examine the predictive accuracies of probabilistic models of topic transitions for individuals and groups of users. we explore temporal dynamics by comparing the accuracy of the models for predicting topic transitions at increasingly distant times in the future. we report on a study of topic dynamics for pages visited by a sample of people using msn search. finally, we discuss directions for applying models of search topic dynamics. analysis of the rich data provided by usage logs promises to leadto insights about user goals, improved quality of search results, and new forms of search personalization. instead of inferringtopics of interest from queries, which are often very short and ambiguous, we identify topics associated with urls visited. wereport the predictive power of individual models versus modelsbuilt for groups of users, and consider temporal dynamics. the web provides opportunities for gathering and analyzing largedata sets that reflect users interactions with web based services. we describe researchthat examines characteristics of the topics and transitions amongtopics associated with page visits by users engaged in web search. the ability to predict users search and browsing behaviors has been explored by researchers in several areas. the analysis ofurl access patterns has been used to improve web caching. recent work summarizes the topics that people search for onthe web and explores how page importance measureslike pagerank can be specialized to different topics. topics are also used to construct user profiles via explicit specification ofinterests or automatic analysis of web pages visited. in our experiments, we examine topic dynamics over a week period of time with a large number of users. in particular, we build user profiles based on activity at the search site itself and study the use of these profiles to provide personalized search results. these profiles were then used to re rank the search results and the rank order of the user examined results before and after re ranking were compared. user profiles, descriptions of user interests, can be used by search engines to provide personalized search results. many approaches to creating user profiles collect user information through proxy servers or desktop bots. both these techniques require participation of the user to install the proxy server or the bot. in this study, we explore the use of a less invasive means of gathering user information for personalized search. by implementing a wrapper around the google search engine, we were able to collect information about individual user search activities. in particular, we collected the queries for which at least one search result was examined, and the snippets for each examined result. user profiles were created by classifying the collected information into concepts in a reference concept hierarchy. our study found that user profiles based on queries were as effective as those based on snippets. we also found that our personalized re ranking resulted in a improvement in the rank order of the user selected results. explicit customization has been widely used to personalize the look and content of many web sites, but we concentrate on personalized search approaches that focus on implicitly building and exploiting user profiles. this information could be used to narrow down the number of topics considered when retrieving the results, increasing the likelihood of including the most interesting results from the user perspective. companies that provide marketing data report that search engines are utilized more and more as referrals to web sites, rather than direct navigation via web links. as search engines perform a larger role in commercial applications, the desire to increase their effectiveness grows. however, search engines order their results based on the small amount of information available in the user queries and by web site popularity, rather than individual user interests. thus, all users see the same results for the same query, even if they have wildly different interests and backgrounds. to address this issue, interest in susan gauch electrical engineering and computer science university of kansas lawrence, ks sgauch ku edu personalized search had grown in the last several years, and user profile construction is an important component of any personalization system. another issue facing search engines is that natural language queries are inherently ambiguous. for example, consider a user issuing the query canon book. due to the ambiguity of the query terms, we will obtain results that are either related to religion or photography. according to an analysis of their log file data conducted by onestat com over a month period of time, the most common query length submitted to a search engine was only two words long and of all queries were three words long or less. these short queries are often ambiguous, providing little information to a search engine on which to base its selection of the most relevant web pages among millions. a user profile that represents the interests of a specific user can be used to supplement information about the search that, currently, is represented only by the query itself. for the user in our example, if we knew that she had a strong interest in photography but little or none in religion, the photography related results could be preferentially presented to the user. many approaches create user profiles by capturing browsing histories through proxy servers or desktop activities through the installation of bots on a personal computer. these require the participation of the user in order to install the proxy server or the bot. in this study, we explore the use of a less invasive means of gathering user information for personalized search. our goal is to show that user profiles can be implicitly created out of short phrases such as queries and snippets collected by the search engine itself. we demonstrate that profiles created from this information can be used to identify, and promote, relevant results for individual users. web search engines help users find useful information on the world wide web. generally, each user has different information needs for his her query. therefore, the search result should be adapted to users with different information needs. in this paper, we first propose several approaches to adapting search results according to each user need for relevant information without any user effort, and then verify the effectiveness of our proposed approaches. experimental results show that search systems that adapt to each user preferences can be achieved by constructing user profiles based on modified collaborative filtering with detailed analysis of user browsing history in one day. however, when the same query is submitted by different users, typical search engines return the same result regardless of who submitted the query. it has become increasingly dif cult for users to nd information on the www that satis es their individual needs since information resources on the www continue to grow. under these circumstances, web search engines help users nd useful information on the www. however, when the same query is submitted by different users, most search engines return the same results regardless of who submits the query. www, may, year#, new york, new york, usa. in order to predict such information needs, there are several approaches applying data mining techniques to extract usage patterns from web logs. furthermore, shahabi and chen have pointed out that the item association generated from web server logs might be wrong because web usage data from the server side are not reliable. therefore, these techniques are not so appropriate for web personalization. another novel information systems designed to realize such adaptive systems have been proposed that personalize information or provide more relevant information for users. as far as we know, three types of web search systems provide such information: systems using relevance feedback, systems in which users register their interest or demographic information, and systems that recommend information based on users ratings. in these systems, users have to register personal information such as their interests, age, and so on, beforehand, or users have to provide feedback on relevant or irrelevant judgements, ratings on a scale from to, and so on. therefore, in this paper, we propose several approaches that can be used to adapt search results according to each userinformation need. we then compare the retrieval accuracy of our proposed approaches. compared with our prior works, we scrutinize userbrowsing history in one day closely and it allows each user to perform more ne grained search by capturing changes of each userpreferences without any user effort. such a method is not performed in typical search engines. this paper is organized as follows: in section #, we review related work focusing on personalized search systems. in section #, we propose novel approaches to providing relevant information that satis es each userinformation need by capturing changes in userpreferences without usereffort. for example, for the query java, some users may be interested in documents dealing with the programming language, java, while other users may want documents he is currently working for hitachi, ltd, software division. masatoshi yoshikawa nagoya university furo, chikusa, nagoya, aichi, japan yosikawa itc nagoyau ac jp related to coffee. therefore, web search results should adapt to users with different information needs. in section #, we present the experimental results for evaluating our proposed approaches. finally, we conclude the paper with a summary and directions for future work in section #. in general, each user has different information needs for his her query. however, the discovery of patterns from usage data by itself is not suf cient for performing the personalization tasks. these types of registration, feedback, or ratings can become time consuming and users prefer easier methods. long term search history contains rich information about a user search preferences, which can be used as search context to improve retrieval performance. in this paper, we study statistical language modeling based methods to mine contextual information from long term search history and exploit it for a more accurate estimate of the query language model. experiments on real web search data show that the algorithms are effective in improving search accuracy for both fresh and recurring queries. the best performance is achieved when using clickthrough data of past searches that are related to the current query. first, a userbackground and interests can usually be learned from his her search history by looking at the topics covered by the past queries. in the web search domain, user search history can be obtained by a proxy from web logs, or by a search engine using http redirects. short term search history is limited to a single search session, which contains a sequence of searches with a coherent information need and usually spans a short period of time. unfortunately, most existing studies on longterm search context fail to address this problem, even though they still tend to get positive results; they often use all available context as a whole, without distinguishing between relevant and irrelevant parts. in section #, we develop several algorithms based on statistical language models to mine long term search history. most existing retrieval systems, including the web search engines, suffer from the problem of one size ts all: the decision of which documents to return is made based only on the query, without consideration of a particular userpreferences and search context. when a query is ambiguous, the search results are inevitably mixed in content, which is certainly nonoptimal for the user, who is burdened by the need to sift through the mixed results. therefore, instead of relying solely on the query, which is usually just a few keywords, retrieval systems should exploit the usersearch context, which can reveal more about the usertrue information need. indeed, contextual retrieval has been identi ed as a major challenge in information retrieval research. there are a wide variety of search contexts, from the userbackground and interests, personal document collection, to what activities the user is doing before submitting the query. in this paper, we focus on the usersearch history, which is often kept in log format and records what queries the user made in the past and what results he she chose to view. this is arguably the most important form of search context for the reasons below. for example, if there were many queries such as debugging and cgi code, the user is probably interested in programming and python is likely to mean the programming language. second, from the userpast indication of document relevance we can predict his her reaction to the current retrieved documents. for example, if the user searched with the same query python before and clicked on python language websitelink, we have high con dence that the user would do it again this time, and it makes good sense to list that webpage in the top. even when there is no exact occurrence of the current query in history, we may still nd similar queries like python doc helpful. because the relevance judgment is usually only inferred from user activities, this belongs to the category of implicit feedback, which has been studied in and shown to effectively improve retrieval performance. finally, search history is readily available without extra user efforts. if privacy is a concern, we can use browser plugins and perform result reranking at the client side. search history can be divided into short term and long term types. often, a user composes an initial query, views the returned documents, and if unsatis ed, modi es the query and repeats the search process. all these activities, which form the short term search history, shed light on the current information need and make useful search context. as shown in, queries and clickthrough data in the short term search history provide implicit feedback that can be used to estimate a more accurate query language model and improve retrieval performance. long term search history is, in contrast, unlimited in time scope and may include all search activities in the past. compared with short term search history, it has several advantages. there is no need to detect session boundaries, which is often a dif cult task. nor do we need to limit the context to the contiguous chain of searches in a session; any search in the past that is related to the current one should be leveraged. this also means that we may nd context for the very rst query in a chain, which is impossible if the search history is constrained to be short term. although the extension from short term to long term seems natural and promising, the full potential of long term search history cannot be reached easily. this is because long term history inevitably involves a lot of noisy information that is irrelevant to the current search; only those searches that are related to the current one should be considered as useful context. for example, searches like world cup that are irrelevant to the current query python would not be helpful, and such noise can overwhelm the signal of related past searches. for this reason, when exploiting short term search history we need to detect session boundaries rst, so that only those searches with the same information need are used. such work includes, which interpolates the current query with different chunks of history for personalized search, and, which constructs user pro les from indexed desktop documents for search result reranking. in this paper, we systematically study how to exploit a userlong term search history to improve retrieval accuracy. we propose mixture models to represent a userinformation need and apply statistical language modeling techniques to discover relevant context from the search history, and exploit it to obtain improved estimates of the query model. we then evaluate the methods on a test set of web search histories collected from some real users. we nd that mined search history information, can substantially improve retrieval performance for both recurring and fresh queries, and works best when clickthrough data is used with a discriminative weighting scheme for past searches. we also nd that although recent history tends to be much more useful than remote history, all of the entire history is helpful for improving the search accuracy of recurring queries. the rest of the paper is organized as follows. in section #, we introduce a context sensitive information retrieval approach that allows us to incorporate contextual information mined from search history. in section #, we de ne the search history mining task and cast it as a query language model estimation problem. we describe how we build a test set by collecting users web search history in section # and present our experiment results in section #. we formulate and study search algorithms that consider a user prior interactions with a wide variety of content to personalize that user current web search. rather than relying on the unrealistic assumption that people will precisely specify their intent when searching, we pursue techniques that leverage implicit information about the user interests. this information is used to re rank web search results within a relevance feedback framework. we explore rich models of user interests, built from both search related information, such as previously issued queries and previously visited web pages, and other information about the user such as documents and email the user has read and created. our research suggests that rich representations of the user and the corpus are important for personalization, but that it is possible to approximate these representations and provide efficient client side algorithms for personalizing search. we show that such personalization algorithms can significantly improve on current web search. in most previous work on personalized search algorithms, the results for all queries are personalized in the same manner. however, as we show in this paper, there is a lot of variation across queries in the benefits that can be achieved through personalization. for some queries, everyone who issues the query is looking for the same thing. for other queries, different people want very different results even though they express their need in the same way. we examine variability in user intent using both explicit relevance judgments and large scale log analysis of user behavior patterns. while variation in user behavior is correlated with variation in explicit relevance judgments the same query, there are many other factors, such as result entropy, result quality, and task that can also affect the variation in behavior. we characterize queries using a variety of features of the query, the results returned for the query, and people interaction history with the query. using these features we build predictive models to identify queries that can benefit from personalization. a number of factors are important to consider when ranking web documents in response to a query. of primary importance is the topical relevance of each document, or how well each document matches the query, and much research in information retrieval has focused on addressing this problem. however, search on the web goes beyond ad hoc retrieval tasks based on topical relevance in several ways. peopleweb queries are short, varied, and include navigational and resource queries. there are often many more documents that match a web query than a searcher has time to view, and ranking becomes a problem not only of identifying topically relevant documents, but also of identifying those that are of particular interest to the searcher. fidel and crandall have shown that in addition to topic relevance, variables such as recency, genre, level of detail, and project relevance are important in determining relevance. algorithms like pagerank and hits take advantage of aggregate link information to get at some of these non content features. in addition, teevan et al have reported individual variation in what different people personally consider relevant to the same queries. these differences result in a large gap between how well search engines could perform if they personalized results for an individual, and how well they actually do perform by returning a single list designed to satisfy everyone. recent work on personalized search systems has focused on developing algorithms that personalize results using a representation of an individualinterests. in these systems, personalization is applied to all queries. however, as found by dou et al, personalization only improves the results for some queries, and can actually harm other queries. this can happen when unreliable personal information swamps the effects of aggregate group information that is based on considerably more information. aggregate information can be collected in large quantities for queries an individual has never issued before, and this may be particularly useful when different peopleintents for the same query are similar. on the other hand, when there is a lot of information available about what an individual it interested in related to a query, or when a query is very vague, it may make sense to focus primarily on the individual during ranking. in this paper, we first examine the variability in user intent for a large number of queries using both implicit and explicit measures. we study how well variation in the implicit measures predicts variation in the explicit measures, and look at what other factors can account for variation in the implicit measures. queries are characterized using a variety of features of the query, the results returned for the query, and the queryinteraction history. using these features we build predictive models to identify the queries that will benefit most from personalization, and explore which features are the most valuable for prediction. this paper presents an algorithm that predicts with very high accuracy which web search result a user will click for one sixth of all web queries. prediction is done via a straightforward form of personalization that takes advantage of the fact that people often use search engines to re find previously viewed resources. in our approach, an individual past navigational behavior is identified via query log analysis and used to forecast identical future navigational behavior by the same individual. we compare the potential value of personal navigation with general navigation identified using aggregate user behavior. although consistent navigational behavior across users can be useful for identifying a subset of navigational queries, different people often use the same queries to navigate to different resources. this is true even for queries comprised of unambiguous company names or urls and typically thought of as navigational. we build an understanding of what personal navigation looks like, and identify ways to improve its coverage and accuracy by taking advantage of people consistency over time and across groups of individuals. one common way that web search engines are used is to navigate to particular information resources. for example, a person looking to buy a book on web search and data mining may, instead of searching directly for a book on the topic, issue the query amazon in order to navigate to the amazon com website where a relevant book can then be identified and purchased. over of all queries are navigational in nature, according to an in situ survey of people actively searching the web. if search engines are able to identify that a query is navigational, and to identify the queryintended navigational target, they can use that information provide significant benefit to their users. at a most basic level, they can display the target in a prominent manner that is easy for users to find and select. additionally, this can be done quickly via better caching for navigational queries, and the interface can be designed to help support the desired permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. intent by providing, for example, links directly into the sitecontent or access to appropriate meta data or site functionality. search engines may also be able to provide their users with more appropriate advertisements. several approaches have been explored to identify navigational queries, including analysis of the query string and behavioral data. but while some queries are used to navigate to a particular resource by all who issue them, there are many more queries with navigational intent where the intent or intended resource is not obvious, even when it seems like it should be from the query string. for example, the reader of this paper may use a search engine to navigate to the wsdm year# homepage via the query wsdm, while a person interested in country music in the midwest may use the same query to navigate to the wsdm fm radio station homepage. others may not use the query wsdm for navigation at all, but rather issue it with an informational intent to learn more about web services distributed management. to truly understand whether a particular instance of query is navigational requires understanding the individual userintent when they issue it. we find it is possible to easily and accurately identify a significant portion of queries with navigational intent and the associated target by using an individualpast search behavior via an approach that we call personal navigation. we identify personal navigational behavior once a user has used a query to navigate to a particular result twice before. for example, someone who has searched for wsdm several times and clicked on http: wsdm org every time they did can be expected to click on the same result the next time they issue the query. personal navigation presents a real opportunity for search engines to take a first step into safe, low risk web search personalization. most personalization approaches rely on explicit or inferred user profiles to guess what new content might be of interest to a user for a given query. here we look at how to capture the lowhanging fruit of personalizing results for repeat queries. our ability to reliably identify navigational intent for queries that appear informational suggests navigational behavior may be more common than previously believed. what is more, there is the potential to significantly benefit users with the identification of these queries, as the identified targets are more likely to be ranked low in the result list than typical clicked search results. after a brief description of the query logs used for our analysis, we explore general navigational behavior where everyone is assumed to use the same query to navigate to the same result. we expose several flaws in this approach, and introduce personal navigation as an alternative. we present a straightforward algorithm for identifying personal navigation behavior, and show that many queries can be easily and accurately identified in this way. we explore how our ability to predict personal navigation is impacted by the consistency of the behavior over time and across individuals, and conclude with a discussion of how repeat behavior can be used to improve the search experience. personalized web search takes advantage of information about an individual to identify the most relevant results for that person. a challenge for personalization lies in collecting user profiles that are rich enough to do this successfully. one way an individual profile can be augmented is by using data from other people. to better understand whether groups of people can be used to benefit personalized search, we explore the similarity of query selection, desktop information, and explicit relevance judgments across people grouped in different ways. the groupings we explore fall along two dimensions: the longevity of the group members relationship, and how explicitly the group is formed. we find that some groupings provide valuable insight into what members consider relevant to queries related to the group focus, but that it can be difficult to identify valuable groups implicitly. building on these findings, we explore an algorithm to groupize web search results that leads to a significant improvement in result ranking on group relevant queries. web search personalization algorithms improve the web search experience by using an individualdata to identify the results that are the most relevant to that individual. for example, a searcherquery can be modified to reflect a particular interest, or results may be re ranked so that personally relevant results appear higher in the list. previous research suggests personalization algorithms perform best when there is a large amount of data available about an individual. for this reason, we propose combining an individualdata with that of other related people to enhance the performance of personalized search. we call the use of group information for personalization groupization. one challenge in the use of group data for personalization lies in the identification of related groups of people. to develop an understanding of what factors are important for building groups for groupization, we conducted two studies of a total of people. the data we collected enabled us to understand whether people grouped by various properties were similar in the queries they selected, the information they had on their desktop, or the relevance judgments they assigned to search results. we explored groupings that varied based on the longevity of the relationship and on whether the group was formed explicitly or implicitly. specific grouping criteria included task, interests, demographics, geographic location, occupation, work group, query selection, and the content on their desktop computers. by correlating group membership with the similarities of the group members explicit relevance judgments, we are able to understand what types of groups are most likely to receive value from groupization. it appears that some attributes are more useful than others for identifying people who find the same results relevant, and, in particular, that group membership provides information about what members consider relevant to group related queries. using the data we collected to understand group properties, we explore combining information about group members to produce a groupized result list. we find that it is possible to aggregate personalization scores from different group members to create a groupized result list that is of higher quality than each individualpersonalized list. consistent with the understanding we develop of the different attributes, groupization appears most useful for queries related to the group. we begin the paper with a discussion of related work in the areas of personalization, collaborative filtering, and collaborative web search. by analyzing the collected data, we explore the within group variation of relevance judgments, query selection, and user profile information. we then describe a groupization algorithm that extends personalization techniques to include group data. we analyze the value of groupization for the groups represented in our study, and conclude with a discussion of practical issues, including techniques for identifying groups and group related queries outside of experimental settings. search and browsing activity is known to be a valuable source of information about user search intent. it is extensively utilized by most of modern search engines to improve ranking by constructing certain ranking features as well as by personalizing search. personalization aims at two major goals: extraction of stable preferences of a user and specification and disambiguation of the current query. the common way to approach these problems is to extract information from user search and browsing long term history and to utilize short term history to determine the context of a given query. personalization of the web search for the first queries in new search sessions of new users is more difficult due to the lack of both long and short term data. to be more precise, we restrict our attention to the set of initial queries of search sessions. these, with the lack of contextual information, are known to be the most challenging for short term personalization and are not covered by previous studies on the subject. we apply a widespread framework for personalization of search results based on the re ranking approach and evaluate our methods on the large scale data. the proposed methods are shown to significantly improve non personalized ranking of one of the major commercial search engines. when we restrict the use of our method to the queries with largest expected gain, the resulting benefit of personalization increases significantly. in this paper we study the problem of short term personalization. to approach this problem in the absence of the search context, we employ short term browsing context. to the best of our knowledge this is the first study addressing the problem of short term personalization based on recent browsing history. we find that performance of this re ranking approach can be reasonably predicted given a query. the classical approach to web retrieval considers a userrequest aside from her personality, context of the request, nature of the search intent. now modern commercial search engines tend to employ not only information about the query itself, but also all knowledge about searcherpersonality expressed in her long term interests, context of the current query and her engagement with certain web pages. recent studies show that properly processed clickthrough and browsing logs can signi cantly improve quality of the classical ranking. browsing and search sessions constitute the major part of user activity studied by researchers and provide one of the most powerful sources of data for personalization of search results and context sensitive ranking. search session is a series of intent related userrequests to a search engine. similarly, a series of web pages visited by a user with similar intent are referred to as a browsing session. modern studies prove that usage of search click through logs helps to disambiguate the current information need of a searcher as well as to construct her interests pro le. at the same time, there is not much known about applicability of the browsing sessions to the same tasks, though several works provide comprehensive studies of the nature of the post search browsing sessions and suggest various ways to incorporate them into the design of a search engine. in the current study we address the problem of personalization of the web search for new users. since long term search history is sparse or completely absent for them, we perform only short term personalization. moreover, we restrict our attention to the most arduous queries: rst queries in the search session with lack of even short term search context. the research questions motivated our study are: what portion of queries cannot be covered by personalization methods based on the short term search context. to what extent could short term browsing context improve ranking on those queries. how to select exactly those queries that have potential to bene. from the given personalization approach by analyzing the search and browsing logs of a major commercial search engine we demonstrate that a considerable volume of queries has very little short term contextual information from the search session itself. these queries are challenging for the state of the art short term personalization methods based on search context, so we adopt a personalization framework based on re ranking using short term browsing context. we also extract various statistics from browsing sessions of other users to improve ranking for the current user. each re ranking approach has its applicability limits and inappropriate employment could even harm the quality of a search engine. to cope with this issue we present a method of automatic ltering of queries suitable for context aware personalization. our study is completely data driven, ie, both sets for training and evaluation are extracted from user logs. to sum up, the contributions of the present study are: utilization of short browsing context in a personalization framework based on re ranking. most importantly, it allows to improve ranking for queries with the lack of search context and for users without long term interest pro les. development of a framework for automatic learning of. utilization of knowledge about recent browsing experience of other users to improve search experience of a new user. comprehensive analysis of importance of various sources of data and features extracted from them for shortterm personalization based on browsing context. the rest of the paper is organized as follows. in section # we describe related work relevant to our research. in section # we give examples of short and long term based personalization cases and observe some statistics about typical search sessions to motivate our study. in section # we describe in detail our method based on a com mon framework for personalization. we discuss features employed in our study in section # and experiments themselves in section #. finally, we conclude our research in section #. in the past decade the situation has drastically changed. how does the length of the utilized browsing session. each re ranking algorithm has its own scope, and the ltering directly depends on a personalization algorithm. describe the data employed for further evaluations in section #. in this paper, we present a systematic study of the effectiveness of five variant sources of contextual information for user interest modeling. post query navigation and general browsing behaviors far outweigh direct search engine interaction as an information gathering activity. the five contextual information sources used are: social, historic, task, collection, and user interaction. our findings demonstrate that the sources perform differently depending on the duration of the time window used for future prediction, and that context overlap outperforms any isolated source. search and recommendation systems must include contextual information to effectively model users interests. therefore we conducted this study with a focus on website recommendations rather than search results. we evaluate the utility of these sources, and overlaps between them, based on how effectively they predict users future interests. designers of website suggestion systems can use our findings to provide improved support for post query navigation and general browsing behaviors. this abstraction is necessary given the scale constraints under which these systems operate. others include time of day, user gender, age, ethnicity, locality, etc. these interest models can be effective for identifying aspects of user information needs; however, users spend more time engaged in post query navigation and general browsing than using search engines. although context information has been used to support post query navigation and general browsing, little is known about the value of different contextual sources for this purpose. in this paper we describe a systematic, log based study of numerous contextual sources for modeling user interests during web interaction. this is the first study to systematically assess contextual variants for user interest modeling. we describe their evaluation in section #, and present the findings in section #. we discuss our findings in section #, and conclude in section #. modeling user interests to meet individual user needs is an important challenge for personalization and information filtering applications, such as recommender systems. information behavior is embedded within an external context that motivates the problem situation and influences interaction behavior. meeting user requirements involves a thorough understanding of their interests expressed explicitly through search engine queries or implicitly through browsing behavior and search context. the information retrieval community has theorized about context, developed models for context sensitive search, and performed user studies investigating the role of context in the information seeking process. largescale ir systems such as web search engines assume queries are context independent. user modeling systems have fewer constraints and typically process past user permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. consumption data, search related interactions, or explicit ratings to obtain a representation of user interests stored in a user interest model. such models are suitable for predicting future behavior, augmenting search engine queries, or suggesting relevant items during post query navigation or general browsing. the historical information employed in user interest modeling is one source of contextual evidence about the current session. the polyrepresentation principle suggests that the overlap between numerous contexts associated with the current session can be used to locate pertinent items. the querying and result examination behavior of search system users supports the development of rudimentary user interest models that are based solely on the interaction context. the core task for any user modeling system is predicting future behavior, and we evaluate the informativeness of different sources of contextual evidence based on their informativeness for predicting users future interests at different temporal durations. we assume that the user has browsed to a web page and the task is to leverage context to predict their future interests. the use of the current page and five distinct sources of context are evaluated: interaction: recent interaction behavior preceding the current page; collection: pages with hyperlinks to the current page; task: pages related to the current page by sharing the same search engine queries; historic: the longterm interests for the current user, and; social: the combined interests of other users that also visit the current page. we also study the use of overlap between sources as a stronger source of contextual signal. as we will show, the performance of contextual variants depends on the time duration used to represent future interests, and overlap between contexts yields more effective interest models than any model itself. understanding which sources and source combinations best predict future user interests is critical for the development of effective website recommendation systems. the remainder of this paper is structured as follows. section # presents related work on at least contextual ir, user modeling, and recommendation systems. section # describes the log data used to perform our study. the user interest models developed based on each contextual source are described in section #. a query considered in isolation offers limited information about a searcher intent. query context that considers pre query activity, can provide richer information about search intentions. in this paper, we describe a study in which we developed and evaluated user interest models for the current query, its context, and their combination, which we refer to as intent. using large scale logs, we evaluate how accurately each model predicts the user short term interests under various experimental conditions. in our study we: determine the extent of opportunity for using context to model intent; compare the utility of different sources of behavioral evidence for building predictive interest models, and; investigate optimally combining the query and its context by learning a model that predicts the context weight for each query. our findings demonstrate significant opportunity in leveraging contextual information, show that context and source influence predictive accuracy, and show that we can learn a near optimal combination of the query and context for each query. the findings can inform the design of search systems that leverage contextual information to better understand, model, and serve searchers information needs. search behavior resides within an external context that motivates the problem situation and influences interaction behavior for the duration of the search session and beyond. satisfying searchers information needs involves a thorough understanding of their interests expressed explicitly through search queries, or implicitly through search engine result page clicks or post serp browsing behavior. the information retrieval community has theorized about context, developed context sensitive search models, and performed user studies investigating the role of context in the search process. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. most ir systems assume that queries are context independent. this abstraction is necessary in cranfield style evaluations where relevance judgments are gathered independent of any user or interaction context. in larger operational systems such as web search engines, scale constraints have often favored simple context independent approaches. recent research suggests that this may be changing as log data and machine learning techniques are applied to model activity based context in applications such as query suggestion, query classification, web page recommendation, and web search result ranking. however, this research is often specific to particular applications, and an assessment of the value of modeling activity based context that is applicable to a broad range of search and recommendation settings is required. in this paper we describe a systematic study of the value of contextual information during web search activity. we construct interest models of the current query, its context comprising preceding session activity such as previous queries or previous clicks on search results, the combination of the query and its context, and evaluate the predictive effectiveness of these models using future actions. figure # illustrates each of the models and their role in representing users interests. queries are de accurate understanding of current interests and prediction of future interests are core tasks for user modeling, with a range of possible applications. for example, a query such as could be interpreted differently depending on whether they previous query was vs. this contextual knowledge could be used to re rank search results, classify the query, or suggest alternative query formulations. similarly, an accurate understanding of current and future interests could be used to dynamically adapt search interfaces to support different tasks. in our study we: determine the fraction of search engine queries for which context could be leveraged, measure the value of different models and sources for predicting future interests, and investigate learning the optimal combination of query and context on a per query basis, and use the learned models to improve the accuracy of our predictions. we use a logbased methodology as logs contain behavioral evidence at scale and cover many classes of information needs. this is important since performance differences may not hold for all search tasks. the remainder of this paper is structured as follows. in section # we present related work on implicit profile generation from user activity, on representing interests with topical categories, on query analysis, and on the development of predictive interest models. section # describes how we define and construct the models developed for this study. we describe the study and the findings from our analysis in section #. we discuss findings and their implications in section # and conclude in section #. online services rely on unique identifiers of machines to tailor offerings to their users. an implicit assumption is made that each machine identifier maps to an individual. however, shared ma chines are common, leading to interwoven search histories and noisy signals for applications such as personalized search and ad vertising. we present methods for attributing search activity to individual searchers. using ground truth data for a sample of almost four million. web searchers containing both machine identifiers and person identifiers we show that over half of the machine identifiers comprise the queries of multiple people. we characterize variations in features of topic, time, and other aspects such as the complexity of the information sought per the number of searchers on a machine, and show significant differences in all measures. based on these insights, we develop models to accurately estimate when multiple people contribute to the logs ascribed to a single machine identifier. we also develop models to cluster search behavior on a machine, allowing us to attribute historical data accurately and automatically assign new search activity to the correct searcher. the findings have implications for the design of applications such as personalized search and advertising that rely heavily on machine identifiers to custom tailor their services. user identifiers are central to a range of applications on the web, including behavioral analysis, personalized search, and online advertising. with single identifiers tied to a machine, applications and services operate under the implicit assumption that identifiers refer to users. however, for shared machines in homes and workplaces this is often an erroneous assumption. iw reserves the right to provide a hyperlink to the author site if the material is used in electronic media. percentages of machine identifiers in our dataset comprised of each number of searchers, from to. it is striking from the figure that of machine identifiers comprise the search activity of more than one person. although we report statistics from only one data source, the data are purposely gathered from a representative sample of united states households. this inference alone could help decide which identifiers contain noise, inform decisions about when long term histories can be trusted, and when more computationally expensive methods, such as clustering of search histories, need to be applied, and; leverage the estimated number of searchers from the previous step, we cluster search activity based on a range of similar features and show that we can accurately assign new search activity to the correct searcher. section # presents a characterization of the data. section # uses the output of the regression and through clustering, addresses the challenge of assigning activity from a particular machine identifier to the correct searcher. these machine identifiers are assigned to the machine via mechanisms such as browser cookies or toolbars. households have a computer, in most households machines are shared between multiple people. different people may use the shared machine at different times, but to a remote observer all activity is associated with a single identifier, and peoplesearch behaviors will be intertwined in search logs. this creates a noisy behavioral signal, and importantly, a challenge for analyzing search behavior, especially long term behavior that has utility in many applications, such as search personalization. let us consider some real world data gathered from a panel of millions of web searchers recruited by comscore, an copyright is held by the international world wide web conference committee. http: dx doi org year# number of searchers figure #. in addition to a machine identifier, similar to that obtainable via web browser cookies and other applications, panelists have a person identifier and are required to signin prior to use to indicate that they are searching on the machine at a particular time. since we have both machine identifiers and person identifiers, we can compute the frequency with which multiple people are observed searching on a particular machine, as well as other characterizations of searching and searcher interests reported later. we can also use these data as ground truth in developing models to estimate the number of searchers within a machine identifier, and in attributing search activity observed historically to specific searchers. figure # shows the fraction of machine identifiers in the dataset that are comprised of different numbers of searchers. the mean and median number of searchers per machine observed in the data are and respectively, aligning well with. we note that the comscore data used in our study are not proprietary; other researchers can purchase the logs from comscore, and can replicate and extend our findings. we envisage that the performance of personalization and ad matching would likely be enhanced if user centric signals and analyses were used. we also see privacy benefits of being able to accurately segregate searcher activity within a machine. providing a means to preventing the unintended sharing of sensitive information between the searchers on the same machine and help ensure that only necessary information is shared with search providers. despite the importance of accurately attributing search activity, to our knowledge, we know of no prior research on this topic beyond the level of machine identifiers. we address this shortcoming in this paper by characterizing variations in search behaviors within machines and developing predictive models to assign observed search activity to the correct individual. in doing so, we can capitalize on well documented aspects of human behavior such as the bursty nature in which human events typically occur. the estimated searcher count from the regression alone is insufficient since we do not have a representation of those searchers activity, and need such a representation to handle the assignment of new queries. we focus on the web search domain given the nature of the data available and its importance, but the activity attribution challenge applies to a number of other domains including online advertising, audio signal processing, and fraud detection. the remainder of this paper is structured as follows. section # describes related work in areas such as behavioral analysis and personalization. section # provides an overview of the data used in the study. section # describes the prediction of whether multiple searchers comprise a machine identifier and estimating the number of searchers that contribute to the search activity associated with a machine identifier. we discuss our findings and their implications for the design of online services in section #, and conclude in section #. the context of a search query often provides a search engine meaningful hints for answering the current query better. previous studies on context aware search were either focused on the development of context models or limited to a relatively small scale investigation under a controlled laboratory setting. particularly, about context aware ranking for web search, the following two critical problems are largely remained unsolved. first, how can we take advantage of different types of contexts in ranking second, how can we integrate context information into a ranking model in this paper, we tackle the above two essential problems analytically and empirically. we develop different ranking principles for different types of contexts. moreover, we adopt a learning to rank approach and integrate the ranking principles into a state of the art ranking model by encoding the context information as features of the model. we empirically test our approach using a large search log data set obtained from a major commercial search engine. our evaluation uses both human judgments and implicit user click data. the experimental results clearly show that our context aware ranking approach improves the ranking of a commercial search engine which ignores context information. furthermore, our method outperforms a baseline method which considers context information in ranking. in web search, given a query, a search engine returns the matched documents in a ranked list to meet the userinformation need. ranking models play a central role in search engines. currently, almost all the existing ranking models consider only the current query and the documents, but do not take into account any context information such as the previous queries in the same session and the answers clicked on or skipped by the user to the previous queries. in other words, almost all the current ranking models are insensitive to context. information retrieval research has well recognized that context information is very helpful in achieving good search results. context information may provide hints about users search intent and help to make better matching with documents. for example, if a user raises a query jaguar after she searches bmw, it is very likely that the user is seeking for information about a jaguar car rather than a jaguar as an animal. the absence of context information in document ranking models is probably partially due to the di culty of obtaining context information. only recently have large amounts of search session data become available, which enable large scale empirical studies on context aware methods for web search. several recent studies explore context aware search methods from di erent angles. shen et al presented a contextaware ranking method by assuming that context information can better represent search intent. they incorporated the context information to build context aware language models, which were assumed to give rise to documents not only similar to the current query but also similar to the previous queries and the summaries of the documents clicked on. ectiveness of the ranking model on trec data. however, the evaluation was based on a small data set consisting of only thirty sessions from three subjects under a controlled laboratory setting. it is unclear whether the assumption in the study holds for web search engines in the real world. more recently, cao et al extracted context information in web search sessions by modeling search sessions as sequences of user queries and clicks. they learned sequential prediction models such hidden markov model and conditional random fields from search log data. di erent from our study here, their models were designed for predicting search intents based on context information, but not for ranking. therefore, the models are more suitable for query suggestion, query categorization, and url recommendation than search results ranking, as will be further analyzed in section #. in spite of the several existing studies on context aware search methods, the following two critical problems about context aware ranking for web search are largely remained unsolved. first, how can we take advantage of di erent types of contexts in ranking second, how can we integrate context information into a ranking model in this paper, we tackle the above two essential problems analytically and empirically and make the following contributions. we develop four di erent ranking principles for di erent types of contexts. those principles promote or demote documents according to the context of the current query. we evaluate the four principles using real web search sessions, and con rm the. ectiveness of three principles through the signi cance test on the data. ective principles is consistent with the ndings obtained by shen et al on trec data, indicating that web search is quite di erent from search on trec data. moreover, we adopt a learning to rank approach and integrate the ranking principles into a state of the art ranking model, ranksvm, by encoding the context information as features. we empirically test our approach using a large search log data set obtained from a major commercial search engine. our evaluation uses both human judgments and implicit user click data. the experimental results clearly show that our context aware ranking approach improves the ranking of a commercial search engine which ignores context information. furthermore, our method outperforms a baseline method which considers context information in ranking. the rest of the paper is organized as follows. we discuss the types of contexts, propose ranking principles, and evaluate the. we incorporate context information into a learning to rank model in section #. the experimental results are reported in section #. web search engines utilize behavioral signals to develop search experiences tailored to individual users. to be effective, such personalization relies on access to sufficient information about each user interests and intentions. for new users or new queries, profile information may be sparse or non existent. to handle these cases, and perhaps also improve personalization for those with profiles, search engines can employ signals from users who are similar along one or more dimensions, ie, those in the same cohort. in this paper we describe a characterization and evaluation of the use of such cohort modeling to enhance search personalization. we experiment with three pre defined cohorts topic, location, and top level domain preference independently and in combination, and also evaluate methods to learn cohorts dynamically. we show via extensive experimentation with large scale logs from a commercial search engine that leveraging cohort behavior can yield significant relevance gains when combined with a production search engine ranking algorithm that uses similar classes of personalization signal but at the individual searcher level. additional experiments show that our gains can be extended when we dynamically learn cohorts and target easily identifiable classes of ambiguous or unseen queries.