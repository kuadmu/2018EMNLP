our work relates to existing research in improving the ef ciency of online experimentation, as discussed below. tang et al described the multi layer framework of overlapping experiments used in google. the motivation of this framework is to build a scalable online experimentation mechanism. for instance, one of these experiments can compare two ranking algorithms, another can evaluate changes in the ui, etc. a similar framework is used by the bing search engine. our work shares the same goal: to make the online experimentation pipeline more. even in the framework proposed by tang et al, in some layers we might want to run the most promising experiments earlier or experiments might arrive faster than they could be processed. orts have been applied to improve the sensitivity of the interleaving experiments, so that each interleaving experiment can be nished faster. yue et al proposed a machine learned approach to interpret click feedback from the users, so that the intrinsic noise of the clicks is reduced. chapelle et al additionally considered some simple heuristic weighting schemes that can be used while aggregating the userclicks in a single impression. we study a complimentary approach to increase the experimentation. ciency: under the optimising schedule, more successful experiments can be performed. hofmann et al proposed to estimate the interleaving comparison outcomes by treating historical user sessions as comparison events between tested alternatives. kharitonov et al used the historical click data to calculate the expected di erence in the number of clicks each interleaving alternative receives after an interleaving experiment is deployed. in a recent work, li et al proposed to leverage historical click data and natural variance in the search engineresult pages to predict the results of atests. in this work, we also use historical click data to predict the interleaving experiment outcome. indeed, predicting an outcome of an experiment is just one of the steps of our proposed experiment scheduling approach. moreover, the historical click data forms only a part of the features we use in our study: we additionally consider features that are based on the. radlinski and craswell studied the agreement between the. ine evaluation metrics, such as ndcgor precision, and the results of interleaving experiments. their work is related to our research, since they demonstrated that some metrics, such as ndcg, have a statistically signi cant correlation with the outcomes of interleaving experiments. ine evaluation metrics can be useful in predicting the interleaving experiment results. a similar experiment was performed by chapelle et al, who measured the correlation between dcg and the absolute online metrics used in atests. however, since the agreements reported in are not perfect, the following question arises: can a better prediction can be achieved by using other features apart from the search result. ectiveness in our evaluation study we address this question. further, radlinski and craswell performed their study on a dataset containing three experiments with major changes, and two experiments with minor improvements. however, major changes are rare in modern commercial search engines. similarly, the analysis of the absolute online metrics used in atests, performed by chapelle et al used a dataset of comparisons. in contrast, we use two datasets, containing real life interleaving experiments and atests, performed by a commercial search engine as part of its development. overall, to the best of our knowledge, our paper is the rst work that studies the optimisation of the schedule of the online experiments.