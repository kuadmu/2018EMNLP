interleaving is an increasingly popular technique for evaluating information retrieval systems based on implicit user feedback. while a number of isolated studies have analyzed how this technique agrees with conventional offline evaluation approaches and other online techniques, a complete picture of its efficiency and effectiveness is still lacking. in particular, we analyze the agreement of interleaving with manual relevance judgments and observational implicit feedback measures, estimate the statistical efficiency of interleaving, and explore the relative performance of different interleaving variants. we also show how to learn improved credit assignment functions for clicks that further increase the sensitivity of interleaving. in this paper we extend and combine the body of empirical evidence regarding interleaving, and provide a comprehensive analysis of interleaving using data from two major commercial search engines and a retrieval system for scientific literature. while the conventional approach of using expert judgments has proven itself effective in many respects, it has at least two limitations. as such, more exible and ef cient evaluation methods are required especially for applications with resource constraints including desktop search, personalized web search, intranet search, helpdesk support, and academic literature search. permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies show this notice on the rst page or initial screen of a display along with the full citation. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior speci. permissions may be requested from publications dept, acm, inc, penn plaza, suite, new york, ny year# usa, fax, or permissions acm org. year# acm year# year# art doi year# http: doi acm org year#. the basic idea behind all variants of the interleaving approach is to perform paired online comparisons of two rankings. the algorithm used to produce the interleaved ranking is designed to be fair, so that users clicks can be interpreted as unbiased judgments about the relative quality of the two rankings. in this way, interleaving interactively modi es, in a controlled experiment, the search results presented to the user so that the observed user behavior is more interpretable. this avoids the problem of post hoc interpretation of observational data common to most other approaches to interpreting implicit feedback. speci cally, this paper reviews, analyzes, and extends the balanced interleaving and team draft interleaving methods. after introducing the two interleaving methods and the systems used for evaluation in the next two sections, we validate and analyze interleaved evaluation by answering a series of speci. finally, we address the limitations of interleaved evaluation as compared to other approaches in depth. in particular, we would like to acknowledge the contributions of madhu kurup, nick craswell, and yue gao and ya zhang. proper evaluation of search quality is essential for developing effective information retrieval systems. we aim to provide a comprehensive body of evidence regarding its effectiveness, accuracy, and limitations. first, expert judgments may not re ect the actual relevance and utility that users experience while using a retrieval system, especially since judges often cannot reliably estimate the users intents. second, its associated cost and turnaround times are substantial and often prohibitive. this research was funded in part through nsf award iis and through a gift from yahoo authoraddress: chapelle; email: chap yahoo inc com. chapelle et al these limitations have motivated research on alternative approaches to retrieval evaluation, especially approaches based on observable user behavior such as clicks, query reformulations, and response times. unlike expert judgments, usage data can be collected at essentially zero cost, is available in real time, and re ects the judgments of the users rather than those of judges far removed from the users context at the time of the information need. the key problem with retrieval evaluation based on usage data lies in its proper interpretation, in particular understanding how certain observable statistics relate to retrieval quality. in this article, we analyze the interleaving approach to solving this key problem. this involves merging the two rankings into a single interleaved ranking, and then presenting the interleaved ranking to the user. the analysis relies on results from a new large scale eld study on the yahoo search engine, tied into published and additional unpublished data from experiments on the bing search engine, and the fulltext search of the arxiv org repository of scienti. we ask whether interleaving agrees with the conventional evaluation approach based on relevance judgments collected from experts; whether it agrees with other online metrics; how the statistical sensitivity and reliability of these different alternatives compares; and how to select among different credit assignment schemes for clicks. to provide a complete picture of interleaving as an evaluation technique, this paper combines new data and experiments with data and results from past collaborations with other authors. while numerous metrics for information retrieval are available in the case of binary relevance, there is only one commonly used metric for graded relevance, namely the discounted cumulative gain. a drawback of dcg is its additive nature and the underlying independence assumption: a document in a given position has always the same gain and discount independently of the documents shown above it. inspired by the cascade user model, we present a new editorial metric for graded relevance which overcomes this difficulty and implicitly discounts documents which are shown below very relevant documents. more precisely, this new metric is defined as the expected reciprocal length of time that the user will take to find a relevant document. this can be seen as an extension of the classical reciprocal rank to the graded relevance case and we call this metric expected reciprocal rank. we conduct an extensive evaluation on the query logs of a commercial search engine and show that err correlates better with clicks metrics than other editorial metrics. as with any application of machine learning, web search ranking requires labeled data. the labels usually come in the form of relevance assessments made by editors. click logs can also provide an important source of implicit feedback and can be used as a cheap proxy for editorial labels. the main difficulty however comes from the so called position bias urls appearing in lower positions are less likely to be clicked even if they are relevant. in this paper, we propose a dynamic bayesian network which aims at providing us with unbiased estimation of the relevance from the click logs. experiments show that the proposed click model outperforms other existing click models in predicting both click through rate and relevance. web page ranking has been traditionally based on hand designed ranking functions such as bm. with the inclusion of thousands of features for ranking, hand tuning of ranking function becomes intractable. several machine learning algorithms have been applied to automatically optimize ranking functions. machine learned ranking requires a large number of training examples, with relevance labels indicating the degree of relevance for each querydocument pair. the cost of the editorial labeling is usually quite expensive. moreover, the relevance labels of the training examples could change over time. for example, if the query is time sensitive or recurrent, a search engine is expected to return the copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. most up to date documents sites to the users. however, it would be prohibitive to keep all the relevance labels up to date. click logs embed important information about user satisfaction with a search engine and can provide a highly valuable source of relevance information. compare to editorial labels, clicks are much cheaper to obtain and always re ect current relevance. clicks have been used in multiple ways by a search engine: to tune search parameters, to evaluate di erent ranking functions, or as signals to directly in uence ranking. however, clicks are known to be biased, by the presentation order, the appearance of the documents, and the reputation of individual sites. many studies have attempted to account the position bias of click. carterette and jones proposed to model the relationship between clicks and relevance so that clicks can be used to unbiasedly evaluate search engine when lack of editorial relevance judgment. other research attempted to model user click behavior during search so that future clicks may be accurately predicted based on observations of past clicks. two di erent types of the click models are position models and the cascade model. a position model assumes that a click depends on both relevance and examination. each rank has a certain probability of being examined, which decays by rank and depends only on rank. a click on a url indicates that the url is examined and considered relevant by the user. however this model treats the individual urls in a search result page independently and fails to capture the interaction among urls in the examination probability. take for example two equally relevant urls for a query: a user may only click on the top one, feel satis ed, and then leave the search result page. in this case, the positional bias cannot fully explain the lack of clicks for the second url. the cascade model assumes that users examine the results sequentially and stop as soon as a relevant document is clicked. here, the probability of examination is indirectly determined by two factors: the rank of the url and the relevance of all previous urls. the cascade model makes a strong assumption that there is only one click per search and hence it could not explain the abandoned search or search with more than one clicks. even though the cascade model is quite restrictive, the authors of that paper showed that we refer to url as a shorthand for the entire display block consisting of the title, abstract and url of the corresponding result. it can predict click through rates more accurately than the position models described above. none of the above models distinguish perceived relevance and actual relevance. because users cannot examine the content of a document until they click on the url, the decision to click is made based on perceived relevance. while there is a strong correlation between perceived relevance and actual relevance, there are also many cases where they di er. in this paper, a dynamic bayesian network model is proposed to model the users browsing behavior. as in the position model, we assume that a click occurs if and only if the user has examined the url and deemed it relevant. similar to the cascade model, our model assumes that users make a linear transversal through the results and decide whether to click based on the perceived relevance of the document. the user chooses to examine the next url if he she is unsatis ed with the clicked url. our model di ers from the cascade model in two aspects: because a click does not necessarily mean that the user is satis ed with the clicked document, we attempt to distinguish the perceived relevance and actual relevance. interleaved comparison methods, which compare rankers using click data, are a promising alternative to traditional information retrieval evaluation methods that require expensive explicit judgments. a major limitation of these methods is that they assume access to live data, meaning that new data must be collected for every pair of rankers compared. we investigate the use of previously collected click data for interleaved comparisons. we start by analyzing to what degree existing interleaved comparison methods can be applied and find that a recent probabilistic method allows such data reuse, even though it is biased when applied to historical data. we then propose an interleaved comparison method that is based on the probabilistic approach but uses importance sampling to compensate for bias. we experimentally confirm that probabilistic methods make the use of historical data for interleaved comparisons possible and effective. interleaved comparison methods, which compare ran kers using naturally occuring user interactions such as clicks, are quickly gaining interest as a complement to traditional evaluations for information retrieval. compared to evaluations based on manual judgments from expert annotators, interleaved comparison methods rely only on data that can be collected cheaply and unobtrusively. since this data is based on the behavior of real users, it more accurately re ects how well their information needs are met. existing interleaved comparison methods suffer from a drawback: they assume access to live data, ie, data gathered during the evaluation itself. comparing two rankers requires presenting users with interleaved result lists based on those rankers and observing how permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. since historical data, collected while comparing two different rankers, is unlikely to contain the same result lists, it is unclear how it can be exploited by interleaved comparison methods. without the ability to estimate comparison outcomes using historical data, the practical utility of interleaved comparison methods is limited. if all comparisons are done with live data, then applications such as learning to rank, which perform many comparisons, need prohibitive amounts of data. since interleaving result lists may affect the userexperience of a search engine, the collection of live data is complicated by the need to rst control the quality of the compared rankers using alternative evaluation setups. using previously collected data is already possible for evaluation methods that use explicit assessments. thus, although obtaining explicit judgments is initially expensive, this cost can be amortized over the whole set of evaluations that reuse the same judgments. interleaved comparisons are less expensive initially but cannot be amortized, as live data is needed for each new evaluation. we remedy this shortcoming by investigating the use of historical data by interleaved comparison methods. first, we analyze to what degree existing approaches can exploit historical data. we show that, while the most widely known approach, called team draft, cannot do so effectively, a recently developed probabilistic method can. this method makes ef cient use of sample data by marginalizing over the ways in which observed result lists may have been constructed. in the live data setting, the probabilistic method is unbiased, ie, its expected value equals the expected outcome. however, it is biased when applied to historical data. second, we introduce an interleaved comparison method that is based on the probabilistic approach but uses importance sampling to correct for bias. third, we present an empirical analysis of the use of historical data by interleaved comparison methods. because the estimated values are ultimately used only to make a binary decision, the original, biased approach is surprisingly robust. when lots of historical data is available, the unbiased approach performs better. the reuse of historical data enables the application of interleaved comparisons to learning to rank and large scale evaluation feasible. evaluating rankers using implicit feedback, such as clicks on documents in a result list, is an increasingly popular alternative to traditional evaluation methods based on explicit relevance judgments. previous work has shown that so called interleaved comparison methods can utilize click data to detect small differences between rankers and can be applied to learn ranking functions online. in this paper, we analyze three existing interleaved comparison methods and find that they are all either biased or insensitive to some differences between rankers. to address these problems, we present a new method based on a probabilistic interleaving process. we derive an unbiased estimator of comparison outcomes and show how marginalizing over possible comparison outcomes given the observed click data can make this estimator even more effective. we validate our approach using a recently developed simulation framework based on a learning to rank dataset and a model of click behavior. our experiments confirm the results of our analysis and show that our method is both more accurate and more robust to noise than existing methods. in information retrieval, most techniques for evaluating rankers require explicit feedback, in which assessors manually judge the relevance of documents for given queries. since annotators usually judge documents for queries they did not formulate, the judgments may be biased towards their interpretation of the underlying information needs. or commercial advantage and that copies bear this notice and the full citation on the rst page. rankers using implicit feedback such as click data is a promising alternative. since click data is a by product of the normal interactions between the user and the retrieval system, it can be collected unobtrusively and at minimal cost. in addition, since it is based on the behavior of real users, it more accurately re ects how well their actual information needs are met. previous work demonstrated that two rankers can be successfully compared using click data. in the balanced interleave method, an interleaved list is generated for each query based on the two rankers. the userclicks on the interleaved list are attributed to each ranker based on how they ranked the clicked documents, and the ranker that obtains more clicks is deemed superior. these comparisons are repeated for a large number of queries, to obtain a reliable estimate of the relative performance of the two rankers. unfortunately, the balanced interleave approach suffers from a bias that can make it prefer the wrong ranker. the alternative team draft method avoids this form of bias by assigning each document to the ranker that contributed it to the interleaved list and only crediting clicks to the ranker to which the clicked document is assigned. in contrast to the balanced interleave and team draft methods, the document constraint method takes relations between documents into account. in this paper, we analyze existing interleaved comparison methods and nd that the document constraint method suffers from a bias similar to that of the balanced interleave method. we also nd that the team draft method, though unbiased, is insensitive to some differences between rankers. we illustrate these problems using examples, and show how they can affect comparison outcomes. furthermore, we propose a new approach for comparing rankers that utilizes a probabilistic model of the interleave process and is based on the team draft method. the main idea is to replace the original rankers with softmax functions that de ne probability distributions over documents. contributing a document to an interleaved list is formulated as sampling without replacement from a probability distribution. because this estimator is based on our probabilistic model, it smooths over click assignments, making the method both unbiased and sensitive to small differences between rankers. finally, we show how comparisons can make more effective use of observed data by marginalizing over all possible comparison outcomes for each observed sample. we present experiments using a large set of explicitly labeled data and a recently developed user click model that simulates the resulting implicit feedback. these experiments con rm the results of our analysis, and show that our method can identify the correct ranker more reliably than existing methods. furthermore, our method improves over previous methods in terms of ef ciency and robustness to noise. in the next section we give an overview of related work. we present and discuss experimental outcomes in ยง and conclude in ยง. consequently, evaluating permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. to infer comparison outcomes, we derive an unbiased estimator. we then formulate requirements for interleaved comparison methods and analyze existing methods. our probabilistic method is described in ยง and experiments are detailed in ยง. unfortunately, obtaining such explicit judgments is expensive and error prone. modern large retrieval environments tend to overwhelm their users by their large output. the third one computes the relative to the ideal performance of ir techniques, based on the cumulative gain they are able to yield. since all documents are not of equal relevance to their users, highly relevant documents should be identified and ranked first for presentation. in order to develop ir techniques in this direction, it is necessary to develop evaluation approaches and methods that credit ir methods for their ability to retrieve highly relevant documents. this can be done by extending traditional evaluation methods, that is, recall and precision based on binary relevance judgments, to graded relevance judgments. alternatively, novel measures based on graded relevance judgments may be developed. this article proposes several novel measures that compute the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. the first one accumulates the relevance scores of retrieved documents along the ranked result list. the second one is similar but applies a discount factor to the relevance scores in order to devaluate late retrieved documents. these novel measures are defined and discussed and their use is demonstrated in a case study using trec data: sample system run results for queries in trec. as a relevance base we used novel graded relevance judgments on a four point scale. the test results indicate that the proposed measures credit ir methods for their ability to retrieve highly relevant documents and allow testing of statistical significance of effectiveness differences. the graphs based on the measures also provide insight into the performance ir techniques and allow interpretation, for example, from the user point of view. or commercial advantage, the copyright notice, the title of the publication, and its date appear, and notice is given that copying is by permission of the acm, inc. to bring such differences into daylight, both graded relevance judgments and a method for using them are required. in most laboratory tests in ir documents are judged relevant or irrelevant with regard to the request. more often relevance is con ated into two categories al at the analysis phase because of the calculation of precision and recall. the third one computes the relative to theideal performance of ir techniques, based on the cumulated gain they are able to yield. the graphs based on the measures also provide insight into the performance ir techniques and allow interpretation, for example, from the user point of view. modern large retrieval environments tend to overwhelm their users by their large output. since all documents are not of equal relevance to their users, highly relevant documents, or document components, should be identi ed and ranked rst for presentation. this is often desirable from the user point of view. in order to develop ir techniques in this direction, it is necessary to develop this research was supported by the academy of finland under the grant numbers and. authors address: department of information studies, fin university of tampere, finland; email: uta. permission to make digital hard copy of part or all of this work for personal or classroom use is granted without fee provided that the copies are not made or distributed for pro. to copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior speci. year# acm year# year# acm transactions on information systems, vol. evaluation approaches and methods that credit ir methods for their ability to retrieve highly relevant documents. the current practice of liberal binary judgment of topical relevance gives equal credit for a retrieval technique for retrieving highly and marginally relevant documents. for example, trec is based on binary relevance judgments with a very low threshold for accepting a document as relevant the document needs to have at least one sentence pertaining to the request to count as relevant. therefore differences between sloppy and excellent retrieval techniques, regarding highly relevant documents, may not become apparent in evaluation. in some studies relevance judgments are allowed to fall into more than two categories, but only a few tests actually take advantage of different relevance levels. however, graded relevance judgments may be collected in eld studies and also produced for laboratory test collections, so they are available. graded relevance judgments may be used for ir evaluation, rst, by extending traditional evaluation measures, such as recall and precision andr curves, to use them. al al arvelin, arvelin and kek ainen propose the use of each relevance level separately in recall and precision calculation. thus differentr curves are drawn for each level. they demonstrate that differing performance of ir techniques at different levels of relevancemaythusbeobservedandanalyzed kek ainenandj al arvelin generalize recall and precision calculation to directly utilize graded document relevance scores. they consider precision as a function of recall, but the approach extends to dcv based recall and precision as well. they demonstrate that the relative effectiveness of ir techniques, and the statistical signi cance of their performance differences, may vary according to the relevance scales used. in the present article we develop several new evaluation measures that seek to estimate the cumulative relevance gain the user receives by examining the retrieval result up to a given rank. the rst one accumulates the relevance scores of retrieved documents along the ranked result list. the second one is similar but applies a discount factor to the relevance scores in order to devaluate late retrieved documents. the rst two were originally presented inal arvelin and kek ainen and were also applied in the trec web track year# and in a text summarization experiment by sakai and sparck jones. these novel measures are akin to the average search length, sliding ratio, and normalized recall measures. they also have some resemblance to the ranked half life and relative relevance measures proposed by borlund and ingwersen for interactive ir. however, they offer several advantages by taking both the degree of relevance and the rank position of a document into account. the novel measures are rst de ned and discussed and then their use is demonstrated in a case study on the effectiveness of trec runs in retrieving documents of various degrees of relevance. the results indicate that the proposed measures credit ir methods for their ability to retrieve highly relevant documents and allow testing of statistical signi cance of effectiveness differences. section # explains our evaluation measures: the cumulated gain based evaluation measures. the test environment, relevance judgments, and the retrieval results are reported. section # contains discussion and section # conclusions. this paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. while previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. this makes them difficult and expensive to apply. the goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query log of the search engine in connection with the log of links the users clicked on in the presented ranking. such clickthrough data is available in abundance and can be recorded at very low cost. taking a support vector machine approach, this paper presents a method for learning retrieval functions. from a theoretical perspective, this method is shown to be well founded in a risk minimization framework. furthermore, it is shown to be feasible even for large sets of queries and features. the theoretical results are verified in a controlled experiment. it shows that the method can effectively adapt the retrieval function of a meta search engine to a particular group of users, outperforming google in terms of retrieval quality after only a couple of hundred training examples. which www page does a user actually want to re trieve when he types some keywords into a search engine there are typically thousands of pages that contain these words, but the user is interested in a much smaller subset. if we knew the set of pages actually relevant to the user query, we could use this as training data for optimizing the retrieval function. unfortunately, experience shows that users are only rarely willing to give explicit feedback. however, this paper argues that sufficient information is already hidden in the logfiles of www search engines. ceive millions of queries per day, such data is available in abundance. compared to explicit feedback data, which is typically elicited in laborious user studies, any information that can be extracted from logfiles is virtually free and sub stantially more timely. this leads to a problem of learning with preference examples like for query, document, should be ranked higher than document db. more generally, will formulate the problem of learning a ranking function over a finite domain in terms of empirical risk minimization. for this formulation, will present a support vector machine algorithm that leads to a convex program and that can be extended to non linear ranking functions. it starts with a defi nition of what clickthrough data is, how it can be recorded, and how it can be used to generate training examples in the form of preferences. section # then introduces a gen eral framework for learning retrieval functions, leading to an svm algorithm for learning parameterized orderings in section #. section # evaluates the method based on experi mental results. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific pemaission and or a fee. one could simply ask the user for feedback. since major search engines re permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. this paper presents an approach to learning retrieval func tions by analyzing which links the users click on in the pre sented ranking. experi ments show that the method can successfully learn a highly effective retrieval function for a meta search engine. interleaving is an online evaluation method to compare two alternative ranking functions based on the users implicit feedback. an important property of interleaving methods is their sensitivity, ie, their ability to reliably derive the comparison outcome with a relatively small amount of user behaviour data. this allows testing of changes in the search engine ranking functions frequently and, as a result, rapid iterations in developing search quality improvements can be achieved. in this paper we propose a novel approach to further improve interleaving sensitivity by using pre experimental user behaviour data. the probabilities of presenting these interleaved result pages to the users are then optimised, such that the sensitivity of interleaving is maximised. in order to evaluate the proposed approach, we re use data from six actual interleaving experiments, previously performed by a commercial search engine. our results demonstrate that the proposed approach outperforms a state of the art baseline, achieving up to a median of reduction in the number of impressions for the same level of confidence. in an interleaving experiment, the results from two ranking functions are merged in a single result list and presented to the users. the users click feedback on the merged result list is analysed to derive preferences over the ranking functions. in particular, the click history is used to train a click model, which is then used to predict which interleaved result pages are likely to contribute to the experiment outcome. the evaluation of retrieval systems is vital to ensure progress in information retrieval. historically, the system based evaluation with manual judgements motivated by cran eldexperiments has proven popular. however, this approach has several limitations, as discussed by voorhees. firstly, often the manual judgements are expensive to collect, since labelling requires labour from the trained professionals. on the other hand, the online user based experimental methods, such as atesting and interleaving, can overcome these limitations. as these methods leverage the live query stream to infer the users preferences, they are believed to represent the actual preferences of the users. the idea behind this method is the following. following radlinski and craswell, we de ne the sensitivity of an interleaving method as its ability to derive reliable experiment outcomes with little data. firstly, sensitive methods allow search engines to evaluate improvements and iterate fast, ie, to make more business decisions in a unit of time and progress quickly. secondly, with a sensitive method, it is possible to evaluate even subtle improvements in a short time. as we will discuss in the next section, this problem has received a considerable attention from the research community. however, we address this problem from a new perspective. our hypothesis is that by using the user behaviour data prior to the start of the experiment, it is possible to adjust how often a particular interleaved result list is shown, to achieve higher levels of sensitivity. the remainder of the paper is organised as follows. after discussing the related work in section #, we derive a problem of optimisation of the interleaving experiment parameters in section #. next, in section # we discuss how this optimisation problem can be formulated and solved before deploying the experiment online. the dataset and the evaluation methodology used in our study are discussed in sections and, respectively. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. secondly, document relevance can be hard to de ne, especially when the userintent can be ambiguous or the result set is personalised. interleaving was rst proposed by joachims et al as an online unbiased evaluation method. given two ranking algorithms we want to compare, a and, we run an interleaving experiment on a portion of the query stream. ected by the experiment, the results of a andare mixed into an interleaved result list that is shown to the user just as a regular search result page. next, the userclick behaviour is interpreted in order to derive which of the tested algorithms provides the users with a better results ranking. high levels of sensitivity are important in web search retrieval for several reasons. finally, since a considerable part of the evaluated changes may lead to a degradation of the user experience, sensitive methods can help to identify such changes early and reduce the user frustration. in this work, we concentrate on improving the sensitivity of the interleaving methods. in order to test this hypothesis, we propose and evaluate a method to leverage historical user behaviour data observed in the click log to optimise the parameters of the interleaving experiment. the contributions of this paper are three fold: we propose a theoretically motivated approach to increase the sensitivity of the interleaving experiments by adjusting how often a particular interleaved result page is shown; we propose a user click model based approach to predict the parameters of this optimisation problem before the experiment is performed; we perform a thorough experimental study of the proposed algorithms. further, we report our results in section # and close the paper with conclusions and a discussion of the possible future work in section #. online controlled experiments are often utilized to make data driven decisions at amazon, microsoft, ebay, facebook, google, yahoo, zynga, and at many other companies. while the theory of a controlled experiment is simple, and dates back to sir ronald. fisher experiments at the rothamsted agricultural experimental station in england in the year#s, the deployment and mining of online controlled experiments at scale thousands of experiments now has taught us many lessons. these exemplify the proverb that the difference between theory and practice is greater in practice than in theory. we present our learnings as they happened: puzzling outcomes of controlled experiments that we analyzed deeply to understand and explain. each of these took multiple person weeks to months to properly analyze and get to the often surprising root cause. the root causes behind these puzzling results are not isolated incidents; these issues generalized to multiple experiments. the heightened awareness should help readers increase the trustworthiness of the results coming out of controlled experiments. at microsoft bing, it is not uncommon to see experiments that impact annual revenue by millions of dollars, thus getting trustworthy results is critical and investing in understanding anomalies has tremendous payoff: reversing a single incorrect decision based on the results of an experiment can fund a whole team of analysts. the topics we cover include: the oec, click tracking, effect trends, experiment length and power, and carryover effects. web facing companies, including amazon, ebay, etsy, facebook, google, groupon, intuit, linkedin, microsoft, netflix, shop direct, stumbleupon, yahoo, and zynga use online controlled experiments to guide product development and accelerate innovation. at microsoft bing, the use of controlled experiments has grown exponentially over time, with over concurrent experiments now running on any given day. running experiments at large scale requires addressing multiple challenges in three areas: cultural organizational, engineering, and trustworthiness. on the cultural and organizational front, the larger organization needs to learn the reasons for running controlled experiments and the tradeoffs between controlled experiments and other methods of evaluating ideas. we discuss why negative experiments, which degrade the user experience short term, should be run, given the learning value and long term benefits. on the engineering side, we architected a highly scalable system, able to handle data at massive scale: hundreds of concurrent experiments, each containing millions of users. classical testing and debugging techniques no longer apply when there are billions of live variants of the site, so alerts are used to identify issues rather than relying on heavy up front testing. on the trustworthiness front, we have a high occurrence of false positives that we address, and we alert experimenters to statistical interactions between experiments. the bing experimentation system is credited with having accelerated innovation and increased annual revenues by hundreds of millions of dollars, by allowing us to find and focus on key ideas evaluated through thousands of controlled experiments. a improvement to revenue equals more than annually in the us, yet many ideas impact key metrics by and are not well estimated a priori. the system has also identified many negative features that we avoided deploying, despite key stakeholders early excitement, saving us similar large amounts. many web facing companies use online controlled experiments to guide product development and prioritize ideas, including amazon, ebay, etsy, facebook, google, groupon, intuit, linkedin, microsoft, netflix, shop direct, stumbleupon, yahoo, and zynga. controlled experiments permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. are especially useful in combination with agile development, steve blankcustomer development process, and mvps popularized by eric rieslean startup. large scale can have multiple dimensions, including the number of users and the number of experiments. we are dealing with big data and must scale on both dimensions: each experiment typically exposes several million users to a treatment, and over experiments are running concurrently. while running online controlled experiments requires a sufficient number of users, teams working on products with thousands to tens of thousands of users are typically looking for larger effects, which are easier to detect than the small effects that large sites worry about. for example, to increase the experiment sensitivity by a factor of, say from delta to, you need. controlled experiments thus naturally scale from small startups to the largest of web sites. our focus in this paper is on scaling the number of experiments: how can organizations evaluate more hypotheses, increasing the velocity of validated learnings, per time unit. we share our experiences, how we addressed challenges, and key lessons from having run thousands of online controlled experiments at bing, part of microsoftonline services division. office and windows follow sinofskylong planning and execution cycles. bing has thousands of developers, program managers, and testers, using online controlled experiments heavily to prioritize ideas and decide which changes to ship to all users. bingexperimentation system is one of the largest in the world, and pushes the envelope on multiple axes, including culture, engineering, and trustworthiness. in the us alone, it distributes traffic from about million monthly users executing over queries a month to over experiments running concurrently. almost every user is in some experiment: of users eligible for experimentation are each rotated into over concurrent experiments, while are put into a holdout group to assess the overall impact of the experimentation system and to help with alerting. analysis of an experiment utilizing of eligible users over weeks processes about tb of data to generate a summary scorecard. with about experiments in each one of concurrent experimentation areas, users end up in one of. automated analyses, or scorecards, are generated on clusters consisting of tens of thousands of machines to help guide product releases, to shorten product development cycles, measure progress, and gain valuable customer feedback. alerts fire automatically when experiments hurt the user experience, or interact with other experiments. while the overall system has significant costs associated with it, its value acm year#. far outweighs those costs: ideas that were implemented by small teams, and were not even prioritized high by the team implementing them, have had surprisingly large effects on key metrics. for example, two small changes, which took days to develop, each increased ad revenue by about million annually. motivating example we begin with a motivating visual example of a controlled experiment that ran at bing. the team wanted to add a feature allowing advertisers to provide links to the target site. the rationale is that this will improve ads quality by giving users more information about what the advertisersite provides and allow users to directly navigate to the sub category matching their intent. visuals of the existing ads layout and the new ads layout with site links added are shown in figure # figure #: exponential growth in experimentation over time. in the hierarchy of possible designs, controlled experiments are the gold standard in science. if an organization wants to make data driven decisions to drive product development, with customers actual behavior as the source of data for decisions, one of the key goals is to enable experimentation at scale: support running many experiments and lower the cost of experimentation. this must be done without lowering the trustworthiness of the overall system. the difference might not be obvious at first but it is worth tens of millions of dollars in a controlled experiment, users are randomly split between the variants in a persistent manner. their interactions with the site are instrumented and key metrics computed. in this experiment, the overall evaluation criterion was simple: increasing average revenue per user without degrading key user engagement metrics. results showed that the newly added site links increased revenue, but also degraded user metrics and page load time, likely because of increased vertical space usage. even offsetting the space by lowering the average number of mainline ads shown per query, this feature improved revenue by tens of millions of dollars per year with neutral user impact, resulting in extremely high roi. while the example above is a visual change for monetization, we use controlled experiments for many areas at bing. visual changes range from small tweaks like changing colors, to improving search result captions, to bigger changes like adding video to the homepage, and to a complete makeover of bingsearch result page that rolled out in may year# and included a new social pane. we also test usability improvements, such as query auto suggest, did you mean, and search history. backend changes such as relevance rankers, ad optimization, and performance improvements are constantly being experimented with. finally, we also experiment with changes to sites generating traffic to bing, such as msn. the experimentation system the problem that the bing experimentation system addresses is how to guide product development and allow the organization to assess the roi of projects, leading to a healthy focus on key ideas that move metrics of interest. while there are many ways to design and evaluate products, our choice of controlled experiments for knowledge discovery derives from the desire to with the mission of accelerating software innovation through trustworthy experimentation, the use of experimentation at bing grew exponentially fast over time, as shown in figure #. the bing experimentation system is one of the largest systems in the world for running online controlled experiments, with over experiments running concurrently, exposing about million active monthly customers to billions of bing variants that include implementations of new ideas and variations of existing ones. related work and contributions multiple papers and books have been written on how to run an online controlled experiment and we will not address that here; we follow the terminology of controlled experiments on the web: survey and practical guide. we build upon that work and share how to scale experimentation, ie, how to run many experiments to accelerate innovation in product development. we are aware of only one paper that focused on this aspect of experiment scale, an excellent paper by diane tang et al about overlapping experiments at google. because that topic is well covered in that paper, and bingsystem is similar, we chose not to discuss it here. to the best of our knowledge, most of the lessons we share here are novel and not previously covered. our contributions are as follows: we share key tenets, or principles, which an organization should adopt before using online controlled experiments at scale. experimentation is not a panacea for everyone, and the assumptions should be understood. in a lean startup approach, businesses rely on validated learning, scientific experimentation, and iterative product releases to shorten product development cycles, measure progress, and gain valuable customer feedback. a standard approach to estimating online click based metrics of a ranking function is to run it in a controlled experiment on live users. while reliable and popular in practice, configuring and running an online experiment is cumbersome and time intensive. in this work, inspired by recent successes of offline evaluation techniques for recommender systems, we study an alternative that uses historical search log to reliably predict online click based metrics of a \emph ranking function, without actually running it on live users. to tackle novel challenges encountered in web search, variations of the basic techniques are proposed. the first is to take advantage of diversified behavior of a search engine over a long period of time to simulate randomized data collection, so that our approach can be used at very low cost. the second is to replace exact matching by \emph matching to increase data efficiency, via a better trade off of bias and variance. extensive experimental results based on large scale real search data from a major commercial search engine in the us market demonstrate our approach is promising and has potential for wide use in web search. first, to the best of our knowledge, our work is the rst to apply causal inference techniques to. very often, such user feedback strongly indicates quality of the system. in advertising and recommender systems, for instance, it is natural to prefer a system with high adoption conversion rate and possibly the revenue associated with it. in this paper, we consider the problem of estimating the average value of such pre de ned online signals across user visits. often the metric of interest depends on user feedback, and hence on system output as well therefore, one usually does not know what the user feedback would have been if the system output changes, unless a highly accurate user model is available. such an observation leads to a fundamental challenge when estimating a metric of a search engine: based on search log collected by running a version of the engine on users in the past, it is often di cult or even impossible to reliably estimate a metric of a modi ed engine. metrics that do not depend on user feedback are typically easy to estimate from historical data. driven decisions to improve web based services, they are expensive for a few reasons. ight: as the new ranker will serve live users, substantial care has to be taken to ensure the ranker does not lead to catastrophic results. ine evaluation is therefore of great, practical interest. it aims to predict online metrics of a search engine without actually running it on live users, thus addressing all three di culties above and substantially improving experimentation agility and quality. the key of these techniques is to rst collect randomized data and then properly correcting sampling bias when evaluating a new system. in the context of web search ranking, however, the problem is even more challenging due to the exponentially large number of possible serps for any given query. first, data collection is very expensive since all possible serps will need to be shown with nonzero probability, a condition often required by reliable. second,ine evaluation results variance roughly depends reciprocally on the number of possible seprs and can be too large in practice to be useful. while aexperiments are highly valuable for making data in this paper, we focus on metrics that depend on user feedback. many web based services are by nature systems that interact with users. a search engine, for example, starts with a query issued by a user, returns a ranked list of documents, and observes user feedback often in the form of clicks and document consumption. other examples are advertising and recommender systems that suggest one or multiple items to a user and in return receive feedback such as whether the user adopts purchases the item or not. in web search ranking, the problem we focus on in this work, useful signals can be extracted from user feedback to infer whether a search event is successful or not. commonly used examples include click through rate, time to click on the search result page, and mean reciprocal of click positions, etc. although human judgment labels are commonly used in practice to optimize such web based systems, they are inherently approximations to the quality of end user experience. this limitation suggests the need and importance to measure and optimize online metrics, even though label based optimization can be a very useful starting point. the standard approach to this problem is to run an aexperiment, also known as a randomized controlled experiment: incoming users are randomly split into two groups, where the control group is served by a baseline system and the treatment group by a variant. after running the experiment for a period of time, online metrics of the two systems are compared, and the one with higher metric values wins. first, the turn around time is long: it usually takes days or even weeks before comparisons of the control and treatment engines start to be statistically signi cant. quite often, multiple experiments compete for search tra, and it is challenging to run several experiments at the same time. ine evaluation has been studied and successfully applied to recommendation and advertising, using statistical tools from causal inference. ine evaluate metrics of a web search ranker. second, in order to address novel challenges encountered in web search, two variations of the basic. ine evaluation technique are proposed and studied, making the method realistic for this application domain. third, we demonstrate the proposed solution is promising through extensive experiments using large scale real search log from a commercial search engine. information retrieval effectiveness is usually evaluated using measures such as normalized discounted cumulative gain, mean average precision and precision at some cutoff on a set of judged queries. recent research has suggested an alternative, evaluating information retrieval systems based on user behavior. particularly promising are experiments that interleave two rankings and track user clicks. according to a recent study, interleaving experiments can identify large differences in retrieval effectiveness with much better reliability than other click based methods. we study interleaving in more detail, comparing it with traditional measures in terms of reliability, sensitivity and agreement. to detect very small differences in retrieval effectiveness, a reliable outcome with standard metrics requires about, judged queries, and this is about as reliable as interleaving with, user impressions. amongst the traditional measures, ndcg has the strongest correlation with interleaving. finally, we present some new forms of analysis, including an approach to enhance interleaving sensitivity. a tremendous amount of research has improved information retrieval systems over the last few decades. ective approaches mature and relative improvements become smaller, the sensitivity of evaluation metrics and their delity to actual user experience becomes increasingly critical. without sensitive measurement we might reject a small but signi cant improvement. this becomes a problem if we reject a large number of independent small improvements, because we have forgone an overall large improvement. without delity in measurement, a small change in a retrieval permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. nick craswell microsoft redmond, wa, usa nickcr microsoft com model might be taking into account some bias of relevance judges, rather than the preferences of real users. the predominant form of evaluation in information retrieval is based on test collections comprising query topics, a document corpus and human relevance judgments of topic document pairs. this allows the application of standard metrics such as ndcg, map and precision. fidelity depends on whether the test collection re ects realworld search behavior. for example, the trec web track found that changing from informational to navigational assumptions when judging can change the outcome of an evaluation. an alternate evaluation approach is based on user behavior, estimating user success by measuring click, re querying and general browsing patterns on search results. on delity, judges are usually far removed from the search process, so may generate unrealistic query topics from observed queries, and have a hard time assessing documents in a way that re ects a useractual information need. additionally, traditional measures combine document judgments to obtain a score per query, for example based on discount and gain, but these may not match real user experience. finally, judgments are slow and expensive to collect. for a system with real users, usage based evaluation is far cheaper, despite the fact that the click data collected may not be reusable in the way that most test collections are. this paper considers the reliability, sensitivity and agreement of these competing evaluation approaches. on the cran eld trec side, we consider relevance judgments for up to, queries. on the user metric side, we perform click based tests involving the interleaving of two retrieval functions over, user impressions, which we de ne as events where a user runs a query and clicks a result. using a large commercial dataset, we establish results that we believe would also hold true in an academic setting. we test sensitivity by measuring outcomes with varying numbers of queries impressions. this is done on pairs of retrieval functions with varying degrees of di erence, including one pair with a very small di erence in. we test agreement in overall outcomes between traditional measures and interleaving. we tend to nd agreement, which is an indication of the delity of the judgmentbased metric, since it is agreeing with an experiment involving real users. we then study various new ways of aggregating and analyzing the interleaving data, showing how to improve agreement with traditional metrics and also attain reliability with fewer impressions. we also show that, in contrast to judgment based metrics, interleaving can measure the fraction of users for whom a ranking change was meaningful. this allows assessment to move beyond an assumption that relevance for all users is identical, and that relevance of individual documents should be aggregated identically for all queries. sensitivity depends on the number of topics and judgments. this can be motivated on grounds of delity and cost. our results show that both approaches can be very sensitive, but judged evaluation may require thousands of judged queries to obtain the required sensitivity. interleaving is an online evaluation technique for comparing the relative quality of information retrieval functions by combining their result lists and tracking clicks. a sequence of such algorithms have been proposed, each being shown to address problems in earlier algorithms. in this paper, we formalize and generalize this process, while introducing a formal model: we identify a set of desirable properties for interleaving, then show that an interleaving algorithm can be obtained as the solution to an optimization problem within those constraints. our approach makes explicit the parameters of the algorithm, as well as assumptions about user behavior. further, we show that our approach leads to an unbiased and more efficient interleaving algorithm than any previous approach, using a novel log based analysis of user search behavior. in most studies retrieval evaluation is performed using manual relevance judgments that assess the relevance of particular documents to particular queries, or by observing user behavior in an actual retrieval system. in both cases, the goals are clear: sensitivity to small improvements in retrieval quality for a given cost of evaluation, and delity to the actual user experience were real users to directly compare particular retrieval systems. among other bene ts, this most easily allows for reproducibility and reusability: a retrieval algorithm can be run on a document collection for a particular query set for which judgments are known. performance can be measured using any number of metrics such as mean average precision, discounted cumulative gain, or permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. di erent researchers can apply the same retrieval algorithms to the same collection to reproduce results. further, the data can be used to evaluate future retrieval methods on the same document collection and query set. in contrast, online evaluation involves real users searching for actual and current information needs. based on the user interface, users select results, reformulate or revise their information need, and continue with other tasks. their behavior in selecting queries issued and documents clicked can be interpreted as feedback about the documents retrieved. reproducing an evaluation requires showing new users similar results. if the new users are substantially different, or have substantially di erent needs or behavior, the outcome may change. furthermore, a record of behavior given particular documents returned to users does not tell the researcher how the users would have behaved had other documents been shown, so observed behavior is not easily reusable when evaluating new retrieval methods. however, online evaluation bene ts from involving real users, particularly when conducted on real queries in situ. in that case there is no uncertainty in how a judge should interpret the query sdsu, nor how to trade. the relevance of new and old documents to the query wsdm. all aspects of the users context and state of knowledge are present, some of which may be di cult to realistically capture in a test collection. finally, as usage data can be collected essentially for free by any active information retrieval system, its cost can be much lower than that of obtaining su cient relevance judgments from experts to detect small relevance improvements. for these reasons, we focus on online evaluation. among online evaluation approaches in the context of web search, two methods dominate today: the rst involves measuring properties of user responses to retrieval, such as the the time it takes users to click, or the positions of these clicks and other observable behavior. this can be used to compute a score for a given retrieval algorithm that can be compared across systems. the second approach invovles showing users a combination of results retrieved by di erent ranked retrieval algorithms, and observing which results users select from this combination. a number of authors have demonstrated the higher sensitivty of the interleaved approach, largely due to the within user comparison that is taking place: the same user with the same information need at the same time is shown the best results proposed by both systems, and directly chooses between them. rankings produced by two retrieval functions for the query napa valley, are combined into an interleaved combination. in this paper, we address the question of how to obtain an optimal interleaving algorithm. further, additional variants of interleaving algorithms, for example involving how credit is assigned for clicks have been proposed. this leads to the question of what the best interleaving algorithm would be, and why. also, the two most recent interleaving algorithms addressed unexpected aws in previous algorithms. this suggests that analysis of interleavings algorithm is di cult, and that the properties encoded by each algorithm implicitly make assumptions that are di cult to verify. the designer of an algorithm risks creating new problems, even while xing existing ones. we thus invert the problem of deriving an interleaving algorithm: starting with properties that we wish the algorithm to have, we formulate interleaving as the solution to an optimization problem. this is the key contribution of our work. a second contribution is our evaluation method: following a similar approach to li et al, we show how di erent interleaving algorithms can be evaluated on historical log data, without running a new algorithm on new users. we then describe our approach, interleaving as the solution to an optimization problem. after formulating the optimization problem, we show theoretical guarantees and solve for two interleaving algorithms. we conclude with an evaluation comparing our approach with previous interleaving algorithms using a real usage data. in particular, a number of interleaving algorithms have been proposed, including balanced interleaving, team draft interleaving and probabalistic interleaving. the users can enter search terms, and documents are retrieved. in particular, we investigate two paired comparison tests that analyze clickthrough data from an interleaved presentation of ranking pairs, and we find that both give accurate and consistent results. automatically judging the quality of retrieval functions based on observable user behavior holds promise for making retrieval evaluation faster, cheaper, and more user centered. however, the relationship between observable user behavior and retrieval quality is not yet fully understood. we present a sequence of studies investigating this relationship for an operational search engine on the arxiv orgprint archive. we find that none of the eight absolute usage metrics we explore reliably reflect retrieval quality for the sample sizes we consider. however, we find that paired experiment designs adapted from sensory analysis produce accurate and reliable statements about the relative quality of two retrieval functions. we conclude that both paired comparison tests give substantially more accurate and sensitive evaluation results than absolute usage metrics in our domain. interleaving experiments are an attractive methodology for evaluating retrieval functions through implicit feedback. designed as a blind and unbiased test for eliciting a preference between two retrieval functions, an interleaved ranking of the results of two retrieval functions is presented to the users. it is then observed whether the users click more on results from one retrieval function or the other. while it was shown that such interleaving experiments reliably identify the better of the two retrieval functions, the naive approach of counting all clicks equally leads to a suboptimal test. we present new methods for learning how to score different types of clicks so that the resulting test statistic optimizes the statistical power of the experiment. our methods are evaluated on an operational search engine over a collection of scientific articles. this can lead to substantial savings in the amount of data required for reaching a target confidence level.