in prior work we studied a language construct tt restrict tt that allows programmers to specify that certain pointers are not aliased to other pointers used within a lexical scope. in this paper we continue the study of tt restrict tt and introduce the construct tt confine tt. we present a type and effect system for checking the correctness of these annotations, and we develop efficient constraint based algorithms implementing these type checking systems. to make it easier to use tt restrict tt and tt confine tt in practice, we show how to automatically infer such annotations without programmer assistance. among other applications, programming with these constructs helps program analysis tools locally recover strong updates, which can improve the tracking of state in flow sensitive analyses. in experiments on locking in linux device drivers, tt confine tt inference can automatically recover strong updates to eliminate of the type errors resulting from weak updates. almost all program analyses for languages with pointers must perform some form of alias analysis: when a program indirectly loads or stores through a pointer, the analysis must determine to which location points. the research literature abounds with proposed alias analysis techniques, some of which scale to very large programs. that is, such an analysis takes a bare program and infers all possible aliasing. this paper is about aliasing in programs, but the purpose is different from previous work on automatic alias analysis. and developed further here, is the topic of this paper. one application of cqual is to verify properties of locking. if all goes well in the example in figure #, cqual infers that has type unlocked lock at point, the type locked lockat point, and the type unlocked lock at point in this way, cqual checks code for deadlocks caused by reacquiring a lock that is already held or releasing a lock that has not been acquired. cqual models state by mapping every program variable to an abstract location if two program quantities may alias each other, they are mapped to the same abstract location. in the example, because our alias analysis cannot distinguish different elements of an array, all elements of the array reside at the same abstract location. similarly, lpoints to location, meaning that lis stored at location, and thus both land all array elements may alias. however, it is nots state that is changed, but the state ofs abstract location to accomplish this, cqual also needs to know how the functions spin lock and spin unlock change the state of locks. stands for other locks, too namely the other locks in the array, which are still in the unlockedstate. thus after the call to spin lock the static information about. degrades to knowing only that any locks it represents may be either in the lockedor the unlockedstate, and the veri cation of any locking properties on any of these locks becomes impossible. the dif culty is that the single abstract location. stands for multiple concrete locks, and the call to spin lock only changes the state of a single lock. what we need for accurate analysis, though, is a strong update: we want to change the state offrom unlocked to locked and not affect the status of any other lock. the need to perform strong updates is not speci. this problem arises in any static analysis where there are both collections such as arrays or lists and we want to track state changes of values. if we knew that do with lock could only accessthrough its formal parameter, and not through some alias it holds through, eg, a global variable, then locally within do with lockwe could ignore the aliases ofexternal to do with lock and perform strong updates ons location. the recent standard for theprogramming language provides a way to say almost exactly this. change the de nition of do with lockto void do with lock at a high level, the restrict keyword means that no alias ofde ned outside of do with lock is used during the functionexecution. although there may be many aliases ofin the program, locally we knowis the only way for do with lock to access. this notion of locally unaliased pointers is missing from conventional ow insensitive may alias analysis, where pointers are either aliased or not and the only scope of interest is the entire program. notice that while context sensitive or parameterized alias analysis may help our do with lockexample, we can also use restrict to indicate local non aliasing within nested scopes smaller than function scopes. indeed, we make use of this feature in our experiments. another key feature of restrict is that it provides a form of program documentation: it allows the programmer to specify a particular kind of non aliasing. combined with a checking system such as we propose, we believe that restrict is not only bene cial for tools like cqual, but also for the programmer when writing their program. in, restrict is trusted and unchecked by the compiler it amounts to a license for compilers to perform aggressive optimizations that would be unsound in the presence of aliases. we believe restrictis even more useful in program checking tools, and not just forprograms, but for programs written in any language with references. while there are important exceptions, such as functions that copy data or pointers, we believe many pointers in practice can be marked restricted. the thesis of this paper is that restricted references are common in real programs, and that exploiting this structure is important to software engineering tools such as cqual that need to reason about references. more speci cally, the contributions of this paper are: we develop a formal semantics of restrict and also present an informal description and examples. we give a type and effect system for checking that a restrict annotated program is correct with respect to our semantics. we give a constraint based algorithm for verifying restrict annotations, whereis the size of the typed program andis the number of restrict annotations in the program. the type system for restrict is described brie. in prior work, but this is the rst description of the type checking algorithm. in using cqual we have found it necessary to add many more restrict annotations to programs than we would like to do by hand. furthermore, in many applications we wish to restrict not just a variable, but an expression. this extension of restrict introduces two new problems. first, to treat an arbitrary expression as a name, it must be referentially transparent, which introduces additional constraints beyond what is required for restrict. we call this stronger condition con ning an expression and likewise name the associated construct confine. second, for confine inference we have the additional problem of inferring in what scope an expression can be con ned. in this experiment, confine is very effective at identifying the program points where strong updates can aid the analysis. alias analysis is a key ingredient in many program checking systems and compiler optimizations. almost all of these techniques are fully automatic. after the call to spin lock is the union of the old state and the new state. we present the results of experiments with confineinference, in which we use cqual to analyze the locking behavior of linux device drivers. our motivation comes from experience developing and using cqual, a system for extendingwith user de ned type quali ers. consider the partial program shown in figure #. we use this example to tell a story, the moral of which is that we needed a new form of alias analysis to make cqual and similar analyses work in practice; that new form of alias analysis, previously sketched brie. cqual usestwo non standard, ow sensitive type quali ers lockedand unlockedto re ne the type lock. ow sensitive analysis, which means that cqual must be able to assign ldifferent types at different points in the program. assume that all locks in the array begin in the state unlocked lock. the call to spin lock changes the state ofto a locked lock. void foo void do with lock figure #: example program. in ow sensitive analysis, this is known as a weak update. this motivates the idea of restrict inference: not just checking user supplied restrictannotations, but automatically inferring restricts in a program with no restrict annotations. one of the primary challenges in building and evolving large object oriented systems is understanding aliasing between objects. unexpected aliasing can lead to broken invariants, mistaken assumptions, security holes, and surprising side effects, all of which may lead to software defects and complicate software evolution this paper presents aliasjava, a capability based alias annotation system for java that makes alias patterns explicit in the source code, enabling developers to reason more effectively about the interactions in a complex system. we describe our implementation, prove the soundness of the annotation system, and give an algorithm for automatically inferring alias annotations. our experience suggests that the annotation system is practical, that annotation inference is efficient and yields appropriate annotations, and that the annotations can express important invariants of data structures and of software architectures. understanding and evolving large software systems is one of the most pressing challenges confronting software engineers today. when evolving a complex system in the face of changing requirements, developers need to understand how the system is organized in order to work effectively. for example, to avoid introducing program defects, programmers need to be able to predict the effect of making a software change. also, while fixing permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. defects, programmers need to be able to track value flow within a program in order to understand how an erroneous value was produced. in an object oriented program, all of these tasks require understanding the data sharing relationships within the program. these relationships may be very complex at worst, a reference could point to any object of compatible type and current languages do not provide much help in understanding them. data sharing problems can also compromise the security of a system. for example, in version of the java standard library, the security system function class getsigners returned a pointer to an internal array, rather than a copy. clients could then modify the array, compromising the security of the sandbox that isolates java applets and potentially allowing malicious applets to pose as trusted code. existing languages provide poor support for preventing security problems that arise from improper data sharing. in this paper, we describe and evaluate aliasjava, a type annotation system for specifying data sharing relationships in java programs. the annotations provide automatically checked documentation about data sharing within a program, while allowing software engineers to program in much the same style as before. we have also applied aliasjava to specify the data sharing relationships within a software architecture, as expressed in the architecture description language archjava. aliasjavaannotations capture several common forms of sharing in object oriented systems. first, objects are often shared in a structurally bounded way: an object might be shared within the implementation of a subsystem, but not beyond it. in aliasjava, objects that are part of a subsystemrepresentation are specified with an owned type annotation; the subsystem can grant trusted external objects the capability to access its owned state using a simple form of ownership parameterization. second, objects are sometimes shared in a time bounded way: an object may be passed as a parameter to a method, which uses the object for the duration of the call, but does not store a persistent reference to the object. aliasjava specifies this kind of time bounded access capability with a lent type annotation. finally, our type system also includes the best case unique annotation for unshared objects and the worst case shared annotation for objects that have no owning subsystem. the contributions of this paper are the following: a capability based type annotation system that combines uniqueness and ownership style encapsulation; an implementation in java and a discussion of issues including concurrency, inner classes, iterators, and casts; class linkedlist public unique object getitem public unique linkedlist getnext unique linkedlist list new linkedlist, null; list new linkedlist, list; unique objectlist getitem; list list getnext; figure #. a linked list class with unique links and items a formalization of our type annotation system for a subset of java and a proof outline of several key invariants; a novel algorithm for inferring alias annotations; and an empirical evaluation of aliasjava on a non trivial program and on part of the java collection class library. the rest of this paper is organized as follows. the next section introduces aliasjava with a series of examples. section # formalizes our type system and outlines proofs of key properties. we evaluate our system in section # on a realistic program and on the java collection libraries. section # discusses related work, and we conclude in section #. this paper reports on a new approach to solving a subset based points to analysis for java using binary decision diagrams. in the model checking community, bdds have been shown very effective for representing large sets and solving very large verification problems. our work shows that bdds can also be very effective for developing a points to analysis that is simple to implement and that scales well, in both space and time, to large programs the paper first introduces bdds and operations on bdds using some simple points to examples. then, a complete subset based points to algorithm is presented, expressed completely using bdds and bdd operations. this algorithm is then refined by finding appropriate variable orderings and by making the algorithm propagate sets incrementally, in order to arrive at a very efficient algorithm. experimental results are given to justify the choice of variable ordering, to demonstrate the improvement due to incrementalization, and to compare the performance of the bdd based solver to an efficient hand coded graph based solver. finally, based on the results of the bdd based solver, a variety of bdd based queries are presented, including the points to query. in this paper, we take a well known problem from the compiler optimization community, points to analysis, and we show how to this work was supported, in part, by nserc and a tomlinson graduate fellowship. special thanks to jrn lind nielsen for his publicly available buddy package. we would also like to thank the pldi reviewers who provided several insightful comments. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to solve this problem ef ciently using reduced ordered binary decision diagrams which have been shown to be very effective in the model checking community. whole program analyses, such as points to analysis, require approaches that can scale well to large programs. two popular approaches to ow insensitive points to analysis have been pursued in the past, equality based approaches like those pioneered by steensgaard, and subset based approaches like the analysis rst suggested by andersen. the subset based approaches give more accurate results, but they also lead to greater challenges for ef cient implementations. for this paper, we have chosen to implement a subset based pointsto analysis for java. at a very high level, one can see this problem as nding the allocation sites that reach a variable in the program. consider an allocation statement: a new a; if a variableis used in some other part of the program, then one would like to know whethercan refer to an object allocated at. a key problem in developing ef cient solvers for the subset based points to analysis is that for large programs there are many pointsto sets, and each points to set can become very large. often, many of these points to sets are equal or almost equal. several methods of representing them compactly have been studied in the past, including collapsing equivalent variables and designing new representations for sets; the bdd based approach that we introduce in this paper is another example of such a compact representation. since bdds have been shown to be effective for compactly representing large sets and for solving large state space problems like those generated in model checking, it seemed like an interesting question to see if bdds could also be used to ef ciently solve the points to problem for java. in particular, we wanted to examine three aspects of the bdd solution: execution time for the solver, memory usage, and ease of specifying the points to algorithm using a standard bdd package. in summary, our experience was that bdds were very effective in all three aspects. the contributions of this paper include: we propose and develop an ef cient bdd based algorithm for subset based points to analysis for java. to our knowledge, we are the rst group to successfully use bdds to solve such an analysis ef ciently. we provide new insights into how to make the bdd based implementation ef cient in terms of space and time. first, we used a systematic approach to nd a good variable ordering for the points to set. second, we noted that the al republish, to post on servers or to redistribute to lists, requires prior speci. in the remainder of this paper we simply refer to bdds, meaning permission and or a fee. a more detailed description of related work is found in section #. gorithm should propagate the sets incrementally, and presented an incremental version. this general idea of incrementalizing the algorithm may be useful in solving other program analysis problems using bdds. third, we found that specifying an analysis using high level bdd operations allowed us to specify our analysis very compactly and it was very simple to experiment with a wide variety of algorithms. our source code contains many different variations that can be enabled by switches. we experimentally validated the bdd based approach by comparing its performance to a previously existing ef cient solver. for small problem sizes, we found that the time and space requirements are similar, but for larger problems, the bdd based approach requires less memory, and scales better. although we initially intended to compute only points to sets, we found that the bdd approach leads to a solution that can be used to answer a variety of queries, of which the points to query is only one. in future work we plan to develop this aspect of our work further. the rest of this paper is organized as follows. in section # we provide an introduction to bdds and operations on bdds using small examples based on the points to problem. given this introductory material, we then introduce our points to algorithm and its implementation using bdds in section #. then, in section #, we show how to improve the performance of the algorithm by choosing the correct variable ordering and making the algorithm incremental. in section # we give experimental results for our best bdd algorithm and compare its performance to a hand coded and optimized solver based on spark. in section # we discuss possible applications for the results of our algorithm, which includes answering points to queries. finally, section # gives a discussion of related work and section # gives conclusions and future work. we present practical approximation methods for computing interprocedural aliases and side effects for a program written in a language that includes pointers, reference parameters and recursion. we present the following results: an algorithm for flow sensitive interprocedural alias analysis which is more precise and efficient than the best interprocedural method known. an extension of traditional flow insensitive alias analysis which accommodates pointers and provides a framework for a family of algorithms which trade off precision for efficiency. an algorithm which correctly computes side effects in the presence of pointers. pointers cannot be correctly handled by conventional methods for side effect analysis. an alias naming technique which handles dynamically allocated objects and guarantees the correctness of data flow analysis. a compact representation based on transitive reduction which does not result in a loss of precision and improves precision in some case. a method for intraprocedural alias analysis which is based on a sparse representation. object oriented programming languages allow inter object aliasing. although necessary to construct linked data structures and networks of interacting objects, aliasing is problematic in that an aggregate object state can change via an alias to one of its components, without the aggregate being aware of any aliasing. ownership types form a static type system that indicates object ownership. this provides a flexible mechanism to limit the visibility of object references and restrict access paths to objects, thus controlling a system dynamic topology. the type system is shown to be sound, and the specific aliasing properties that a system object graph satisfies are formulated and proven invariant for well typed programs. this paper describes a new algorithm for flow and context insensitive pointer analysis ofprograms. our studies show that the most common use of pointers inprograms is in passing the addresses of composite objects or updateable values as arguments to procedures. therefore, we have designed a low cost algorithm that handles this common case accurately. in terms of both precision and running time, this algorithm lies between steensgaard algorithm, which treats assignments bi directionally using unification, and andersen algorithm, which treats assignments directionally using subtyping. our one level flow algorithm uses a restricted form of subtyping to avoid unification of symbols at the top levels of pointer chains in the points to graph, while using unification elsewhere in the graph. for instance, we are able to analyze a mloc program in two minutes, using less than mb of memory. at the same time, the precision of our algorithm is very close to that of andersen algorithm. on all of the integer benchmark programs from spec, the one level flow algorithm and andersen algorithm produce either identical or essentially identical points to information. therefore, we claim that our algorithm provides a method for obtaining precise flow insensitive points to information for largeprograms. this paper reports on the design, implementation, and empirical results of a new method for dealing with the aliasing problem in. the method is based on approximating the points to relationships between accessible stack locations, and can be used to generate alias pairs, or used directly for other analyses and transformations. our method provides context sensitive interprocedural information based on analysis over invocation graphs that capture all calling contexts including recursive and mutually recursive calling contexts. furthermore, the method allows the smooth integration for handling general function pointers in. we illustrate the effectiveness of the method with empirical results from an implementation in the mccat optimizing parallelizingcompiler. linearity provides powerful reasoning about state changes, but at the price of restrictions on aliasing. the hard division between linear and nonlinear types forces the programmer to make a trade off between checking a protocol on an object and aliasing the object. because of this, checking a protocol on an object imposes aliasing restrictions on any data structure that directly or indirectly points to the object. we propose a new type system that reduces these restrictions with the adoption and focus constructs. adoption safely allows a programmer to alias objects on which she is checking protocols, and focus allows the reverse. we discuss how we implemented these ideas in the vault programming language. a type system with linearity is useful for checking software protocols andresource management at compile time. most onerous is the restriction that any type with a linear component must itself be linear. a programmer can alias data structures that point to linear objects and use focus for safe access to those objects. rules governing proper interaction with that interface must be gleaned from the componentdocumentation or, as is often the case, learned from local folklore. as a familiar example,le systeminterface protocol typically has the following rules:le must be opened before it is read or written;le may be read or written until it is closed; and every le that is opened must eventually be closed. in the context of the vault programming language, we studied a type system that tracks the lifetime and symbolic state of objects. to enforceour lesystemprotocol in vault, wegivethe le type the states open and closed and specify that the read and write functions expect an open le and that the close function changes. for instance, the code sequence close; read obeys our interface protocol if a andrefer to di erent les, but is incorrect if these variables alias the same le. to solve this problem, vaulttype system splits the programvalues into two groups: those on which we can check protocols, but to which aliasing restrictions apply; and those on which we cannot check protocols, but which are free of aliasing restrictions. although this distinction is necessary for tractability, the trade. the division between linear and nonlinear types presents one further annoyance. for instance, if a andwere nonlinear records whose. elds refer to linear les, then the code sequence close; read could be unsafe, since a andcould be aliases due to their nonlinear types. because of these restrictions, checking protocols on more than a few types of objects in a large program is impractical. the aliasing restrictions quickly spread to all of the programdata structures, including those, like graphs and caches, which inherently involve aliasing. we introduce the adoption construct, which safely allows aliasing of objects on which we check protocols and those that refer to them. we introduce the focus operator, which provides a temporary scope in which we can check a protocol on an aliased object. the rst focus temporarily gives log, an alias for msgs, a linear type so that its eldmay be updated. for safety, in the scope of this focus, access to any potential alias of msgs is illegal. the potential aliases are known due to the presence of guards on nonlinear types. the same guardmeans errs and msgs could be aliases. through the use of guards, focus prevents access to potential aliases. successful use of a software component often requires more than just familiarity with the types of the functions and data in the componentinterface. these rules, which we call the interface protocol, govern the order in which the interfacefunctions may be called and its data accessed. checking the states associated with objects requires the ability to tell di erent objects apart. between protocol checking and aliasing is an annoyance to programmers. a linear type system typically restricts how a programmer must design her data structures by forbidding a nonlinear type from having linear components. to prevent such safety violations, a linear type system restricts aliasing not only to those objects on which we check protocols, but also to any object which directly or indirectly refers to them. to make protocol checking more practical, this paper presents a new type system that removes these restrictions. our contributions are: we allow linear components in nonlinear containers, but control access to ensure safety. together, these features allow us to check vault code like the following: struct fileptr; void reset logs previous linear type systems would reject both the declaration of the nonlinear type fileptr due to its linear. eldand the code in reset logs due to the potential aliasing between msgs and errs our type systemacceptance of this code demonstrates the focus operation. we automatically infer one focus operation around the rst three statements and a second focus operation around the last three statements. the adoption operation allows a fileptr object to be allocated at a linear type, then given a nonlinear type to allow it to be aliased. this paper reports on the design and implementation of a practical shape analysis for. the purpose of the analysis is to aid in the disambiguation of heap allocated data structures by estimating the shape of the data structure accessible from each heap directed pointer. this shape information can be used to improve dependence testing and in parallelization, and to guide the choice of more complex heap analyses the method has been implemented as a context sensitive interprocedural analysis in the mccat conlpiler. experimental results and observations are given for benchmark programs. these results show that the analysis gives accurate and useful results for an important group of applications. during the past two decades many different pointer analysis algorithms have been published. although some descriptions include measurements of the effectiveness of the algorithm, qualitative comparisons among algorithms are difficult because of varying infrastructure, benchmarks, and performance metrics. without such comparisons it is not only difficult for an implementor to determine which pointer analysis is appropriate for their application, but also for a researcher to know which algorithms should be used as a basis for future advances. this paper describes an empirical comparison of the effectiveness of five pointer analysis algorithms onprograms. the algorithms vary in their use of control flow information and alias data structure, resulting in worst case complexity from linear to polynomial. the effectiveness of the analyses is quantified in terms of compile time precision and efficiency. in addition to measuring the direct effects of pointer analysis, precision is also reported by determining how the information computed by the five pointer analyses affects typical client analyses of pointer information: mod ref analysis, live variable analysis and dead assignment identification, reaching definitions analysis, dependence analysis, and conditional constant propagation and unreachable code identification. efficiency is reported by measuring analysis time and memory consumption of the pointer analyses and their clients. in this model data objects are directed graphs. in the authors introduced the concept of binding time optimization and presented a series of data flow analytic methods for determining some of the binding time characteristics of programs. if this set is small or regular in structure, this information can be used to optimize the program execution, mainly by use of more efficient storage allocation schemes in the first part we show how to construct from a program without selective updating a tree grammar whose nonterminals generate the desired sets of graphs; in this case they will all be trees. the tree grammars are of a more general form than is usually studied, so we show that they may be converted to the usual form. the resulting tree grammar could naturally be viewed as a recursive type definition of the values the variables may assume. further, standard algorithms may be employed to test for infiniteness, emptiness or linearity of the tree structure in the second part selective updating is allowed, so an alternate semantics is introduced which more closely resembles traditional lisp implementations, and which is equivalent to the tree model for programs without selective updating. we devise a finite approximation method which provides enough information to detect cell sharing and cyclic structures whenever they can possibly occur. this information can be used to recognize when the use of garbage collection or of reference counts may be avoided the work reported in the second part of this paper extends that of schwartz and cousot and cousot. they have developed methods for determining whether the values of two or more variables share cells, while we provide information on the detailed structure of what is shared. the ability to detect cycles is also new. it also extends the work of kaplan, who distinguishes only binary relations among the variables of a program, does not handle cycles, and does not distinguish selectors. today module systems do not effectively support information hiding in the presence of shared mutable objects, causing serious problems in the development and evolution of large software systems. ownership types have been proposed as a solution to this problem, but current systems have ad hoc access restrictions and are limited to java like languages in this paper, we describe systemown, an extension of systemwith references and ownership. our design shows both how ownership fits into standard type theory and the encapsulation benefits it can provide in languages with first class functions, abstract data types, and parametric polymorphism. by looking at ownership in the setting of systemf, we were able to develop a design that is more principled and flexible than previous ownership type systems, while also providing stronger encapsulation guarantees. during execution, when two or more names exist for the same location at some program point, we call them aliases. in a language which allows arbitrary pointers, the problem of determining aliases at a program point is space hard. we present an algorithm for the conditional may alias problem, which can be used to safely approximate interprocedural may alias in the presence of pointers. this algorithm is as precise as possible in the worst case and has been implemented in a prototype analysis tool forprograms. of particular interest is the approach to assigning attributes from the high level relations to physical domains in the underlying bdds, which is done by expressing the constraints as a sat problem and using a modern sat solver to compute the solution. further, a runtime system is defined that handles memory management issues and supports a browsable profiling tool for tuning the key bdd operations the motivation for designing jedd was to support the development of whole program analyses based on bdds, and we have used jedd to express five key interrelated whole program analyses in our soot compiler framework. in this paper we present jedd, a language extension to java that supports a convenient way of programming with binary decision diagrams. the jedd language abstracts bdds as database style relations and operations on relations, and provides static type rules to ensure that relational operations are used correctly the paper provides a description of the jedd language and reports on the design and implementation of the jedd translator and associated runtime system. we provide some examples of this application and discuss our experiences using jedd. ef cient solvers for whole program analyses like points to analysis. based on our very positive experience with using bdds for program analysis, we embarked on a project to express a number of key, interrelated whole program analyses for java using bdds inside our java compiler framework, soot. the implicit nature of the bdd representation made these errors dif cult to track down. finally, we found that tuning a bdd based algorithm requires pro ling information about the size and shape of the underlying bdds at each program step. in developing program analyses using bdds, we have found that this is a more appropriate level of abstraction. static and dynamic type checking: when using a bdd library directly, there is very little type information to help the programmer determine if bdd operations are used in a consistent and correct fashion. the problem of assigning physical domains turns out to be np complete. these require the programmer to explicitly manipulate the reference counts, which is error prone and does not. we also found that the physical domain assignment algorithm worked well, ran in acceptable times, and provided good mappings of attributes to physical domains. the jeddccompiler is composed of a front end and a back end. the physical domain assignment module calls an external sat solver tool. the remainder of this paper is structured as follows. finally, in section #, we discuss related work, and in section #, we conclude and suggest future work. binary decision diagrams are widely used for ef ciently solving problems in model checking, and recently, we demonstrated that bdds are very useful for de ning compact and this work was supported, in part, by nserc. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. as bdds have been in use for some time, there exist several excellent libraries providing ef cient representations, algorithms and memory management techniques for bdds, including two cbased libraries we have been using, buddy and cudd. we still wanted to use existing ef cientbased libraries, but now we required a clean and ef cient interface between the java code of our compiler and our bdd based algorithms. in developing our approach, it soon became apparent that a simple strategy of providing a java wrapper to interface with a bdd library was not a good solution, for many reasons. first, we found that the interface provided by the existing bdd libraries is very low level, and as we attempted to express several complex interrelated analyses, understanding and maintaining our code became dif cult. moreover, programming at such a low level was error prone, and errors in our code led to either the bdd library aborting, or worse, to incorrect results. furthermore, we found that it is quite dif cult to match the memory management in java with the reference counter based schemes employed in the bdd packages. we had previously developed some ad hoc methods for visualizing this information, but a more automated approach was needed. our solution, and the topic of this paper, was the development of: jedd, a language extension to java, which provides a high level way of programming bdd based algorithms based on relations and operations on relations; an associated translator which automatically translates jedd to java code that ef ciently interacts with back end solvers; and run time support for memory management, debugging and pro ling of bdd operations. the key aspects of our approach, and the main contributions of this paper, are: bdds abstracted as relations: rather than expose bdds and their low level operations directly, our jedd language provides a more abstract data type based on database style relations, and operations on those relations. in the jedd approach, all operations on relations have static type rules which help to eliminate many programmer errors. properties that cannot be checked statically are enforced by runtime checks. code generation strategy: we provide a strategy to convert the high level relational operations into low level bdd operations, and a mechanism for interfacing to several different bdd back ends. algorithm for physical domain assignment: an important issue in programming with bdds is how to assign physical domains of bdd variables to the problem being solved. when programming directly with bdds, the programmer must explicitly make all of the assignments and ensure that bdd operations are applied to the correct physical domains, which is a tedious process. furthermore, a small change in physical domain mappings may require many changes in the program. when specifying a program using the jedd language, the user speci es only the important assignments, and the translator completes a consistent mapping for the remainder of the program. we provide an algorithm to express it as an instance of the sat problem, and we show that, using modern sat solvers, the time to nd a solution is very acceptable. in cases where no solution exists, we provide information back to the programmer to help them modify the program to make the problem solvable. run time support for memory management: bdd solvers make use of reference counter memory management techniques to ef ciently reclaim the bdd data structures. jedd frees the programmer from this task by automatically managing all reference counts, and freeing bdds as soon as it is safe to do so. bdd pro ler: in our previous and current work with bdds, we found that tuning the bdd based algorithms required pro ling the size and shape of the bdd data structures at each program point. our jedd system allows the user to automatically generate pro ling information that can be browsed using any html browser, and which provides both counts of the number of operations applied, and graphical gures showing the size and shape of the underlying bdd data structures at each program point. proof of concept applications: in order to verify that our approach works, we have implemented several interrelated whole program analyses using the jedd system. we found that the algorithms were quite easy to specify, compact, and that the resulting bdd solvers were ef cient. a high level overview of the complete jedd system is given in figure #. jedd programs are written in our extension to java, and are provided as input to the jeddccompiler. the output of jeddc is in the form of standard java les which can be incorporated into any java project. the java les produced by jeddc, along with other ordinary java source making up a project, are compiled to class les using a standard java compiler such as javac. unless the code written in jedd is modi ed, jeddc is not needed when recompiling the java part of the project. the resulting class les contain calls to the jedd runtime library, which interfaces using jni to a bdd package. a jvm is used to execute the classes along with the jedd runtime. the runtime also includes a pro ler, which writes pro le information into a sql database. when combined with cgi scripts accessing the jeddc solution profiler views figure #: overview of jedd system database, an html browser can be used to navigate pro ler views of bdd operations. in section #, we give an introduction to the jedd language, along with some illustrative examples from our application of jedd to program analysis. in section #, we explain the key aspects of the jeddc compiler, with a particular emphasis on how we handle code generation and physical domain assignment. in section #, we describe the important elements of the design of our runtime system and pro ler, and in section #, we brie. report on our experiences with using jedd to implement ve interrelated whole program analyses in the soot compiler framework. program analyses and optimizations of java programs require reference information that determines the instances that may be accessed through dereferences. reference information can be computed using reference analysis. this paper presents a set of studies that evaluate the precision of two existing approaches for identifying instances and one approach for computing reference information in a reference analysis. the studies use dynamic reference information collected during run time as a lower bound approximation to the precise reference information. the studies measure the precision of an existing approach by comparing the information computed using the approach with the lower bound approximation. the paper also presents case studies that attempt to identify the cases under which an existing approach is not effective. the presented studies provide information that may guide the usage of existing reference analysis techniques and the development of new reference analysis techniques. program analyses and optimization of java programs require reference information that speci es the instances of classes that may be accessed through dereferences. reference variables in java programs and pointer variables inprograms have many similarities. thus, researchers have suggested the use of points to analysis algorithms, originally developed for use in analyzing in the remainder of the paper, we refer to instances of classes as instances. permission to make digital or hard copies of part or all of this work or personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and or a fee. pointers inprograms, for the computation of reference information for java programs. recent work has presented several approaches for extending ow insensitive and context insensitive points to analysis algorithms for analyzing java programs. this work has also evaluated the performance of various extension approaches. in previous work, we showed that, by taking advantage of the simplicity of the common ways in which java programs are written, the. ow and context insensitive pointsto analysis algorithm can be improved signi cantly for java programs. despite these results, many issues regarding static reference analysis remain unresolved. one important issue is determining the precision of the existing algorithms. this information will help determine whether there is a need to develop more precise static reference analyses. another important issue is identifying the cases in which existing algorithms are imprecise. this information can be used to develop more. ective techniques for handling such cases during static reference analysis, to achieve a better trade. to address these issues, we must measure the precision of static reference analysis algorithms. measuring the absolute precision of such an algorithm is di cult, however, because the problem of computing precise reference information is undecidable. mock et al used dynamic points to information, collected by executingprograms, to evaluate the precision of the static points to information computed by several existing points to analysis algorithms. they reported that, for the programs they studied, static points to information may contain a signi cant amount of spurious information. although these studies give insight into the precision of static points to information for several existing algorithms, they are limited to evaluation of analyses onprograms. to measure the precision of reference analysis algorithms for java, we use a similar approach. we use dynamic reference information, collected during run time, as a lower bound approximation to the precise reference information. we then measure the precision of a reference analysis algorithm by comparing the information computed by the algorithm with this lower bound approximation. if the comparison always yields little di erence, we can conclude that the information computed by the algorithm has high precision. if, in some cases, the comparison yields a signi cant di erence, we can study these cases and develop, if possible, more effective techniques for handling them. this paper presents a set of empirical studies, using this approach, that evaluate the precision of static reference analysis algorithms using dynamic reference information. the studies evaluate two aspects of static reference analysis: the approach for identifying instances and the mechanism for computing the reference information. a static reference analysis can be imprecise because it must use. xed number of names to identify an unbounded number of instances created during the execution of a program. a static reference analysis can also be imprecise because it cannot precisely determine whether a program path is executable. ciency, to compute reference information, an algorithm may use a small number of names to identify the instances, and use. ow insensitive approach that assumes any sequence of the program statements is executable. such an algorithm may compute very imprecise information. our studies evaluate the precision of some existing approaches for identifying instances and computing reference information. guided by the results of the studies, we performed two case studies that attempt to identify the cases under which an approach is not effective. identifying and investigating these cases may guide the development of new reference analyses. one major contribution of this paper is that it presents the results of the rst set of empirical studies that evaluate various approaches for identifying instances in java programs. one of these approaches identifying heapallocated memory using the allocation site has been widely used in points to analysis forwithout any empirical evaluation. our studies show that these approaches may be su ciently precise for instances allocated at most allocation sites in a program. our studies also show that using call strings to distinguish instances allocated at the same allocation site may improve the precision of static reference analysis. another major contribution of this paper is that it presents the results of the rst set of empirical studies that compare reference information computed by andersenalgorithm,ow insensitive and contextinsensitive algorithm, with the reference information recorded during the execution of the program. our studies show that andersenalgorithm can compute very precise information for many allocation sites in some subjects, but can compute very imprecise information for many allocation sites in other subjects. new static reference analysis techniques need to be developed to compute more precise information. a third major contribution of this paper is that it. presents the results of two case studies in which we investigate the cause of the imprecision in identifying instances and in computing points to information using some existing approaches. we found that these approaches may compute very imprecise information for instances that are used to construct sophisticated data structures, such as graphs. therefore, to compute more precise information, new static analysis techniques must provide more. this paper proposes a pointer alias analysis for automatic error detection. state of the art pointer alias analyses are either too slow or too imprecise for finding errors in real life programs. we propose a hybrid pointer analysis that tracks actively manipulated pointers held in local variables and parameters accurately with path and context sensitivity and handles pointers stored in recursive data structures less precisely but efficiently. we make the unsound assumption that pointers passed into a procedure, in parameters, global variables, and locations reached by applying simple access paths to parameters and global variables, are all distinct from each other and from any other locations. this assumption matches the semantics of many functions, reduces spurious aliases and speeds up the analysis we present a program representation, called ipssa, which captures intraprocedural and interprocedural definition use relationships of directly and indirectly accessed memory locations. this representation makes it easy to create demand driven path sensitive and context sensitive analyses we demonstrate how a program checker based on ipssa can be used to find security violations. our checker, when applied to programs, found new violations and previously reported ones. the checker generated only one false warning, suggesting that our approach is effective in creating practical and easy to use bug detection tools. a number of practical auditing and error detection tools have been shown to be. ective in nding errors in existing software systems. among the errors detected are bu er overruns and format string vulnerabilities, which account for a large number of reported security attacks. a common requirement for such tools is the ability to follow the ow of data. ciently: from a de nition to all its possible uses or from a use to all its possible de nitions. what makes this problem di cult for languages likeis the presence of pointers; without performing some sort of pointer analysis it is impossible to say what locations an indirect load or store will access. the current state of the art pointer alias analyses are either too imprecise or too slow to use for bug detection. pointer alias analysis used in program optimizations is necessarily sound: two pointers are considered to be possibly aliased if it cannot be proven otherwise. while owinsensitive and context insensitive analyses are fast, they generate many spurious aliases. program checkers built upon these techniques would raise too many false alarms and would thus be unusable. on the other hand, ow and context sensitive algorithms have been proposed, but are too slow to be used on real life programs. practical auditing tools often use unsound pointer alias analyses instead. not only are the analyses ow and context sensitive, they are even path sensitive. while they track memory locations held by local variables and parameters with precision, they often assume unsoundly that all other indirect memory references are unaliased. because these tools report errors based on information that is most likely to be true, they generate fewer false warnings. unfortunately it is sometimes hard to understand which errors go undetected because of the ad hoc techniques used. contributions our goal is to develop a pointer alias analysis expressly to be used for software auditing and error detection. the following outlines the major contributions of this work. ord the expense of a context sensitive and path sensitive analysis throughout the whole program, nor can we. thus, instead of using a uniform algorithm to analyze all pointers in a program, we use a hybrid approach. first, we propose a precise path and context sensitive alias analysis to track locations referred to by simple access paths originating from parameters and local variables. distinguishing between di erent eld structures is crucial into achieve good precision, thus elds are kept separate in our representation. cient, but imprecise pointer alias analysis to handle all the other references. we currently use steensgaarduni cation based analysis, which is ow and context insensitive and does not distinguish between elds within structures. our design is geared toward catching errors that arise from inconsistencies around procedure boundaries and along exceptional control ow paths without producing too many false alarms. an unsound assumption on aliases unsoundness should be introduced in such a way that still allows users to reason about the results and to understand when the results may be incorrect. the unsound assumption we make has the dual bene. of speeding up the analysis and suppressing warnings that are likely to be false. we assume that pointers passed into a procedure, in parameters, global variables, and locations reached by applying simple access paths to parameters and global variables, are all distinct from each other and from any other locations. such an assumption reduces the complexity of our pointer analysis, as it allows the. ects of each procedure to be summarized succinctly. handling potential aliases between parameters precisely has proven to be di cult. previous approaches either disallow strong updates, which would not be precise enough, or use techniques such as partial transfer functions to create summaries only for observed contexts. none of the ow sensitive and contextsensitive techniques have been demonstrated to scale to large programs. this assumption also matches well with how most programs are written. for modularity, the semantics of a function is often independent of the presence of possible aliases among incoming parameters. if that is not the case, a defensive programmer would insert explicit tests in the code to ensure that the potential aliases are handled properly. in these cases, our unsound assumption would not cause a fully path sensitive analyzer to produce any inaccurate result. our system issues a warning if it concludes the unsound assumption is de nitely violated and that the results of the analysis are necessarily incorrect. a low level warning is also reported if the assumption may be violated, but we expect that the low level warnings are too numerous to be helpful. handling paths and contexts ef ciently since analyzing all potential paths in a program is infeasible, simulation based approaches tend to use heuristics and ad hoc solutions to limit the number of paths explored. our solution is to use a demand driven approach to concentrate the resources on those paths found to be of interest. for handling contexts, we rst analyze the program to create summaries of the. cient whole program context sensitive analysis is made possible by our unsound assumption that incoming parameters are unaliased. once this information is available, we can analyze certain context sensitive paths. similarly, our algorithm rst performs a whole program analysis to nd all the potential de nition use relationships in. on demand, the predicates of the path of interest are analyzed. ipssa: a representation for bug detection because all auditing and bug detection tools inneed to handle indirect memory accesses, we propose to analyze pointers in an application independent manner and make the results accessible to various tools. our representation, called ipssa, extends the basic concept of ssa to include de nition use relationships due to pointer dereferences and procedure calls. empirical results: security violations we demonstrate the practicality of our approach by building a tool that nds bu er overruns and format string violations using the ipssa representation. our tool found security vulnerabilities in application programs. more importantly, it reported only one false warning, which is signi cantly fewer in number than existing tools. these preliminary results suggest that our approach is. ective in creating practical and easy to use bug detection tools. paper organization section # presents an overview and design rationale of our approach. section # describes our algorithm for constructing the ipssa representation. section # presents our tool for detecting security vulnerabilities and our experience in using the tool. section # discusses related work and section # concludes. the goal of points to analysis for java is to determine the set of objects pointed to by a reference variable or a reference objet field. improving the precision of practical points to analysis is important because points to information has a wide variety of client applications in optimizing compilers and software engineering tools. in this paper we present object sensitivity, a new form of context sensitivity for flow insensitive points to analysis for java. the key idea of our approach is to analyze a method separately for each of the objects on which this method is invoked. to ensure flexibility and practicality, we propose a parameterization framework that allows analysis designers to control the tradeoffs between cost and precision in the object sensitive analysis. side effect analysis determines the memory locations that may be modified by the execution of a program statement. this information is needed for various compiler optimizations and software engineering tools. we present a new form of side effect analysis for java which is based on object sensitive points to analysis we have implemented one instantiation of our parameterized object sensitive points to analysis. we compare this instantiation with a context insensitive points to analysis for java which is based on andersen analysis for. on a set of java programs, our experiments show that the two analyses have comparable cost. in some cases the object sensitive analysis is actually faster than the context insensitive analysis. our results also show that object sensitivity significantly improves the precision of side effect analysis, call graph construction, and virtual call resolution. these experiments demonstrate that object sensitive analyses can achieve significantly better precision than context insensitive ones, while at the same time remaining efficient and practical. points to analysis is a fundamental static analysis used by optimizing java compilers and software engineering tools to determine the set of objects whose addresses may be stored in reference variables and reference object elds. these points to sets are typically computed by constructing one or more points to graphs, which serve as abstractions of the run time memory states of the analyzed program. optimizing java compilers can use points to information to perform various optimizations such as virtual call resolution, removal of unnecessary synchronization, and stackbased object allocation. points to analysis is also a prerequisite for a variety of other analyses for example, sidee ect analysis, which determines the memory locations that may be modi ed by the execution of a statement, and defuse analysis, which identi es pairs of statements that set the value of a memory location and subsequently use that value. these analyses are necessary to perform compiler optimizations such as code motion and partial redundancy elimination. in addition, such analyses are needed in the context of software engineering tools: for example, def use analysis is needed for program slicing and data ow based testing. points to analysis is a crucial prerequisite for employing these analyses and optimizations. because of this wide range of applications, it is important to investigate approaches for precise and. the two major dimensions in the design space of points to analysis are ow sensitivity and context sensitivity. intuitively, ow sensitive analyses take into account the ow of control between program points inside a method, and compute separate solutions for these points. flow insensitive analyses ignore the ow of control between program points, and therefore can be less precise and more. contextsensitive analyses distinguish between the di erent contexts under which a method is invoked, and analyze the method separately for each context. context insensitive analyses do not separate the di erent invocation contexts for a method, which improves. ciency at the expense of some possible precision loss. recent work has shown that ow and context insensitive points to analysis for java can be. cient and practical even for large programs, and therefore is a realistic candidate for use in optimizing compilers and software engineering tools. however, context insensitivity inherently compromises the precision of points to analysis for object oriented languages such as java. this imprecision results from fundamental object oriented features and programming idioms. the imprecision decreases the impact of the points to analysis on client optimizations and leads to less precise client analyses. to make existing ow and context insensitive analyses more useful, it is important to introduce context sensitivity that targets the sources of imprecision that are speci. at the same time, the introduction of context sensitivity should not increase analysis cost to the point of compromising the practicality of the analysis. in this paper we propose object sensitivity as a new form of context sensitivity for ow insensitive points to analysis for java. our approach uses the receiver object at a method invocation site to distinguish di erent calling contexts. conceptually, every method is replicated for each possible receiver object. the analysis computes separate points to sets for each replica of a local variable; each of those points to sets is valid for method invocations with the corresponding receiver object. we propose a parameterization framework that allows precision improvement through object sensitivity without incurring the cost of non discriminatory replication of all variables. the analysis is parameterized by the set of variables for which the analysis designer wants to maintain multiple points to sets. this targeted replication allows analysis designers to tune directly the cost of the analysis. the framework space ranges from context insensitive analysis to precise object sensitive analysis for which every local variable is replicated for every possible receiver object of its enclosing method. in this paper we discuss parameterized object sensitive points to analysis that is based on an andersen style pointsto analysis for java. andersenanalysis for is a wellknown ow and context insensitive points to analysis. recent work shows how to extend this analysis for java. although we demonstrate our technique on andersenanalysis, parameterized object sensitivity can be trivially applied to enhance the precision of other ow and context insensitive analyses for java. ect analysis determines, for each statement, the set of objects that may be modi ed by that statement. similarly, use analysis computes the set of objects that may be read by a statement. this information plays an important role in optimizing compilers and software productivity tools. ect analysis requires the output of a points to analysis. we de ne and evaluate a new objectsensitive mod analysis that is based on the parameterized object sensitive points to analysis. although we omit the discussion, our approach also applies to the corresponding use analysis. we have implemented one instantiation of our parameterized object sensitive analysis. we compare this instantiation with an andersen style ow and context insensitive class class static void main figure #: sample program and its points to graph. for a set of java programs, our experiments show that the cost of the two analyses is comparable. in some cases the object sensitive analysis is actually faster than the context insensitive analysis. we also evaluate the precision of the two analyses with respect to several client applications. mod analysis based on object sensitive points to analysis is signi cantly more precise than the corresponding mod analysis based on context insensitive pointsto analysis. in addition, object sensitivity improves the precision of call graph construction and virtual call resolution. our experimental results show that object sensitive analyses are capable of achieving signi cantly better precision than context insensitive ones, while at the same time remaining. the contributions of our work are the following: we propose object sensitivity as a new form of context sensitivity for ow insensitive points to analysis for java. we also de ne a parameterization framework that allows analysis designers to control the degree of object sensitivity and the cost precision tradeo. we de ne a new object sensitive side. ect analysis for java which is based on our parameterized objectsensitive points to analysis. we compare one instantiation of our parameterized object sensitive analysis with an andersen style owand context insensitive analysis. our experiments on a large set of programs show that the object sensitive analysis is practical and signi cantly improves the precision of mod analysis, call graph construction, and virtual call resolution. the rest of the paper is organized as follows. section # describes andersenanalysis for java and discusses some sources of imprecision due to context insensitivity. section # de nes our object sensitive analysis. section # discusses parameterized object sensitivity and section # describes techniques for its. the new mod analysis is de ned in section #. the experimental results are presented in section #. section # discusses related work and section # presents conclusions and future work. in this paper, we compare the behavior of pointers inprograms, as approximated by static pointer analysis algorithms, with the actual behavior of pointers when these programs are run. in order to perform this comparison, we have implemented several well known pointer analysis algorithms, and we have built an instrumentation infrastructure for tracking pointer values during program execution. our experiments show that for a number of programs from the spec and spec benchmark suites, the pointer information produced by existing scalable static pointer analyses is far worse than the actual behavior observed at run time. first, a tool like ours can be used to supplement static program understanding tools in situations where the static pointer information is too coarse to be usable. second, a feedback directed compiler can use profile data on pointer values to improve program performance by ignoring aliases that do not arise at run time. as an example, we were able to obtain a factor of speedup on a frequently executed routine from ksim. recent work on alias analysis in the presence of pointers has concentrated on context sensitive interprocedural analyses, which treat multiple calls to a single procedure independently rather than constructing a single approximation to a procedure effect on all of its callers. while context sensitive modeling offers the potential for greater precision by considering only realizable call return paths, its empirical benefits have yet to be measured. this paper compares the precision of a simple, efficient, context insensitive points to analysis for theprogramming language with that of a maximally context sensitive version of the same analysis. we demonstrate that, for a number of pointer intensive benchmark programs, context insensitivity exerts little to no precision penalty. we also describe techniques for using the output of context insensitive analysis to improve the efficiency of context sensitive analysis without affecting precision. this paper presents the first scalable context sensitive, inclusion based pointer alias analysis for java programs. our approach to context sensitivity is to create a clone of a method for every context of interest, and run a context insensitive algorithm over the expanded call graph to get context sensitive results. for precision, we generate a clone for every acyclic path through a program call graph, treating methods in a strongly connected component as a single node. normally, this formulation is hopelessly intractable as a call graph often has acyclic paths or more. we show that these exponential relations can be computed efficiently using binary decision diagrams. key to the scalability of the technique is a context numbering scheme that exposes the commonalities across contexts. we applied our algorithm to the most popular applications available on sourceforge, and found that the largest programs, with hundreds of thousands of java bytecodes, can be analyzed in under minutes this paper shows that pointer analysis, and many other queries and algorithms, can be described succinctly and declaratively using datalog, a logic programming language. we have developed a system called bddbddb that automatically translates datalog programs into highly efficient bdd implementations. we used this approach to develop a variety of context sensitive algorithms including side effect analysis, type analysis, and escape analysis. many applications of program analysis, such as program optimization, parallelization, error detection and program understanding, need pointer alias information. scalable pointer analyses developed to date are imprecise because they are either contextinsensitive or uni cation based. a context insensitive analysis does not distinguish between different calling contexts of a method and allows information from one caller to propagate erroneously to another caller of the same method. in uni cation based approaches, pointers are assumed to be either unaliased or are pointing to the same set of locations. in contrast, inclusion based approaches are more ef cient but also more expensive, as they allow two aliased pointers to point to overlapping but different sets of locations. we have developed a context sensitive and inclusion based pointer alias analysis that scales to hundreds of thousands of java bytecodes. the analysis is eld sensitive, meaning that it tracks the individual elds of individual pointers. our analysis is mostly ow insensitive, using ow sensitivity only in the analysis of local pointers in each function. the results of this analysis, as we show in this paper, can be easily used to answer users queries and to build more advanced analyses and programming tools. cloning to achieve context sensitivity our approach to context sensitivity is based on the notion of cloning. cloning conceptually generates multiple instances of a method such that every distinct calling context invokes a different instance, thus preventing information from one context to ow to another. cloning makes generating context sensitive results algorithmically trivial: we can simply apply a context insensitive algorithm to the cloned program to obtain context sensitive results. note that our analysis does not clone the code per se; it simply produces a separate answer for each clone. the context of a method invocation is often distinguished by its call path, which is simply the call sites, or return addresses, on the invocationcall stack. in the case of a recursive program, there are an unbounded number of calling contexts. to limit the number of calling contexts, shivers proposed the concept ofcfa whereby one remembers only the lastcall sites. emami et al suggested distinguishing contexts by their full call paths if they are acyclic. for cyclic paths, they suggested including each call site in recursive cycles only once. our ap proach also uses entire call paths to distinguish between contexts in programs without recursion. to handle recursion, call paths are reduced by eliminating all invocations whose callers and callees belong to the same strongly connected component in the call graph. these reduced call paths are used to identify contexts. it was not obvious, at least to us at the beginning of this project, that a cloning based approach would be feasible. the number of reduced call paths in a program grows exponentially with the number of methods, and a cloning based approach must compute the result of every one of these contexts. emami et al have only reported context sensitive points to results on small programs. realistic programs have many contexts; for example, the megamekapplication has over year# contexts. the size of the nal results alone appears to be prohibitive. we show that we can scale a cloning based points to analysis by representing the context sensitive relations using ordered binary decision diagrams. bdds, originally designed for hard ware veri cation, have previously been used in a number of program analyses, and more recently for points to analysis. we show that it is possible to compute context sensitive points to results for over year# contexts. in contrast, most context sensitive pointer alias analyses developed to date are summary based. parameterized sum maries are created for each method and used in creating the summaries of its callers. it is not necessary to represent the results for the exponentially many contexts explicitly with this approach, because the result of a context can be computed independently using the summaries. however, to answer queries as simple as which variables point to a certain object would require all the results to be computed. the readers may be interested to know that, despite much effort, we tried but did not succeed in creating a scalable summary based algorithm using bdds. contributions the contributions of this paper are not limited to just an algorithm for computing context sensitive and inclusion based points to information. the methodology, speci cation language, representation, and tools we used in deriving our pointer analysis are applicable to creating many other algorithms. we demonstrate this by using the approach to create a variety of queries and algorithms. scalable cloning based context sensitive points to analysis using bdds. the algorithm we have developed is remarkably simple. we rst create a cloned call graph where a clone is created for every distinct calling context. we then run a simple contextinsensitive algorithm over the cloned call graph to get contextsensitive results. we handle the large number of contexts by representing them in bdds and using an encoding scheme that allows commonalities among similar contexts to be exploited. we improve the ef ciency of the algorithm by using an automatic tool that searches for an effective variable ordering. datalog as a high level language for bdd based program analyses. instead of writing our program analyses directly in terms of bdd operations, we store all program information and results as relations and express our analyses in datalog, a logic programming language used in deductive databases. because datalog is suc cinct and declarative, we can express points to analyses and many other algorithms simply and intuitively in just a few datalog rules. to aid our algorithm research, we have developed a deductive database system called bddbddb that automatically translates datalog programs into bdd algorithms. we provide a high level summary of the optimizations in this paper; the details are beyond the scope of this paper. our experience is that programs generated by bddbddb are faster than their manually optimized counterparts. more importantly, datalog programs are orders of magnitude easier to write. they are so succinct and easy to understand that we use them to explain all our algorithms here directly. all the experimental results reported in this paper are obtained by running the bdd programs automatically generated by bddbddb. the contextsensitive points to results, the simple cloning based approach to context sensitivity, and the bddbddb system make it easy to write new analyses. we show some representative examples in each of the following categories: simple queries. the results from our context sensitive pointer analysis provide a wealth of information of interest to programmers. we show how a few lines of datalog can help programmers debug a memory leak and nd potential security vulnerabilities. we use datalog because its set based operation semantics matches the semantics of bdd operations well. this paper presents a combined pointer and escape analysis algorithm for java programs. the algorithm is based on the abstraction of points to escape graphs, which characterize how local variables and fields in objects refer to other objects. each points to escape graph also contains escape information, which characterizes how objects allocated in one region of the program can escape to be accessed by another region. the algorithm is designed to analyze arbitrary regions of complete or incomplete programs, obtaining complete information for objects that do not escape the analyzed regions. we have developed an implementation that uses the escape information to eliminate synchronization for objects that are accessed by only one thread and to allocate objects on the stack instead of in the heap. we were able to analyze programs tens of thousands of lines long. for our benchmark programs, our algorithms enable the elimination of between and of the synchronization operations. they also enable the stack allocation of between and of the objects. a partial transfer function describes the behavior of a procedure assuming that certain alias relationships hold when it is called. our empirical results demonstrate that this technique is successful a single ptf per procedure is usually sufficient to obtain completely context sensitive results. this paper proposes an efficient technique for context sensitive pointer analysis that is applicable to realprograms. for efficiency, we summarize the effects of procedures using partial transfer functions. we can reuse a ptf in many calling contexts as long as the aliases among the inputs to the procedure are the same. because manyprograms use features such as type casts and pointer arithmetic to circumvent the high level type system, our algorithm is based on a low level representation of memory locations that safely handles all the features of. we have implemented our algorithm in the suif compiler system and we show that it runs efficiently for a set ofbenchmarks.