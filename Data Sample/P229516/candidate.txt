evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance. we present a real world study of modeling the behavior of web search users to predict web search result preferences. accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks. our key insight to improving robustness of interpreting implicit feedback is to model query dependent deviations from the expected noisy user behavior. we show that our model of clickthrough interpretation improves prediction accuracy over state of the art clickthrough methods. we generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone. we report results of a large scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods. relevance measurement is crucial to web search and to information retrieval in general. traditionally, search relevance is measured by using human assessors to judge the relevance of querydocument pairs. however, explicit human ratings are expensive and difficult to obtain. at the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results. if we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems. recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search. however, most traditional permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. susan dumais robert ragno microsoft research microsoft research sdumais microsoft com rragno microsoft com ir work was performed over controlled test collections and carefully selected query sets and tasks. therefore, it is not clear whether these techniques will work for general real world web search. a significant distinction is that web search is not controlled. individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered. but the amount of the user interaction data is orders of magnitude larger than anything available in a non web search setting. by using the aggregated behavior of large numbers of users we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting. furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage. hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions. automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings. we present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results. our contributions include: a distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions. extensions of existing clickthrough strategies to include richer browsing and interaction features. a thorough evaluation of our user behavior models, as well as of previously published state of the art techniques, over a large set of web search sessions. we discuss our results and outline future directions and various applications of this work in section #, which concludes the paper. understanding the impact of individual and task differences on search result page examination strategies is important in developing improved search engines. characterizing these effects using query and click data alone is common but insufficient since they provide an incomplete picture of result examination behavior. cursor or gaze tracking studies reveal richer interaction patterns but are often done in small scale laboratory settings. in this paper we leverage large scale rich behavioral log data in a naturalistic setting. we examine queries, clicks, cursor movements, scrolling, and text highlighting for millions of queries on the bing commercial search engine to better understand the impact of user, task, and user task interactions on user behavior on search result pages. by clustering users based on cursor features, we identify individual, task, and user task differences in how users examine results which are similar to those observed in small scale studies. our findings have implications for developing search support for behaviorally similar searcher cohorts, modeling search behavior, and designing search systems that leverage implicit feedback. better understanding how users interact with web search engines is important for improving the search experience. the information science community has studied individual differences in search strategies, tactics, and performance, and identified important factors such as prior experience, gender, age, cognitive styles, interface design, and domain expertise that influence search strategies and task performance. most of these studies are conducted in a controlled laboratory setting which limits the number of participants and the naturalness of the tasks selected for study. thus it is unclear the extent to which these findings generalize to the wide diversity of searchers tasks seen in web search settings. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. the web provides unprecedented opportunities to evaluate alternative design, interaction, and algorithmic methods at scale and in situ with actual people doing their own tasks in their own environments. studies of searcher engagement with search engine result pages focus primarily on search result clickthrough behavior. these studies provide insights regarding the order in which results are clicked. however, they fail to capture behaviors that do not lead to clicks or subjective impressions. there are two main ways to capture detailed behavioral patterns in search: gaze tracking and mouse cursor logging. gaze tracking studies can provide more detailed insights about how visual attention is distributed on the serp. however these studies are typically conducted in laboratories using a small number of participants with assigned tasks, with summaries of gaze behavior aggregated across participants and tasks. some studies examined individual and task differences in gaze patterns, and found individual differences in the strategies with which users inspect results, and different clusters of users who exhibit similar result examination behaviors. others found that the type of search task influenced task completion time and time spent reviewing documents. gaze tracking can provide valuable insights but the technology is expensive and needs calibration, meaning that it does not scale well to non laboratory settings. a new technique, viewser, allows for approximate tracking of gaze at much larger scale. it does so by blurring the serp and only revealing the region proximal to the mouse pointer in more detail. however, this method influences the serpvisual presentation which likely also affects users serp examination strategies. an alternative to gaze tracking is mouse cursor tracking. recent research has shown that cursor movements correlate with eye gaze, and may therefore be an effective indicator of user attention. small scale laboratory studies have observed participants making many uses of the cursor on serps beyond hyperlink clicking. these uses include moving the cursor as a reading aid, using it to mark interesting results, using it to interact with controls on the screen, or simply positioning the cursor so that it does not occlude web page content. however, these studies were in small scale laboratory settings which limit what inferences can be made about more naturalistic search behavior. cursor tracking provides an efficient and unobtrusive way to track mouse movement behavior and can be deployed at scale. we believe that rich cursor tracking data affords a detailed analysis of user, task and user task differences that is not possible with current evaluation methodologies. in the research reported in this paper, we use rich cursor logs gathered from a deployment on the bing commercial web search engine to better understand individual and task effects on serp interaction. in addition to tracking mouse behaviors such as cursor movements and clicks, we also logged the location of all areas of interest, viewport size, scrolling activity, and text selections. rich data of this type were captured for over million queries using a methodology similar to that proposed by huang et al. this provides sufficient data to allow us to investigate the effects of user and task differences, and interactions between them, and reach conclusions which are potentially generalizable. as we show in our analysis there are distinct user clusters exhibiting specific serp interaction strategies that can be observed from these data, and are particularly apparent when we also consider the effect of search task on users serp examination behaviors. we make the following contributions with this research: gather and use rich cursor interaction log data on a web scale; automatically identify distinct users clusters at scale based on serp examination behaviors, and relate these clusters to findings from previous smaller scale user studies; study the effect of task type and consider their impact on the user clustering, and; propose design implications based on our behavioral clustering. the remainder of this paper is structured as follows. section # describes related work on individual differences in search behaviors, past work on the effect of search task on search behavior, and previous work on gaze and cursor tracking. section # describes the large scale cursor tracking data, including the methodology used to gather the data and summary statistics on serp interaction. section # describes the features that we extracted from the data, the findings of our analysis of user differences, task differences, and any interactions between them. we discuss findings and their implications in section #, and conclude in section #. as with any application of machine learning, web search ranking requires labeled data. the labels usually come in the form of relevance assessments made by editors. click logs can also provide an important source of implicit feedback and can be used as a cheap proxy for editorial labels. the main difficulty however comes from the so called position bias urls appearing in lower positions are less likely to be clicked even if they are relevant. in this paper, we propose a dynamic bayesian network which aims at providing us with unbiased estimation of the relevance from the click logs. experiments show that the proposed click model outperforms other existing click models in predicting both click through rate and relevance. web page ranking has been traditionally based on hand designed ranking functions such as bm. with the inclusion of thousands of features for ranking, hand tuning of ranking function becomes intractable. several machine learning algorithms have been applied to automatically optimize ranking functions. machine learned ranking requires a large number of training examples, with relevance labels indicating the degree of relevance for each querydocument pair. the cost of the editorial labeling is usually quite expensive. moreover, the relevance labels of the training examples could change over time. for example, if the query is time sensitive or recurrent, a search engine is expected to return the copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. most up to date documents sites to the users. however, it would be prohibitive to keep all the relevance labels up to date. click logs embed important information about user satisfaction with a search engine and can provide a highly valuable source of relevance information. compare to editorial labels, clicks are much cheaper to obtain and always re ect current relevance. clicks have been used in multiple ways by a search engine: to tune search parameters, to evaluate di erent ranking functions, or as signals to directly in uence ranking. however, clicks are known to be biased, by the presentation order, the appearance of the documents, and the reputation of individual sites. many studies have attempted to account the position bias of click. carterette and jones proposed to model the relationship between clicks and relevance so that clicks can be used to unbiasedly evaluate search engine when lack of editorial relevance judgment. other research attempted to model user click behavior during search so that future clicks may be accurately predicted based on observations of past clicks. two di erent types of the click models are position models and the cascade model. a position model assumes that a click depends on both relevance and examination. each rank has a certain probability of being examined, which decays by rank and depends only on rank. a click on a url indicates that the url is examined and considered relevant by the user. however this model treats the individual urls in a search result page independently and fails to capture the interaction among urls in the examination probability. take for example two equally relevant urls for a query: a user may only click on the top one, feel satis ed, and then leave the search result page. in this case, the positional bias cannot fully explain the lack of clicks for the second url. the cascade model assumes that users examine the results sequentially and stop as soon as a relevant document is clicked. here, the probability of examination is indirectly determined by two factors: the rank of the url and the relevance of all previous urls. the cascade model makes a strong assumption that there is only one click per search and hence it could not explain the abandoned search or search with more than one clicks. even though the cascade model is quite restrictive, the authors of that paper showed that we refer to url as a shorthand for the entire display block consisting of the title, abstract and url of the corresponding result. it can predict click through rates more accurately than the position models described above. none of the above models distinguish perceived relevance and actual relevance. because users cannot examine the content of a document until they click on the url, the decision to click is made based on perceived relevance. while there is a strong correlation between perceived relevance and actual relevance, there are also many cases where they di er. in this paper, a dynamic bayesian network model is proposed to model the users browsing behavior. as in the position model, we assume that a click occurs if and only if the user has examined the url and deemed it relevant. similar to the cascade model, our model assumes that users make a linear transversal through the results and decide whether to click based on the perceived relevance of the document. the user chooses to examine the next url if he she is unsatis ed with the clicked url. our model di ers from the cascade model in two aspects: because a click does not necessarily mean that the user is satis ed with the clicked document, we attempt to distinguish the perceived relevance and actual relevance. in this paper, we study user click behavior in federated search. click models have been positioned as an effective approach to interpret user click behavior in search engines. existing click models mostly focus on traditional web search that considers only ten homogeneous web html documents that appear on the first search result page. however, in modern commercial search engines, more and more web search results are federated from multiple sources and contain non html results returned by other heterogeneous vertical engines, such as video or image search engines. we observed that user click behavior in federated search is highly different from that in traditional web search, making it difficult to interpret using existing click models. in response, we propose a novel federated click model to interpret user click behavior in federated search. in particular, we take into considerations two new biases in fcm. the first comes from the observation that users tend to be attracted by vertical results and their visual attention on them may increase the examination probability of other nearby web results. the other illustrates that user click behavior on vertical results may lead to more clues of search relevance due to their presentation style in federated search. with these biases and an effective model to correct them, fcm is more accurate in characterizing user click behavior in federated search. our extensive experimental results show that fcm can outperform other click models in interpreting user click behavior in federated search and achieve significant improvements in terms of both perplexity and log likelihood. craswell et al later formalized this idea as the examinationhypothesis, which states that adocumentis clicked if and only if it is both examined and relevant. utilizing user click behavior in log data to understand user preference of search results is one of the most essential techniques for commercial search engines. log data contains valuableinformation about userpreference and canbe collected at a low cost in most commercial search engines. this may in turn help search engines better entertain their users or deliver user preferable advertisements. however, user click behavior in a commercial search engine may contain noise and biases. many attempts have been made to address these challenges and most of them formalized the problem of learning an unbiased document relevance from user click data as the click model problem. a well known bias that needs to be corrected in learning an. ective click model is the position bias, where a document appearing in a higher position will attract more user clicks even it is not as relevant as other documents appearing in lower positions. as a result, the often used metric click through rate is not an ideal measure of document relevance. rstly observed this bias in their eye tracking experiments and a lot of research has been done since then with the goal of inferring an unbiased relevance. richardson et al proposed to increase the relevance of documents in lower positions by a multiplicative factor. more recent work extended these methodstobetterinterpret user clickdatain either organicweb search or online advertisingin sponsored search. despitetheirsuccess, existing click modelsinorganicweb search mostly focus on traditional web search that returns ten homogeneous html documents, which are often referred to as ten blue links in prominent literature. in modern commercial search engines, more and more web search results are federated from multiple sources and contain non html results returned by other heterogeneous vertical engines. forexample, theresultof thequery michaeljordan in bing is shown in figure #. besides the traditional html documents, it also contains results from vertical search engines such as image, video and news. it is clear that the vertical resultsdi erfrom traditionalhtmldocumentsdue to the di erence in presentation, layout, attractiveness, etc, which may in turn. ect users browsing behavior and their decision to perform clicks. however, previous click models are unable to address this issue since they assume only the availability of the ten blue links. as a result, modeling user click behavior in the federated search is an important but under explored research problem. figure #: screenshot of the bing serp page showing vertical search results at di erent slots. in this paper, we rst study user behavior in federated search and then put forward an observation that user behavior in federated search is highly di erent from that in traditional web search. to better characterize user behavior in federated search, we propose a novel bayesian model calledfederatedclickmodel, whichintroducestwo new biases in order to capture the distinctive user behavior in federated search. the rst illustrates that users tend to be attracted by vertical results and the visual attention on these vertical results will increase the examination probability of other nearby web results. this motivates us to reconsider the examination probability of each document in federated search anddevelop an attention modelbeyond the cascade hypothesis. the other bias is due to the fact that vertial results have a special presentation style in federated search, user clicks are more highly correlated with the relevance of the results, which may impact the examinationprobability of the otherwebdocuments. fcm takes thesebiasesinto account, leading tobetter characterizations of user click behavior in federated search. we conduct extensive experiments on a large scale commercial dataset to evaluate the. experimentalresultsdemonstrate thatfcm outperforms other existing click models and achieves signi cant improvements in terms of both perplexity and log likelihood. in recent years many models have been proposed that are aimed at predicting clicks of web search users. in addition, some information retrieval evaluation metrics have been built on top of a user model. in this paper we bring these two directions together and propose a common approach to converting any click model into an evaluation metric. we then put the resulting model based metrics as well as traditional metrics into a common evaluation framework and compare them along a number of dimensions. one of the dimensions we are particularly interested in is the agreement between offline and online experimental outcomes. it is widely believed, especially in an industrial setting, that online atesting and interleaving experiments are generally better at capturing system quality than offline measurements. we show that offline metrics that are based on click models are more strongly correlated with online experimental outcomes than traditional offline metrics, especially in situations when we have incomplete relevance judgements. there are currently two orthogonal approaches to evaluating the quality of ranking systems. the rst approach is usually called the cran eld approach and is done. xed set of queries and documents judged by permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than acm or the author must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. ranking systems are then evaluated by comparing how good their ranked lists are among other things, a system is expected to place relevant documents higher than irrelevant ones. another approach described by kohavi et al makes use of real online users by assigning some portion of the users to test groups. the simplest variant, called atesting, randomly assigns some users to the control group and the treatment group. ranking systems are then compared by analysing the clicks of the users in the control against those in the treatment group. in the interleaving method by joachims users are presented with a combined list made out of two rankings. then the system that receives more clicks is assumed to be better. one of the main advantages of online evaluation schemes is that they are user based and, as a result, often assumed to give us more realistic insights into the real system quality. interleaving experiments are now widely being used by large commercial search engines like bing and yahoo as well as studied in academia. ine measurements, whereas in the traditional cran eld approach one can re use the same set of judged documents to evaluate any ranking. ine editor based evaluation methods unavoidable during the early development phase of ranking algorithms. one should take care, however, that the resulting editor based measurements agree with the outcomes of online experiments online comparison is often used as the nal validation step before releasing a new version of a ranking algorithm. in order to bring the two evaluation approaches closer to each other, we propose a method for building an. ine information retrieval metric from a user click model. click models, probabilistic models of the behavior of web search users, have been studied extensively by the ir community during the last ve years. the main purpose of predicting clicks, as seen in previous works, is: modeling user behavior when real users are not available; improving ranking using relevance inferred from clicks. we hypothesize that click models can also be turned into. ine metrics and the resulting click modelbased metrics should be closely tied to the user and hence should better correlate with online measurements than traditional. in addition, there is a growing trend to ground. ine metrics in a user model and that is exactly copyright year# acm year#. what click modeling does trying to propose a better user model. so, the question is why not use better user models, based on click behavior, as the basis for. ine metrics we put our proposal for transforming click models into metrics to the test through a set of thorough comparisons with online measurements. our comparison includes an analysis of correlations with the outcomes of interleaving experiments, an analysis of correlations with absolute online metrics, an analysis of correlations between traditional. ine metrics and our new click model based metrics, as well as an analysis of the discriminative power of the various metrics. one dimension to which we devote special attention in our comparison framework concerns unjudged documents. as was shown by buckley and voorhees, having partially judged result pages in the evaluation pool may result in biased measurements. we also show that in situations when we cannot. ord to use only fully judged data, we can still make good use of the available data by making adjustments, by either a technique called condensation or a new threshold method that we propose. the main research questions that we address in this work are: how do click model based ir metrics compare to the traditional. ine ir metrics agree with online experiments do click model based metrics show higher agreement how well do di erent. ine metrics perform in the pres ence of unjudged documents how can we modify. ine metrics to enhance agreement with online experiments our main contributions in this paper are a method for converting click models into click model based. click model based metrics with online measurements and traditional. the rest of the paper is organized as follows. section # shows how to transform a click model into a model based. in section # we examine click model based and traditional. we nish with a conclusion and discussion in section #. secondly, we present a thorough analysis and comparison of speci. a key source of bias is presentation order: the probability of click is influenced by a document position in the results page. this paper focuses on explaining that bias, modelling how probability of click depends on position. we carry out a large data gathering effort, where we perturb the ranking of a major search engine, to see how clicks are affected. we then explore which of the four hypotheses best explains the real world position effects, and compare these to a simple logistic regression model. a cascade model, where users view results from top to bottom and leave as soon as they see a worthwhile document, is our best explanation for position bias in early ranks. search engine click logs provide an invaluable source of relevance information, but this information is biased. we propose four simple hypotheses about how position bias might arise. the data are not well explained by simple position models, where some users click indiscriminately on rank or there is a simple decay of attention over ranks. as people search the web, certain of their actions can be logged by a search engine. they can also be indicative of success or failure of the engine. or commercial advantage and that copies bear this notice and the full citation on the rst page. these record which results page elements were selected for which query. click log information can be fed back into the engine, to tune search parameters or even used as direct evidence to in uence ranking. a fundamental problem in click data is position bias. the probability of a document being clicked depends not only on its relevance, but on its position in the results page. in top results lists, the probability of observing a click decays with rank. eye tracking experiments show that the user is less likely to examine results near the bottom of the list, although click probability decays faster than examination probability so there are probably additional sources of bias. our approach is to consider several such hypotheses for how position bias arises, formalising each as a simple probabilistic model. we then collect click data from a major web search engine, while deliberately ipping positions of documents in the ranked list. we nally evaluate the position bias models using the ip data, to see which is the best explanation of real world position. although our experiment involves ips, our goal is to model position bias so we can correct for it, without relying on ips. with such a model it should be possible to process a click log and extract estimates of a search resultabsolute click relevance. patterns of behaviour in logs can give an idea of the scope of user activity. when deciding which search results to permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. bill ramsey microsoft research, redmond usa brams microsoft com present, click logs are of particular interest. such estimates could be used in applications where an estimate of probability of click is useful such as ad ranking or evaluation. perhaps because search is used in such a huge variety of tasks and contexts, the user interface must strike a careful balance to meet all user needs. we describe a study that used eye tracking methodologies to explore the effects of changes in the presentation of search results. we found that adding information to the contextual snippet significantly improved performance for informational tasks but degraded performance for navigational tasks. we discuss possible reasons for this difference and the design implications for better presentation of search results. web search services are among the most heavily used applications on the world wide web. we propose a new model to interpret the clickthrough logs of a web search engine. this model is based on explicit assumptions on the user behavior. in particular, we draw conclusions on a document relevance by observing the user behavior after he examined the document and not based on whether a user clicks or not a document url. this results in a model based on intrinsic relevance, as opposed to perceived relevance. we use the model to predict document relevance and then use this as feature for a learning to rank machine learning algorithm. comparing the ranking functions obtained by training the algorithm with and without the new feature we observe surprisingly good results. this is particularly notable given that the baseline we use is the heavily optimized ranking function of a leading commercial search engine. a deeper analysis shows that the new feature is particularly helpful for non navigational queries and queries with a large abandonment rate or a large average number of queries per session. this is important because these types of query is considered to be the most difficult to solve. search engine click logs provide an invaluable source of relevance information but this information is biased because we ignore which documents from the result list the users have actually seen before and after they clicked. otherwise, we could estimate document relevance by simple counting. in this paper, we propose a set of assumptions on user browsing behavior that allows the estimation of the probability that a document is seen, thereby providing an unbiased estimate of document relevance. our solution outperforms very significantly all previous models. they also explain why documents situated just after a very relevant document are clicked more often. to train, test and compare our model to the best alternatives described in the literature, we gather a large set of real data and proceed to an extensive cross validation experiment. as a side effect, we gain insight into the browsing behavior of users and we can compare it to the conclusions of an eye tracking experiments by joachims et al. in particular, our findings confirm that a user almost always see the document directly after a clicked document. users permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bearthisnoticeand thefull citationonthe rstpage tocopy otherwise, to republish, topost on servers or to redistribute tolists, requiresprior speci. benjamin piwowarski yahoo research latin america bpiwowar yahoo inc com areincreasingly understood tobethedrivingforceof theinternet and many initiatives are aimed at empowering them. social search, as its name implies, supposes participation from users who tag, bookmark, andcomment their search results. in addition to this information explicitly provided by users, there is a much larger source of implicit data which is collectedby search engines. itis apoll of millions of users over an enormous variety of topics. examples of applicationsincludewebpersonalization, web spam detection, query term recommendation. unlike human tags and bookmarks, implicit feedback is also not biased towards socially active web users. that is, the data is collected from all users, notjust usersthat choosetoedit a wikipage, orjoin a social network such asmyspace orfriendster. click data seems the perfect source of information when deciding whichdocuments to showin answerto a query. this information can be fed back into the engine, to tune search parameters or even used asdirect evidencetoin uence ranking. nevertheless, they cannot be used without further processing: a fundamental problem is the position bias. the probability of a document being clicked depends not only on its relevance, but on other factors as its position in the result page. contributions user activity models within web search can be broadly divided in three categories: analysis models where the aim istogaininsightintotypical userbehavior, modelsthat try topredictthenext useraction, and eventually models that estimate the attractiveness or perceived relevance of a document independently of the layout in uence. this work focusses on thelatter, usingas the only source ofinformation the web search logs produced by the search engines. yet users do not browse the whole list and documents situated earlier in the rankinghave ahigherprobability ofbeing examined. as a consequence, they also have a higher probability of being clicked independently of how relevant they are. if we could estimatetheprobability that adocumentis examined by the user, we could estimate its relevance as the ratio of the number of times a user clicked on the document to the expected number of times the document is examined. the main contribution of this work is a model of user browsing behavior when consulting a page of search results. this model estimates the probability of examination of a documentgiven the rankofthedocument andthedistance to the last clicked document. our model sheds light on user behavior, is in agreement with the user experiments ofgranka et al and extends andquanti esthe user model ofjoachims et al. in section # we review the literature for click models and we present our contributions. in section # we compare the predicting abilities on unseen data of the di erent models. westudy in moredetailstheimplications of theuserbrowsing model and we relate the ndings with the eye tracking experiments of insection. social search is quickly gaining acceptance as a promising way of harnessing the common knowledge of millions of users to help each other and search more. arguably, this is a long term trend that started with kleinberg idea of hubs and authorities, which proposed that a hyperlink from one document to another was a vote in favor of the document linked to, an idea in practice exploited in the pagerank algorithm. thisfeedbackprovidesdetailed and valuable information about users interactions with the system as theissuedquery, thepresentedurls, the selected documents and their ranking. it has been used in many ways to mine user interests and preferences. it can be thought as the result of users voting in favor of the documents they nd interesting. in top results lists, the probability of observing a click decays with rank. eye tracking experiments show that a user is less likely to examine results near the bottom of the list, although click probability decays faster than examination probability so there are probably additional sources of bias. experiments also show that a document is not clicked with the same frequency if situated after a highly relevant or a mediocre document. if the users looked with attention all the documents in the ranking list, the relevance of one of them could be estimated simply by counting thenumberof timesitisselected. we investigate how users interact with the results page of a www search engine using eye tracking. the goal is to gain insight into how users browse the presented abstracts and how they select links for further exploration. such understanding is valuable for improved interface design, as well as for more accurate interpretations of implicit feedback for machine learning. the following presents initial results, focusing on the amount of time spent viewing the presented abstracts, the total number of abstract viewed, as well as measures of how thoroughly searchers evaluate their results set. how do users interact with the list of ranked results of www search engines do they read the abstracts sequentially from top to bottom, or do they skip links how many of the results do users evaluate before clicking on a link or reformulating the search the answers to these questions will be beneficial in at least three ways. first, they provide the basis for improved interfaces. second, they suggest more targeted metrics for evaluating the retrieval performance in www search. and third, they help interpreting implicit feedback like clickthrough and reading times for machine learning of improved retrieval functions. in particular, better understanding of user behavior will allow us to draw more accurate inferences about how implicit feedback relates to relative relevance judgments. the following presents the results of an eye tracking study that we conducted. previous studies have analyzed directly observable data like query word frequency. however, unlike eyetracking, these measurements can at best give indirect evidence of how users perceive and respond to the search results. to the best of our knowledge, only one previous study has used eye tracking in the context of information retrieval evaluation. this study attempted to use eye movements to infer the relevancy of documents in the retrieval phase of an information search. the researchers linked relevancy judgments to increases in pupil diameter, as a larger diameter typically signifies high interest in the content matter. however, the sample size and search tasks in this experiment were not robust enough to generate predictable patterns of user search and scanning behavior, which is what our study is able to attain. our findings help us better understand how searchers use cursors on serps and can help design more effective search systems. understanding how people interact with search engines is important in improving search quality. web search engines typically analyze queries and clicked results, but these actions provide limited signals regarding search interaction. laboratory studies often use richer methods such as gaze tracking, but this is impractical at web scale. in this paper, we examine mouse cursor behavior on search engine results pages, including not only clicks but also cursor movements and hovers over different page regions. we: report an eye tracking study showing that cursor position is closely related to eye gaze, especially on serps; present a scalable approach to capture cursor movements, and an analysis of search result examination behavior evident in these large scale cursor data; and describe two applications that demonstrate the value of capturing cursor data. our scalable cursor tracking method may also be useful in non search settings. modern large retrieval environments tend to overwhelm their users by their large output. the third one computes the relative to the ideal performance of ir techniques, based on the cumulative gain they are able to yield. since all documents are not of equal relevance to their users, highly relevant documents should be identified and ranked first for presentation. in order to develop ir techniques in this direction, it is necessary to develop evaluation approaches and methods that credit ir methods for their ability to retrieve highly relevant documents. this can be done by extending traditional evaluation methods, that is, recall and precision based on binary relevance judgments, to graded relevance judgments. alternatively, novel measures based on graded relevance judgments may be developed. this article proposes several novel measures that compute the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. the first one accumulates the relevance scores of retrieved documents along the ranked result list. the second one is similar but applies a discount factor to the relevance scores in order to devaluate late retrieved documents. these novel measures are defined and discussed and their use is demonstrated in a case study using trec data: sample system run results for queries in trec. as a relevance base we used novel graded relevance judgments on a four point scale. the test results indicate that the proposed measures credit ir methods for their ability to retrieve highly relevant documents and allow testing of statistical significance of effectiveness differences. the graphs based on the measures also provide insight into the performance ir techniques and allow interpretation, for example, from the user point of view. or commercial advantage, the copyright notice, the title of the publication, and its date appear, and notice is given that copying is by permission of the acm, inc. to bring such differences into daylight, both graded relevance judgments and a method for using them are required. in most laboratory tests in ir documents are judged relevant or irrelevant with regard to the request. more often relevance is con ated into two categories al at the analysis phase because of the calculation of precision and recall. the third one computes the relative to theideal performance of ir techniques, based on the cumulated gain they are able to yield. the graphs based on the measures also provide insight into the performance ir techniques and allow interpretation, for example, from the user point of view. modern large retrieval environments tend to overwhelm their users by their large output. since all documents are not of equal relevance to their users, highly relevant documents, or document components, should be identi ed and ranked rst for presentation. this is often desirable from the user point of view. in order to develop ir techniques in this direction, it is necessary to develop this research was supported by the academy of finland under the grant numbers and. authors address: department of information studies, fin university of tampere, finland; email: uta. permission to make digital hard copy of part or all of this work for personal or classroom use is granted without fee provided that the copies are not made or distributed for pro. to copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior speci. year# acm year# year# acm transactions on information systems, vol. evaluation approaches and methods that credit ir methods for their ability to retrieve highly relevant documents. the current practice of liberal binary judgment of topical relevance gives equal credit for a retrieval technique for retrieving highly and marginally relevant documents. for example, trec is based on binary relevance judgments with a very low threshold for accepting a document as relevant the document needs to have at least one sentence pertaining to the request to count as relevant. therefore differences between sloppy and excellent retrieval techniques, regarding highly relevant documents, may not become apparent in evaluation. in some studies relevance judgments are allowed to fall into more than two categories, but only a few tests actually take advantage of different relevance levels. however, graded relevance judgments may be collected in eld studies and also produced for laboratory test collections, so they are available. graded relevance judgments may be used for ir evaluation, rst, by extending traditional evaluation measures, such as recall and precision andr curves, to use them. al al arvelin, arvelin and kek ainen propose the use of each relevance level separately in recall and precision calculation. thus differentr curves are drawn for each level. they demonstrate that differing performance of ir techniques at different levels of relevancemaythusbeobservedandanalyzed kek ainenandj al arvelin generalize recall and precision calculation to directly utilize graded document relevance scores. they consider precision as a function of recall, but the approach extends to dcv based recall and precision as well. they demonstrate that the relative effectiveness of ir techniques, and the statistical signi cance of their performance differences, may vary according to the relevance scales used. in the present article we develop several new evaluation measures that seek to estimate the cumulative relevance gain the user receives by examining the retrieval result up to a given rank. the rst one accumulates the relevance scores of retrieved documents along the ranked result list. the second one is similar but applies a discount factor to the relevance scores in order to devaluate late retrieved documents. the rst two were originally presented inal arvelin and kek ainen and were also applied in the trec web track year# and in a text summarization experiment by sakai and sparck jones. these novel measures are akin to the average search length, sliding ratio, and normalized recall measures. they also have some resemblance to the ranked half life and relative relevance measures proposed by borlund and ingwersen for interactive ir. however, they offer several advantages by taking both the degree of relevance and the rank position of a document into account. the novel measures are rst de ned and discussed and then their use is demonstrated in a case study on the effectiveness of trec runs in retrieving documents of various degrees of relevance. the results indicate that the proposed measures credit ir methods for their ability to retrieve highly relevant documents and allow testing of statistical signi cance of effectiveness differences. section # explains our evaluation measures: the cumulated gain based evaluation measures. the test environment, relevance judgments, and the retrieval results are reported. section # contains discussion and section # conclusions. today popular web search engines expand the search process beyond crawled web pages to specialized corpora like images, videos, news, local, sports, finance, shopping etc, each with its own specialized search engine. search federation deals with problems of the selection of search engines to query and merging of their results into a single result set. despite a few recent advances, the problem is still very challenging. first, due to the heterogeneous nature of different verticals, how the system merges the vertical results with the web documents to serve the user information need is still an open problem. moreover, the scale of the search engine and the increasing number of vertical properties requires a solution which is efficient and scaleable. in this paper, we propose a unified framework for the search federation problem. we model the search federation as a contextual bandit problem. the system uses reward as a proxy for user satisfaction. given a query, our system predicts the expected reward for each vertical, then organizes the search result page in a way which maximizes the total reward. instead of relying on human judges, our system leverages implicit user feedback to learn the model. the method is efficient to implement and can be applied to verticals of different nature. we have successfully deployed the system to three different markets, and it handles multiple verticals in each market. the system is now serving hundreds of millions of queries live each day, and has improved user metrics considerably. modern search engines have gone beyond presenting only web text documents. they have extended their services to include results from specialized corpora or verticals likes news, local search, movie, shopping etc. when a user issues a query, the search engine sends it to di erent vertical search engines, and the results returned by these engines are then aggregated together with the web results and composed into a search results page, as the example shown in figure #. this process is usually referred as aggregated search or federated search. a good search federation system can help enhancing the usersearch experience. the results from the verticals are presented to the users in a visually appealing interface and contain useful information. for example, a movie result usually contains the movie poster and showtime information, and a local search result contains the address and telephone of the target business. given a user query, the system should rst decide the possible vertical intent of the query and send it to the corresponding vertical backend. some queries may express the intent for vertical content explicitly, other queries intent maybe implicit. a user who searches for co ee bean may look for a local shop which sells co ee beans, plan to buy co ee beans online, or search for an article which recommends coffee beans. therefore, how to compare the relevance of these items, and place the verticals in the correct figure #: an example serp for the query co ee shows image, food, local and shopping verticals at di erent positions. position within the web text documents become particularly challenging. this problem is di erent from the core ranking problem in web search, where the textual web corpus of the same properties are compared with each other. in federated search, even though vertical databases usually have more structured representation, they are unlikely to share common features due to their heterogeneous nature. hence we can not directly apply the same machinery used in web ranking. model based triggering approach has become popular in recent years, and machine learning techniques have been used to. a new challenge that comes with this approach is how to gather enough data with labels to train the model. hiring editors to judge the results can be expensive and is not scaleable. the judgements can be biased because many queries contain multiple intents. how does the system know if a user who searches for starbucks is looking for a nearby starbucks store or intends to navigate to starbucks. ce site furthermore, the intent of a query may also shift over time. in such cases, it is crucial to quickly identify the new intent. another possible solution is by analyzing users click logs and obtaining training data for the model. several studies have been conducted for the web documents ranking application, but few on the federated search setup. di erent from the traditional web ranking problem, where all the documents are presented to the user in a ranked order, if a vertical has not been triggered given a query and or has not been slotted above a web document, the system will receive no feedback, and thus will have no knowledge as to whether it matches the userintent or if its results are preferred over a web document. several research projects have attempted to address related issues from di erent perspectives. in this paper, we present a uni ed search federation system. it addresses the challenge of aggregating the vertical results on the serp in a uni ed and principled approach. the system currently serves three major markets of yahoo usa, taiwan and hongkong, and controls a dozen verticals in total. it is serving hundreds of millions of queries live each day, and has improved user metrics considerably. we formulated it as a contextual bandit problem, an approach which collects training data using an exploration exploitation strategy, and updates its model based on the user click feedback to maximize the total user satisfaction in the long run. in section #, we present the method we implemented in each part of our system, and an. finally, we show the performance improvement of our system over the old system on real web search tra. we will review the related work in section #. then, we formalize the de nition of our problem, and introduce the framework of our federation system. this paper examines the reliability of implicit feedback generated from clickthrough data in www search. analyzing the users decision process using eyetracking and comparing implicit feedback against manual relevance judgments, we conclude that clicks are informative but biased. while this makes the interpretation of clicks as absolute relevance judgments difficult, we show that relative preferences derived from clicks are reasonably accurate on average. the idea of adapting a retrieval system to particular groups of users and particular collections of documents promises further improvements in retrieval quality for at least two reasons. second, as evident from the trec evaluations, differences between document collections make it necessary to tune retrieval functions with respect to the collection for optimum retrieval performance. since manually adapting a retrieval function is time consuming or even impractical, research on automatic adaptation using machine learning is receiving much attention. ithaca, ny, usa cornell edu however, a great bottleneck in the application of machine learning techniques is the availability of training data. in this paper we explore and evaluate strategies for how to automatically generate training examples for learning retrieval functions from observed user behavior. however, implicit feedback is more di cult to interpret and potentially noisy. in this paper we analyze which types of implicit feedback can be reliably extracted from observed user behavior, in particular clickthrough data in www search. the study is designed to analyze how users interact with the list of ranked results from the google search engine and how their behavior can be interpreted as relevance judgments. first, we use eyetracking to understand how users behave on googleresults page. do users scan the results from top to bottom how many abstracts do they read before clicking how does their behavior change, if we arti cially manipulate googleranking answers to these questions give insight into the users decision process and suggest in how far clicks are the result of an informed decision. based on these results, we propose several strategies for generating feedback from clicks. to evaluate the degree to which feedback signals indicate relevance, we compare the implicit feedback against explicit feedback we collected manually. the study presented in this paper is di erent in at least two respects from previous work assessing the reliability of implicit feedback. first, our study provides detailed insight into the users decision making process through the use of eyetracking. second, we evaluate relative preference signals derived from user behavior. this is in contrast to previous studies that primarily evaluated absolute feedback. our results show that users make informed decisions among the abstracts they observe and that clicks re ect relevance judgments. however, we show that clicking decisions are biased in at least two ways. first, we show that there is a trust bias which leads to more clicks on links ranked highly by google, even if those abstracts are less relevant than other abstracts the user viewed. second, there is a quality bias: the users clicking decision is not only in uenced by the relevance of the clicked link, but also by the overall quality of the other abstracts in the ranking. we propose several strategies for extracting such relative relevance judgments from clicks and show that they accurately agree with explicit relevance judgments collected manually. first, a one size ts all retrieval function is necessarily a compromise in environments with heterogeneous users and is therefore likely to act suboptimally for many users. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. in contrast to explicit feedback, such implicit feedback has the advantage that it can be collected at much lower cost, in much larger quantities, and without burden on the user of the retrieval system. to evaluate the reliability of implicit feedback signals, we conducted a user study. we performed two types of analysis in this study. this shows that clicks have to be interpreted relative to the order of presentation and relative to the other abstracts. relevance estimation with the two stage examination model also outperforms that with a single stage examination model. user examination of search results is a key concept involved in all the click models. however, most studies assumed that eye fixation means examination and no further study has been carried out to better understand user examination behavior. in this study, we design an experimental search engine to collect both the user feedback on their examinations and the eye tracking click through data. to our surprise, a large proportion of the results fixated by users are not recognized as being read. looking into the tracking data, we found that before the user actually reads the result, there is often a skimming step in which the user quickly looks at the result without reading it. we thus propose a two stage examination model which composes of a first from skimming to reading stage and a second from reading to clicking stage. we found that the biases considered in many studies impact in different ways in stage and stage, which suggests that users make judgments according to different signals in different stages. we also show that the two stage examination behaviors can be predicted with mouse movement behavior, which can be collected at large scale. this study shows that the user examination of search results is a complex cognitive process that needs to be investigated in greater depth and this may have a significant impact on web search. web search has reached a level at which a good understanding of user interactions may significantly impact its quality. among all kinds of user interactions, examination is an important one that attracted much attention. our understanding on how users allocate their limited attention to search engine result pages can contribute to improving search ui designing, result ranking, ad delivery and many other research issues in web search. it also plays a central role in the examination hypothesis, which assumes that one result on serp will be clicked only if it is examined. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. http: dx doi org year# many previous investigations on user examination bahavior relied on eye tracking. richardson and joachims are among the first to point out that users are more likely to examine results near the top of serps based on findings in eye tracking experiment. cutrell and buscher found that eye movements of users with different search intents are quite different. wang and diaz found that different result appearances may lead to different eye movement behaviors on both vertical and ordinary results. while all these studies based on eye tracking have revealed a number of important findings in search users examination process, they generally assumed that when a user fixes eye on a result for a certain time, the result is examined. the eye fixation sequence was assumed to be that of examination, and the remaining eye movements were ignored. these studies follows the strong eye mind hypothesis, which supposes that what the eyes fixate on is what the mind processes. however, this strong assumption is not always validated. for example, just et al found that while the duration of the gaze is closely related to the duration of cognitive processes, they are not necessarily identical. with a number of experiments, they found that the gaze duration may at best provide a rough estimate of the absolute duration of processing. therefore, although the eye fixation sequence helps us understand users examination patterns on serps, it may not necessarily reflect the true examination sequence of the user. in order to construct better models for user interactions, we need to better understand the relationship between fixation, reading and clicking. this study is an attempt to address the question. to this end, we design an experimental search engine system that collects simultaneously eye tracking, mouse movement, click through behavior and userexplicit feedback on result reading. we analyze in depth the examination process of web search users and we find that the results fixated by users are different from those which users remember to have read. it shows that user examination process is non trival and more complex than what eye fixation sequence shows. an example in figure # shows a user eye fixation sequences and the explicit feedback on reading during a search session. we can see that the user quickly looked through the first two results before focusing on the third one. although all three results are fixated for some time, the user explicit feedback showed that he she only regarded the third result as being read. this typical example shows that the user result reading process may not always be aligned with eye fixation. therefore, simply treating fixation as examination may be misleading for the understanding of web search user behaviors. in addition, we also observe that before reading, there are rich eye movements over the results. they are important to understand how users examine the results, but are typically discarded in the previous studies. in this paper, these movements are considered to correspond to skimming a quick overlook at the result without reading. reading a result requires that the result be first skimmed. similarly, not all results read by the user are clicked by users while almost all clicked results are read by users. this observation motivates us to consider an examination as a two stage process. in the first skimming to reading stage which is featured by sometimes unconscious eye fixations, users quickly look through results and decide whether one result should be ignored or paid further attention to. in figure #, the user attention on the first two results corresponds to stage. in this example, the user chose to ignore these results. in the second reading to clicking stage which is usually remembered by users, they carefully read and comprehend the results selected from stage and based on the reading, decide whether to click on it or not. in figure #, the user examination on the third result might come into stage, and the user remembers that it has been read. a user eye fixation sequence and the corresponding explicit feedback on result reading for top results in a search session of query. our proposed two stage examination model and the choice of two stages are inspired by the attention selection mechanism which is widely accepted in cognitive psychology studies. it says that human attention consists of two functionally independent, hierarchical stages: an early, pre attentive stage that operates without capacity limitation and in parallel across the entire visual field, followed by a later, attentive limited capacity stage that can deal with only one item at a time. attention selection is one of the basic cognitive mechanisms of human beings and the two stage examiantion model can be regarded as an attempt to explain how the mechanism works in web search environment. what we propose in this paper is as follows: with analysis of usersearch interaction process, we show that users may examine serps with a two stage strategy. this two stage examination model reveals the relationship among eye fixation, result reading and click through behaviors. it also helps us to understand the mechanism with which search users allocate their attention selectively. while revisiting the search behavior biases including position bias, domain bias and attractiveness bias, we found that these biases have different impacts on user behavior in different examination stages. it means that users may rely on different signals to make decisions in different stages. a prediction model is constructed to identify result examination in different stages with mouse movement information that could be collected at large scale. after that, a learning method is proposed to estimate the relevance of a result based on the two stage examination model. the two stage model is found to significantly outperform the orginal single stage model. the remainder of this paper is organized as follows. in section # we review some related studies on user interactions in web search. section # describes the framework of the experimental system and the interaction data collected in our study. section # analyzes the relationship between fixation, reading and click through behaviors and proposes the two stage examining model. section # focuses on the behavior biases in the two stage model. in section # we attempt to predict two stage examination behavior using mouse movement information and then use this information to estimate result relevance. in section # we discuss the extension of our work before some concluding remarks. these findings also reaffirm the necessity of the proposed two stage model. the process of fixation identification separating and labeling fixations and saccades in eye tracking protocols is an essential part of eye movement data analysis and can have a dramatic impact on higher level analyses. however, algorithms for performing fixation identification are often described informally and rarely compared in a meaningful way. in this paper we propose a taxonomy of fixation identification algorithms that classifies algorithms in terms of how they utilize spatial and temporal information in eye tracking protocols. using this taxonomy, we describe five algorithms that are representative of different classes in the taxonomy and are based on commonly employed techniques. we then evaluate and compare these algorithms with respect to a number of qualitative characteristics. the results of these comparisons offer interesting implications for the use of the various algorithms in future work. eye tracking has been gaining in popularity over the past decade as a window into observers visual and cognitive processes. for instance, researchers have utilized eye tracking to study behavior in such domains as image scanning, driving, arithmetic, analogy, and reading. in these and other domains, researchers typically analyze eye movements in terms of fixations. common analysis metrics include fixation or gaze durations, saccadic velocities, saccadic amplitudes, and various transition basod parameters between fixations and or regions of interest. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advan tage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee, eye tracking research applications symposium year# palm beach gardens, fl, usa; year# acm isbn. of industrial and manufacturing engineering pennsylvania state university leonhard building university park, pa year# usa year# jgolclberg psu edu the analysis of fixations and saccades requires some form of fixation identification bthat is, the translation from raw eye movement data points to fixation locations on the visual display. fixation identification significantly reduces the size and complexity of the eye movement protocol, removing raw saccade data points and collapsing raw fixation points into a single representative tuple. this reduction is useful for at least two reasons. first, little or no visual processing can be achieved during a saccade, and thus the actual paths traveled during saccades are typically irrelevant for many research applications. second, smaller eye movements that occur during fixations, such as tremors, drifts, and flicks, often mean little in higher level analyses. thus, fixation identification is a convenient method of minimizing the complexity of eye tracking data while retaining its most essential characteristics for the purposes of understanding cognitive and visual processing behavior. fixation identification is an inherently statistical description of observed eye movement behaviors. while it is generally agreed upon that visual and cognitive processing do occur during fixations, it is less clear exactly when fixations start and when they end. thus, regardless of the precision and flexibility associated with identification algorithms, the identification problem is still a subjective process. therefore one efficient way to validate these algorithms is to compare resultant fixations to an observer subjective impressions. though widely employed, fixation identification and its various implementations have often been given short shrift in the eye movement literature, particularly in literature concerning the interaction of eye movements and higher level cognition. however, identification is often a critical aspect of eye movement data analysis that can have significant effects on later analyses. for instance, poorly defined identification algorithms may produce too many or too few fixations, or may be overly sensitive to outlier data points, thus biasing interpretation. karsh and breitenbach provided a rigorous demonstration of how different identification algorithms can produce vastly different interpretations even when analyzing identical protocols. thus, good identification algorithms ensure valid fixation and saccade locations and durations which in turn influence inferences such as processing complexity and visual search paths. in addition to the frequent underspecification of identification algorithms, little work has been done in evaluating and comparing different possible algorithms. researchers and practitioners often have minimal information to guide their decision of which algorithm to use in particular situations. not surprisingly, this problem leads to the somewhat haphazard application of different algorithms, making it difficult to compare results derived by different methods of identification. in this paper we address these problems by proposing a novel taxonomy of fixation identification algorithms and evaluating representative algorithms in the context of this taxonomy. the taxonomy classifies algorithms with respect to five spatial and temporal criteria. the spatial criteria divide algorithms in terms of their use of velocity, dispersion, and area of interest information. the temporal criteria divide algorithms in terms of their use of duration information and their local adaptivity. the taxonomy provides a meaningful labeling and classification of existing algorithms so that they may be more easily compared in a systematic way to guide the choice of algorithms for particular applications. to demonstrate the usefulness of the taxonomy, we identify and describe five algorithms that are representative of different classes in the taxonomy. we first provide rigorous descriptions of these algorithms in pseudocode form to best illustrate the similarities and differences between the algorithms. we then evaluate and compare the methods with respect to several qualitative characteristics including algorithm speed, accuracy, robustness, parameter space, and ease of implementation. we conclude with a discussion of implications and recommendations for how the various methods can be best utilized for future applications. the ndcg measure is found to correlate best with user preferences compared to a selection of other well known measures. the work revealed a number of new results, but also showed that there is much scope for future work refining effectiveness measures to better capture user preferences. this paper presents results comparing user preference for search engine rankings with measures of effectiveness computed from a test collection. it establishes that preferences and evaluation measures correlate: systems measured as better on a test collection are preferred by users. this correlation is established for both conventional web retrieval and for retrieval that emphasizes diverse results. unlike previous studies in this area, this examination involved a large population of users, gathered through crowd sourcing, exposed to a wide range of retrieval systems, test collections and search tasks. reasons for user preferences were also gathered and analyzed. one of the striking aspects of almost all the early work in test collections is that the predictions about users implied from such measurements were rarely, if ever, validated. given that test collections are used to simulate users, that so little validation took place is perhaps surprising. there is a long tradition of encouraging conducting, and researching evaluation of search systems in the ir community. a test collection and an evaluation measure are together used as a tool to make a prediction about the behavior of users on the ir systems being measured. if measurement using the collection reveals that system a is more effective than system, it is assumed that users will prefer a overin an operational setting. in the last ten years a series of papers employing a range of methods conducted such validation. the papers produced contradictory results, some failing to find any link between test collection measures and user preferences, performance, or satisfaction; others finding links, but only when differences between ir systems were large. much of the past work involved a small number of topics, systems, and users; and or introduced some form of artificial manipulation of search results as part of their experimental method. there was also a strong focus on test collections and not on the relative merits of different evaluation measures. therefore, it was decided to examine, on a larger scale, if test collections and their associated evaluation measures do in fact predict user preferences across multiple ir systems, examining different measures and topic types. the study involved users, working with topics, comparing user preferences across runs submitted to a recent trec evaluation. the research questions of the study were as follows. does effectiveness measured on a test collection predict user preferences for one ir system over another. if such a predictive power exists, does the strength of prediction vary across different search tasks and topic types. if present, does the predictive power vary when different effectiveness measures are employed. when choosing one system over another, what are the reasons given by users for their choice the rest of this paper starts with a literature review, followed by a description of the data sets and methods used in the study. next, the results of experiments are described, the methods are reflected upon, conclusions are drawn, and future work is detailed. in modern search engines, an increasing number of search result pages are federated from multiple specialized search engines. as an effective approach to interpret users click through behavior as feedback information, most click models were designed to reduce the position bias and improve ranking performance of ordinary search results, which have homogeneous appearances. however, when vertical results are combined with ordinary ones, significant differences in presentation may lead to user behavior biases and thus failure of state of the art click models. with the help of a popular commercial search engine in china, we collected a large scale log data set which contains behavior information on both vertical and ordinary results. we also performed eye tracking analysis to study user real world examining behavior. according these analysis, we found that different result appearances may cause different behavior biases both for vertical results and for the whole result lists. these biases include: examine bias for vertical results, trust bias for result lists with vertical results, and a higher probability of result revisitation for vertical results. based on these findings, a novel click model considering these biases besides position bias was constructed to describe interaction with serps containing verticals. experimental results show that the new vertical aware click model is better at interpreting user click behavior on federated searches in terms of both log likelihood and perplexity than existing models. as the web search click logs reflect users preferences regarding search result documents, these logs are considered to be invaluable sources of information for improving search performance. the information stored in click through behavior data can be used in many research areas such as click through rate prediction, web search ranking, query recommendation, and so on. with the help of these applications, search engine can better help users to satisfy their information needs. while analyzing click through data, key concerns include how to construct a click model to interpret users examination and clicking preferences and how to obtain unbiased document relevance estimations. much effort has been made on this research topic. state of art click models such as user browsing model, click chain model and dynamic bayesian network click model have shown their power in fitting the real world data and predicting future clicks. although these existing click models have gained much success in modeling ordinary search results, they were not designed for result lists with non web page results, which were provided by multiple heterogeneous vertical search engines and incorporated into a large number of serps. according to their appearances, we classify vertical results into three categories: text vertical: the text vertical is made up of a few blue links and textual snippets, such as news search results or wiki information shown in serps. multimedia vertical: the multimedia vertical is made up of a group of multimedia components, such as video or photo search results. application vertical: the application vertical contains a button or a form embedded into serps to help users finish certain tasks, such as a download button or an exchange rate calculator. we can see that the vertical results have different layout and presentation forms compared to ordinary search results. it is reasonable to suppose that they may lead to different user examination behavior and click preference. therefore, most of previous click models, which assume all results are homogeneous, may not describe user behavior on these serps correctly. chen et al made the first step to model user behavior in vertical results. they found that users were more likely to examine the vertical and the ordinary web documents nearby. they also indicated that users are more likely to be satisfied with vertical results and end the whole search session. however, although they also divided verticals into several kinds, they didntake into account that different vertical types influence users behavior differently. with the help of a popular commercial search engine in china, we collected a large number of log data which contain behavior information on both vertical and ordinary results. by analyzing the logs we found that more than serps of this chinese commercial search engines contain verticals, and different result appearances caused different behavior biases both for the vertical results and for the whole result lists. so when analyzing user behavior for modern search engines, taking verticals into account is very important. we also performed eye tracking experiments to look into users actual permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. different kinds of vertical results federated into serps examining behaviors on vertical and ordinary results. based on these findings, a novel click model named vertical aware click model was constructed to take these biases into consideration. the major contributions of this work include: vertical results are grouped into three categories according to their appearances on serps. how users interact with these vertical results and other results on a same serp are analyzed in terms of both large scale click through log and laboratory eye tracking analysis. millions of users submit queries to search engines every day. this paper presents partially observable markov model with duration, a statistical method that addresses the challenge of understanding sophisticated user behaviors from the search log in which some user actions, such as reading and skipping search results, cannot be observed and recorded. pomd utilizes not only the positional but also the temporal information of the clicks in the log. pomd differs from the traditional hidden markov model in that not all the hidden state transitions emit observable events, and that the duration of staying in each state is explicitly factored into the core statistical model. first, we show that the search behavioral patterns inferred by pomd match well with those reported in the eye tracking experiments. secondly, through a series of acomparison experiments, we demonstrate that pomd can distinguish the ranking qualities of different search engine configurations much better than the patterns inferred by the model proposed in the previous work. in this work, we treat the user engagements with a search engine as a markov process, and model the unobservable engagements as hidden states. to address the training and decoding issues emerged as the results of the variations, we propose an iterative two stage training algorithm and a greedy segmental decoding algorithm respectively. we validate the proposed algorithm with two sets of experiments. both of the experimental results suggest that pomd can provide a statistical and quantitative way to understand the sophisticated search behaviors by simply mining the search logs. ective way to gain invaluable insights into evaluating and designing human computer interaction applications. for this purpose, eye tracking has been a widely accepted method to study user behaviors and many have applied this technique to gain insights into analyzing the. for example, it has been shown that users read the search results in an uneven fashion, with results on top of the page receiving more attention than the others. ected by either the search task or the rank positions of the search results, and these observations lead to the use of reading time to assess the quality of clicked documents. however, the eye tracking experiments are expensive to conduct, not only due to the di culties of calibrating the tracking or recording devices, but also the labor intensive nature to perform the experiments on human subjects. as a result, eye tracking experiments were typically not conducted at scale, and it becomes an attractive alternative to apply advanced data mining techniques to the massive and easy to collect search logs that record the interactions between the users and the web applications. however, a fundamental challenge underlying search log mining is that the search log can only record observable user actions, such as clicks and query submissions. how users interact with the majority of the search results that are read but skipped remains not recordable in the search logs. to gain a holistic view, a statistical model, called the partially observable markov model, was recently proposed to infer the unobservable search behaviors based on the recorded user events in the search log. pom uses a markov chain to model the sequence of the search results read by the user and allows not all but only some of them to emit recordable observations. this property makes it suitable to model the search results that are read but skipped and hence leave no behavioral traces in the logs. the partially observable process, though realistic in modeling the search behaviors, brings technical challenges for its implementation, the most prominent one being that, for any given observable event sequence, there are in theory an in nite number of hidden state transitions that can generate this event sequence. ective in processing large scale log data, and the hidden sequences thus uncovered were shown to bear close agreements with the physical measurements of human behaviors using the eye tracking method. despite the success of pom in uncovering the unobservable user behaviors from the search logs, an important behavioral evidence, the duration that the user spends on reading a search result, was left out of the core model in pom. in the literature, this behavioral cue is widely shown to play a critical role in precisely assessing the user perceived quality of the search results. empirically, we have also observed that pom occasionally would produce unintuitive hypotheses that may be attributed to its lack of an explicit duration model. the main contribution of this work, called pom with duration or pomd for short, is to correct this omission by augmenting the underlying framework of pom with a more advanced statistical model, and with it to demonstrate the importance and pinpoint the role the duration plays in understanding the search behaviors. although the necessary revisions of the mathematical formulation are signi cant, the primary objective of the work remains the same. as such, the usefulness of pomd can be assessed in a similar manner as pom from the following perspectives. first, since the goal of pomd is also to infer the unobservable search behaviors, a straightforward way to assess the results of pomd is to compare them with those obtained from physical measurements, say, through eye tracking experiments. the comparison is qualitative in nature because, as elaborated in, it remains challenging to conduct precise and quantitative scanpath analysis from the eye tracking data. ectiveness of pomd can accordingly be evaluated by investigating how well search quality metrics can be designed based on the model. the rest of the paper is organized as follows. to make the general pomd realizable, we describe in section # the assumptions made for pomd. implementation, starting with the greedy decoding algorithm in section #. in section #, we lay out the training process, rst deriving the em algorithm and the viterbi approximation that estimate the parameters of the poisson mixture process and the hidden markov process, respectively, and then the iterative two stage training algorithm for pomd. in section #, we evaluate the proposed model with two sets of experiments. first, we show that the search behavioral patterns inferred by pomd are in good agreement with those reported in the eye tracking experiments. secondly, through a series of acomparison experiments, we demonstrate that pomd can distinguish the ranking qualities of di erent search engine con gurations much better than the patterns inferred by the model proposed in the previous work. ectiveness of and designing better approaches to web search. known as the positional bias, this observed behavior has been the motivation behind the work in search quality analysis and relevance ranking. it also has been observed that the duration of the reading time is. pom addresses this challenge with a segmental decoding technique to reduce the algorithmic complexity from having to consider the in nite number of hidden sequences down to a manageable few. in contrast, through the use of probabilistic modeling in pomd, the variabilities in user behaviors are naturally taken into account in the probabilistic distribution in a mathematically tractable manner. secondly, the analytical nature of pomd lends itself to be a viable tool to measure the search quality. in this paper, we describe a series of experiments based on the acomparison paradigm in which we deliberately introduce certain search engine con gurations that are known to produce inferior quality than the corresponding control conditions. a metric, and its underlying model, can therefore be deemed more. ective if it can provide a higher resolution to tell apart the experimental conditions. we rst review the motivation of this work in section #. in section #, we present the general mathematical framework of pomd that highlights the main di erences and necessary revisions from pom. in the following sections, we then describe the speci. finally in section #, we give a summary on the model described in this paper. click through behaviors are treated as invaluable sources of user feedback and they have been leveraged in several commercial search engines in recent years. however, estimating unbiased relevance is always a challenging task because of position bias. to solve this problem, many researchers have proposed a variety of assumptions to model click through behaviors. nevertheless, this model cannot draw a complete picture of information seeking behaviors. many eye tracking studies find that user interactions are not sequential but contain revisiting patterns. if a user clicks on a higher ranked document after having clicked on a lower ranked one, we call this scenario a revisiting pattern, and we believe that the revisiting patterns are important signals regarding a user click preferences. this paper incorporates revisiting behaviors into click models and introduces a novel click model named temporal hidden click model. this model dynamically models users click behaviors with a temporal order. in our experiment, we collect over million query sessions from a widely used commercial search engine and then conduct a comparative analysis between our model and several state of the art click models. the experimental results show that the thcm model achieves a significant improvement in the normalized discounted cumulative gain, the click perplexity and click distributions metrics. most of these models share a common examination hypothesis, which is that users examine search results from the top to the bottom. with the explosive growth of information on the web, search engines have become indispensable information acquisition tools for users. how to obtain an ideal ranking is always a challenging task with respect to search engines. however, the manual labeling process is both expensive and time consuming, especially for updating some labels. updating these labels is a necessary yet di cult job because some queries require the up to date results. for example, if a user submits wsdm as a query, results related to wsdm are more likely to be expected documents than results related to wsdm. click through logs record user interactions with search engines and can be collected at a low cost. in addition, click through data is treated as an important signal of users click preferences because click through data can provide fresh and timely information. previous studies show that click logs are informative but biased. this phenomenon is represented as follows: a higher ranked document has a higher probability to be examined and clicked even if it is not as relevant as lower ones. based on position bias problem, researchers have proposed many click models to obtain an unbiased estimation of document relevance. the cascade model assumes that each user examines the results from top to bottom sequentially. a strong assumption is that a user will end the current search session as soon as he clicks a document; therefore, this model is only suitable for single click situations and cannot draw a whole picture of multi click sessions. to solve this problem, the dcm model proposes the following assumption: a user who clicks a document has a a probability of continuing and a of abandoning the search. here a depends on the rank of this document. thus, the dcm model extends the cascade model to multi click situation. guo et al introduce the skipping behavior into the ccm model by assuming that user can choose to click or skip according to the current relevance. eye tracking studies are used as direct microcosmic evidences for user behaviors. according to, the eye tracking processing is categorized into two parts: the depth rst strategy and the breadth rst strategy. the depth rst model assumes that the user examines the result list from top to bottom and decides immediately whether to click. the breadth rst strategy is described as follows: the user looks ahead at a series of results and then revisits the most relevant results to click on. most of previous click models are rooted in the depth rst model. lorigo et al performed a series of eye tracking experiments and used scan path to characterize users browsing behaviors. they found that only percent of the scan paths are linear while over percent of sessions contain revisiting or skipping behaviors which cannot be covered by the depth rst model. their experimental results indicated that a majority of users may not, in general, follow the presentation order. according to their ndings, empirical seeking behavior is very complex, and the depth rst model is a simpli cation. the revisiting behaviors are acceptable supplements to the depth rst model. however, little work has been done for incorporating revisiting behavior into recent click models. according to our analysis of click through data containing over million user sessions, we discover that of multi click sessions contain revisiting behaviors. this result coincides with the eye tracking study performed by. based on these ndings, we can see that the revisiting behaviors cover a large number of search sessions and should not be ignored in the construction of a practical click model. as a result, we will introduce revisiting behaviors to solve the position bias problem. to the best of our knowledge, this study is the rst attempt to incorporate revisiting behaviors into click models. for each position on a search engine results page, a user may move down for lower results, may stop and abandon the search or may review the higher ranked results. previous models only estimate the probability of going down for lower results and the probability of stopping or abandoning the search. in our model, we will determine the probability of revisiting higher results. thus, our model provides a more complete simulation of a userinformation seeking behaviors. in other words, the probability of being examined is estimated based on both the higher ranked and the lower ranked results. to make our model generative, we build up a new model with a dynamic temporal order. our model is named temporal hidden click model, and it di ers from previous models in two respects: revisiting behaviors are taken into consideration; user interaction is organized in a temporal order instead of ranking order. the users interactions are organized in a temporal order. this order is more reasonable than the ranking order because users do not always follow the ranking order. a practical method based on the thcm model is also proposed to solve the relevance inference and parameter estimation problems with an acceptable scalability in both time and space. the remainder of this paper is organized as follows. we rst present some important hypotheses and existing click models in section #. after providing a detailed description of our model in section #, the process of relevance inference and parameter estimation are represented in section #. in section #, we conduct experimental studies and evaluation, and we discuss and conclude our work in section #. previous studies have developed a number of ranking optimization algorithms for manually labeled data. as a result, click through logs are widely adopted in both sponsored searches and web searches. givenresults in a serp, theth result is accessed from two perspectives: going down from the higher ranked results and revisiting from the lower ones. the main contributions of our work are as follows: a novel click model thcm is proposed to incorporate revisiting behaviors in the whole search process and improves the performance of the click models. previous studies on search engine click modeling have identified two presentation factors that affect users behavior: position bias: the same result will get a different number of clicks when displayed in different positions and externalities: the same result might get more clicks when displayed with results of relatively lower quality than when shown with higher quality results. in this paper we focus on analyzing the sequence of user actions to model users click behavior on sponsored listings shown on the search results page. we first show that temporal click sequences are good indicators of externalities in the advertising domain. we then describe the positional rationality hypothesis to explain both the position bias and the externalities, and based on this hypothesis we further propose the temporal click model, a bayesian framework that is scalable and computationally efficient. to the best of our knowledge, this is the first attempt in the literature to estimate positional bias, externalities and unbiased user perceived ad quality from user click logs in a combined model. we finally evaluate the proposed model on two real datasets, each containing over million ad impressions obtained from a commercial search engine. the experimental results show that tcm outperforms two other competitive methods at click prediction. commercial web search engines typicallygenerate revenue bypresentingsponsored results as well as organic web results to satisfy a user query. the most commonly employed payment model is pay per click, where an advertiser pays the search engine when a user clicks ontheir ad. the cost of a clickdepends on thequality andbid of competing ads, and is usuallydeterminedby a secondprice auction. marketsbybidding only onthesearch terms interesting to them, to explore di erent markets with minimal risk, and toiterate andimprovequickly their campaignsby using immediate feedback from performance. the combination of these features makes sponsored search one of the most attractive andpro table advertising approaches. the objective of the search enginein the sponsored search modelisto maximizeitsrevenueoverthelong term. this involves adelicatebalance ofpossibly con icting objectives: maximize the revenue per search, minimize thenegativeimpact of adsonthe user experience, and maximize the return on investment for advertisers. estimating the probability that users click on ads displayed in response to their queries is essential to sponsored search, because accurate predictions are necessary to address the objectives above. in particular, the click probability is a factor in ranking, placement, ltering, and pricing of ads. several studies have been published recently analyzing user clickbehaviorin organic web search andin sponsored search advertising. joachims et al conducted an eye tracking experiment to understandthedecision makingprocessofuserswhenbrowsing search engine results. an important nding of this study, now well known as position bias, is that users tend to click less on documents that are shown in lower positions, even when the results were presented in reverse order. to explain the position bias phenomenon, richardson et al proposed the examination hypothesis, which assumes that a result mustbe examinedbeforebeing clicked, and theprobability of being clicked after being examined depends on its user perceived quality. craswell et al later proposed the cascade hypothesis, which assumes that users always examine results in order from top to bottom. under this assumption, results displayed at the top are more likely to be examined than results shown at the bottom, regardless oftheirquality. thecascade modelproposedin makes another strong assumption: the user session ends after the rst click on a result. the right four graphs are statistical analyses for sad. most of the subsequent research on click modeling for search results have focused on relaxing this assumption. the click chain model and the bayesian browsing model both allow the user session to continue with a probability that is dependent on the relevance of the clicked ad. the dynamic bayesian network model proposed by chapelle and zhang alsolets the user session continue after a click, butin their model the probability of continuing the session depends on the quality of the landing page. their model is the rst to separate the perceived relevance of the ads from the actual quality of the landing page. the click models discussed so far do not account for the attractivenessor relevanceof the resultsbelowwhenconsidering the probability of click on a particular ad. in guo andchapelle models, theprobability of clickforthe results might be. ected by the results shown immediately above them, since the examination probability depends on the previous result quality. however, the click probabilities cannot be in uenced by the results presented below. this can be a signi cant limitation, especially in sponsored search, where the advertisers compete andpayforthe userattention. ect of the set of ads on the user behavior is widely accepted and referred to as externalities. ghosh et al proposed the rationality hypothesis to explain this behavior. under their hypothesis users are assumed to be rational. given a set of ads displayed, a user rst compares the qualities of the ads and clicks on the best one. kempe and mahdian later proposed a variation of the cascade model based on this hypothesis. instead of using the top to bottom order of scanning the ads, their model allows each user to have a di erent ordering over the positions. unfortunately both of these models were used only in the context of analyzing the auction mechanisms, therefore there are no results on the accuracy of the click models proposed, or even analysis to show that the externalities exist in the data. in this paper, our objective is to rst show that externalities are indeed present in the sponsored search domain and then investigate how to explain both position bias and externalities in a combined model. we exploit the temporal order of useractions for this analysis. our contributions can be summarized in three points. first, we present a statistical analysis demonstrating that temporal user click sequences are good indicators of ad externalities. second, we propose the positional rationality hypothesis which explains both position bias and externalities. third, based on this hypothesis we develop the temporal click model, which has the properties: foundation: to the best of our knowledge, this is the rst attempt in the literature to estimate positional bias, externalities and unbiased userperceived adquality fromuserclicklogsinacombinedmodel; framework: tcm is based on a strict bayesian framework. closed form representations ofthe adquality and user behavior posteriors can be derived using this framework, making it scalable and computationally. cient to handle the challenges imposed by the voluminous click logs;ectiveness: tcm consistently outperforms two stateof the art models in a number of metrics at click prediction. this paper studies quality of human labels used to train search engines rankers. our specific focus is performance improvements obtained by using overlapping relevance labels, which is by collecting multiple human judgments for each training sample. the paper explores whether, when, and for which samples one should obtain overlapping training labels, as well as how many labels per sample are needed. the proposed selective labeling scheme collects additional labels only for a subset of training samples, specifically for those that are labeled relevant by a judge. our experiments show that this labeling scheme improves the ndcg of two web search rankers on several real world test sets, with a low labeling overhead of around labels per sample. this labeling scheme also outperforms several methods of using overlapping labels, such as simpleoverlap, majority vote, the highest labels, etc. finally, the paper presents a study of how many overlapping labels are needed to get the best improvement in retrieval accuracy. the retrieval accuracy of a learned model depends both on the quality of the training labels and on the amount of training examples. as expected, the higher the quality of the training labels, and the more the training examples, the better the accuracy of the learned model. a large set of training data is commonly used to improve a modelretrieval accuracy. recently, however, researchers found that the improvement of the retrieval accuracy of a learned model stops after the number of training examples reaches a certain threshold. when more training examples are not able to further improve a modelaccuracy, improving the quality of labels is a solution. collecting high quality labels is a challenging task. label quality permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. depends both on the expertise of the labelers and on the number of labelers. for a given training sample, the more expert the labelers, and the more labelers, the higher the label quality. therefore, the labels with the best quality should result from obtaining overlapping labels from multiple expert labelers. however, obtaining overlapping labels from multiple experts is expensive. one alternative is to obtain overlapping labels from nonexperts, for example, labelers from amazonmechanical turk, which is an online labor market where workers are paid small amounts of money to complete human intelligence tasks. there is ongoing research on how to effectively use services such as mturk to obtain more labels. unfortunately, labels from non experts are often unreliable; and such unreliable labels decrease the retrieval accuracy of a learned model. another alternative to obtaining overlapping labels from experts is collecting one label from one expert for each sample. this single labeling scheme is affordable in general, and is widely used in supervised learning. however, since only one expert is involved in determining the relevance of a sample, the single expertopinion may contain a personal bias, which may introduce noise, and thus possibly interfere with the learning of the ranking model. for example, the web document at www svmsolutions com may be labeled as good for the query svm by a given expert, but the same expert may label the document www svmlight joachims org as bad if he is not an expert in machine learning. hence, the single labeling scheme may create unreliable labels because not every judge is an expert for every query. figure # depicts agreements between aggregated overlapping labels and the ground truth, and agreement between the best judge and the ground truth, for web queries and urls. figure # shows that when the number of overlapping labels is greater than, the aggregated labels achieve better quality than even the best single judge does. this motivates the use of overlapping labels instead of a single expert judge. figure #: aggregated overlapping labels are better than best judge. in this paper, we present a new scheme of collecting high quality labels at low cost. in particular, the paper focuses on how to cheaply and effectively produce and employ overlapping labels from multiple experts to improve web search accuracy. the proposed labeling scheme requests additional labels from more experts only when a sample is labeled as relevant by one expert; otherwise, only the single label from that expert is used. our experiments show that this selective labeling scheme improves web search accuracy, which is measured in ndcg, of both lambdarank and lambdamart rankers on several real world web test sets, with a low labeling overhead of around labels per sample. the proposed new labeling scheme also outperforms several methods of using overlapping labels, such as majority vote, overlap labels, the highest labels, etc. furthermore, our paper also describes how many additional labels are needed to get the best improvement in retrieval accuracy. although this paper focuses on the task of web search, the techniques presented can be generally applied to many research areas, such as computational linguistics, where manual labels are useful for training and evaluation. first, the paper explores whether, when, and for which samples one should obtain overlapping training labels, as well as how many overlapping labels are needed. second, the proposed if goodscheme creates high quality labels at low cost, which makes the approach promising for application to search engine training or other supervised training applications. the remainder of the paper is organized as the follows. section # introduces the web search task and the label distributions of multiple experts. section # discusses issues of using overlapping labels. section # shows the experimental results and compares our approach to other commonly used overlapping labeling schemes. section # discusses the proposed scheme, and section # concludes the paper. leveraging clickthrough data has become a popular approach for evaluating and optimizing information retrieval systems. although data is plentiful, one must take care when interpreting clicks, since user behavior can be affected by various sources of presentation bias. while the issue of position bias in clickthrough data has been the topic of much study, other presentation bias effects have received comparatively little attention. for instance, since users must decide whether to click on a result based on its summary, one might expect clicks to favor more attractive results. in this paper, we examine result summary attractiveness as a potential source of presentation bias. our experiments conducted on the google web search engine show substantial evidence of presentation bias in clicks towards results with more attractive titles. this study distinguishes itself from prior work by aiming to detect systematic biases in click behavior due to attractive summaries inflating perceived relevance. evaluating the quality of search result rankings has traditionally relied on explicit human judgments or editorial labels. ective, the prohibitive cost of acquiring human judgments makes it di cult to apply these evaluation approaches at scale for large search services such as commercial search engines. it is also impractical to acquire manually labeled query document relevance judgments for every possible retrieval domain, such as medical, law, physics, etc. as this study was conducted while the rst author was an intern at google. copyright is held by the international world wide web conference committee. such, collecting implicit user feedback typically click logs has grown tremendously in popularity in recent years. but what can we interpret from clicks it is well known that users are biased towards clicking on higher ranked results this is the so called position bias. but users must also judge relevance based on summaries rather than the actual results themselves. summaries typically include titles, urls, and query dependent abstracts, and often have matching query terms highlighted using boldface font. as a simple thought experiment, consider two equally relevant results with one having more bolded query terms in the title. intuitively, we might expect click behavior to favor the more attractive title. thus, even in the absence of position bias, a resultperceived relevance might noticeably di er from its actual relevance. a particularly illuminating study by clarke et al found that click inversions cannot be entirely explained by the lower ranked document being more relevant. they found that click inversions tend to co occur with additional factors such as lower ranked documents having comparatively more matching query terms in the titles. ect of bolded keyword matches in the title and abstracts on the attractiveness of the result. our analysis controls for both position bias and rated relevance, and is based on data collected using a portion of search tra. to control for position bias, we collected data using the fairpairs algorithm, which allows us to interpret clicks as preference judgments between two documents. our ndings show that clicks are measurably biased by attractive titles in ating perceived relevance. we will also discuss possible ways to adjust for title attractiveness bias. for the rest of this paper, we proceed by rst overviewing related work. section # describes our data collection methodology, which includes collecting both clickthrough data and explicit human judgments. section # describes our analysis on measuring attractiveness bias. we then discuss ways to adjust for bias in section #, and conclude with a discussion of limitations and avenues for future work. distribution of these papers is limited to classroom use, and personal use by others. www year#, april, year#, raleigh, north carolina, usa. to control for quality, we gathered human preference judgments for a subset of our clickthrough data.