in this paper, we propose a novel context aware query suggestion approach which is in two steps. in the offine model learning step, to address data sparseness, queries are summarized into concepts by clustering a click through bipartite. then, from session data a concept sequence suffix tree is constructed as the query suggestion model. query suggestion plays an important role in improving the usability of search engines. although some recently proposed methods can make meaningful query suggestions by mining query patterns from search logs, none of them are context aware they do not take into account the immediately preceding queries as context in query suggestion. in the online query suggestion step, a user search context is captured by mapping the query sequence submitted by the user to a sequence of concepts. by looking up the context in the concept sequence sufix tree, our approach suggests queries to the user in a context aware manner. we test our approach on a large scale search log of a commercial search engine containing billion search queries, billion clicks, and million query sessions. the experimental results clearly show that our approach outperforms two baseline methods in both coverage and quality of suggestions. ectiveness of information retrieval from the web largely depends on whether users can issue queries to search engines, which properly describe their information needs. writing queries is never easy, because usually queries are short and words are ambiguous. therefore, there is no standard or optimal way to issue queries to search engines, and it is well recognized that query formulation is a bottleneck issue in the usability of search engines. that is, by guessing a usersearch intent, a search engine suggests queries which may better re ect the userinformation need. it is hard to determine the usersearch intent, ie, whether the user is interested in the history of gladiator, famous gladiators, or the lm gladiator. moreover, the user is probably searching the lms played by russell crowe. in this paper, we propose a novel context aware query suggestion approach by mining click through data and session data. first, instead of mining patterns of individual queries which may be sparse, we summarize queries into concepts. to tackle these challenges, we develop a novel, highly scalable yet. we develop a novel structure of concept sequence su. third, we empirically study a large scale search log containing billion search queries, billion clicks, and million query sessions. we explore several interesting properties of the click through bipartite and illustrate several important statistics of the session data. the clustering algorithm and the query suggestion method are described in sections and, respectively. to make the problem even more complicated, di erent search engines may respond di erently to the same query. recently, most commercial search engines such as google, yahoo, live search, ask, and baidu provide query suggestions to improve usability. a commonly used query suggestion method is to nd similar queries in search logs and use those queries as suggestions for each other. another approach mines pairs of queries which are adjacent or co occur in the same query sessions. although the existing methods may suggest good queries in some cases, none of them are context aware they do not take into account the immediately preceding queries as context in query suggestion. without looking at the context of search, the existing methods often suggest many queries for various possible intents, and thus may have a low accuracy in query suggestion. if we nd that the user submits a query beautiful mind before gladiator, it is very likely that the user is interested in the lm gladiator. the query context which consists of the recent queries issued by the user can help to better understand the usersearch intent and enable us to make more meaningful suggestions. a concept is a group of similar queries. although mining concepts of queries can be reduced to a clustering problem on a bipartite graph, the very large data size and the curse of dimensionality pose great challenges. we may have millions of unique queries involving millions of unique urls, which may result in hundreds of thousands of concepts. second, there are often a huge number of patterns that can be used for query suggestion. how to mine those patterns and organize them properly for fast query suggestion is far from trivial. the data set in this study is several magnitudes larger than those reported in previous work. last, we test our query suggestion approach on the search log. the experimental results clearly show that our approach outperforms two baseline methods in both coverage and quality of suggestions. the rest of the paper is organized as follows. we rst present the framework of our approach in section # and review the related work in section #. we report an empirical study in section #. the open directory project is clearly one of the largest collaborative efforts to manually annotate web pages. this effort involves over, editors and resulted in metadata specifying topic and importance for more than million web pages. still, given that this number is just about percent of the web pages indexed by google, is this effort enough to make a difference in this paper we discuss how these metadata can be exploited to achieve high quality personalized web search. first, we address this by introducing an additional criterion for web page ranking, namely the distance between a user profile defined using odp topics and the sets of odp topics covered by each url returned in regular web search. we empirically show that this enhancement yields better results than current web search using google. then, in the second part of the paper, we investigate the boundaries of biasing pagerank on subtopics of the odp in order to automatically extend these metadata to the whole web. probably almost everybody is equally convinced that we will not be able to manually annotate all web pages. this is one of the largest efforts to manually annotate web pages, exporting all this metadata information in rdf format. everyone working in the context of the semantic web is convinced of the utility of metadata describing the content and various other interesting properties of web pages and relationships between them. but do we really need to this paper focuses on manually entered metadata expressing topical categorizations of web pages, as well as on the importance of these pages. this kind of metadata was one of the rst metadata available on the web in signi cant quantities, because it is useful to provide hierarchically structured access to high quality content on the web, starting with efforts like the yahoo directory, collected and put together by a group of human editors. by inserting a web page into one or more categories, basically a content classi cation is annotated to the document. most notable is the annotation categorization done in the context of the open directory project. over, editors are busy keeping the directory reasonably up to date, and the odp now provides access to over million web pages in the odp catalog. still, given the fact that google now indexes more than billion pages, the odp effort still only covers about percent of the web pages indexed by google. so does search using these metadata stand any chance against google one good use these metadata can be put to is to personalize search, ie, returning search results which are both relevant to the user pro le, as well as of good quality. this paper investigates the possibilities we have for building such a personalized search engine based on odp or similar directory metadata and investigates the quality and effectiveness of such personalization. speci cally, this paper investigates two ways to personalize search and makes the following contributions: first, using odp entries directly, we show how to generalize personalized search in catalogs such as odp and google directory beyond the currently available search restricted to speci. the precision of this personalized search signi cantly surpassed the precision offered by google in a set of experiments on topic related searches. second, extending the manual odp classi cations from its cur rent million entries to a billion web in an automated way is fea pagerank and personalized pagerank sible, based on an analysis of how topic classi cations for a small but important subset of a large page collection can be extended to this large collection via topic sensitive biasing of pagerank values. this generalizes earlier approaches which already investigated topic sensitive page ranks, but relied on very simple classi cations using only topics. the paper is organized as follows: in section # we will give a short overview of the open directory project, as well as of page rank and personalized pagerank as relevant algorithms for this paper. in section # we discuss how we can directly use odp and google directory entries to implement personalized search based on user pro les corresponding to topic vectors from the odp hierarchy, and discuss a user study comparing google and odp search with these personalized versions. section # builds on the idea that sets of odp or other directory entries can be used to bias pagerank appropriately, and thus to implicitly extend such annotations to the rest of the web. we speci cally investigate when biasing on such a set actually makes a difference to non biased pagerank, presenting experiments with various kinds of biasing sets. we then use these results to analyze biasing sets from the odp year# crawl used in and show that all biasing sets we investigated can be successfully used for biasing. search engines can record which documents were clicked for which query, and use these query document pairs as soft relevance judgments. however, compared to the true judgments, click logs give noisy and sparse relevance information. we apply a markov random walk model to a large click log, producing a probabilistic ranking of documents for a given query. a key advantage of the model is its ability to retrieve relevant documents that have not yet been clicked for that query and rank those effectively. we conduct experiments on click logs from image search, comparing our random walk model to a different random walk, varying parameters such as walk length and self transition probability. the most effective combination is a long backward walk with high self transition probability. a search engine can track which of its search results were clicked for which query. for a popular system, these click records can amount to millions of query document pairs per day. each pair can be viewed as a weak indication of relevance: that the user decided to at least view the document, based on its description in the search results. although clicks are not real judgments, there is evidence that they are useful, for example as training data, as annotations, for query suggestion or directly as evidence for ranking. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. we can use the clicks of past users to improve the current search results. however, the clicked set of documents is likely to di er from the current userrelevant set. other di erences are due to presentation issues; for example, the user must decide whether to click based on a short summary and is in uenced by the ordering of results. for any given search, a large number of documents are never seen by the user, therefore not clicked. from the perspective of a user conducting a search, documents that are clicked but not relevant constitute noise in the click data. documents that are relevant but not clicked constitute sparsity in the click data. one class of approaches attempts to reduce noise in click data, by building a click model that may use additional information about the userbehaviour. these approaches can signi cantly reduce noise, by identifying some clicked documents as irrelevant. this paper focuses on the sparsity problem, although our model also has noise reduction properties. the model gives a probabilistic ranking of documents, which includes relevant documents that have not yet been clicked for the current query. the sparsity problem is evidenced by power law distributions observed in click logs. most queries in the click log have a small number of clicked documents. in such cases, it is useful to identify additional relevant documents. we rst describe the click information as a graph, and survey a range of click graph applications. then we detail our markov random walk model for nding relevant documents. the subsequent sections describe a real click dataset, and empirical evaluation of the new methods. some differences arise because we are aggregating clicks across users, who may simply disagree about which documents are relevant. for example, taking into account the userbrowsing patterns after clicking a document. then, personalization is achieved by re ranking the search results of related queries using the user profile. within the information overload on the web and the diversity of the user interests, it is increasingly difficult for search engines to satisfy the user information needs. personalized search tackles this problem by considering the user profile during the search. this paper describes a personalized search approach involving a semantic graph based user profile issued from ontology. user profile refers to the user interest in a specific search session defined as a sequence of related queries. it is built using a score propagation that activates a set of semantically related concepts and maintained in the same search session using a graph based merging scheme. we also define a session boundary recognition mechanism based on tracking changes in the dominant concepts held by the user profile relatively to a new submitted query using the kendall rank correlation measure. our experimental evaluation is carried out using the hard year# trec collection and shows that our approach is effective. for example, the keyword query python may refer to python as a snake as well as the python programming language. personalized ir has become a promising area for disambiguating the web search and therefore improving retrieval. the ucair system identi es session boundaries using a semantic similarity measure between successive queries using term relations. indeed, ontology provides a highly expressive ground for describing user interests and a rich variety of interrelations among them and allows encountering new topics of interests. in this paper, we describe a personalized search approach that represents the user pro le as a weighted graph of semantically related concepts of prede ned ontology, namely the odp. second, we build a single user interest across related queries using a session boundary recognition mechanism based on a topical dependant similarity measure. the experimental evaluation and results are presented in section #. most existing search engines are classical content based systems characterized by one size ts all approaches, where the information retrieval process is based on the query document matching pattern. such pattern provides the same results for the same keyword queries even though these latter are submitted by different users with di erent intentions. ectiveness by modeling the user pro le by his interests and preferences. user pro le could be inferred from the whole search history to model long term user interests or from the recent search history to model short term user interests. mining short term user interests in a personalized retrieval task requires a session boundary mechanism that allows grouping related queries together. concerning the representation model, user interests could be represented as a set of keyword vectors or class vectors, a set of concepts or an instance of prede ned ontology. the user pro le is built by accumulating graph based query pro les in the same search session. we de ne also a session boundary recognition mechanism that allows using the appropriate user pro le to re rank search results of queries allocated in the same search session. unlike previously cited work, our approach has several new features. first, the user pro le is represented as a http: www dmoz org graph of the most relevant concepts of ontology in a speci. search session and not as an instance of the entire ontology. this allows using the most suitable user interest to personalize the search. the user interest is built for a single query in where the issue of related queries is not tackled in real web environment. the remaining of this paper is organized as follows. in section #, we describe our approach for building and maintaining the ontological user pro le and setting a session boundary recognition mechanism. in section #, we present our search personalization method. in the nal section, we present our conclusion and plan for future work. although personalized search has been proposed for many years and many personalization strategies have been investigated, it is still unclear whether personalization is consistently effective on different queries for different users, and under different search contexts. we present a large scale evaluation framework for personalized search based on query logs, and then evaluate five personalized search strategies using day msn query logs. by analyzing the results, we reveal that personalized search has significant improvement over common web search on some queries but it also has little effect on other queries. it even harms search accuracy under some situations. furthermore, we show that straightforward click based personalization strategies perform consistently and considerably well, while profile based ones are unstable in our experiments. in this paper, we study this problem and get some preliminary conclusions. we also reveal that both long term and short term contexts are very important in improving search performance for profile based personalized search strategies. incorporating features extracted from clickthrough data has been demonstrated to significantly improve the performance of ranking models for web search applications. such benefits, however, are severely limited by the data sparseness problem, ie, many queries and documents have no or very few clicks. the ranker thus cannot rely strongly on clickthrough features for document ranking. this paper presents two smoothing methods to expand clickthrough data: query clustering via random walk on click graphs and a discounting method inspired by the good turing estimator. both methods are evaluated on real world data in three web search domains. experimental results show that the ranking models trained on smoothed clickthrough features consistently outperform those trained on unsmoothed features. this study demonstrates both the importance and the benefits of dealing with the sparseness problem in clickthrough data. we consider the task of ranking web search results, ie, a set of retrieved web documents are ordered by relevance to a query issued by a user. in this paper we assume that the task is performed using a ranking model that is learned on labeled training data, ie, human judged querydocument pairs. the ranking model is a function that maps the feature vector of a query document pair to a real valued relevance score. such a learned ranking model is shown to be superior to classical retrieval models largely due to its ability to integrate both traditional criteria such as tf idf and bm values, and non traditional features such as hyperlinks. in general web search, a document can be described by multiple text streams. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. however, clickthrough data typically suffer from the sparseness problem. first, for a query, users only click on a very limited number of documents, thus the clicks are not complete. we refer to it as the incomplete click problem. second, for many queries and documents, no click at all is made by users. as a consequence, the clickthrough streams for most of documents are either short or empty. although one can use such raw text streams to extract some clickthrough features as in previous studies, their potential is severely limited because of the following reasons: first, with incomplete clicks, the clickrelated features that we can generate for a document query pair are also incomplete and unreliable. second, no clickthrough features can be generated for pairs without clicks. in the rankers used in most previous studies, this is equivalent to assigning zero values for clickthrough features. in ranker training, the zero valued features make a categorical difference between the documents with and without clicks, and severely penalize the documents without clicks. however, in reality, wkh wuxh gliihrence between these documents may be much smaller because a document could be unclicked for a variety of reasons even if the document is relevant. the missing click problem bears a strong resemblance to the problem of determining the frequency or probability of an unseen event, which has been well studied in the context of estimating ngram language models. various smoothing techniques have been proposed and successfully used to deal with this problem, including clustering and discounting. in the case of clickthrough data, we can consider a click for a document query pair as angram. then clickthrough data can also be smoothed in two directions: by clustering similar queries or by assigning non zero values to the clickthrough features of unclicked documents through discounting. in this paper, we propose to perform query clustering via random walk on click graphs, and a discounting method inspired by the good turing estimator. the random walk method is intended to address the incomplete click problem. in some particular settings, such as image retrieval and query classification, it has been shown that expanding clicks to similar documents and queries via random walk can lead to significant improvements. however, to our knowledge, no study has been carried out on general web search applications showing a similar improvement. our experiments show that the expanded clickthrough data is noisy, and it should be used with caution. effective improvement is possible only when we extract those features that are robust to noise for. message web design home www message uk com. msn web messenger webmessenger msn com year#: high school baseball web www hsbaseballweb come message boards htm. sprintpcs way sms messaging sprintpcs com sml guestcompose do. yahoo messenger chat, instant message messenger yahoo com year#: yahoo message boards home messages yahoo com figure #. kh txhu\ vhvvlrq iru wkh txhu\ zhe phvvdjh marked in bold are the links the user clicked on. notice that documents and queries with no click cannot be enriched through random walk. thus, inspired by the good turing method, we present a discounting method to estimate the values of the clickthrough features for the documents without clicks. our experiments will show that both smoothing techniques can significantly improve the retrieval effectiveness compared to the utilization of raw clickthrough data. in particular, the simple discounting method will prove to be effective on all the three test datasets. this series of experiments strongly indicate that sparseness is a crucial problem in clickthrough data, and an appropriate solution to this problem allows us to better take advantage of clickthrough data. in the rest of the paper, section # describes background information on clickthrough data and rankers. related work and conclusions are presented in sections and. send a wireless web message messaging sprintpcs com. we present a personalization approach that builds a user interest profile using users complete browsing behavior, then uses this model to rerank web results. personalizing web search results has long been recognized as an avenue to greatly improve the search experience. we show that using a combination of content and previously visited websites provides effective personalization. we extend previous work by proposing a number of techniques for filtering previously viewed content that greatly improve the user model used for personalization. our approaches are compared to previous work in offline experiments and are evaluated against unpersonalized web search in large scale online tests. additionally, previous research has noted that the vast majority of search queries are short and ambiguous. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. second, we improve upon the evaluation methodology, by performing the rst large online comparative evaluation of personalization strategies. second, most previous work on search personalization has involved an evaluation using either a small number of users evaluating the relevance of documents for a small set of search queries not representative of a real workload, the trec query and document collection, and simulating a personalized search setting, or an after the fact log based analysis. in this work, we start by using document judgments obtained from a small number of users for queries to assess potential approaches. we then describe our evaluation approach in detail, with results from our. ine evaluation in section #, and online evaluation in section #. clearly, di erent users would prefer di erent results. often, di erent users consider the same query to mean di erent things. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. two alternatives are: ask users to label documents as more personally relevant or not, and infer personal relevance automatically. this data was collected using a firefox add on created for this purpose. the key di erence from previous work in the pro les we construct is that we parse web page structure, using term extraction and part of speech tagging to extract noun phrases to re ne the user model. we show that this yields signi cant retrieval improvements over web search and other personalization methods, without requiring any. ort on the userpart, and without changing the usersearch environment. improvements found using these methods do not necessarily translate to actual improvements in user search experience on a real query workload. we use an interleaving evaluation approach, that has been shown to accurately re ect di erences in ranking relevance. this can be illustrated with the search query ajax. ort from users, we opt for the latter. although web search has become an essential part of our lives, there is still room for improvement. in particular, a major de ciency of current retrieval systems is that they are not adaptive enough to users individual needs and interests. this query will return results about ajax based web development, about the dutch football team ajax amsterdam, and websites about the cleaning product ajax. personalized search is a potential solution to all these problems. personalizing web search has received a lot of attention by the research community. we improve upon this work in two key ways: first, we build an improved user pro le for personalizing web search results. to successfully personalize search results, it is essential to be able to identify what types of results are relevant to users. in particular, the content of all the web pages visited by users, along with the users particular behavior on web search results, is used to build a user model. the pro le constructed is then used to rerank the top search results returned by a non personalized web search engine. we then select three methods for complete online evaluation, with our personalized search system being used by users for two months to issue thousands of queries as part of their day to day web search activities. after reviewing related work next, we give an overview of the user pro le generation and re ranking strategies investigated in section #. a major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored. in this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting. we propose several context sensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents. we use the trec ap data to create a test collection with search context information, and quantitatively evaluate our models using this test set. experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially. we de ne implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results. in most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents. from a single query, however, the retrieval system can only have very limited clue about the userinformation need. an optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available. indeed, context sensitive retrieval has been identi ed as a major challenge in information retrieval research. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. there are many kinds of context that we can exploit. relevance feedback can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy. however, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents. since it forces the user to engage additional activities while the bene ts are not always obvious to the user, a user is often reluctant to provide such feedback information. thus the effectiveness of relevance feedback may be limited in real applications. for this reason, implicit feedback has attracted much attention recently. in general, the retrieval results using the userinitial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval ranking accuracy. for a complex or dif cult information need, the user may need to modify his her query and view ranked documents with many iterations before the information need is completely satis ed. in such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document. a major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort. for example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the java programming language or the java island in indonesia. as a result, the retrieved documents will likely have both kinds of documents some may be about the programming language and some may be about the island. however, any particular user is unlikely searching for both types of documents. such an ambiguity can be resolved by exploiting history information. for example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for. implicit feedback was studied in several previous works. in, joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people. in, a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated. in, some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user. other related work on using context includes personalized search, query log analysis, context factors, and implicit queries. while the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context sensitive language models for retrieval. speci cally, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy. we use the kl divergence retrieval model as the basis and propose to treat context sensitive retrieval as estimating a query language model based on the current query and any search context information. we propose several statistical language models to incorporate query and clickthrough history into the kl divergence model. one challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation. we thus use the trec ap data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models. to the best of our knowledge, this is the rst test set for implicit feedback. we evaluate the proposed models using this data set. the experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user. in section #, we attempt to de ne the problem of implicit feedback and introduce some terms that we will use later. in section #, we propose several implicit feedback models based on statistical language models. in section #, we describe how we create the data set for implicit feedback experiments. in section #, we evaluate different implicit feedback models on the created data set. section # is our conclusions and future work. in particular, we build user profiles based on activity at the search site itself and study the use of these profiles to provide personalized search results. these profiles were then used to re rank the search results and the rank order of the user examined results before and after re ranking were compared. user profiles, descriptions of user interests, can be used by search engines to provide personalized search results. many approaches to creating user profiles collect user information through proxy servers or desktop bots. both these techniques require participation of the user to install the proxy server or the bot. in this study, we explore the use of a less invasive means of gathering user information for personalized search. by implementing a wrapper around the google search engine, we were able to collect information about individual user search activities. in particular, we collected the queries for which at least one search result was examined, and the snippets for each examined result. user profiles were created by classifying the collected information into concepts in a reference concept hierarchy. our study found that user profiles based on queries were as effective as those based on snippets. we also found that our personalized re ranking resulted in a improvement in the rank order of the user selected results. explicit customization has been widely used to personalize the look and content of many web sites, but we concentrate on personalized search approaches that focus on implicitly building and exploiting user profiles. this information could be used to narrow down the number of topics considered when retrieving the results, increasing the likelihood of including the most interesting results from the user perspective. companies that provide marketing data report that search engines are utilized more and more as referrals to web sites, rather than direct navigation via web links. as search engines perform a larger role in commercial applications, the desire to increase their effectiveness grows. however, search engines order their results based on the small amount of information available in the user queries and by web site popularity, rather than individual user interests. thus, all users see the same results for the same query, even if they have wildly different interests and backgrounds. to address this issue, interest in susan gauch electrical engineering and computer science university of kansas lawrence, ks sgauch ku edu personalized search had grown in the last several years, and user profile construction is an important component of any personalization system. another issue facing search engines is that natural language queries are inherently ambiguous. for example, consider a user issuing the query canon book. due to the ambiguity of the query terms, we will obtain results that are either related to religion or photography. according to an analysis of their log file data conducted by onestat com over a month period of time, the most common query length submitted to a search engine was only two words long and of all queries were three words long or less. these short queries are often ambiguous, providing little information to a search engine on which to base its selection of the most relevant web pages among millions. a user profile that represents the interests of a specific user can be used to supplement information about the search that, currently, is represented only by the query itself. for the user in our example, if we knew that she had a strong interest in photography but little or none in religion, the photography related results could be preferentially presented to the user. many approaches create user profiles by capturing browsing histories through proxy servers or desktop activities through the installation of bots on a personal computer. these require the participation of the user in order to install the proxy server or the bot. in this study, we explore the use of a less invasive means of gathering user information for personalized search. our goal is to show that user profiles can be implicitly created out of short phrases such as queries and snippets collected by the search engine itself. we demonstrate that profiles created from this information can be used to identify, and promote, relevant results for individual users. long term search history contains rich information about a user search preferences, which can be used as search context to improve retrieval performance. in this paper, we study statistical language modeling based methods to mine contextual information from long term search history and exploit it for a more accurate estimate of the query language model. experiments on real web search data show that the algorithms are effective in improving search accuracy for both fresh and recurring queries. the best performance is achieved when using clickthrough data of past searches that are related to the current query. first, a userbackground and interests can usually be learned from his her search history by looking at the topics covered by the past queries. in the web search domain, user search history can be obtained by a proxy from web logs, or by a search engine using http redirects. short term search history is limited to a single search session, which contains a sequence of searches with a coherent information need and usually spans a short period of time. unfortunately, most existing studies on longterm search context fail to address this problem, even though they still tend to get positive results; they often use all available context as a whole, without distinguishing between relevant and irrelevant parts. in section #, we develop several algorithms based on statistical language models to mine long term search history. most existing retrieval systems, including the web search engines, suffer from the problem of one size ts all: the decision of which documents to return is made based only on the query, without consideration of a particular userpreferences and search context. when a query is ambiguous, the search results are inevitably mixed in content, which is certainly nonoptimal for the user, who is burdened by the need to sift through the mixed results. therefore, instead of relying solely on the query, which is usually just a few keywords, retrieval systems should exploit the usersearch context, which can reveal more about the usertrue information need. indeed, contextual retrieval has been identi ed as a major challenge in information retrieval research. there are a wide variety of search contexts, from the userbackground and interests, personal document collection, to what activities the user is doing before submitting the query. in this paper, we focus on the usersearch history, which is often kept in log format and records what queries the user made in the past and what results he she chose to view. this is arguably the most important form of search context for the reasons below. for example, if there were many queries such as debugging and cgi code, the user is probably interested in programming and python is likely to mean the programming language. second, from the userpast indication of document relevance we can predict his her reaction to the current retrieved documents. for example, if the user searched with the same query python before and clicked on python language websitelink, we have high con dence that the user would do it again this time, and it makes good sense to list that webpage in the top. even when there is no exact occurrence of the current query in history, we may still nd similar queries like python doc helpful. because the relevance judgment is usually only inferred from user activities, this belongs to the category of implicit feedback, which has been studied in and shown to effectively improve retrieval performance. finally, search history is readily available without extra user efforts. if privacy is a concern, we can use browser plugins and perform result reranking at the client side. search history can be divided into short term and long term types. often, a user composes an initial query, views the returned documents, and if unsatis ed, modi es the query and repeats the search process. all these activities, which form the short term search history, shed light on the current information need and make useful search context. as shown in, queries and clickthrough data in the short term search history provide implicit feedback that can be used to estimate a more accurate query language model and improve retrieval performance. long term search history is, in contrast, unlimited in time scope and may include all search activities in the past. compared with short term search history, it has several advantages. there is no need to detect session boundaries, which is often a dif cult task. nor do we need to limit the context to the contiguous chain of searches in a session; any search in the past that is related to the current one should be leveraged. this also means that we may nd context for the very rst query in a chain, which is impossible if the search history is constrained to be short term. although the extension from short term to long term seems natural and promising, the full potential of long term search history cannot be reached easily. this is because long term history inevitably involves a lot of noisy information that is irrelevant to the current search; only those searches that are related to the current one should be considered as useful context. for example, searches like world cup that are irrelevant to the current query python would not be helpful, and such noise can overwhelm the signal of related past searches. for this reason, when exploiting short term search history we need to detect session boundaries rst, so that only those searches with the same information need are used. such work includes, which interpolates the current query with different chunks of history for personalized search, and, which constructs user pro les from indexed desktop documents for search result reranking. in this paper, we systematically study how to exploit a userlong term search history to improve retrieval accuracy. we propose mixture models to represent a userinformation need and apply statistical language modeling techniques to discover relevant context from the search history, and exploit it to obtain improved estimates of the query model. we then evaluate the methods on a test set of web search histories collected from some real users. we nd that mined search history information, can substantially improve retrieval performance for both recurring and fresh queries, and works best when clickthrough data is used with a discriminative weighting scheme for past searches. we also nd that although recent history tends to be much more useful than remote history, all of the entire history is helpful for improving the search accuracy of recurring queries. the rest of the paper is organized as follows. in section #, we introduce a context sensitive information retrieval approach that allows us to incorporate contextual information mined from search history. in section #, we de ne the search history mining task and cast it as a query language model estimation problem. we describe how we build a test set by collecting users web search history in section # and present our experiment results in section #. we formulate and study search algorithms that consider a user prior interactions with a wide variety of content to personalize that user current web search. rather than relying on the unrealistic assumption that people will precisely specify their intent when searching, we pursue techniques that leverage implicit information about the user interests. this information is used to re rank web search results within a relevance feedback framework. we explore rich models of user interests, built from both search related information, such as previously issued queries and previously visited web pages, and other information about the user such as documents and email the user has read and created. our research suggests that rich representations of the user and the corpus are important for personalization, but that it is possible to approximate these representations and provide efficient client side algorithms for personalizing search. we show that such personalization algorithms can significantly improve on current web search. in most previous work on personalized search algorithms, the results for all queries are personalized in the same manner. however, as we show in this paper, there is a lot of variation across queries in the benefits that can be achieved through personalization. for some queries, everyone who issues the query is looking for the same thing. for other queries, different people want very different results even though they express their need in the same way. we examine variability in user intent using both explicit relevance judgments and large scale log analysis of user behavior patterns. while variation in user behavior is correlated with variation in explicit relevance judgments the same query, there are many other factors, such as result entropy, result quality, and task that can also affect the variation in behavior. we characterize queries using a variety of features of the query, the results returned for the query, and people interaction history with the query. using these features we build predictive models to identify queries that can benefit from personalization. a number of factors are important to consider when ranking web documents in response to a query. of primary importance is the topical relevance of each document, or how well each document matches the query, and much research in information retrieval has focused on addressing this problem. however, search on the web goes beyond ad hoc retrieval tasks based on topical relevance in several ways. peopleweb queries are short, varied, and include navigational and resource queries. there are often many more documents that match a web query than a searcher has time to view, and ranking becomes a problem not only of identifying topically relevant documents, but also of identifying those that are of particular interest to the searcher. fidel and crandall have shown that in addition to topic relevance, variables such as recency, genre, level of detail, and project relevance are important in determining relevance. algorithms like pagerank and hits take advantage of aggregate link information to get at some of these non content features. in addition, teevan et al have reported individual variation in what different people personally consider relevant to the same queries. these differences result in a large gap between how well search engines could perform if they personalized results for an individual, and how well they actually do perform by returning a single list designed to satisfy everyone. recent work on personalized search systems has focused on developing algorithms that personalize results using a representation of an individualinterests. in these systems, personalization is applied to all queries. however, as found by dou et al, personalization only improves the results for some queries, and can actually harm other queries. this can happen when unreliable personal information swamps the effects of aggregate group information that is based on considerably more information. aggregate information can be collected in large quantities for queries an individual has never issued before, and this may be particularly useful when different peopleintents for the same query are similar. on the other hand, when there is a lot of information available about what an individual it interested in related to a query, or when a query is very vague, it may make sense to focus primarily on the individual during ranking. in this paper, we first examine the variability in user intent for a large number of queries using both implicit and explicit measures. we study how well variation in the implicit measures predicts variation in the explicit measures, and look at what other factors can account for variation in the implicit measures. queries are characterized using a variety of features of the query, the results returned for the query, and the queryinteraction history. using these features we build predictive models to identify the queries that will benefit most from personalization, and explore which features are the most valuable for prediction. in this article, we report our efforts in mining the information encoded as clickthrough data in the server logs to evaluate and monitor the relevance ranking quality of a commercial web search engine. we describe a metric called pskip that aims to quantify the ranking quality by estimating the probability of users encountering non relevant results that cost them the efforts to read and skip. a search engine with a lower pskip is regarded as having a better ranking quality. a key design goal of pskip is to integrate the findings from two sets of user studies that utilize eye tracking devices to track users browsing patterns on the search result pages, and that use specially instrumented browsers to actively solicit users explicit judgments on their search activities. we present the derivation of the maximum likelihood estimation of pskip and demonstrate its efficacy in describing the user study data. the mathematical properties of pskip are further analyzed and compared with several objective metrics as well as the cumulated gain method that uses subjective judgments. experimental data show that pskip can measure aspects of the search quality that these existing metrics are not designed or fail to address, such as identifying the real search intents expressed in the ambiguous queries. although effective and superior in many ways, we also report a series of experiments that show pskip may be influenced by system issues that are not directly related to relevance ranking, suggesting that measurements complementary to pskip are still needed in order to form a holistic and accurate characterization of the ranking quality. in this decade we have witnessed the evolution of web search from an altruistic service into a highly lucrative business that changes not only how people find but also put information on the web. continuously improving the search quality is therefore a paramount goal for the service providers, and a key aspect of the search quality is the relevance of the search results. this paper describes our efforts for the past few years in evaluating and mon itoring the relevance ranking quality of a commercial web search service. our approach consists of two methods: one that utilizes explicit judgments from trained human assessors and the other, the clickthrough data as the implicit feedback from the users. for the first method we follow the cranfield methodology used in the nist text retrieval conference to evaluate informa tion retrieval systems on the web documents. as reviewed recently in, the methodology has three essential components: a collection of documents that forms the retrieval base, a set of statements, called topics, that represent the information needs of the users, and finally the relevance judgments carried out by hu man assessors that specify which documents in the collection should be retrieved for which topics. despite its wide adoption, aspects of the methodology, especially the implied assumptions made to facilitate the evaluations, have been the subject of con troversies, many of which are made more pronounced as ir is being applied to the web whose large scale has amplified issues that could otherwise be comfortably ignored. for instance, the evaluation framework for the web must be able to cope with a very large document collection whose size is unknown and con tents constantly changing. it has been pointed out that the conven tional relevance judgments and the accompanied pooling tech niques are not scalable with the size of the document collection, and effective methods to address the consequent incomplete and imperfect judgments remain an unsolved research problem. secondly, thanks to the global reach and the diverse user base of the web, a common practice of associating each query with a single relevance set of documents deserves a closer examination. users with different background may have drastically different assessments on the same document with respect to its relevance to a query. this is exacerbated by the reality that, for web search, the queries do not often make explicit the true search in tents of the users and the topics of the information needs that are the bedrock of the cranfield methodology. for example, docu ments describing a car model may be relevant to the query ma trix for interested car buyers, but it is hard to argue their relev ance to the users for which the car model is not even available in their markets. it stands to reason that one document meeting the information needs of a group of users might not be so to another. as data presented later in this article show, disagreements in re levance judgments are often a reflection of diverse intents being realized with the same query. if the information needs behind the queries remain elusive, how to properly execute relevance judg ments becomes an issue. finally, since the goal of ir is to address the userinformation needs, it appears straightforward to evaluate the effectiveness of a retrieval method and avoid misinterpreting the userintent is to directly ask the users if the search results are to their likings. the idea of relevance feedback or interactive relevance has been popu lar since the dawn of ir and has been applied to web search. one of the notable efforts is the curious browser expe riments that utilize specially instrumented browsers to actively solicit userexplicit feedback on their web search ac tivities. while the experimental data offer highly valuable obser vations, in our deployment attempts we have found that, in order to have meaningful interpretation, a large amount of data have to be collected due to the high variability in the user behaviors. finding enough users to participate in the curious browser expe riment is not only expensive but also involves many social and privacy concerns that cannot be easily overcome. in addition, the data collected from the instrumented experiments appear statisti cally different from the controlled environments, suggesting the users might have altered their behaviors when participating in the experiments. how to collect sizeable and unbiased data remains a challenging question. in contrast to cranfield or curious browser, web server log data are inexpensive to collect and easy to scale. the idea of using the server log data for web search has been of great interests in the community. a straightforward view is to regard the clickthrough data as users voting with their mouse clicks: the more clicks a link receives, the more likely the linked document is relevant to the query. however, since the server data do not capture userexplicit feedback, such a straightforward view has been found to be not suitable for all applications. in fact, it has been convincingly shown that the clicks are clearly biased, and proper ways to adjust for the click biases are critical in cor rectly use and interpret the search log data. in this paper, we describe pskip, a measurement designed to eva luate the relevance ranking of a commercial web search engine based on the log data. short for probability of skipping, pskip is motivated by an observation that the click biases seem to be re lated to and hence can be adjusted by considering the patterns of users reading the search result pages. since in the ideal case the more relevant documents should appear at the more prominent positions, users will perceive the search engine as having made a ranking error when they see a more relevant result being placed after a non relevant one that they have to skip. we can thus esti mate how often users have to read and skip the search results in order to find something relevant to them and use it as an indicator of relevance ranking quality. in other words, a lower pskip cor responds to a better perceived ranking quality. the design of pskip embraces two essential notions that non clicked results are not necessarily all bad, and the clicked re sults are not necessarily equally good. the first point is largely based on the eye tracking experiments that demonstrate users do not consume the search results in a uniform manner. specifi cally, results that are placed in a less prominent position are viewed less frequently. a non clicked result can therefore be attributed to it not being seen rather than not relevant, and consequently should not always be viewed negatively. the second point is motivated by the user studies on the implicit feedback and our analysis of the data ob tained through the curious browser experiments. in this paper, we zoom in on one specific aspect that explores the correla tions between the relevance of the document and the dwell time, ie, the time duration a user will spend in reading the document with respect to the query topic. the experimental data indicate that a short dwell time is a very effective predictor of a non rele vant document. this finding can be intuitively understood as users will return to the search result page more quickly when they click on a result and find the landing document not relevant. the rest of the paper is organized as follows. we review the user studies that underlie the design of pskip and the related work inspired by these experimental results. to our best know ledge, this work is the first effort that attempts to combine into a single model the findings of the two user experiments that are conducted with very different apparatus and design goals. we describe the derivation of the maximum likelihood estima tion of pskip in details. we show mathematically how pskip is related to widely known metrics such as the mean reciprocal rank, the clickthrough rate, and the expected search length. we also elaborate how pskip can be extended to general search sessions that receive multiple clicks, and explain why pskip is more focused on measuring the ranking quality and less affected by the length of the search session, a confounding factor that we take into account to avoid unfair penalties on the informational type of queries. a bayesian click quality predictor, used in pskip to probabilistically discount the clicks with short dwell time, is presented and its effectiveness validated by showing its ability in predicting the human labels in the cu rious browser data. compares pskip to the alternative, trec style evaluation approach that involves trained human assessors in making explicit relevance judgments. we show the two approaches do not always agree, and present data on the sources of disagreements. the analyses show that, while pskip can leverage the information from the server log effectively, there are factors other than the relevance ranking that can also affect pskip, and therefore complementary measures are still needed to ensure correct interpretation of the clickthrough data. understanding the extent to which people search behaviors differ in terms of the interaction flow and information targeted is important in designing interfaces to help world wide web users search more effectively. in this paper we describe a longitudinal log based study that investigated variability in people. interaction behavior when engaged in search related activities on the web allwe analyze the search interactions of more than two thousand volunteer users over a five month period, with the aim of characterizing differences in their interaction styles allthe findings of our study suggest that there are dramatic differences in variability in key aspects of the interaction within and between users, and within and between the search queries they submit allour findings also suggest two classes of extreme user. whose search interaction is highly consistent or highly variable. lessons learned from these users can inform the design of tools to support effective web search interactions for everyone. search has emerged as a key enabling technology to facilitate access to information for the general user population of the world wide web. everyday, millions of users submit millions of queries to commercial search engines such as google, yahoo, and windows live search. drucker microsoft live labs one microsoft way redmond, wa usa sdrucker microsoft com example, when we can model and identify consistent behavior, we have a chance to adapt user interfaces to take advantage of predicted behavior. through the research in areas such as information foraging, sensemaking, orienteering search interface design, and information visualization, the research community is at the forefront of developing search technology that serves a diverse range of purposes. however, large scale commercial search engines have not yet been able to effectively apply this rich and varied research, and still favor the traditional ranked list style of result presentation. in this paper we present a study of interaction behavior for users engaged in web search activities that originate with the submission of a query to a search engine. to better understand what users are doing when they are searching, we place a particular emphasis on post query navigation trails. through client side logging of, users over a five month period, we gathered sufficient interaction log data to perform a detailed analysis of variability in search behavior within and between users and within and between the query statements they issued. understanding variability given a user or a query has a range of implications in areas such as the design of search interfaces, predictive document retrieval, and user modeling. although there has been related research on examining user trails, studying browsing behavior within web sites, developing user and task models, and investigating individual differences in user behavior, this is the first study to focus explicitly on behavioral variability in web search. in this investigation we wanted to characterize differences in the interaction styles of users, and better understand just how different users in particular, we focus on search interactions actually are. two research questions: how variable are search interactions within each user and between all users and how variable areooscurane tm caeiwes te fits a ap rahterhrslpsnt, hrhae search interactions within each query and between all queries to search interface is shown to all users for each query they submit. there is good reason for this: users benefit from familiarity with the interface, and the cost on interface designers is minimized. however, as users perform more tasks using search engines, there is a growing need to understand more precisely what users are doing during the search process. it is only through this understanding that we will be able to build more effective interfaces to cater to mrues qisnsacigtlsfor oeruraehserydn. copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. answer these questions we analyze interaction log data for a large sample of users and a set of queries sampled from the logs. in this analysis we focus on interaction patterns, features of the interaction such as time and structure, and features of the information that users interact with, such as the web domain. as well as providing a better understanding of behavioral variability in web search, the answers motivate the creation of a tailored set of design recommendations for supporting the most and least variable users and queries that can be offered as optional interface functionality for all web searchers. the driving force behind this research is a desire to improve the web search experience for all users. in the remainder of this paper we present a discussion of related work, describe the study performed, present the findings and their implications, and conclude. a query considered in isolation offers limited information about a searcher intent. query context that considers pre query activity, can provide richer information about search intentions. in this paper, we describe a study in which we developed and evaluated user interest models for the current query, its context, and their combination, which we refer to as intent. using large scale logs, we evaluate how accurately each model predicts the user short term interests under various experimental conditions. in our study we: determine the extent of opportunity for using context to model intent; compare the utility of different sources of behavioral evidence for building predictive interest models, and; investigate optimally combining the query and its context by learning a model that predicts the context weight for each query. our findings demonstrate significant opportunity in leveraging contextual information, show that context and source influence predictive accuracy, and show that we can learn a near optimal combination of the query and context for each query. the findings can inform the design of search systems that leverage contextual information to better understand, model, and serve searchers information needs. search behavior resides within an external context that motivates the problem situation and influences interaction behavior for the duration of the search session and beyond. satisfying searchers information needs involves a thorough understanding of their interests expressed explicitly through search queries, or implicitly through search engine result page clicks or post serp browsing behavior. the information retrieval community has theorized about context, developed context sensitive search models, and performed user studies investigating the role of context in the search process. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. most ir systems assume that queries are context independent. this abstraction is necessary in cranfield style evaluations where relevance judgments are gathered independent of any user or interaction context. in larger operational systems such as web search engines, scale constraints have often favored simple context independent approaches. recent research suggests that this may be changing as log data and machine learning techniques are applied to model activity based context in applications such as query suggestion, query classification, web page recommendation, and web search result ranking. however, this research is often specific to particular applications, and an assessment of the value of modeling activity based context that is applicable to a broad range of search and recommendation settings is required. in this paper we describe a systematic study of the value of contextual information during web search activity. we construct interest models of the current query, its context comprising preceding session activity such as previous queries or previous clicks on search results, the combination of the query and its context, and evaluate the predictive effectiveness of these models using future actions. figure # illustrates each of the models and their role in representing users interests. queries are de accurate understanding of current interests and prediction of future interests are core tasks for user modeling, with a range of possible applications. for example, a query such as could be interpreted differently depending on whether they previous query was vs. this contextual knowledge could be used to re rank search results, classify the query, or suggest alternative query formulations. similarly, an accurate understanding of current and future interests could be used to dynamically adapt search interfaces to support different tasks. in our study we: determine the fraction of search engine queries for which context could be leveraged, measure the value of different models and sources for predicting future interests, and investigate learning the optimal combination of query and context on a per query basis, and use the learned models to improve the accuracy of our predictions. we use a logbased methodology as logs contain behavioral evidence at scale and cover many classes of information needs. this is important since performance differences may not hold for all search tasks. the remainder of this paper is structured as follows. in section # we present related work on implicit profile generation from user activity, on representing interests with topical categories, on query analysis, and on the development of predictive interest models. section # describes how we define and construct the models developed for this study. we describe the study and the findings from our analysis in section #. we discuss findings and their implications in section # and conclude in section #. we present a non traditional retrieval problem we call subtopic retrieval. the subtopic retrieval problem is concerned with finding documents that cover many different subtopics of a query topic. in such a problem, the utility of a document in a ranking is dependent on other documents in the ranking, violating the assumption of independent relevance which is assumed in most traditional retrieval methods. subtopic retrieval poses challenges for evaluating performance, as well as for developing effective algorithms. we propose a framework for evaluating subtopic retrieval which generalizes the traditional precision and recall metrics by accounting for intrinsic topic difficulty as well as redundancy in documents. we propose and systematically evaluate several methods for performing subtopic retrieval using statistical language models and a maximal marginal relevance ranking strategy. a mixture model combined with query likelihood relevance ranking is shown to modestly outperform a baseline relevance ranking on a data set used in the trec interactive track.