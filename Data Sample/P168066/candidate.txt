evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance. we present a real world study of modeling the behavior of web search users to predict web search result preferences. accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks. our key insight to improving robustness of interpreting implicit feedback is to model query dependent deviations from the expected noisy user behavior. we show that our model of clickthrough interpretation improves prediction accuracy over state of the art clickthrough methods. we generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone. we report results of a large scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods. relevance measurement is crucial to web search and to information retrieval in general. traditionally, search relevance is measured by using human assessors to judge the relevance of querydocument pairs. however, explicit human ratings are expensive and difficult to obtain. at the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results. if we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems. recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search. however, most traditional permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. susan dumais robert ragno microsoft research microsoft research sdumais microsoft com rragno microsoft com ir work was performed over controlled test collections and carefully selected query sets and tasks. therefore, it is not clear whether these techniques will work for general real world web search. a significant distinction is that web search is not controlled. individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered. but the amount of the user interaction data is orders of magnitude larger than anything available in a non web search setting. by using the aggregated behavior of large numbers of users we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting. furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage. hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions. automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings. we present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results. our contributions include: a distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions. extensions of existing clickthrough strategies to include richer browsing and interaction features. a thorough evaluation of our user behavior models, as well as of previously published state of the art techniques, over a large set of web search sessions. we discuss our results and outline future directions and various applications of this work in section #, which concludes the paper. we give a novel way of transforming clicks into weighted, directed graphs inspired by eye tracking studies and then devise an objective function for finding cuts in these graphs that induce a good labeling. in its full generality, the problem is np hard, but we show that, in the case of two labels, an optimum labeling can be found in linear time. for the more general case, we propose heuristic solutions. each training point is a pair that is labeled by a human judge who assigns a score of perfect, excellent, etc, depending on how well the url matches the query. in this paper, we study whether clicks can be used to automatically generate good labels. intuitively, documents that are clicked in aggregate can indicate relevance. experiments on real click logs show that click based labels align with the opinion of a panel of judges, especially as the consensus of the panel grows stronger. the ranking function used by search engines to order results is learned from labeled training data. search engines order web results via a ranking function that, given a query and a document, produces a score indicating how well the document matches the query. first, the preference rules de ned by joachims et al assume a speci. second, the work tacitly assumes a relatively controlled environment with a stable search engine, ie, one that always produces search results in the same order. each preference rule produces a consistent set of preferences between urls. the ranking function is itself learned via a machine learning algorithm such as. the nal label of the pair is then derived by aggregating the multiple judgments. consequently, there is a pressing need for search engines to automate the labeling process as much as possible. aggregation of the activities of many users provides a powerful signal about the quality of a pair. these rules when applied to a click log produce pairwise preferences between the urls of a query, which are then used as a training set for a learning algorithm. when applying preference rules on click logs, many contradicting pairwise preferences must be reconciled to obtain a consistent labeling. third, the goal is to generate pairwise preferences which are then used directly for training. for such algorithms it is important to produce labels for the pairs. contributions: the main contribution of this paper is a method for automatically generating labels for pairs from click activity. we model the collection of pairwise preferences as a graph and formulate the label generation problem as a novel graph partitioning problem, where the goal is to partition nodes in the graph into labeled classes, such that we maximize the number of users that agree with the labeling minus the number of users that disagree with the labeling. the maximum ordered partition problem is of independent theoretical interest. we show that in the case of nding two labels, ie, relevant and not relevant, it surprisingly turns out that the optimum labeling can be found in linear time. on the other hand, we show that the problem of nding the optimumlabeling, whereis the number of vertices in the graph, is np hard. we propose heuristics for addressing the problem of label generation for multiple labels. our methods compute a linear ordering of the nodes in the graph, and then nd the optimal partition using dynamic programming. the input to the learning algorithm is typically a collection of pairs, labeled with relevance labels such as perfect, excellent, good, fair or bad indicating how well the document matches permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. obtaining labels of high quality is of critical importance: the quality of training data heavily in uences the quality of the ranking function. currently, the labels for training data are collected using human judges. typically, each pair is assigned to a single judge. however, this leads to error prone judgments since it is very hard for a single judge to capture all the intents and nuances of a query posed by a user. to alleviate such errors, a panel of judges can be used to obtain multiple judgments for the same pair. this manual way of obtaining labels is time consuming, laborintensive, and costly. furthermore, as ranking models become more complex, the amount of labeled data needed to learn an accurate model increases. further, to ensure temporal relevance of labels, the labeling process must be repeated quite often. it has been observed that the click logs of a search engine can be used to automate the generation of training data. the click log records all queries posed to a search engine, the urls that were shown to the user as a result of the query, and the clicks. the logs capture the preferences of the users: clicked documents are most likely relevant to the needs and the intent of the user, while skipped documents are most likely not. this data can thus be used for the task of automatically generating labels. joachims et alseminal work proposes a collection of preference rules for interpreting click logs. applying the ideas of joachims et al to a search engine click log uncovers several shortcomings. user browsing model that is overly simpli ed. as we outline in section #, the rules do not fully capture the aggregate behavior of users and also limits the generation of training data. it also assumes a small set of users that behave consistently. these assumptions are not satis ed in the environment of a real search engine: the observed behavior for a query changes dynamically over time, different orderings may be shown to different users, and the same users may exhibit different behavior depending on the time of day. the method directly address the three shortcomings mentioned above. more concretely, we propose a new interpretation of the click log that utilizes the probability a user has seen a speci. this new model more accurately captures the aggregate behavior of users. we conduct an extensive experimental study of both the preference rules and the labeling methods. the experiments compare click inferences to the aggregate view of a panel of judges. we demonstrate that our probabilistic interpretation of the click log is more accurate than prior deterministic interpretations via this panel. further, we show that the stronger the consensus of the panel, the more likely our click labels are to agree with the consensus. in the event that click labels disagree with a strong consensus, we show that it can often be attributed to short queries where the intent is ambiguous. however, several learning algorithms operate on labeled training data. as with any application of machine learning, web search ranking requires labeled data. the labels usually come in the form of relevance assessments made by editors. click logs can also provide an important source of implicit feedback and can be used as a cheap proxy for editorial labels. the main difficulty however comes from the so called position bias urls appearing in lower positions are less likely to be clicked even if they are relevant. in this paper, we propose a dynamic bayesian network which aims at providing us with unbiased estimation of the relevance from the click logs. experiments show that the proposed click model outperforms other existing click models in predicting both click through rate and relevance. web page ranking has been traditionally based on hand designed ranking functions such as bm. with the inclusion of thousands of features for ranking, hand tuning of ranking function becomes intractable. several machine learning algorithms have been applied to automatically optimize ranking functions. machine learned ranking requires a large number of training examples, with relevance labels indicating the degree of relevance for each querydocument pair. the cost of the editorial labeling is usually quite expensive. moreover, the relevance labels of the training examples could change over time. for example, if the query is time sensitive or recurrent, a search engine is expected to return the copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. most up to date documents sites to the users. however, it would be prohibitive to keep all the relevance labels up to date. click logs embed important information about user satisfaction with a search engine and can provide a highly valuable source of relevance information. compare to editorial labels, clicks are much cheaper to obtain and always re ect current relevance. clicks have been used in multiple ways by a search engine: to tune search parameters, to evaluate di erent ranking functions, or as signals to directly in uence ranking. however, clicks are known to be biased, by the presentation order, the appearance of the documents, and the reputation of individual sites. many studies have attempted to account the position bias of click. carterette and jones proposed to model the relationship between clicks and relevance so that clicks can be used to unbiasedly evaluate search engine when lack of editorial relevance judgment. other research attempted to model user click behavior during search so that future clicks may be accurately predicted based on observations of past clicks. two di erent types of the click models are position models and the cascade model. a position model assumes that a click depends on both relevance and examination. each rank has a certain probability of being examined, which decays by rank and depends only on rank. a click on a url indicates that the url is examined and considered relevant by the user. however this model treats the individual urls in a search result page independently and fails to capture the interaction among urls in the examination probability. take for example two equally relevant urls for a query: a user may only click on the top one, feel satis ed, and then leave the search result page. in this case, the positional bias cannot fully explain the lack of clicks for the second url. the cascade model assumes that users examine the results sequentially and stop as soon as a relevant document is clicked. here, the probability of examination is indirectly determined by two factors: the rank of the url and the relevance of all previous urls. the cascade model makes a strong assumption that there is only one click per search and hence it could not explain the abandoned search or search with more than one clicks. even though the cascade model is quite restrictive, the authors of that paper showed that we refer to url as a shorthand for the entire display block consisting of the title, abstract and url of the corresponding result. it can predict click through rates more accurately than the position models described above. none of the above models distinguish perceived relevance and actual relevance. because users cannot examine the content of a document until they click on the url, the decision to click is made based on perceived relevance. while there is a strong correlation between perceived relevance and actual relevance, there are also many cases where they di er. in this paper, a dynamic bayesian network model is proposed to model the users browsing behavior. as in the position model, we assume that a click occurs if and only if the user has examined the url and deemed it relevant. similar to the cascade model, our model assumes that users make a linear transversal through the results and decide whether to click based on the perceived relevance of the document. the user chooses to examine the next url if he she is unsatis ed with the clicked url. our model di ers from the cascade model in two aspects: because a click does not necessarily mean that the user is satis ed with the clicked document, we attempt to distinguish the perceived relevance and actual relevance. recent studies show that a majority of web page accesses are referred by search engines. we then analytically estimate how much longer it takes for a new page to attract a large number of web users when search engines return only popular pages at the top of search results. our result shows that search engines can have an immensely worrisome impact on the discovery of new web pages. in this paper we study the widespread use of web search engines and its impact on the ecology of the web. in particular, we study how much impact search engines have on the popularity evolution of web pages. for example, given that search engines return currently popular pages at the top of search results, are we somehow penalizing newly created pages that are not very well known yet are popular pages getting even more popular and new pages completely ignored we first show that this unfortunate trend indeed exists on the web through an experimental study based on real web data. since the arrival of the web in early, the web search engines have become an indispensable tool in our everyday life. when we seek information, we often go to our favorite search engine and look at the returned pages. given the sheer quantity of information available on the web, the widespread use of search engines is not surprising. therefore, a page that is not indexed by google is unlikely to be viewed by many web users. roughly speaking, the pagerank metric considers a page important or of high quality if the page is linked to by many other pages on the web for example, google puts a page at the top of a search result when the page is linked to by the most other pages on the web. the problem of this popularity based ranking is that it is inherently biased against unknown pages. while google takes more than factors into account in determining the nal ranking of a page, the core of their ranking algorithm is based on the pagerank metric. results, more web users will discover and look at those pages, increasing their popularity even further. this rich get richer phenomenon can be particularly problematic for the high quality pages that were recently created. even if a page is of high quality, the page may be completely ignored by web users simply because its current popularity is very low. new and valuable pages are ignored just because they have not been given a chance to be noticed by people. in this paper, we investigate the magnitude of this searchengine bias through experimental and theoretical studies: experimental study: we rst study whether the rich get richer phenomenon is happening in the current web by examining real web data collected over months. the result strongly indicates that this phenomenon is indeed happening. from our experimental data, we could observe that the top of the pages with the highest number of incoming links obtained of the new links after months, while the bottom of the pages obtained virtually no new incoming links during that period. an individual simply cannot read billions of pages available on the web, so he gets help from search engines to zoom in to a small number of pages worth looking at. despite search engines usefulness, we note that their widespread use may introduce a signi cant bias to peopleperception of the web. for example, in a recent news article, a web commentator stated that if your page is not indexed by google, your page does not exist on the web. while this statement may be an exaggeration, it contains an alarming bit of truth. to nd a page on the web, many web users go to google issue keyword queries, and look at the results. if the users cannot nd relevant pages after several iterations of keyword queries, they are likely to give up and stop looking for further pages on the web. the main question that we may ask is, then, how search engines rank web pages given a query. if search engines fairly judge the quality and relevance of every page and return the pages of highest quality, this search engine bias may not be a signi cant problem. unfortunately, the quality of a page is a very subjective notion and dif cult to measure in practice, so most existing search engines use a link popularity metric, called pagerank, to measure the quality of a page. in short, currently popular pages are repeatedly returned at the top of the search results by major search engines. that is, when search engines constantly return popular pages at the top of their search more precise description of the pagerank metric is provided in section #. in contrast, a currentlyunpopular page will not be returned by search engines, so few new users will discover those pages and create a link to it, pushing the pageranking even further down. this situation is clearly unfortunate both for web page authors and the overall web users. in a number of recent studies researchers have found that because search engines repeatedly return currently popular pages at the top of search results, popular pages tend to get even more popular, while unpopular pages get ignored by an average user. this rich get richer phenomenon is particularly problematic for new and high quality pages because they may never get a chance to get users attention, decreasing the overall quality of search results in the long run. in this paper, we propose a new ranking function, called page quality that can alleviate the problem of popularity based ranking. we first present a formal framework to study the search engine bias by discussing what is an ideal way to measure the intrinsic quality of a page. we then compare how pagerank, the current ranking metric used by major search engines, differs from this ideal quality metric. this framework will help us investigate the search engine bias in more concrete terms and provide clear understanding why pagerank is effective in many cases and exactly when it is problematic. we then propose a practical way to estimate the intrinsic page quality to avoid the inherent bias of pagerank. we derive our proposed quality estimator through a careful analysis of a reasonable web user model, and we present experimental results that show the potential of our proposed estimator. we believe that our quality estimator has the potential to alleviate the rich get richer phenomenon and help new and high quality pages get the attention that they deserve. a key source of bias is presentation order: the probability of click is influenced by a document position in the results page. this paper focuses on explaining that bias, modelling how probability of click depends on position. we carry out a large data gathering effort, where we perturb the ranking of a major search engine, to see how clicks are affected. we then explore which of the four hypotheses best explains the real world position effects, and compare these to a simple logistic regression model. a cascade model, where users view results from top to bottom and leave as soon as they see a worthwhile document, is our best explanation for position bias in early ranks. search engine click logs provide an invaluable source of relevance information, but this information is biased. we propose four simple hypotheses about how position bias might arise. the data are not well explained by simple position models, where some users click indiscriminately on rank or there is a simple decay of attention over ranks. as people search the web, certain of their actions can be logged by a search engine. they can also be indicative of success or failure of the engine. or commercial advantage and that copies bear this notice and the full citation on the rst page. these record which results page elements were selected for which query. click log information can be fed back into the engine, to tune search parameters or even used as direct evidence to in uence ranking. a fundamental problem in click data is position bias. the probability of a document being clicked depends not only on its relevance, but on its position in the results page. in top results lists, the probability of observing a click decays with rank. eye tracking experiments show that the user is less likely to examine results near the bottom of the list, although click probability decays faster than examination probability so there are probably additional sources of bias. our approach is to consider several such hypotheses for how position bias arises, formalising each as a simple probabilistic model. we then collect click data from a major web search engine, while deliberately ipping positions of documents in the ranked list. we nally evaluate the position bias models using the ip data, to see which is the best explanation of real world position. although our experiment involves ips, our goal is to model position bias so we can correct for it, without relying on ips. with such a model it should be possible to process a click log and extract estimates of a search resultabsolute click relevance. patterns of behaviour in logs can give an idea of the scope of user activity. when deciding which search results to permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. bill ramsey microsoft research, redmond usa brams microsoft com present, click logs are of particular interest. such estimates could be used in applications where an estimate of probability of click is useful such as ad ranking or evaluation. search engine click logs provide an invaluable source of relevance information but this information is biased because we ignore which documents from the result list the users have actually seen before and after they clicked. otherwise, we could estimate document relevance by simple counting. in this paper, we propose a set of assumptions on user browsing behavior that allows the estimation of the probability that a document is seen, thereby providing an unbiased estimate of document relevance. our solution outperforms very significantly all previous models. they also explain why documents situated just after a very relevant document are clicked more often. to train, test and compare our model to the best alternatives described in the literature, we gather a large set of real data and proceed to an extensive cross validation experiment. as a side effect, we gain insight into the browsing behavior of users and we can compare it to the conclusions of an eye tracking experiments by joachims et al. in particular, our findings confirm that a user almost always see the document directly after a clicked document. users permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bearthisnoticeand thefull citationonthe rstpage tocopy otherwise, to republish, topost on servers or to redistribute tolists, requiresprior speci. benjamin piwowarski yahoo research latin america bpiwowar yahoo inc com areincreasingly understood tobethedrivingforceof theinternet and many initiatives are aimed at empowering them. social search, as its name implies, supposes participation from users who tag, bookmark, andcomment their search results. in addition to this information explicitly provided by users, there is a much larger source of implicit data which is collectedby search engines. itis apoll of millions of users over an enormous variety of topics. examples of applicationsincludewebpersonalization, web spam detection, query term recommendation. unlike human tags and bookmarks, implicit feedback is also not biased towards socially active web users. that is, the data is collected from all users, notjust usersthat choosetoedit a wikipage, orjoin a social network such asmyspace orfriendster. click data seems the perfect source of information when deciding whichdocuments to showin answerto a query. this information can be fed back into the engine, to tune search parameters or even used asdirect evidencetoin uence ranking. nevertheless, they cannot be used without further processing: a fundamental problem is the position bias. the probability of a document being clicked depends not only on its relevance, but on other factors as its position in the result page. contributions user activity models within web search can be broadly divided in three categories: analysis models where the aim istogaininsightintotypical userbehavior, modelsthat try topredictthenext useraction, and eventually models that estimate the attractiveness or perceived relevance of a document independently of the layout in uence. this work focusses on thelatter, usingas the only source ofinformation the web search logs produced by the search engines. yet users do not browse the whole list and documents situated earlier in the rankinghave ahigherprobability ofbeing examined. as a consequence, they also have a higher probability of being clicked independently of how relevant they are. if we could estimatetheprobability that adocumentis examined by the user, we could estimate its relevance as the ratio of the number of times a user clicked on the document to the expected number of times the document is examined. the main contribution of this work is a model of user browsing behavior when consulting a page of search results. this model estimates the probability of examination of a documentgiven the rankofthedocument andthedistance to the last clicked document. our model sheds light on user behavior, is in agreement with the user experiments ofgranka et al and extends andquanti esthe user model ofjoachims et al. in section # we review the literature for click models and we present our contributions. in section # we compare the predicting abilities on unseen data of the di erent models. westudy in moredetailstheimplications of theuserbrowsing model and we relate the ndings with the eye tracking experiments of insection. social search is quickly gaining acceptance as a promising way of harnessing the common knowledge of millions of users to help each other and search more. arguably, this is a long term trend that started with kleinberg idea of hubs and authorities, which proposed that a hyperlink from one document to another was a vote in favor of the document linked to, an idea in practice exploited in the pagerank algorithm. thisfeedbackprovidesdetailed and valuable information about users interactions with the system as theissuedquery, thepresentedurls, the selected documents and their ranking. it has been used in many ways to mine user interests and preferences. it can be thought as the result of users voting in favor of the documents they nd interesting. in top results lists, the probability of observing a click decays with rank. eye tracking experiments show that a user is less likely to examine results near the bottom of the list, although click probability decays faster than examination probability so there are probably additional sources of bias. experiments also show that a document is not clicked with the same frequency if situated after a highly relevant or a mediocre document. if the users looked with attention all the documents in the ranking list, the relevance of one of them could be estimated simply by counting thenumberof timesitisselected. web search engines present search results in a rank ordered list. this works when what a user wants is near the top, but sometimes the information that the user really wants is located at the bottom of the page. this study examined how users search behaviors vary when target results were displayed at various positions for informational and navigational tasks. we found that when targets were placed relatively low in the first page of search results, people spent more time searching and were less successful in finding the target, especially for informational tasks. further analysis of eye movements showed that the decrease in search performance was partially due to the fact that users rarely looked at lower ranking results. the large decrease in performance for informational search is probably because users have high confidence in the search engine ranking; in contrast to navigational tasks, where the target is more obvious from information presented in the results, in informational tasks, users try out the top ranked results even if these results are perceived as less relevant for the task. given a terabyte click log, can we build an efficient and effective click model it is commonly believed that web search click logs are a gold mine for search business, because they reflect users preference over web documents presented by the search engine. click models provide a principled approach to inferring user perceived relevance of web documents, which can be leveraged in numerous applications in search businesses. due to the huge volume of click data, scalability is a must. we present the click chain model, which is based on a solid, bayesian framework. it is both scalable and incremental, perfectly meeting the computational challenges imposed by the voluminous click logs that constantly grow. we conduct an extensive experimental study on a data set containing million query sessions obtained in july year# from a commercial search engine. ccm consistently outperforms two state of the art competitors in a number of metrics, with over better log likelihood, over better click perplexity and much more robust prediction of the first and the last clicked position. important attributes of these search activities are automatically logged as implicit user feedbacks. web search click logs are probably the most extensive, albeit indirect, surveys on user experience, which can be part of this work was done when the rst author was on a summer internship with microsoft research. for example, the topic of utilizing click data to optimize search ranker has been well explored and evaluated by quite a few academic and industrial researchers since the beginning of this century. joachims et al carried out eye tracking experiments to study participants decision process as they scan through search results, and further compared implicit click feedback against explicit relevance judgments. they found that clicks are accurate enough as relative judgement to indicate userpreferences for certain pairs of documents, but they are not reliable as absolute relevance judgement, ie, clicks are informative but biased. a particular example is that users tend to click more on web documents in higher positions even if the ranking is reversed. richardson et al proposed the examination hypothesis to explain the position bias of clicks. under this hypothesis, a web document must be examined before being clicked, and user perceived document relevance is de ned as the conditional probability of being clicked after being examined. click models provide a principled way of integrating knowledge of user search behaviors to infer user perceived relevance of web documents, which can be leveraged in a number of search related applications, including: automated ranking alterations: the top part of ranking can be adjusted based on the inferred relevance so that they are aligned with users preference. search quality metrics: the inferred relevance and user examination probabilities can be used to compose search quality metrics, which correlate with user satisfaction. adaptive search: when the meaning of a query changes over time, so do user click patterns. based on the inferred relevance that shifts with click data, the search engine can be adaptive. judge of the judges: the inferred rst party relevance judgement could be contrasted reconciled with well trained human judges for improved quality. online advertising: the user interaction model can be adapted to a number of sponsored search applications such as ad auctions. an ideal model of clicks should, in addition to enabling reliable relevance inference, have two other important properties scalability and incremental computation; scalability enables processing of large amounts of clicklogs data and the incremental computation enables updating the model as new data are recorded. two click models are recently proposed which are based on the same examination hypothesis but with di erent assumptions about user behaviors. the user browsing model proposed by dupret and piwowarski is demonstrated to outperform the cascade model in predicting click probabilities. however, the iterative nature of the inference algorithm of ubm requires multiple scans of the data, which not only increases the computation cost but renders incremental update obscure as well. cient than ubm, but its performance on tail queries could be further improved. in this paper, we propose the click chain model, that has the following desirable properties: foundation: it is based on a solid, bayesian framework. a closed form representation of the relevance posterior can be derived from the proposed approximation inference scheme. scalability: it is fast and nimble, with excellent scalability with respect to both time and space; it can also work in an incremental fashion. ccm consistently shows performance improvements over two of the state of the art competitors in a number of evaluation metrics such as log likelihood, click perplexity and click prediction robustness. algorithms for relevance inference, parameter estimation and incremental computation are detailed in section #. billions of queries are submitted to search engines on the web every day. these attributes include, for each query session, the query string, the time stamp, the list of web documents shown in the search result and whether each document is clicked or not. copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. extracting key statistics or patterns from these tera byte logs is of much interest to both search engine providers, who could obtain objective measures of user experience and useful features to improve their services, and to world wide web researchers, who could better understand user behavior and calibrate their hypotheses and models. a number of studies have been conducted previously on analyzing user behavior in web search and their relationship to click data. top ranked documents may have more chance to be examined than those ranked below, regardless of their relevance. craswell et al further proposed the cascade model for describing mathematically how the rst click arises when users linearly scan through search results. however, the cascade model assumes that users abandon the query session after the rst click and hence does not provide a complete picture of how multiple clicks arise in a query session and how to estimate document relevance from such data. the dependent click model which appears in our previous work is naturally incremental, and is an order of magnitude more. the rest of this paper is organized as follows. we rst survey existing click models in section #, and then present ccm in section #. this paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. while previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. this makes them difficult and expensive to apply. the goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query log of the search engine in connection with the log of links the users clicked on in the presented ranking. such clickthrough data is available in abundance and can be recorded at very low cost. taking a support vector machine approach, this paper presents a method for learning retrieval functions. from a theoretical perspective, this method is shown to be well founded in a risk minimization framework. furthermore, it is shown to be feasible even for large sets of queries and features. the theoretical results are verified in a controlled experiment. it shows that the method can effectively adapt the retrieval function of a meta search engine to a particular group of users, outperforming google in terms of retrieval quality after only a couple of hundred training examples. which www page does a user actually want to re trieve when he types some keywords into a search engine there are typically thousands of pages that contain these words, but the user is interested in a much smaller subset. if we knew the set of pages actually relevant to the user query, we could use this as training data for optimizing the retrieval function. unfortunately, experience shows that users are only rarely willing to give explicit feedback. however, this paper argues that sufficient information is already hidden in the logfiles of www search engines. ceive millions of queries per day, such data is available in abundance. compared to explicit feedback data, which is typically elicited in laborious user studies, any information that can be extracted from logfiles is virtually free and sub stantially more timely. this leads to a problem of learning with preference examples like for query, document, should be ranked higher than document db. more generally, will formulate the problem of learning a ranking function over a finite domain in terms of empirical risk minimization. for this formulation, will present a support vector machine algorithm that leads to a convex program and that can be extended to non linear ranking functions. it starts with a defi nition of what clickthrough data is, how it can be recorded, and how it can be used to generate training examples in the form of preferences. section # then introduces a gen eral framework for learning retrieval functions, leading to an svm algorithm for learning parameterized orderings in section #. section # evaluates the method based on experi mental results. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific pemaission and or a fee. one could simply ask the user for feedback. since major search engines re permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. this paper presents an approach to learning retrieval func tions by analyzing which links the users click on in the pre sented ranking. experi ments show that the method can successfully learn a highly effective retrieval function for a meta search engine. this paper examines the reliability of implicit feedback generated from clickthrough data in www search. analyzing the users decision process using eyetracking and comparing implicit feedback against manual relevance judgments, we conclude that clicks are informative but biased. while this makes the interpretation of clicks as absolute relevance judgments difficult, we show that relative preferences derived from clicks are reasonably accurate on average. the idea of adapting a retrieval system to particular groups of users and particular collections of documents promises further improvements in retrieval quality for at least two reasons. second, as evident from the trec evaluations, differences between document collections make it necessary to tune retrieval functions with respect to the collection for optimum retrieval performance. since manually adapting a retrieval function is time consuming or even impractical, research on automatic adaptation using machine learning is receiving much attention. ithaca, ny, usa cornell edu however, a great bottleneck in the application of machine learning techniques is the availability of training data. in this paper we explore and evaluate strategies for how to automatically generate training examples for learning retrieval functions from observed user behavior. however, implicit feedback is more di cult to interpret and potentially noisy. in this paper we analyze which types of implicit feedback can be reliably extracted from observed user behavior, in particular clickthrough data in www search. the study is designed to analyze how users interact with the list of ranked results from the google search engine and how their behavior can be interpreted as relevance judgments. first, we use eyetracking to understand how users behave on googleresults page. do users scan the results from top to bottom how many abstracts do they read before clicking how does their behavior change, if we arti cially manipulate googleranking answers to these questions give insight into the users decision process and suggest in how far clicks are the result of an informed decision. based on these results, we propose several strategies for generating feedback from clicks. to evaluate the degree to which feedback signals indicate relevance, we compare the implicit feedback against explicit feedback we collected manually. the study presented in this paper is di erent in at least two respects from previous work assessing the reliability of implicit feedback. first, our study provides detailed insight into the users decision making process through the use of eyetracking. second, we evaluate relative preference signals derived from user behavior. this is in contrast to previous studies that primarily evaluated absolute feedback. our results show that users make informed decisions among the abstracts they observe and that clicks re ect relevance judgments. however, we show that clicking decisions are biased in at least two ways. first, we show that there is a trust bias which leads to more clicks on links ranked highly by google, even if those abstracts are less relevant than other abstracts the user viewed. second, there is a quality bias: the users clicking decision is not only in uenced by the relevance of the clicked link, but also by the overall quality of the other abstracts in the ranking. we propose several strategies for extracting such relative relevance judgments from clicks and show that they accurately agree with explicit relevance judgments collected manually. first, a one size ts all retrieval function is necessarily a compromise in environments with heterogeneous users and is therefore likely to act suboptimally for many users. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. in contrast to explicit feedback, such implicit feedback has the advantage that it can be collected at much lower cost, in much larger quantities, and without burden on the user of the retrieval system. to evaluate the reliability of implicit feedback signals, we conducted a user study. we performed two types of analysis in this study. this shows that clicks have to be interpreted relative to the order of presentation and relative to the other abstracts. this article examines the reliability of implicit feedback generated from clickthrough data and query reformulations in world wide web search. analyzing the users decision process using eyetracking and comparing implicit feedback against manual relevance judgments, we conclude that clicks are informative but biased. while this makes the interpretation of clicks as absolute relevance judgments difficult, we show that relative preferences derived from clicks are reasonably accurate on average. we find that such relative preferences are accurate not only between results from an individual query, but across multiple sets of results within chains of query reformulations. the idea of adapting a retrieval system to particular groups of users and particular collections of documents promises further improvements in retrieval quality for at least two reasons. second, as evident from the trec evaluations, differences between document collections make it necessary to tune retrieval functions with respect to the collection for optimum retrieval performance. since manually adapting a retrieval function is time consuming or even impractical, research on automatic adaptation using machine learning is receiving much attention. however, a great bottleneck in the application of machine learning techniques is the availability of training data. in contrast to explicit feedback, such implicit feedback has the advantage that it can be collected at much lower cost, in much larger quantities, and without burden on the user of the retrieval system. in this article we analyze which types of implicit feedback can be reliably extracted from observed user behavior, in particular clickthrough data in world wide web search. following and extending prior work reported in radlinski and joachims, joachims et al, and granka et al, we analyze implicit feedback from within individual queries as well as across multiple consecutive queries about the same information need. the feedback strategies across query chains exploit that users typically reformulate their query multiple times before their information need is satis ed. we elaborate on the query chain strategies proposed in radlinski and joachims, as well as propose and explore additional strategies. to evaluate the reliability of these implicit feedback signals, we conducted a user study. the study was designed to analyze how users interact with the list of ranked results from the google search engine and how their behavior can be interpreted as relevance judgments. first, we used eye tracking to understand how users behave on googleresults page. do users scan the results from top to bottom how many abstracts do they read before clicking how does their behavior change, if we arti cially manipulate googleranking answers acm transactions on information systems, vol. evaluating accuracy of implicit feedback in web search to these questions give insight into the users decision process and suggest in how far clicks are the result of an informed decision. based on these results, we propose several strategies for generating feedback from clicks and query reformulations. to evaluate the degree to which feedback signals indicate relevance, we compared the implicit feedback against explicit feedback we collected manually. the study presented in this article is different in at least two respects from previous work assessing the reliability of implicit feedback. second, we evaluate relative preference signals derived from user behavior. this is in contrast to previous studies that primarily evaluated absolute feedback. our results show that users make informed decisions among the abstracts they observe and that clicks re ect relevance judgments. however, we show that clicking decisions are biased in at least two ways. first, we show that there is a trust bias which leads to more clicks on links ranked highly by google, even if those abstracts are less relevant than other abstracts the user viewed. second, there is a quality of context bias: the users clicking decision is not only in uenced by the relevance of the clicked link, but also by the overall quality of the other abstracts in the ranking. this shows that clicks have to be interpreted relative to the order of presentation and relative to the other abstracts. we propose several strategies for extracting such relative relevance judgments from clicks and show that they accurately agree with explicit relevance judgments collected manually. first, our study provides detailed insight into the users decision making process through the use of eyetracking. first, a one size ts all retrieval function is necessarily a compromise in environments with heterogeneous users and is therefore likely to act suboptimally for many users. in this article we explore and evaluate strategies for how to automatically generate training examples for learning retrieval functions from observed user behavior. however, implicit feedback is more dif cult to interpret and potentially noisy. we performed two types of analysis in this study. in this paper, we undertake a large scale study of online user behavior based on search and toolbar logs. we propose a new ccs taxonomy of pageviews consisting of content, communication, and search. we show that roughly half of all pageviews online are content, one third are communications, and the remaining one sixth are search. we then give further breakdowns to characterize the pageviews within each high level category. we then study the extent to which pages of certain types are revisited by the same user over time, and the mechanisms by which users move from page to page, within and across hosts, and within and across page types. we consider robust schemes for assigning responsibility for a pageview to ancestors along the chain of referrals. we show that mail, news, and social networking pageviews are insular in nature, appearing primarily in homogeneous sessions of one type. search pageviews, on the other hand, appear on the path to a disproportionate number of pageviews, but cannot be viewed as the principal mechanism by which those pageviews were reached. finally, we study the burstiness of pageviews associated with a url, and show that by and large, online browsing behavior is not significantly affected by breaking material with non uniform visit frequency. since the inception of the world wide web some fteen years ago, the rate of appearance of new capabilities, data types, and services has remained high, allowing users to meet more of their communications, entertainment, and task completion needs online. every day, new websites appear, exploring new approaches and business models in competition with existing online and. hundreds of this work was performed while the author was at yahoo inc. copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. www year#, april, year#, raleigh, north carolina, usa. atomkins gmail com millions of users engage daily with social networking sites that. er capabilities that were unknown just a few years ago. search of webpages, videos, images, commercial listings, personal ads and so forth continues to grow in volume and sophistication. orded by this crucible, and their behavior evolves as rapidly as the business landscape underlying it. this evolution, however, is di cult to observe, partly because of its velocity and partly because user behavioral data is not generally available for study. thus, we lack an accurate picture of how users engage with the web. in this paper, we perform a broad study of online user behavior based on data collected through the yahoo toolbar. we study a large sample of over fty million user pageviews collected over a seven day period in march of year# from users who have installed the yahoo toolbar and agreed to collection of their data for purposes including this type of analysis. our primary result, which will inform most of the analyses we perform, is a newly proposed ccs taxonomy of online pageviews whose three top level branches are content, communication, and search. we will de ne these in detail, and provide an additional one or two levels of depth to the taxonomy. our summary nding is that roughly half of all pageviews online are content, one third are communication, and the remaining one sixth are search. our development of the ccs taxonomy is based on an editorial labeling of a random sample of pages. with the taxonomy in place, we then develop a series of automated recognizers for pageviews that lie in certain classes of the taxonomy. we employ these larger scale sets of labeled data to study how users navigate within and among di erent types of pages, how search interacts with other types of navigation, and how users revisit pages within each taxonomy category. we show that mail, news, and social networking pageviews are insular in nature, appearing primarily in homogeneous sessions of one type. search pageviews, on the other hand, appear on the path to a disproportionate number of pageviews, but cannot be viewed as the principal mechanism by which those pageviews were reached. finally, we study the burstiness of pageviews associated with a url. we consider a smoothed variant of the observed data, in which each pageview appears throughout the time of our measurements according to an estimated poisson likelihood that maintains overall frequency while removing all burstiness from the data for each url. we show that the inter arrival distribution of this smoothed model appears almost unchanged from that of the original data, allowing us to conclude that breaking material is not a signi cant contributor to the structure of online page visit frequency. the remainder of the paper is organized as follows. section # describes related work, and section # characterizes the data sets we study. section # gives some statistical characterization of users, sessions, time online, inter arrival distributions, and popular destinations. section # presents our taxonomy of pageviews, along with a series of analyses based on this breakdown. section # de nes a notion of a search session and studies user behavior in such sessions. what is the potential opportunity for personalization in this paper, we propose a new way to personalize search, personalization with backoff. ideally, classes would be defined by market segments, demographics and surrogate variables such as time and geography. how many pages are there on the web more less big bets on clusters in the clouds could be wiped out if a small cache of a few million urls could capture much of the value. language modeling techniques are applied to msn search logs to estimate entropy. the perplexity is surprisingly small: millions, not billions. entropy is a powerful tool for sizing challenges and opportunities. if we have relevant data for a particular user, we should use it. but if we dont, back off to larger and larger classes of similar users. as a proof of concept, we use the first few bytes of the ip address to define classes. the coefficients of each backoff class are estimated with an em algorithm. how hard is search how hard are query suggestion mechanisms like auto complete how much does personalization help all these difficult questions can be answered by estimation of entropy from search logs. could this work was done when the rst author was on a summer internship at microsoft research. how big is english one can nd simple answers on the covers of many dictionaries, but we would feel more comfortable with answersfroma moreauthoritativesourcethana marketingdepartment. many academics have contributed to this discussionfrom manyperspectives: education, psychology, statistics, linguistics, and engineering. chomsky and shannon proposed two di erent ways to think about such questions: chomsky: languageisin nite shannon: bitsper character these two answers are very di erent. chomskyanswer isaboutthetotal numberof words; andshannon sansweris abouttheperplexity, orthedi culty of using alanguage. using achomskian argument, wecould argue that there are in nitely many urls. for example, one could write a spider trap such as successor aspx which links to successor aspx which links to successor aspx. although there are a lot of pages out there, there are not that many pages thatpeople actuallygoto. but the logs are tiny, far less thancarlsagan sbillions andbillions. how hard is search how much does personalization help http: www timeanddate com calendar monthly html year year# month country personalization personalization is a hot topic, with a large body of work, not only in the scienti. many people use personalized search products every day. literature such as, you ll have to re ne the query considerably by adding a keyword like sigir. why does personalization help it is useful to know your audience. depending on the user, this query could be looking for the sports arena or thefood additive. the search engine could do a better jobanswering ambiguous queries like this if it had access to demographic data and or log data such as click logs. acs can refer to the american chemical society, the american cancer society, the americancollegeofsurgeons and more. acronyms take on special meanings inside many large organizations andprivateenterprises. and of course, it means other things to other people including: montessori school ofraleigh, momservicerepresentative and my sports radio. pss is a stock ticker for payless shoes, as well an abbreviation of several di erent companies: physicians sales and service, phoenix simulation software, personal search syndication, professional sound system, etc. but inside microsoft, pss refers to product support services. it helps to know your audience in order to know: what the terminology means which questions are likely to come up, and which answers are likely to be appreciated. personalization with backoff but whatif wedo nothavedatafor aparticular user this paper takes a backo. to larger and larger groups of similar users. it would be even better to group customers by market segments and or collaborative ltering. an advertiser such asford, for example, has a wide range ofproducts. for example, the rm may wish to target small trucks to a rural audience and hybrids to a green audience. companies would like to know if they are talking to college students, teenagers, parents with young children, etc. it is useful to know the class of your audience. to higher bytes of ip addresses is better than personalization or no personalization. too little personalization misses the opportunity and too much runs into sparse data. market segments are typically de ned interms of surrogate variablessuch asgeography, and time of day and day of week. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. the crawler can easily consume all available time and space. the bigger the web, the harder the search. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. these large investments be wiped out if a small cache of a few million pages could capture much of the value what if someone found a way to squeeze much of the value of the cluster into a desktop or a mobile device is search more like aneverest expedition or a walk inthepark related questions come up in language. a dictionary could coveralot of words, but not all of themare actively used. inadditiontointentionally maliciousspidertraps, thereare perfectly benign examples such as calendars, where there are in nitely many pages, one for each month, with links fromeachmonth tothenext. itisalltooeasy tobuild aweb crawlerthat ndsitself attempting to materializeanin nite set with nite resources. with all the talk about the long tail, one would think that the web was astronomical. as we will see, entropy is a powerful tool for sizing challenges and opportunities. the rst few pages of results are dominated by the commercial practice. if we have the relevant data for a particular user, we should use it. as a proof of concept, users are grouped into equivalence classes based on the most signi cant bytes of their ip address. personalization is then conducted by combining estimates based on all four bytes of the ip address, the rst three bytes, the rst two, and so on. customers are assigned to equivalence classes based on pro le features such as age, income, occupation, etc. it is useful for an advertiser to know who it is talking to so that it can target the message appropriately to the audience. some products are more attractive to some customers, and other products are more attractive to other customers. we nd that a little bit of personalization is better than too much or too little. instead of assigning each customertohis own class, it is common to assign customers to market segments. these surrogate variables are easy to work with, and hopefully, they are well correlated with the more sensitive demographic variables such as those mentioned above. how many pages are there on the web more less how hard is search how much does personalization help all are di cult but crucial questions to search business. search engines make large investments in expensive computer centers in the cloud to index billions of pages. thispaper will estimate entropy of urls based on logs from microsoft swww live com. we nd thatittakesjust bits to guess the next url. a query for personalized search returns millions of page hits. forexample, for mostpeople, msr means mountain safety research, but inside microsoft, it means microsoft research. if we do not have data for a particular user, back. we seek to gain improved insight into how web search engines shouldcope with the evolving web, in an attempt to provide users with themost up to date results possible. for this purpose we collectedweekly snapshots of some web sites over the course of one year, and measured the evolution of content and link structure. for pages that persistover time we found that, perhaps surprisingly, the degree of contentshift as measured using tf idf cosine distance does not appear to beconsistently correlated with the frequency of contentupdating. despite this apparent non correlation, the rate of content shift of a given page is likely to remain consistent over time. that is, pages that change a great deal in one week will likely change by a similarly large degree in the following week. conversely, pages that experience little change will continue to experience little change. we conclude the paper with a discussion of the potential implications ofour results for the design of effective web search engines. as the web grows larger and more diverse, search engines are becoming the killer app of the web. whenever users want to look up information, they typically go to a search engine, issue queries and look at the results. recent studies con rm the growing importance of search engines. according to, for example, web users spend a total of million hours per month interacting with google alone. search engines typically crawl web pages in advance to build local copies and or indexes of the pages. this local index is then used later to identify relevant pages and answer users queries quickly. given that web pages are changing constantly, search engines need to update their index periodically in order to keep up with the evolving web. an obsolete index leads to irrelevant or broken search results, wasting users time and causing frustration. in this paper, we study the evolution of the web from the perspective of a search engine, so that we can get a better understanding on how search engines should cope with the evolving web. a number of existing studies have already investigated the evolution of the web. while some parts of our study have commonalities with the existing work, we believe that the following aspects make our study unique, revealing new and important details of the evolving web. link structure evolution: search engines rely on both the content and the link structure of the web to select the pages to return. for example, google uses pagerank as their primary ranking metric, which exploits the web link structure to evaluate the importance of a page. in this respect, the evolution of the link structure is an important aspect that search engines should know, but not much work has been done before. as far as we know, our work is the rst study investigating the evolution of the link structure. new pages on the web: while a large fraction of existing pages change over time, a signi cant fraction of changes on the web are due to new pages that are created over time. in this paper, we study how many new pages are being created every week, how much new content is being introduced and what are the characteristics of the newly created pages. search centric change metric: we study the changes in the existing web pages using metrics directly relevant to search engines. search engines typically use variations of tf idf distance metric to evaluate the relevance of a page to a query, and they often use an inverted index to speed up the relevance computation. in our pa per, we measure the changes in the existing pages using both the tf idf distance metric and the number of new words introduced in each update. the study of the tf idf distance will shed light on how much relevance change a page goes through over time. the number of new words will tell us what fraction of an inverted index is subject to updating. in this paper, we study the above aspects of the evolving web, by monitoring pages in web sites on a weekly basis for one year and analyzing the evolution of these sites. we can summarize some of the main ndings from this study as following: whatnew on the web we estimate that new pages are created at the rate of per week. assuming that the current web has billion pages, this result corresponds to million new pages every week, which is roughly terabytes in size we also estimate that only of the pages available today will be still accessible after one year. given this result, we believe that creation and deletion of new pages is a very signi cant part of the changes on the web and search engines need to dedicate substantial resources detecting these changes. while a large number of new pages are created every week, the new pages seem to borrow a signi cant portion of their content from exiting pages. in our experiments, we observe that about of new content is being introduced every week given new pages and new content, we estimate that at most of the content in the newly created pages is new. after a year, about of the content on the web is new. the link structure of the web is signi cantly more dynamic than the content on the web. after a year, about of the links on the web are replaced with new ones. this result indicates that search engines need to update linkbased ranking metrics very often. given changes every week, a week old ranking may not re ect the current ranking of the pages very well. search engine advertising has become a significant element of the web browsing experience. choosing the right ads for the query and the order in which they are displayed greatly affects the probability that a user will see and click on each ad. this ranking has a strong impact on the revenue the search engine receives from the ads. further, showing the user an ad that they prefer to click on improves user satisfaction. for these reasons, it is important to be able to accurately estimate the click through rate of ads in the system. for ads that have been displayed repeatedly, this is empirically measurable, but for new ads, other means must be used. we show that we can use features of ads, terms, and advertisers to learn a model that accurately predicts the click though rate for new ads. we also show that using our model improves the convergence and performance of an advertising system. as a result, our model increases both revenue and user satisfaction. most major search engines today are funded through textual advertising placed next to their search results. the market for these search advertisements has exploded in the last decade to billion, and is expected to double again by year#. the most notable example is google, which earned billion in revenue for the third quarter of year# from search advertising alone. though there are many forms of online advertising, in this paper we will restrict ourselves to the most common model: pay per copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. performance with a cost per click billing, which means the search engine is paid every time the ad is clicked by a user. google, yahoo, and microsoft all primarily use this model. to maximize revenue and user satisfaction, pay per performance systems must predict the expected user behavior for each displayed advertisement and must maximize the expectation that a user will act on it. the search system can make expected user behavior predictions based on historical click through performance of the ad. for example, if an ad has been displayed times in the past, and has received clicks, then the system could estimate its click through rate to be. this estimate, however, has very high variance, and may only reasonably be applied to ads that have been shown many times. this poses a particular problem when a new ad enters the system. a new ad has no historical information, so its expected click through rate is completely unknown. in this paper, we address the problem of estimating the probability that an ad will be clicked on, for newly created ads and advertising accounts. we show that we can use information about the ad itself, the page the ad points to, and statistics of related ads, to build a model that reasonably predicts the future ctr of that ad. there has been considerable work on user browsing models for search engine results, both organic and sponsored. the click through rate of a result is the product of the probability of examination times the perceived relevance of the result. past papers have assumed that when the ctr of a result varies based on the pattern of clicks in prior positions, this variation is solely due to changes in the probability of examination. we show that, for sponsored search results, a substantial portion of the change in ctr when conditioned on prior clicks is in fact due to a change in the relevance of results for that query instance, not just due to a change in the probability of examination. we then propose three new user browsing models, which attribute ctr changes solely to changes in relevance, solely to changes in examination, or to both changes in relevance and examination. the model that attributes all the ctr change to relevance yields substantially better predictors of ctr than models that attribute all the change to examination, and does only slightly worse than the model that attributes ctr change to both relevance and examination. for predicting relevance, the model that attributes all the ctr change to relevance again does better than the model that attributes the change to examination. surprisingly, we also find that one model might do better than another in predicting ctr, but worse in predicting relevance. thus it is essential to evaluate user browsing models with respect to accuracy in predicting relevance, not just ctr. web search engines have become an essential tool for navigating the vast amounts of information on the internet. implicit user feedback, speci cally, click through data, is valuable for optimizing search engine results. clickthrough data plays an equally important role in estimating the quality of sponsored search results. any attempt at using click through data for search or sponsored search runs into the following issue: eye tracking studies show that people tend to scan the search results in order. however, they are likely to click on a result as soon as they nd one that they consider helpful, and if that result provides a su ciently helpful answer, may not look at other results. this causes position bias: the same result will get a higher click through rate if it is positioned towards the top of the page. thus algorithms that use click through data have to take position bias into account. initial work on estimating position bias modeled ctr as the product of perceived relevance times the probability of examination. the examination probability was estimated by looking at the ctr of the same result in di erent positions. subsequently, there have been many papers on better estimating the probability of examination by using the pattern of clicks on prior results, or using both prior clicks and the relevance of prior results. we discuss this work in detail in section #. the key point is that all of these papers assume that if ctr changes when conditioned on the pattern of clicks on prior results, the change in ctr is solely due to changes in the probability of examination. the user could be planning to buy the camera immediately, in which case the sponsored results are highly relevant. on the other hand, if the user is just starting to learn about the camera, the sponsored results will be much less relevant. thus a click on the rst sponsored result is a signal that the other sponsored results are also relevant. say we now partition the query instances corresponding to the query canon into two sets based on whether the rst result got a click: the click and no click sets. the second result will be highly relevant for the query instances in the click set, and less relevant for the no click set, even though the query and result are the same. thus the second result will have higher ctr in the click set than in the no click set. however, current user models assume that the relevance is the same in both the click and no click sets, and that all the di erence in ctr for the second result is because users in the click set were more likely to examine the second result than users in the no click set. in this paper, we examine the implications of the above insight. in section #, we show that, for sponsored search results, an increase in relevance is indeed responsible for a substantial portion of the increase in ctr when conditioned on prior clicks. we then propose three new user browsing models in section #, which attribute ctr changes solely to changes in relevance, solely to changes in examination, or to both changes in relevance and examination. we evaluate the accuracy of these models when predicting ctr in section #, and the accuracy when predicting relevance in section #. our results show that, surprisingly, one model might do better than another in predicting ctr but worse in predicting relevance. we conclude with a summary of our results and directions for future work in section #. leveraging clickthrough data has become a popular approach for evaluating and optimizing information retrieval systems. although data is plentiful, one must take care when interpreting clicks, since user behavior can be affected by various sources of presentation bias. while the issue of position bias in clickthrough data has been the topic of much study, other presentation bias effects have received comparatively little attention. for instance, since users must decide whether to click on a result based on its summary, one might expect clicks to favor more attractive results. in this paper, we examine result summary attractiveness as a potential source of presentation bias. our experiments conducted on the google web search engine show substantial evidence of presentation bias in clicks towards results with more attractive titles. this study distinguishes itself from prior work by aiming to detect systematic biases in click behavior due to attractive summaries inflating perceived relevance. evaluating the quality of search result rankings has traditionally relied on explicit human judgments or editorial labels. ective, the prohibitive cost of acquiring human judgments makes it di cult to apply these evaluation approaches at scale for large search services such as commercial search engines. it is also impractical to acquire manually labeled query document relevance judgments for every possible retrieval domain, such as medical, law, physics, etc. as this study was conducted while the rst author was an intern at google. copyright is held by the international world wide web conference committee. such, collecting implicit user feedback typically click logs has grown tremendously in popularity in recent years. but what can we interpret from clicks it is well known that users are biased towards clicking on higher ranked results this is the so called position bias. but users must also judge relevance based on summaries rather than the actual results themselves. summaries typically include titles, urls, and query dependent abstracts, and often have matching query terms highlighted using boldface font. as a simple thought experiment, consider two equally relevant results with one having more bolded query terms in the title. intuitively, we might expect click behavior to favor the more attractive title. thus, even in the absence of position bias, a resultperceived relevance might noticeably di er from its actual relevance. a particularly illuminating study by clarke et al found that click inversions cannot be entirely explained by the lower ranked document being more relevant. they found that click inversions tend to co occur with additional factors such as lower ranked documents having comparatively more matching query terms in the titles. ect of bolded keyword matches in the title and abstracts on the attractiveness of the result. our analysis controls for both position bias and rated relevance, and is based on data collected using a portion of search tra. to control for position bias, we collected data using the fairpairs algorithm, which allows us to interpret clicks as preference judgments between two documents. our ndings show that clicks are measurably biased by attractive titles in ating perceived relevance. we will also discuss possible ways to adjust for title attractiveness bias. for the rest of this paper, we proceed by rst overviewing related work. section # describes our data collection methodology, which includes collecting both clickthrough data and explicit human judgments. section # describes our analysis on measuring attractiveness bias. we then discuss ways to adjust for bias in section #, and conclude with a discussion of limitations and avenues for future work. distribution of these papers is limited to classroom use, and personal use by others. www year#, april, year#, raleigh, north carolina, usa. to control for quality, we gathered human preference judgments for a subset of our clickthrough data.