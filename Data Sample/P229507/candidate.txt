previously published snippet generation algorithms have been primarily based on selecting document fragments most similar to the query, which does not take into account which parts of the document the searchers actually found useful. to achieve this aim, we develop a method for collecting behavioral data with precise association between searcher intent, document examination behavior, and the corresponding document fragments. in turn, this allows us to incorporate page examination behavior signals into a novel behavior biased snippet generation system. by mining searcher examination data, bebs infers document fragments of most interest to users, and combines this evidence with text based features to select the most promising fragments for inclusion in the result summary. our extensive experiments and analysis demonstrate that our method improves the quality of result summaries compared to existing state of the art methods. we believe that this work opens a new direction for improving search result presentation, and we make available the code and the search behavior data used in this study to encourage further research in this area. query biased search result summaries, or snippets, help users decide whether a result is relevant for their information need, and have become increasingly important for helping searchers with difficult or ambiguous search tasks. we present a new approach to improving result summaries by incorporating post click searcher behavior data, such as mouse cursor movements and scrolling over the result documents. capturing and analyzing the detailed eye movements of a user while reading a web page can reveal much about the ways in which web reading occurs. the webgazeanalyzer system described here is a remote camera system, requiring no invasive head mounted apparatus, giving test subjects a normal web use experience when performing web based tasks. while many such systems have been used in the past to collect eye gaze data, webgazeanalyzer brings together several techniques for efficiently collecting, analyzing and re analyzing eye gaze data. we briefly describe techniques for overcoming the inherent inaccuracies of such apparatus, illustrating how we capture and analyze eye gaze data for commercial web design problems. techniques developed here include methods to group fixations along lines of text, and reading analysis to measure reading speed, regressions, and coverage of web page text. we describe a reading detection algorithm and present a preliminary study to find expressive eye movement measures. reading detection is an important step in the process of automatic relevance feedback generation based on eye movements for information retrieval tasks. employing eye tracking data, we keep track of document parts the user read in some way. we use this information on the subdocument level as implicit feedback for query expansion and reranking. we evaluated three different variants incorporating gaze data on the subdocument level and compared them against a baseline based on context on the document level. our results show that considering reading behavior as feedback yields powerful improvements of the search result accuracy of ca. however, the extent of the improvements varies depending on the internal structure of the viewed documents and the type of the current information need. we examine the effect of incorporating gaze based attention feedback from the user on personalizing the search process. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. they take the usercurrent actions into account like viewing, scrolling, and querying behavior. nowadays, they have reached such a state of development that they are unobtrusive and easy to use and that their accuracy is su cient for many practical applications. since their development is progressing further, there is a good chance that they will become more. ordable and, as a consequence, become more widespread. by applying eye trackers as evidence source we are able to gather very detailed and precise data about how the user works with documents on the screen. hence, it is worth studying how eye movement data can be used. eye trackers can be applied speci cally to collect information about which document parts the user looked at and how he or she looked at them. in contrast to most other techniques that gather such implicit feedback on the document level, eye tracking goes down to the subdocument level. extreme examples are textbooks containing many chapters, eg, a text book about medicine containing a chapter about simple aspects of physics important for understanding some processes in the human body. on the side of the search engine, such a heterogeneous topical structure of a document can be taken into account by applying algorithms for passage based retrieval or by decomposing a document in topically di erent parts, for example. thus, one can expect that eye trackers are very useful sources for more precise implicit feedback. based on this information, we extract characteristic terms of the viewed document parts and use them for expanding the userquery. the paper is organized as follows: first, we provide background information on personalization methods and on the utilization of eye tracking in information retrieval. then we describe our study design, rst from the perspective of the participants and then from a technical perspective. the results of the experiment are viewed and analyzed from different angles and on di erent levels of detail. the importance of personalization in information retrieval increased over the last years because the incorporation of user preferences and user context can substantially enhance the quality of search results. they range from analyzing the userpersonal document collection to observing the useractions like issued queries, visited web sites, etc. on the one hand, methods for long term modeling of the userpersistent interests have been developed. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. on the other hand, methods for short term modeling of the userimmediate information needs have been investigated. in our work we focus on short term modeling of the userinterests and especially investigate the. this is important, because documents often contain several topics that are somewhat related but clearly di erent when viewed from a more detailed perspective. however, on the userside, if a document is displayed it is di cult to determine which parts of the document actually matter to the user, ie, the parts he or she actually works with. in this regard, an eye tracker provides very useful information about which parts of the document the user pays attention to. in this paper, we describe a user study to investigate whether and how much di erent variants considering eye movements to elicit the usershort term context can enhance the quality of retrieval by query expansion and reranking. the general scenario is to record which document parts were paid attention to immediately before querying a search engine. the expanded query is then applied for result reranking. we examine three eye tracking based variants for query expansion and compare them against a baseline which operates on the document level, ie, it extracts terms from full text documents. in this paper, we propose a novel context aware query suggestion approach which is in two steps. in the offine model learning step, to address data sparseness, queries are summarized into concepts by clustering a click through bipartite. then, from session data a concept sequence suffix tree is constructed as the query suggestion model. query suggestion plays an important role in improving the usability of search engines. although some recently proposed methods can make meaningful query suggestions by mining query patterns from search logs, none of them are context aware they do not take into account the immediately preceding queries as context in query suggestion. in the online query suggestion step, a user search context is captured by mapping the query sequence submitted by the user to a sequence of concepts. by looking up the context in the concept sequence sufix tree, our approach suggests queries to the user in a context aware manner. we test our approach on a large scale search log of a commercial search engine containing billion search queries, billion clicks, and million query sessions. the experimental results clearly show that our approach outperforms two baseline methods in both coverage and quality of suggestions. ectiveness of information retrieval from the web largely depends on whether users can issue queries to search engines, which properly describe their information needs. writing queries is never easy, because usually queries are short and words are ambiguous. therefore, there is no standard or optimal way to issue queries to search engines, and it is well recognized that query formulation is a bottleneck issue in the usability of search engines. that is, by guessing a usersearch intent, a search engine suggests queries which may better re ect the userinformation need. it is hard to determine the usersearch intent, ie, whether the user is interested in the history of gladiator, famous gladiators, or the lm gladiator. moreover, the user is probably searching the lms played by russell crowe. in this paper, we propose a novel context aware query suggestion approach by mining click through data and session data. first, instead of mining patterns of individual queries which may be sparse, we summarize queries into concepts. to tackle these challenges, we develop a novel, highly scalable yet. we develop a novel structure of concept sequence su. third, we empirically study a large scale search log containing billion search queries, billion clicks, and million query sessions. we explore several interesting properties of the click through bipartite and illustrate several important statistics of the session data. the clustering algorithm and the query suggestion method are described in sections and, respectively. to make the problem even more complicated, di erent search engines may respond di erently to the same query. recently, most commercial search engines such as google, yahoo, live search, ask, and baidu provide query suggestions to improve usability. a commonly used query suggestion method is to nd similar queries in search logs and use those queries as suggestions for each other. another approach mines pairs of queries which are adjacent or co occur in the same query sessions. although the existing methods may suggest good queries in some cases, none of them are context aware they do not take into account the immediately preceding queries as context in query suggestion. without looking at the context of search, the existing methods often suggest many queries for various possible intents, and thus may have a low accuracy in query suggestion. if we nd that the user submits a query beautiful mind before gladiator, it is very likely that the user is interested in the lm gladiator. the query context which consists of the recent queries issued by the user can help to better understand the usersearch intent and enable us to make more meaningful suggestions. a concept is a group of similar queries. although mining concepts of queries can be reduced to a clustering problem on a bipartite graph, the very large data size and the curse of dimensionality pose great challenges. we may have millions of unique queries involving millions of unique urls, which may result in hundreds of thousands of concepts. second, there are often a huge number of patterns that can be used for query suggestion. how to mine those patterns and organize them properly for fast query suggestion is far from trivial. the data set in this study is several magnitudes larger than those reported in previous work. last, we test our query suggestion approach on the search log. the experimental results clearly show that our approach outperforms two baseline methods in both coverage and quality of suggestions. the rest of the paper is organized as follows. we rst present the framework of our approach in section # and review the related work in section #. we report an empirical study in section #. the inherent ambiguity of short keyword queries demands for enhanced methods for web retrieval. in this paper we propose to improve such web queries by expanding them with terms collected from each user personal information repository, thus implicitly personalizing the search output. we introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co occurrence statistics, as well as to using external thesauri. our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings. subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query. a separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach. the booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the web. yet keyword queries are part of this work was performed while the author was visiting yahoo research, barcelona, spain. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. the query canon book for example covers several different areas of interest: religion, photography, literature, and music. clearly, one would prefer search output to be aligned with usertopic of interest, rather than displaying a selection of popular urls from each category. studies have shown that more than of the users would prefer to receive such personalized search results instead of the currently generic ones. query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the web search output accordingly. it has been shown to perform very well over large data sets, especially with short input queries. this is exactly the web search scenario in this paper we propose to enhance web query reformulation by exploiting the userpersonal information repository, ie, the personal collection of text documents, emails, cached web pages, etc. several advantages arise when moving web search personalization down to the desktop level. first is of course the quality of personalization: the local desktop is a rich repository of information, accurately describing most, if not all interests of the user. second, as all pro le information is stored and exploited locally, on the personal machine, another very important bene. search engines should not be able to know about a personinterests, ie, they should not be able to connect a speci. person with the queries she issued, or worse, with the output urls she clicked within the search interface. our algorithms expand web queries with keywords extracted from userpir, thus implicitly personalizing the search output. after a discussion of previous works in section #, we rst investigate the analysis of local desktop query context in section #. we propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the web query best. in section # we move our analysis to the global desktop collection and investigate expansions based on co occurrence metrics and external thesauri. the experiments presented in section # show many of these approaches to perform very well, especially on ambiguous queries, producing ndcg improvements of up to. in section # we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query. this yields an additional improvement of over the previously identi ed best algorithm. we conclude and discuss further work in section #. search engines can map queries at least to ip addresses, for example by using cookies and mining the query logs. however, by moving the user pro le at the desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. traditionally, search engines have ignored the reading difficulty of documents and the reading proficiency of users in computing a document ranking. while there are many important problems in interface design, content filtering, and results presentation related to addressing children search needs, perhaps the most fundamental challenge is simply that of providing relevant results at the right level of reading difficulty. we show how reading level can provide a valuable new relevance signal for both general and personalized web search. we describe models and algorithms to address the three key problems in improving relevance for search using reading difficulty: estimating user proficiency, estimating result difficulty, and re ranking based on the difference between user and result reading level profiles. this is one reason why web search engines do a poor job of serving an important segment of the population: children. at the opposite end of the proficiency spectrum, it may also be valuable for technical users to find more advanced material or to filter out material at lower levels of difficulty, such as tutorials and introductory texts. we evaluate our methods on a large volume of web query traffic and provide a large scale log analysis that highlights the importance of finding results at an appropriate reading level for the user. our goal is to show how modeling reading pro ciency of users and the reading di culty of documents can be used to improve the relevance of web search results. this goal is motivated by the fact that content on the web is written at a wide range of di erent reading levels: from easy introductory texts and material written speci cally for children, to di cult, highly technical material for experts that requires advanced vocabulary knowledge to comprehend. di er widely in their reading pro ciency and ability to understand vocabulary, depending on factors such as age, educational background, and topic interest or expertise. these facts currently impair the ability of users to carry out successful searches by nding material at an appropriate level of reading di culty for them. as an example of the need, and potential, for personalization by reading level, consider the query, whose actual top ranked results from a major search engine are shown in table #. second, we describe how a userreading pro ciency pro le may be estimated automatically from their current and past search behavior. third, we use this pro le to train a re ranking algorithm that combines both relevance and di culty in a principled way, and which generalizes easily to broader tasks such as expertise based re ranking. in this view, the overall relevance of a document is a combination of two factors: a general relevance factor, provided by an existing ranking algorithm, and a user speci. reading di culty model, based on the gap between a userpro ciency level and a documentdi culty level. while users may self identify their desired level of result di culty, such information may not always be provided. we therefore investigate methods for estimating a reading pro ciency pro le for users based on their online search interaction patterns. section # then develops the theoretical models and algorithms we use to address each of these three areas. in section # we contrast the search behavior of two groups: users looking for kids related material and general users, by performing a large scale log analysis of query, session, and result properties. ectiveness of those algorithms according to implicit relevance judgments obtained from actual web search logs comprising queries, search result clicks, and post click navigation events. there are several results, however, including the top two, that contain highly technical research oriented content most appropriate for specialists only. to address this problem, we describe a tripartite approach based on user pro les, document di culty, and re ranking. rank url domain title category reading level insectdiets com insect diet rearing research technical research high imfc cfl rncan gc ca insect diet technical research high www sugar glider store com insect eater diet commercial medium insectdiets com insect rearing research technical research high insectrearing com bio serv entomology division commercial, technical medium www ehow com aquatic insects diet educational medium www exoticnutrition com insect eater diet. ectively combine relevance and di culty signals to improve search quality. this analysis helps provide insights into features that may be useful for improving re ranking by reading level. finally, in sections and we discuss areas for future research and summarize our ndings. web users permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. web search engines, however, typically use algorithms optimized for the average user, not speci. while a younger child may be more likely to have chosen query terms like or, the choice of could have been made by a child doing a class project, or an elementary school teacher searching for low di culty material; parents or middle school science students may require intermediate material, and more advanced high school and college users may require sites describing entomology research, as the top results do here. only one result, at rank position eight, is at a low level of di culty. clearly there is a need for improvement in ranking search results at an appropriate level of reading di culty. first, we discuss how snippets and web pages can be labeled with reading level and combined with open directory project category predictions. in section # we review related work in reading di culty prediction, modeling user expertise, and search systems for children. we then describe three key problems that must be addressed in using reading level to improve web search relevance: estimating table #: top ten results, in rank order, for the query from a commercial search engine, showing the wide variation in reading level that can occur for material retrieved on the same query. reading level here is estimated using the statistical model described in section # and shown in brackets. commercial medium www tutorvista com insect diet: questions answers educational low www encarta msn com dictionary not relevant a deltafarmpress com producers may put sh on insect diet technical news high a pro le of the userreading pro ciency, estimating a documentreading di culty, and re ranking algorithms that can. perhaps because search is used in such a huge variety of tasks and contexts, the user interface must strike a careful balance to meet all user needs. we describe a study that used eye tracking methodologies to explore the effects of changes in the presentation of search results. we found that adding information to the contextual snippet significantly improved performance for informational tasks but degraded performance for navigational tasks. we discuss possible reasons for this difference and the design implications for better presentation of search results. web search services are among the most heavily used applications on the world wide web. especially in on line scenarios there is a great amount of content that is not suitable for their age group. a range of different features based on findings from cognitive sciences and children psychology are discussed and evaluated. finally a comparison to traditional web classification methods as well as human annotator performance reveals that our automatic classifier can reach a performance close to that of human agreement. today children interact more and more frequently with information services. due to the growing importance and ubiquity of the internet in today world, denying children any unsupervised web access is often not possible. this work presents an automatic way of distinguishing web pages for children from those for adults in order to improve child appropriate web search engine performance. we conducted a large scale user study on the suitability of web sites and give detailed information about the insights gained. the age at which children have their rst contact with the internet has become consistently and permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. er safe search functionalities that reliably remove adult material. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. judging which content is relevant for their information need can be hard. children face the same situation, but su er even more from it as they tend to unquestioningly believe in any kind of information and readily absorb new knowledge. it is hardly possible to supervise childreninternet usage continuously in person as would be necessary in order to make sure that children are not exposed to inappropriate content. er high quality childrenpages, but due to the high amount of manual labour involved in their maintenance they are less exible and have a much more limited coverage than an automatic approach could achieve. it is important to note that assuring appropriateness exceeds ltering. this kind of ltering usually takes a topical approach, which is already well understood. in this work, we will show how well studied means of topical classi cation can be augmented by non topical techniques in order to make the multi faceted suitability decision. the contributions of our work are threefold: we identify the criteria of good childrenweb pages based on childrenspeci. the remainder of this paper is structured as follows. section # aims to nd promising subsets of the full feature space. after presenting our performance baseline, we conduct a range of experiments to evaluate our classi erperformance in sections and. finally, we close with a discussion of the insights gained from this study in section #. the use of the internet has become an integral part of most peoplelives. this tendency also holds true for children who naturally adopt the behaviour that is displayed by parents and teachers. even adults often struggle to cope with the amounts of information on the web. while this in general is a desirable and essential characteristic that enables children to easily acquire large amounts of information such as new words, in the case of the internet it imposes potential dangers. several popular web search engines such as google and yahoo even state in their terms of service that their search should not be used by minors. recent surveys found that close to of uk children aged years access the internet without parental guidance or supervision. considering these numbers, an automatic means of determining child appropriateness of web pages would be highly desirable. state of the art childrenweb search engines could greatly bene. at the moment childrensearch resources are typically directories of manually selected web pages. examples of current web search facilities for children are yahoo kids or ask kids. notions of text di culty and age appropriate web site design however are independent of the page topic and should strongly contribute to the decision of showing a certain page to a child. an automatic solution should take care of various aspects, assessing appropriateness in terms of topical relevance, textual content di culty and presentation style. needs and abilities and show how to encode http: www google com accounts tos http: info yahoo com legal us yahoo utos these criteria with features. we conducted a large scale user survey to better understand the nature of childrenweb pages. the results of this survey and its implications for our work are discussed in detail. we describe a classi cation method for web pages, demonstrating that purely topical models can be outperformed by models augmented by non topical aspects. we begin with a brief review of related research in section #. in section #, we introduce a range of web page features and show their relevance to our task. in this paper, we present an in depth analysis of sessions in which people explicitly search for new knowledge on the web based on the log files of a popular search engine. we investigate within session and cross session developments of expertise, focusing on how the language and search behavior of a user on a topic evolves over time. our findings provide insight into how search engines might better help users learn as they search. the internet is the largest source of information in the world. search engines help people navigate the huge space of available data in order to acquire new skills and knowledge. in this way, we identify those sessions and page visits that appear to significantly boost the learning process. our experiments demonstrate a strong connection between clicks and several metrics related to expertise. based on models of the user and their specific context, we present a method capable of automatically predicting, with good accuracy, which clicks will lead to enhanced learning. or commercial advantage and that copies bear this notice and the full citation on the rst page. wsdm, february, year#, new york, new york, usa. jaime teevan, ryen white, susan dumais microsoft research, wa, usa microsoft com in recent years, search systems have employed representations of the userpreferences, current context, and recent behavior to more accurately model user interests and intentions. needs of individual searchers rather than considering only the dominant search intent across all users. previous work has shown that searchers acquire domain knowledge by means of web search. this increase in expertise is a byproduct of the actual search process. commercial search engines often assess relevance using explicit judgments and click through statistics, considering long dwell times as a proxy for searcher satisfaction. following this paradigm of the shortest path to the goal, the searcher might not be exposed to additional, relevant information. based on such sessions, we examine changes in domain expertise within search sessions and show how these changes are sustained across session boundaries. finally, we present an automatic means of predicting the potential for learning from page visits. the remainder of this paper is structured as follows: section # provides an overview of related work in the areas of web search personalization, search intent frameworks, domain expertise, and exploratory search. in section #, we identify web search sessions in which people explicitly seek to acquire knowledge and characterize di erent knowledge acquisition intent types seen in the log les of a popular web search engine. section # studies how domain expertise develops within a session and how it is sustained across sessions. sections and present a discussion of our ndings and their implications on future research, and practical applications in web search. with size estimates of to billion indexed pages on commercial web search engines, the internet is a very large collection of information. ordable and easily available internet connections, searching for information on line has become a natural step in the knowledge acquisition process for modern society. the wealth of available information makes tools such as web search engines indispensable in identifying and accessing information. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. this allows systems to better address the speci. however, even with such advanced retrieval technology, exploratory and open ended information needs can be challenging to satisfy, especially when the searcher is unfamiliar with the domain in question. continued exposure to and interaction with information appears to in uence the users domain expertise over time. this observation is of particular interest with regard to modern relevance models. state of the art search algorithms attempt to maximise the relevance of a list of results retrieved for an expressed information need. in this paper, we study the development of expertise at the session level in order to better understand the value that the journey towards the nal result, as opposed to just that result in isolation, holds for the searcher. our work makes three major contributions over the state of the art in knowledge acquisition: we present an investigation of explicit knowledge seeking sessions dedicated to nding procedural or declarative information. we investigate factors related to changes in domain expertise by studying the connection between page visits and the subsequent development of expertise and searcher behavior. section # investigates the reasons that underly the previously observed learning. in section #, based on page level features, we predict which page visits are most likely to help the users in expanding their domain expertise. query suggestion aims to suggest relevant queries for a given query, which help users better specify their information needs. previously, the suggested terms are mostly in the same language of the input query. this is very important to scenarios of cross language information retrieval and cross lingual keyword bidding for search engine advertisement. instead of relying on existing query translation technologies for clqs, we present an effective means to map the input query of one language to queries of the other language in the query log. important monolingual and cross lingual information such as word translation relations and word co occurrence statistics, etc. are used to estimate the cross lingual query similarity with a discriminative model. benchmarks show that the resulting clqs system significantly out performs a baseline system based on dictionary based query translation. besides, the resulting clqs is tested with french to english clir tasks on trec collections. in this paper, we extend it to cross lingual query suggestion: for a query in one language, we suggest similar or relevant queries in other languages. the results demonstrate higher effectiveness than the traditional query translation methods. query suggestion is a functionality to help users of a search engine to better specify their information need, by narrowing down or expanding the scope of the search with synonymous queries and relevant queries, or by suggesting related queries that have been frequently used by other users. search engines, such as google, yahoo, msn, ask jeeves, all have implemented query suggestion functionality as a valuable addition to their core search method. in addition, the same technology has been leveraged to recommend bidding terms to online advertiser in the pay forperformance search market. query suggestion is closely related to query expansion which extends the original query with new search terms to narrow the scope of the search. but different from query expansion, query suggestion aims to suggest full queries that have been formulated by users so that the query integrity and coherence are preserved in the suggested queries. typical methods for query suggestion exploit query logs and document collections, by assuming that in the same period of time, many users share the same or similar interests, which can be expressed in different manners. by suggesting the related and frequently used formulations, it is hoped that the new query can cover more relevant documents. clqs aims to suggest related queries but in a different language. it has wide applications on world wide web: for cross language search or for suggesting relevant bidding terms in a different language. clqs can be approached as a query translation problem, ie, to suggest the queries that are translations of the original query. dictionaries, large size of parallel corpora and existing commercial machine translation systems can be used for translation. however, these kinds of approaches usually rely on static knowledge and data. it cannot effectively reflect the quickly shifting interests of web users. for instance, this work was done while the author was visiting microsoft research asia. the translated terms can be reasonable translations, but they are not popularly used in the target language. for example, the french query aliment biologique is translated into biologic food by google translation tool, yet the correct formulation nowadays should be organic food. therefore, there exist many mismatch cases between the translated terms and the really used terms in target language. this mismatch makes the suggested terms in the target language ineffective. a natural thinking of solving this mismatch is to map the queries in the source language and the queries in the target language, by using the query log of a search engine. we exploit the fact that the users of search engines in the same period of time have similar interests, and they submit queries on similar topics in different languages. as a result, a query written in a source language likely has an equivalent in a query log in the target language. in particular, if the user intends to perform clir, then original query is even more likely to have its correspondent included in the target language query log. therefore, if a candidate for clqs appears often in the query log, then it is more likely the appropriate one to be suggested. in this paper, we propose a method of calculating the similarity between source language query and the target language query by exploiting, in addition to the translation information, a wide spectrum of bilingual and monolingual information, such as term co occurrences, query logs with click through data, etc. a discriminative model is used to learn the cross lingual query similarity based on a set of manually translated queries. the model is trained by optimizing the cross lingual similarity to best fit the monolingual similarity between one query and the other querytranslation. besides being benchmarked as an independent module, the resulting clqs system is tested as a new means of query translation in clir task on trec collections. the results show that this new translation method is more effective than the traditional query translation method. the remainder of this paper is organized as follows: section # introduces the related work; section # describes in detail the discriminative model for estimating cross lingual query similarity; section # presents a new clir approach using cross lingual query suggestion as a bridge across language boundaries. section # discusses the experiments and benchmarks; finally, the paper is concluded in section #. however, all of the existing studies dealt with monolingual query suggestion and to our knowledge, there is no published study on cross lingual query suggestion. moreover, there are some problems with translated queries in target language. we investigate how users interact with the results page of a www search engine using eye tracking. the goal is to gain insight into how users browse the presented abstracts and how they select links for further exploration. such understanding is valuable for improved interface design, as well as for more accurate interpretations of implicit feedback for machine learning. the following presents initial results, focusing on the amount of time spent viewing the presented abstracts, the total number of abstract viewed, as well as measures of how thoroughly searchers evaluate their results set. how do users interact with the list of ranked results of www search engines do they read the abstracts sequentially from top to bottom, or do they skip links how many of the results do users evaluate before clicking on a link or reformulating the search the answers to these questions will be beneficial in at least three ways. first, they provide the basis for improved interfaces. second, they suggest more targeted metrics for evaluating the retrieval performance in www search. and third, they help interpreting implicit feedback like clickthrough and reading times for machine learning of improved retrieval functions. in particular, better understanding of user behavior will allow us to draw more accurate inferences about how implicit feedback relates to relative relevance judgments. the following presents the results of an eye tracking study that we conducted. previous studies have analyzed directly observable data like query word frequency. however, unlike eyetracking, these measurements can at best give indirect evidence of how users perceive and respond to the search results. to the best of our knowledge, only one previous study has used eye tracking in the context of information retrieval evaluation. this study attempted to use eye movements to infer the relevancy of documents in the retrieval phase of an information search. the researchers linked relevancy judgments to increases in pupil diameter, as a larger diameter typically signifies high interest in the content matter. however, the sample size and search tasks in this experiment were not robust enough to generate predictable patterns of user search and scanning behavior, which is what our study is able to attain. result clickthrough statistics and dwell time on clicked results have been shown valuable for inferring search result relevance, but the interpretation of these signals can vary substantially for different tasks and users. to this end, we identify patterns of examination and interaction behavior that correspond to viewing a relevant or non relevant document, and design a new post click behavior model to capture these patterns. to our knowledge, pcb is the first to successfully incorporate post click searcher interactions such as cursor movements and scrolling on a landing page for estimating document relevance. we evaluate pcb on a dataset collected from a controlled user study that contains interactions gathered from hundreds of unique queries, result clicks, and page examinations. the experimental results show that pcb is significantly more effective than using page dwell time information alone, both for estimating the explicit judgments of each user, and for re ranking the results using the estimated relevance. this paper shows that that post click searcher behavior, such as cursor movement and scrolling, provides additional clues for better estimating document relevance. in this paper, we argue that post click search behavior provides essential evidence for estimating the intrinsic page relevance for a search task. while previous research has made great use of result clickthrough data, the usefulness of clickthrough statistics is limited by a number of presentation biases, which strongly in uence user click behavior. one of the most signi cant limitations of clickthrough data, is that clicks are based primarily on a documentperceived relevance, where a searcher guesses the pagerelevance based on a short summary generated by the search engine. copyright is held by the international world wide web conference committee. www year#, april, year#, lyon, france acm year#. eugene agichtein mathematics computer science department emory university eugene mathcs emory edu however, the perceived relevance may be inconsistent with the actual intrinsic relevance, where a searcher clicks on a result only to nd out that it is not relevant. to address this problem, page dwell time has been proposed as a measure of intrinsic document relevance. the most heavily studied scenario is that of a bounce back, which happens when the searcher returned to the search engine result page shortly after she clicked on a result, indicating low result relevance. in fact, a most frustrating scenario is when a searcher spends a long time searching for relevant information on a seemingly promising page that she clicked, but fails to nd the needed information. yet, based on dwell time alone, this document would be considered highly relevant, and remain high in the search ranking to frustrate future searchers. to address this problem, we propose to use post click searcher behavior to more precisely analyze how the searchers spend their time on the landing pages and the subsequently viewed documents, which would in turn allow for more accurate estimation of intrinsic document relevance. as an illustration, figures show the searchers cursor movement on clicked result pages for the task of nding the phone number of the verizon wireless helpline for massachusetts, where the user spends approximately seconds examining each of the pages. the color intensity in the gures indicates the amount of time the mouse cursor spent over the corresponding document regions, with the exact cursor coordinates indicated by the small crosses. the differences in the examination of a relevant page and a non relevant page are striking. for the former, the searcher was carefully reading the text and using the mouse as a reading aid, while for the latter, the searcher appears to be skimming or scanning the page, without nding relevant information worth careful reading. this example illustrates our underlying hypothesis: that page dwell time alone is not suf cient to distinguish between relevant and non relevant pages, but post click searcher behavior can provide the necessary additional evidence to distinguish the two. relevant non relevant figure #: cursor based reading examination heatmap of a relevant document compared to scanning of a non relevant document, both with equal dwell time. speci cally, we hypothesize that searcher interactions on landing pages such as cursor movements and scrolling can help more accurately interpret searcher viewing behavior, in turn, improve relevance estimation. that is, like eye movements, such interactions can re ect searcher attention. these interactions can be captured with javascript code that is embedded in a browser add on. this would allow estimating whether some parts of the landing page captured the searcherattention and provide additional clues about the document relevance. to test this hypothesis, we rst identify patterns of examination and interaction behavior that correspond to viewing a relevant or non relevant document, and develop a novel model of inferring document relevance that incorporates rich post click behavior such as cursor movements and scrolling that could capture these patterns. in summary, our contributions include: characterizing patterns of examination and interaction behavior that correspond to viewing a relevant or non relevant document. pcb, a novel model of relevance estimation that captures post click behavior. empirical evidence that pcb is more effective than using dwell time information alone, both for estimating the explicit judgments of each user, as well as for ranking the documents using the estimated relevance. survey the background and related work to put our contribution in context. estimating document relevance is at the core of information retrieval ranking and evaluation. unfortunately, this task is notoriously dif cult: even the notion of relevance itself varies for different tasks and users. distribution of these papers is limited to classroom use, and personal use by others. the main intuition is that short dwell time, indicates that a document is non relevant. this heuristic and resulting metrics have been successfully adapted by the major search engines, and have undoubtedly improved search quality by detecting non relevant or even detrimental results. unfortunately, the converse of the short dwell time rule is not true: a long page dwell time does not necessarily imply result relevance. the model is operationalized by converting these interactions into features, which can then be used as input to machine learning algorithms for tasks such as estimating personalized and aggregate document relevance, and improving result ranking. a key problem in information retrieval is inferring the searcher interest in the results, which can be used for implicit feedback, query suggestion, and result ranking and summarization. one important indicator of searcher interest is gaze position that is, the results or the terms in a result listing where a searcher concentrates her attention. capturing this information normally requires eye tracking equipment, which until now has limited the use of gaze based feedback to the laboratory. while previous research has reported a correlation between mouse movement and gaze position, we are not aware of previous work on automatically inferring searcher gaze position from mouse movement or similar interface interactions. in this paper, we report the first results on automatically inferring whether the searcher gaze position is coordinated with the mouse position a crucial step towards predicting the searcher gaze position from the computer mouse movements. web search components such as ranking and query suggestions analyze the user data provided in query and click logs. while this data is easy to collect and provides information about user behavior, it omits user interactions with the search engine that do not hit the server; these logs omit search data such as users cursor movements. we start by exploring recorded user interactions with the search results, both qualitatively and quantitatively. we find that cursor hovering and scrolling are signals telling us which search results were examined, and we use these interactions to reveal latent variables in searcher models to more accurately compute document attractiveness and satisfaction. accuracy is evaluated by computing how well our model using these parameters can predict future clicks for a particular query. we are able to improve the click predictions compared to a basic searcher model for higher ranked search results using the additional log data. just as clicks provide signals for relevance in search results, cursor hovering and scrolling can be additional implicit signals. in this work, we demonstrate a technique to extend models of the user search result examination state to infer document relevance. web search engines allow users to search and retrieve relevant documents from billions of web pages. the user issues a query and the search engine returns a list of results, gen permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. white microsoft research redmond, wa ryenw microsoft com kuansan wang microsoft research redmond, wa kuansanw microsoft com erally ranked in order of relevance. the relevance of the results is based on a number of factors: how well a docu ment matches the query, a documentreputation, and more recently, implicit feedback in the form of past behaviors for that query from many users. records of these user behav iors are commonly sourced from query logs containing users queries and the corresponding clicks, if any. query logs are easy to collect, since they typically already exist as web server access logs without modi cation to the search engine. being able to compute relevance scores from implicit feed back allows a search engine to better rank the search results for future queries. clicks provide a clear signal that users were attracted to the search result, and numerous studies have used click data in searcher models to infer relevance scores. these searcher models track the userstate as they examine search results and use the observable events to infer search result attractiveness and document relevance. however, query logs possess inherent limitations, some of which have been noted in the literature. they are unable to reveal actual user intent, provide little data about uncommon queries, and omit many interactions. furthermore, they are uninforma tive for queries that have no clicks, ie, abandoned queries. in this paper, we introduce richer interaction data that can be used to supplement query and click data. this richer data comprises cursor movements and scrolling on the search engine results page, data which is not collected by commercial search engines but may be potentially useful. we believe cursor movements and scrolling can be additional implicit signals of relevance. these interactions can be cap tured at scale and can be recorded without disrupting the user, as shown in huang et al. actions such as cursor hovering and scrolling can be translated into implicit rele vance feedback when overlaid on the serp. in this work, we explore techniques to extend searcher models by using cur sor hovering and scrolling activity to reveal latent variables in these searcher models to more accurately infer search re sult attractiveness and document relevance. as far as we are aware, this is the rst study that explores the potential of cursor and scrolling interactions for use in searcher models. our primary contribution in this work is our study of extending a popular searcher model by adding hover and scroll data, informed by our analysis of replays of user in teractions on the search results page. we nd qualitative evidence that from a human observerperspective, hover ing and scrolling provide insight into the userintentions and attention as they examine the serp. we nd that we can improve searcher models by estimating whether a search result was viewed based on cursor hover and scroll behavior. in section # we describe related work that characterizes cursor interactions with serps and searcher models. section # describes the cursor data that we used in our study. we present an initial exploratory analysis of the data, which was useful in informing decisions about model features, in section #. in section # we describe an extension to the searcher model using the cursor hover and scroll data, and present results of experiments using them in section #. we discuss the ndings, their implications, and limitations of the method in section #, and conclude in section #. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. the remainder of the paper is structured as follows. our findings help us better understand how searchers use cursors on serps and can help design more effective search systems. understanding how people interact with search engines is important in improving search quality. web search engines typically analyze queries and clicked results, but these actions provide limited signals regarding search interaction. laboratory studies often use richer methods such as gaze tracking, but this is impractical at web scale. in this paper, we examine mouse cursor behavior on search engine results pages, including not only clicks but also cursor movements and hovers over different page regions. we: report an eye tracking study showing that cursor position is closely related to eye gaze, especially on serps; present a scalable approach to capture cursor movements, and an analysis of search result examination behavior evident in these large scale cursor data; and describe two applications that demonstrate the value of capturing cursor data. our scalable cursor tracking method may also be useful in non search settings. this paper examines the reliability of implicit feedback generated from clickthrough data in www search. analyzing the users decision process using eyetracking and comparing implicit feedback against manual relevance judgments, we conclude that clicks are informative but biased. while this makes the interpretation of clicks as absolute relevance judgments difficult, we show that relative preferences derived from clicks are reasonably accurate on average. the idea of adapting a retrieval system to particular groups of users and particular collections of documents promises further improvements in retrieval quality for at least two reasons. second, as evident from the trec evaluations, differences between document collections make it necessary to tune retrieval functions with respect to the collection for optimum retrieval performance. since manually adapting a retrieval function is time consuming or even impractical, research on automatic adaptation using machine learning is receiving much attention. ithaca, ny, usa cornell edu however, a great bottleneck in the application of machine learning techniques is the availability of training data. in this paper we explore and evaluate strategies for how to automatically generate training examples for learning retrieval functions from observed user behavior. however, implicit feedback is more di cult to interpret and potentially noisy. in this paper we analyze which types of implicit feedback can be reliably extracted from observed user behavior, in particular clickthrough data in www search. the study is designed to analyze how users interact with the list of ranked results from the google search engine and how their behavior can be interpreted as relevance judgments. first, we use eyetracking to understand how users behave on googleresults page. do users scan the results from top to bottom how many abstracts do they read before clicking how does their behavior change, if we arti cially manipulate googleranking answers to these questions give insight into the users decision process and suggest in how far clicks are the result of an informed decision. based on these results, we propose several strategies for generating feedback from clicks. to evaluate the degree to which feedback signals indicate relevance, we compare the implicit feedback against explicit feedback we collected manually. the study presented in this paper is di erent in at least two respects from previous work assessing the reliability of implicit feedback. first, our study provides detailed insight into the users decision making process through the use of eyetracking. second, we evaluate relative preference signals derived from user behavior. this is in contrast to previous studies that primarily evaluated absolute feedback. our results show that users make informed decisions among the abstracts they observe and that clicks re ect relevance judgments. however, we show that clicking decisions are biased in at least two ways. first, we show that there is a trust bias which leads to more clicks on links ranked highly by google, even if those abstracts are less relevant than other abstracts the user viewed. second, there is a quality bias: the users clicking decision is not only in uenced by the relevance of the clicked link, but also by the overall quality of the other abstracts in the ranking. we propose several strategies for extracting such relative relevance judgments from clicks and show that they accurately agree with explicit relevance judgments collected manually. first, a one size ts all retrieval function is necessarily a compromise in environments with heterogeneous users and is therefore likely to act suboptimally for many users. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. in contrast to explicit feedback, such implicit feedback has the advantage that it can be collected at much lower cost, in much larger quantities, and without burden on the user of the retrieval system. to evaluate the reliability of implicit feedback signals, we conducted a user study. we performed two types of analysis in this study. this shows that clicks have to be interpreted relative to the order of presentation and relative to the other abstracts. however, most studies have been confined to specific search domains, such as news, and have not considered the effects of task on display time, and the potential impact of this relationship on the effectiveness of display time as implicit feedback. we describe the results of an intensive naturalistic study of the online information seeking behaviors of seven subjects during a fourteen week period. throughout the study, subjects online information seeking activities were monitored with various pieces of logging and evaluation software. subjects were asked to identify the tasks with which they were working, classify the documents that they viewed according to these tasks, and evaluate the usefulness of the documents. results of a user centered analysis demonstrate no general, direct relationship between display time and usefulness, and that display times differ significantly according to specific task, and according to specific user. recent research has had some success using the length of time a user displays a document in their web browser as implicit feedback for document preference. tailoring retrieval to individuals is becoming an important area of research in interactive information retrieval. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. belkin scils rutgers university new brunswick, nj usa nick belkin rutgers edu documents. however, a significant challenge is identifying efficient and reliable techniques for creating and maintaining user models. specifically, determining how to get information about the user into the model is a difficult problem. one approach to this problem is to take advantage of the userprevious information seeking behaviors to identify documents that have been of interest to that person in the past, and use these documents as sources of information for the model. the model could be constructed, for instance, by identifying and recording the documents that the person has looked at and found useful, and automatically classifying those documents according to topic models, derived from the language of the documents. a new search by the user could be associated with one or a few such models, thereby effectively disambiguating the search topic and providing a basis for tailored retrieval. see for a review and classification of this research. a general finding of this research is that users display documents that they find useful longer than those that they do not. however, such studies have been limited because most assume that information seeking behavior is not influenced by contextual factors such as task, topic and collection. in the interactive ir literature, it is generally believed that information seeking behavior is affected by task in a variety of ways. task has been used to explain differences in relevance assessments of information objects and differences in approaches to system use, such as use of search tactics and terms. research on implicit feedback has paid little or no attention to task. most studies have only investigated a single task, such as news or job searching. for instance, morita and shinoda and miller, et al considered the behavior of users interacting with online news services like netnews and usenet. rafter and smyth considered the behavior of users as they interacted with an online job bank. kim, oard, and romanik studied behavior in a more traditional information seeking task, finding sources for a research paper, and cooper and chen investigated how behavior could be used as implicit feedback in an online library card catalog. studies that place no limits on the types of information seeking activities investigated like claypool, et al, make no attempt to measure task, and instead, construe the task as finding useful or interesting information. with little exception, studies of implicit feedback have not characterized informationseeking tasks, or conducted systematic investigations of their impact on the use of observable behaviors as implicit feedback. the exception is kelly and belkin who found that factors like topic familiarity and task type confound the relationship between display time and relevance in complex ways. the current study investigates the relationship between information seeking task and display time, and the potential impact of this relationship on the effectiveness of display time as an implicit measure of document preference. recent research has had some success using information seeking behaviors as implicit feedback for document preference. empirical evidence demonstrating that users exhibit a range of information seeking behaviors that can change with respect to task is mounting. we describe results from a preliminary investigation of the relationship between topic familiarity and information search behavior. these results suggest that it may be possible to infer topic familiarity from information search behavior. two types of information search behaviors are considered: reading time and efficacy. our results indicate that as one familiarity with a topic increases, one searching efficacy increases and one reading time decreases. the underlying assumption is that different objects will be appropriate for different users depending upon their level of familiarity with the topic. our current work focuses on identifying information search behaviors that might be directly related to topic familiarity. studies about the search intermediary and user interaction have demonstrated consistently that search intermediaries attempt to characterize users among numerous facets, including knowledge of the topic. a challenge in digital libraries and ir, in general, is the development of effective techniques for model acquisition. in order to build a user model, the intermediary function, whether human or computer, must somehow elicit information from the user about his her interests and knowledge. human search intermediaries are good at eliciting knowledge from users about their information needs. knowledge of the userfamiliarity with a topic is used by the search intermediary to select appropriate retrieval strategies and information objects for individual users. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. colleen cool gslis queens college, cuny flushing, ny year# usa ccool qc edu in information filtering and retrieval, systems employing user modeling attempt to replicate some aspects of the user modeling function that are performed by human search intermediaries. these systems typically require the user to actively participate in the construction of the user model. however, there are a number of drawbacks with techniques for user model construction. for instance, many users are generally unwilling to explicitly participate in user model construction. additionally, models are often incomplete in that they do not account for changes in the userknowledge states. there has been substantial effort devoted to obtaining information for user modeling implicitly, primarily though the observation of normal human search behavior. behaviors most extensively investigated as sources for implicit feedback have been reading, saving and printing. it has been suggested that contextual factors, such as topic familiarity and task, affect the rate of occurrence of these behaviors. while it is commonly acknowledged that topic familiarity is an important factor influencing information seeking, no one has suggested how to automatically capture knowledge about topic familiarity or how to use this to tailor retrieval. we begin our investigation with the examination of two such behaviors, reading time and efficacy. query formulation is one of the most difficult and important aspects of information seeking and retrieval. to evaluate our approach, we conducted an interactive information retrieval study with subjects and topics. we also investigated the source of the suggestions: approximately half of all subjects were provided with system generated suggestions, while half were provided with user generated suggestions. in this research we combine these two techniques by automatically creating query suggestions using term relevance feedback techniques. two techniques, term relevance feedback and query suggestion, provide methods to help users formulate queries, but each is limited in different ways. each subject completed four topics, half with a term suggestion system and half with a query suggestion system. results show that subjects used more query suggestions than term suggestions and saved more documents with these suggestions, even though there were no significant differences in performance. subjects preferred the query suggestion system and rated it higher along a number of dimensions including its ability to help them think of new approaches to searching. qualitative data provided insight into subjects usage and ratings, and indicated that subjects often used the suggestions even when they did not click on them. this has been attributed to problems related to the design of rf interfaces, task complexity and the userlack of additional cognitive resources, and the amount of extra time and effort required to use such features. hsieh yee found that when working with less familiar topics, subjects were more likely to consult a thesaurus for term suggestion. the display of terms most often consists of a single list. query suggestions provide alternative ways to help users formulate queries and explore unfamiliar areas. freyne, et al describe the integration ofspy with a social navigation component and reported that subjects found query suggestions useful during browsing. however, much of the evidence from interactive studies of rf indicates that such features are not often used. techniques to assist users with query formulation seem particularly important in cases where users are searching for topics about which they have little knowledge or familiarity. users may lack the vocabulary and knowledge to sufficiently cover all aspects of the search topic, and in some cases to even formulate an initial search query. these results suggest query formulation assistance might be particularly useful when users are unfamiliar with topics. one problem with current term rf interfaces is that terms are often presented in isolation, which might make it difficult for users to fully comprehend relationships between terms and their information needs, especially for users who are less familiar with topics. in a previous study, two interfaces that provided term context were compared to a baseline term rf interface which presented a list of terms to users. joho, et al presented users with a list and a menu hierarchy display for query expansion terms and found that subjects selected more terms from the menu hierarchy, although there were no differences in retrieval performance. some early search engines, such as lycos, suggested alternative queries and terms to users, but these techniques were not adopted by many users. recently, however, there has been a revival of interest in query and term suggestion features by major search engine companies and researchers. smyth, et al describe thespy search engine, which incorporates a collaborative ranking function based on similar query document pairs and suggests related queries to users. white, et al compared the effectiveness and usability of a system that suggested queries with one that suggested destinations. furthermore, subjects who rated the query suggestion system more positively indicated they did so because the system saved them from typing queries and helped them generate new ideas for query reformulation. those who rated it unfavorably stated they felt the suggested queries were not relevant. overall, the evidence indicates query suggestion can be a useful feature at least for some types of tasks and users. a necessary condition for query suggestion to occur is that a set of queries that are similar to the current query exists and that the similarity of these queries to one another can be determined. a number of techniques have been proposed to determine query similarity, including the use of term overlap, query hitting time and the examination and comparison of results retrieved in response to queries. moreover, many queries are unique and occur infrequently. many information retrieval techniques have been developed to assist users formulate and reformulate queries, most notably relevance feedback. examples of rf include users adding terms suggested by a system to their queries or indicating to a system passages or documents that are relevant to their information needs. vakkari found that as subjects learned more about their topics they began to use a wider and more specific search vocabulary. however, without appropriate term context it can be difficult for users to understand how terms are used, why terms are suggested, and how such terms might be used to improve retrieval. although queries created with each interface significantly outperformed users initial queries, there were no differences in retrieval performance among interface conditions. subjects stated that the menu hierarchies gave them a better idea of the contents of retrieved documents, which seems to suggest that contextual displays are more useful. however, it is still unclear if term context is important and if so, what form it might take. one way to add context to suggested terms is to combine them with other terms and present them to users as query suggestions rather than term suggestions. most of these techniques work by identifying past queries that are similar to the usercurrent query and suggesting these to the user. results of several evaluations, including ones with subjects, demonstrated that the techniques were effective at improving retrieval. white, et al studied two types of tasks: exploratory and known item, and found an interaction effect according to task type. subjects provided mixed reviews about the suggestion features, but for known item tasks the query suggestion feature was rated most positively. query similarity has been used as a method for identifying terms for automatic query expansion and query rewriting. however, all of these techniques need a sufficiently large number of existing query and or query document pairs to work, and their effectiveness at generating query suggestions for user consumption is unclear. techniques that rely on the existence of previous queries are unlikely to work well in these situations. growing interest in interactive systems for answering complex questions lead to the development of the complex, interactive qa task, introduced for the first time at trec year#. this paper describes the rationale and design of the ciqa task and the evaluation results. thirty complex relationship questions based on five question templates were investigated using the aquaint collection of newswire text. interaction forms were the primary vehicle for defining and capturing user system interactions. in total, six groups participated in the ciqa task and contributed ten different sets of interaction forms. there were two main findings: baseline ir techniques are competitive for complex qa and interaction, at least as defined and implemented in this evaluation, did not appear to improve performance by much. we present initial work towards recognizing reading activities. this paper describes our efforts detect the english skill level of a user and infer which words are difficult for them to understand. we present an initial study of students and show our findings regarding the skill level assessment. we explain a method to spot difficult words. eye tracking is a promising technology to examine and assess a user skill level. reading is a ubiquitous activity that many people even perform in transit, such as while on the bus or while walking. tracking reading enables us to gain more insights about expertise level and potential knowledge of users towards a reading log tracking and improve knowledge acquisition. as a first step towards this vision, in this work we investigate whether different document types can be automatically detected from visual behaviour recorded using a mobile eye tracker. we present an initial recognition approach that com bines special purpose eye movement features as well as machine learning for document type detection. we evaluate our approach in a user study with eight participants and five japanese document types and achieve a recognition performance of using user independent training. reading is one of the most important skills in today society. the ubiquity of this activity has naturally affected many information systems; the only goal of some is the presentation of textual information. one concrete task often performed on a computer and involving reading is finding relevant parts of text. in the current study, we investigated if word level relevance, defined as a binary measure of an individual word being congruent with the reader current informational needs, could be inferred given only the text and eye movements of readers. we found that the number of fixations, first pass fixations, and the total viewing time can be used to predict the relevance of sentence terminal words. in light of what is known about eye movements of readers, knowing which sentence terminal words are relevant can help in an unobtrusive identification of relevant sentences. in this paper, aiming at providing semantically relevant queries for users, we develop a novel, effective and efficient two level query suggestion model by mining clickthrough data, in the form of two bipartite graphs extracted from the clickthrough data. based on this, we first propose a joint matrix factorization method which utilizes two bipartite graphs to learn the low rank query latent feature space, and then build a query similarity graph based on the features. due to the complexity of the web structure and the ambiguity of users inputs, most of the suggestion algorithms suffer from the problem of poor recommendation accuracy. for a given query raised by a specific user, the query suggestion technique aims to recommend relevant queries which potentially suit the information needs of that user. after that, we design an online ranking algorithm to propagate similarities on the query similarity graph, and finally recommend latent semantically relevant queries to users. experimental analysis on the clickthrough data of a commercial search engine shows the effectiveness and the efficiency of our method. or commercial advantage and that copies terface for web users to obtain any kind of information they may seek. queries containing ambiguous terms may confuse the search engine into retrieving web pages which do not satisfy the information needs of users. another consideration, as reported in, is that users tend to submit short queries consisting of only one or two terms under most circumstances, and short queries are more likely to be ambiguous. however, due to the commercial reasons, few public papers have been released to unveil the methods they adopt. in fact, clickthrough data is an ideal source for mining relevant queries. bear this notice and the full citation on the rst page. in this paper, by analyzing the clickthrough data, we develop a query suggestion framework using two level latent semantic analysis. then we build a query graph based on the representation of query space. we evaluate our model from di erent angles: first, it is assessed by a panel of three experts. ciency of our online query suggestion algorithm by measuring how much cpu time that it needs. the results show that our method is both. cient for improving the recommendation quality, as well as generating semantically related queries to users. the rst one is the ambiguity which commonly exists in the natural language. moreover, this noise is not easily removed by machine learning methods. in order to avoid these problems, some additional data sources are likely to be very helpful to improve the recommendation quality. however, most of these references extract only the query url bipartite graph of the clickthrough data for analysis, and ignore the information of users who issued the queries. we rst extract two bipartite graphs, which are user query and query url bipartite graphs. section # presents the similarity propagation model as well as the method for recommending queries. the rest of the paper is organized as follows. with the exponential growth of information on the world wide web, web search engines provide an indispensable in permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. although current commercial search engines have been proved to be successful for recommending the most relevant web pages to users, there are several outstanding issues that can potentially degrade the quality of search results, and these merit investigation. through the analysis of a commercial search enginequery logs recorded over three months in year#, we observe that of web queries are single term queries, and a further of web queries contain only two terms. thirdly, in most cases, the reason why users search is that they have little or even no knowledge about the topic they are searching for. in order to nd satisfactory answers, users have to rephrase their queries constantly. to overcome all of these problems, a valuable technique, query suggestion, has been employed by some famous commercial search engines, such as yahoo, live search, ask and google, to recommend relevant queries to users. typically, query suggestion is based on local and global document analysis, or anchor text analysis. however, these traditional methods have di culty summarizing the latent meaning of a web document due to the huge noise embedded in each web page. in the typical search scenario, a user initiates a query, and submits it to a search engine. the search engine returns a set of ranked related web pages or documents to this user. the user then clicks some pages of interest. some users even re ne their queries in order to nd the desired information. therefore, the collection of queries is likely to well re ect the relatedness of the target web pages. to copy otherwise, to http: www yahoo com republish, to post on servers or to redistribute to lists, requires prior speci. http: www live com permission and or a fee. http: www ask com cikm, october, year#, napa valley, california, usa. actually, users perform as the most important role in the clickthrough data, since all the queries are issued by the users, and which urls to click are also decided by the users. the connections between queries and urls are essentially bridged by di erent kinds of users. moreover, if two distinct users issued the similar set of queries, we can assume that these two users are very similar since they have similar information needs. from the above analysis, we cannot ignore the users in the clickthrough data. then we give solutions to the following two problems: how to learn the query latent feature space from these two bipartite graphs, and how to recommend semantically relevant queries to users as to the rst problem, we develop a joint matrix factorization method which fuse user query and query url bipartite graphs together to learn the low dimensional query latent feature space. in order to address the second problem, we develop a novel,ective, and. cient similarity propagation model, which not only suggests a list of queries relevant to the queries submitted by users, but also ranks the query list based on the similarity scores. we evaluate our model for query suggestion using clickthrough data of a commercial search engine. then, we evaluate it in terms of the ground truth extracted from the odp database. section # describes the construction of two bipartite graphs, and proposes a joint matrix factorization method of learning query latent feature space. in section #, we demonstrate the empirical analysis of our models and algo http: www dmoz org rithms. finally, conclusions and future work are given in section #. generating alternative queries, also known as query suggestion, has long been proved useful to help a user explore and express his information need. in many scenarios, such suggestions can be generated from a large scale graph of queries and other accessory information, such as the clickthrough. however, how to generate suggestions while ensuring their semantic consistency with the original query remains a challenging problem. in this work, we propose a novel query suggestion algorithm based on ranking queries with the hitting time on a large scale bipartite graph. without involvement of twisted heuristics or heavy tuning of parameters, this method clearly captures the semantic consistency between the suggested query and the original query. empirical experiments on a large scale query log of a commercial search engine and a scientific literature collection show that hitting time is effective to generate semantically consistent query suggestions. the proposed algorithm and its variations can successfully boost long tail queries, accommodating personalized query suggestion, as well as finding related authors in research. the explosive growth of web information has not only created a crucial challenge for search engine companies to handle large scale data, but also increased the di culty for a user to manage his information need. it has become increasingly di cult for a user to compose a succinct and precise query to present his search need. instead of pushing this burden to the users, it is common practice for a search engine to provide some types of query suggestions. this work was done when the rst author was on a summer internship at microsoft research. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. when a user types a query msg to the search engines, he will be provided with quite a few alternative potential queries. for example, he will be suggested msg chinese food, msg health, and other names for msg by google, and msg error, msg network, and msg seating chart by yahoo. there are also other query suggestion mechanisms which could automatically complete a query, and automatically correct spelling mistakes. such query suggestion mechanisms are usually developed based on morphological information of queries, or cooccurrence of one query word with other queries. although suchquery suggestions are proved useful in di erent ways, there is usually no guarantee that the suggested queries convey close semantic information with the original query. indeed, it is usually annoying for a researcher who searches for chris burges but is suggested with chris burgess or chris burge ministries. similarly, it is not very helpful to suggest kdd with kbb, kddi, ntt, and harry shum with harry potter. people searching for larry page maybe interested in sergey brin but not yellow page. a good query suggestion system should consider a handful of features, but in most cases it is important to ensure that the semantics of the suggested query do not drift too much from the original one. some users will issue the query msg to search for the sports center in newyork and others use it to search the food additive. msr could mean microsoft research, but also mountain safety research, or even mortgage servicing rights. without the constraint of semantics, a general suggestion to such ambiguous queries would easily be. another big challenge and opportunity for the current query suggestion systems lies in the suggestion of infrequent queries. it has been a well known theory in business that a company could sell less of more by boosting the long tail of the power law distribution. the same question lies in searchengine business, especially in advertising where customers bid for query terms. frequently clicked queries cost more and long tail queries cost less. if a well designed query suggestion system could route the traf all real examples are collected on feb. http: search yahoo com searchchris burges http: search live com results aspxchris burges http: search live com results aspxkdd http: search live com results aspxharry shum. and boost the clickthrough of long tail queries, there is a huge opportunity to maximize thebene ts forboth a search engine company and customers of its advertising system. is there a principled way to suggest semantically similar queries while also boosting long tail queries can such a method also provide a natural solution to personalization it is challenging because semantics is hard to de ne and both long tail queries andpersonalization usually su er from data sparsity. in this paper, we propose a uni ed approach to query suggestion, by computing the hitting time on a large scale bipartite graph of queries and clickthrough. despite its simplicity, this novel approach introduces quite a fewbene ts to query suggestion: the suggestions generated with the proposed algorithm are semantically similar to the original query; the suggestions generated do not have to occur with the original query; this approachboosts the long tail queries as suggestions; and this model provides a natural treatment for personalized query suggestion. empirical experiments on a large scale query log of a commercial search engine, as well as a public available scienti. bibliography dataset show that our proposed algorithm is. ective for semantically coherent query suggestion, which provides a potential new framework, or an important and novel feature for building a real query suggestion system. the approach of using hitting time is quite general, which could provide potential solutions to many other search related problems other than query suggestion. the rest of the paper is organized as follows. in section #, we formally introduce the concept of hitting time on a bipartite graph. in section #, we propose the algorithm of query suggestion using hitting time. we show our experiments and results in section #, introduce the related work in section #, and conclude in section #. net ix spends millions to look for an. ective way to suggest hard to nd movies. the interests, formulated as an implicit query, can then be used in further searches. in the absence of explicit queries, an alternative is to try to infer users interests from implicit feedback signals, such as clickstreams or eye tracking. we formulate this task as a probabilistic model, which can be interpreted as a kind of transfer or meta learning. the probabilistic model is demonstrated to outperform an earlier kernel based method in a small scale information retrieval task. the classic problem in information retrieval is to rank a set of documents according to the usercurrent interest, with the documents most relevant for the user ranked among the rst. the same theme recurs currently in other applications of machine learning such as recommender systems. current ir systems rely mostly on explicit, typed queries to perform the ranking. a straightforward way is to collect explicit feedback, that is, the user labels some of the documents relevant or irrelevant for her interests. it would be ideal if the ir system would be able to unobtrusively collect and use implicit feedback to infer the interest of the user while she works and use this information to improve the quality of the search results. appearing in proceedings of the th international conference on machine learning, helsinki, finland, year#. several forms of implicit feedback, such as clickstream data, time spent during reading, and amount of scrolling and exit behaviour, have been used with some success. gaze patterns are a promising source of information about the attention of the user, and hence of implicit feedback. in the latter, eye tracking based feedback improved information retrieval performance in an experiment where no explicit queries were available, and everything was inferred from the eye movements and the texts. the setup was slightly di erent from standard ir. first, we will use the eye tracking based feedback in a more realistic ir scenario. in we introduced a two stage prediction algorithm, where the latter stage was an svm which classi ed new documents into relevant and irrelevant, given a parameter vector that consists of a weight for each word. the parameter vector was inferred with a regressor which had been trained to predict the weight of a word based on the eye movement pattern on the word. the problem with this method is that learning of the regressor requires a ground truth which is tricky. line learning stage, based on their textual content. the central task is to learn an implicit query for a topic unseen in the original training phase, which translates to transfer learning. the eye movements from which the implicit query is inferred can be considered as meta data for the documents. the browser has been modi ed to record and transmit the eye movement measurements to the wikipedia server. our method, once trained, consists only of a linear discriminator applied to term speci. ciently in linear time whenever the eye tracking data is available. average precision in the re ranked documents was used as the goodness criterion. the main problem in this traditional ir scenario is that it is di cult even for experienced users to formulate good textual queries, and therefore userinterest needs to be inferred partly from other sources. while these sources of feedback are often readily available, they. they have been used for information retrieval in two papers. the users saw sets of ten simpli ed documents, about half of which were relevant to a topic given to them beforehand, while the remaining documents were of other randomly selected topics. based on the gaze pattern, an implicit query was constructed and used to rank unseen documents. the results were signi cantly better than random rankings. instead of randomly sampled documents, the user is shown a ranked list of top results, and the task of the system is to improve the ranking of the yet unseen documents. in the earlier paper we used the parameter vector of an svm taught to classify the relevant and irrelevant documents in the. this is intuitively a sensible strategy, and the experimental results validated it, but the choice is unlikely to be optimal. in this paper, we introduce a probabilistic model for inferring relevance of the documents. incorporating both of the two stages, inference of the relevance of a new document and inference of the implicit query, into a single generative model solves rigorously the problem of getting the ground truth and the learning procedure will be optimal for the task, given our modeling assumptions. learning of the probabilistic model is related to transfer learning and meta learning. the implicit query is expressed in the model as latent variables shared within each search task. we test the method in an experimental scenario designed to closely resemble a real information retrieval setup. the user makes a query within a restricted wikipedia corpus located in our customized wikipedia server. a search engine then ranks the top documents for this query, and the rst ve are shown sequentially to the user using a web browser. we rank the remaining documents by our eye movement based model and aggregate the new and the original ranking to produce an ordering for the remaining documents. collaborative filtering is carried out using the user rating profile model, a state of the art probabilistic latent variable model, computed using markov chain monte carlo techniques. implicit feedback is inferred from eye movement signals, with discriminative hidden markov models estimated from existing data in which explicit relevance feedback is available. we study a new task, proactive information retrieval by combining implicit relevance feedback and collaborative filtering. we have constructed a controlled experimental setting, a prototype application, in which the users try to find interesting scientific articles by browsing their titles. for new document titles the prediction accuracy with eye movements, collaborative filtering, and their combination was significantly better than by chance. the best prediction accuracy still leaves room for improvement but shows that proactive information retrieval and combination of many sources of relevance feedback is feasible. query formulation and efficient navigation through data to reach relevant results are undoubtedly major challenges for image or video retrieval. queries of good quality are typically not available and the search process needs to rely on relevance feedback given by the user, which makes the search process iterative. giving explicit relevance feedback is laborious, not always easy, and may even be impossible in ubiquitous computing scenarios. a central question then is: is it possible to replace or complement scarce explicit feedback with implicit feedback inferred from various sensors not specifically designed for the task in this paper, we present preliminary results on inferring the relevance of images based on implicit feedback about users attention, measured using an eye tracking device. it is shown that, in reasonably controlled setups at least, already fairly simple features and classifiers are capable of detecting the relevance based on eye movements alone, without using any explicit feedback. for text retrieval it is relatively easy to de ne a good set of keywords to be used as query terms. for most other information retrieval tasks, such as image searches, it is far more di cult to construct. existing solutions typically apply either of two predominant strategies: text based queries on metadata, or iterative retrieval based on low level features computed from the content. for the metadata strategy, all the readily available textual ir tools can be used, but the results depend heavily on the quality of the metadata. constructing good metadata is laborious, time consuming, expensive and, furthermore, the metadata cannot cover all aspects of the images. the content based approach sidesteps the need for metadata, but faces the problem that constructing useful and reliable features is di cult. even with a good feature representation, it is not straightforward to formulate a query in terms of the features. in an image retrieval task the features may be complex descriptors of local image characteristics, such as color or texture, and a user cannot be expected to specify those manually. the most practical solution is to use pictorial examples for querying, as is done in the qbic system and in most state of the art methods. regardless of the type of query, the modern content based image retrieval systems, such as picsom, are typically highly interactive. the user is asked to re ne the search by providing explicit feedback on the results, for instance by clicking on the relevant images. the images can then be analyzed and compared with images in a dataset for retrieval. methods for image comparison have been progressing steadily, both in accuracy and in robustness, as reported in yearly benchmarks such as the pascal voc classi cation task. in classi cation, though, the set of classes is known a priori, and there is usually a signi cant number of training samples from each class. for cbir tasks, explicit feedback is accurate, but laborious for the user and limited in complexity. a user may at most be willing to click a few relevant images. in addition to explicit feedback, it is also possible to collect implicit feedback from sources not directly controlled by the user. various sensors can be used for monitoring the user and the context of the search, and the implicit feedback from them could be used to either complement the explicit feedback to improve search quality, or even to replace it completely. indeed this is the approach we take in this paper. it is important to note that sensors are typically noisy and they cannot be focused to measure solely relevance. one particular challenge of this approach is to infer the correct relevance feedback from amongst the noise and nuisance signals. usingeyemovements we consider one particular source of implicit feedback, namely eye movements of the user in an information retrieval task. eye movements can be collected by relatively inexpensive and small scale non invasive equipment, making them a promising source for implicit feedback in practical applications in the near future. eye tracking has been used extensively in the psychology literature, and more recently also in tracking users attention in information retrieval settings. some examples include the human computer interaction aspects of how users perform searches, analysis of user behavior in web search, and using eye movements as implicit relevance feedback in textual ir. in particular, showed that in controlled settings it is possible, to a degree, to infer the relevance of document titles based on the gaze trajectory of the user. the promising results on the textual ir task suggest that using eye movements for relevance determination could be possible also in image retrieval tasks, where they would be even more severely needed. in this paper we report on the rst feasibility studies on using eye movements to improve content based image retrieval. the task of the users is to search for images matching a literal query, and we try to infer the relevance of the images using the eye movements alone. no image features or metadata are used, but instead the relevance is inferred solely based on the trajectory of eye saccades and xations on a collage of images. the focus is on inferring the relevance, not in using the inferred relevance as a feedback source in a real cbir system. however, we also provide a demonstration that the image collection used in the experiments would provide su cient content level information for searching similar images based on the feedback. the test setup of the rst experiments is highly controlled, and even though it is simpli ed it does start to re ect the kinds of activities occurring in a complete cbir system. even though the idea of using eye movements to detect the attention of the user has been discussed earlier, there has been very little work on inferring the relevance of images based on gaze patterns. hence, we considered it worthwhile to start with a controlled setting that can then be expanded later. the preliminary results are very encouraging and show that in the chosen setup it is indeed possible to infer the relevance of the images with reasonably high accuracy, and hence eye movements can be used as a source of implicit relevance feedback. this justi es moving to more advanced test setups, using also contentbased features instead of just the eye movements, applying more advanced learning algorithms, and using the inferred relevance as a feedback source in a real cbir system. eye tracking provides a non invasive way to obtain accurate information from the users, without asking them to figure #: example of a page with no relevant images. perform any additional tasks that might interfere with their main task. as the devices continue to reduce in size and cost, they may become one of the most informative and natural sensor mechanisms for gathering useful user data at low cost. ubiquitous use of such devices would facilitate personalization and adaptivity of user interfaces, and application driven scenarios such as information retrieval. one of the goals of this study is to give evidence about the potential of such systems given the current hardware. query suggestion has been an effective approach to help users narrow down to the information they need. however, most of existing studies focused on only popular head queries. in this paper, we propose an optimal rare query suggestion framework by leveraging implicit feedbacks from users in the query logs. our model resembles the principle of pseudo relevance feedback which assumes that top returned results by search engines are relevant. however, we argue that the clicked urls and skipped urls contain different levels of information and thus should be treated differently. unlike the rocchio algorithm, our learning process does not involve the content of the urls but simply leverages the click and skip counts in the query url bipartite graphs. experimental results on one month query logs from a large commercial search engine with over million rare queries demonstrate the superiority of our framework, with statistical significance, over the traditional random walk models and pseudo relevance feedback models. since rare queries possess much less information than popular queries in the query logs, it is much more difficult to efficiently suggest relevant queries to a rare query. hence, our framework optimally combines both the click and skip information from users and uses a random walk model to optimize the query correlation. our model specifically optimizes two parameters: the restarting rate of random walk, and the combination ratio of click and skip information. consequently, our model is capable of scaling up to the need of commercial search engines. web search engines have completely changed the way people acquire information during the last ten years. by providing a comprehensive portal between the internet users and the web, search engines are able to take a user query and return a ranked list of web pages according to the relevance between queries and the search engine index, which consists a subset of the entire web. the reason of failure is that the length of the queries is usually quite short, so that understanding user intents correctly has been a critical yet quite di cult task for search engines. among a variety of techniques, query suggestion related techniques have become an. among all query suggestion techniques, one of the most important and. speci cally, query logs are server end logs that record user activities in search engines. a typical query log entry contains timestamp, query, clicked urls as well as user personal information. in order to learn a query suggestion model, a commonly used approach is to leverage graph representation which forms query and url relationship into bipartite graphs. the edge between a queryand a urlindicates user clicks ofwhen issuing. as a matter of fact, a myriad of techniques have been proposed. while for rare queries that have only appeared a handful of times in the logs with very few clicks, click graph is unable to capture the underlying relationship between queries. for example, in figure #, and do not have commonly clicked urls, thus a random walk queries urls queries urls audipartstore com audipartstore com audiusa com audiusa com audi parts audi parts audirepair autorepairlocal com audirepair autorepairlocal com audi bodywork nwaaudidealers com audi bodywork nwaaudidealers com audi en wikipedia org wiki audi en wikipedia org wiki audi audi click graph skip graph figure #: an illustrative example of query url click graph and skip graph. while it is well known that in search engines, query frequencies follow a power law distribution where most queries are issued very few times by users, rare queries together constitute a great amount of search tra. ects the relevance and revenue of search engines signi cantly. motivation of our work figure # presents a motivation of our approach. ideally, audi parts should be a good suggested query for audi bodywork. however, after performing a random walk on the click graph, only the query audi can be suggested to audi parts because there is no commonly clicked urls between audi parts and audi bodywork so that their correlation is zero. however, if we leverage the top skipped urls for audi parts and audi bodywork as shown in figure #, it can be clearly observed that both queries skipped their top returned two urls: nwaaudidealers com and en wikipedia org wiki audi. however, for rare queries, many times the top skipped urls contain di erent levels of information than the clicked urls. because top returned urls are more likely to have high static rank scores which are representative of the highlevel topic that the query belongs to. eg, the url is a general entry about audi, while queries audi parts and audi bodywork address di erent aspects of user need of the speci. although users who issued these two queries clicked on more speci. ers a potential topic link between these queries. so if a user clicked the rd ranked url, then the st and nd urls are said to be skipped. fully analyzed query logs from a commercial search engine. figure # shows user session statistics in one of the data sets we use in the experiment which contains million unique queries. the gure compares the query frequency against the number of clicked and skipped urls. it can be observed that when the query frequency is low, more urls are skipped than clicked during the same user session. generally, users are tend to click more often on top returned results for popular queries, while for rare queries, the clicks are more random and thus have higher entropy scores. we further analyzed the quality of skipped urls for rare queries. figure # demonstrates the comparative ratings between skipped and clicked urls. overall, skipped urls indicate a little bit less relevance than clicked urls. this observation further supports our claim that skipped urls should be leveraged for rare queries in the context of relevance measurement. finally, combine two query correlation matrices to form the optimal query correlation matrix, which is used for query suggestion. url skipped url clicked urls clicked urls skipped of urls of urls year# year# query frequency query frequency average url rating figure #: number of urls clicked vs. number of urls skipped in the same user sessions from one week search log. there are more urls skipped than clicked for queries with lower frequencies. in pseudo relevance feedback models, this ratio is the same for both clicked and skipped urls, which is not optimal in practice for rare queries, as we shall see in the empirical analysis. recent study indicates that search is still quite di cult, approximately of times search engines fail to return relevant documents. ective way to interact between users and search engines, hence to improve the relevance of search results. a query url bipartite graph usually consists of two disjoint sets of nodes, corresponding to queries and urls respectively. an example of this bipartite representation has been shown in figure #, where the left hand set of nodes are queries and the righthand set are urls. the click graph possesses large amount of potential information that can be learnt for query suggestion, query clustering, query reformulation and so on. among them, random walk technique is one of the most. however, leveraging only the click information has a serious drawback. that is, the models learnt from click graph can only bene. popular queries which possess enough user click feedbacks. query audi parts and audi bodywork are not correlated if only performs random walk on the click graph, but will be highly correlation if random walk is performed on the skip graph. model which discovers query relationship according to their common clicks is unable to discover any correlation between and. ective proposals to deal with rare queries needs our immediate attentions. the left gure shows the click graph for three queries and ve urls that returned as top serp results. as a result, a random walk on the skip graph will assign a high correlation score to these two queries. our work is inspired by the principle of pseudo relevance feedback which assumes that the topk returned documents from search engines are always relevant to the queries, regardless of whether they are clicked or not. to further back up our argument regarding using both clicked and skipped urls for rare query suggestion, we care we de ne a url to be skipped if it was viewed by the user without being clicked. however, with the increase of query popularity, the click patterns become more stable. we selected, queries which have been issued less than times within a week and asked human judgers to judge the relevance on a scale. on average, clicked urls have a rating of while skipped urls have. contribution of this paper in this paper, we propose a novel graph combinationbased rare query suggestion framework. first, how to choose the optimal restarting rate for the random walk second, given two query url correlation matrices, how to optimally combine them the reason is that the restarting rate directly. ects the transition probability of random walk from nodes to nodes, which. ects the distribution of query relevance scores that is critical for determining the most relevant neighbor nodes. on the other hand, the combination rate decides the level of contributions from click and skip graphs respectively. to the best of our knowledge, we are among the rst to address the importance of the restarting rate of random walk, and optimize the parameter in a principled way. in other random walk like models, this rate is either pre xed, or empirically chosen without any support information. the rest of the paper is organized as follows: section # presents the literature in query suggestion, query clustering related research; section # introduces our framework for optimal rare query suggestions; section # provides empirical results on the performance of our model; nally, section # concludes our proposal with future work. relevance feedback techniques and the selection of search terms for query expansion are major research areas in information retrieval research which seek to improve the effectiveness of interactive information retrieval. query expansion research can be divided into two major areas. firstly, extensive research has been conducted into automatic and semi automatic query expansion techniques. automatic query expansion techniques utilize the text of a userquestion and or retrieved documenth found to be relevant by the user, as input for techniques to derive a set of search terms to retrieve additional relevant documents. the emphasis in automatic query expansion techniques or relevance feedback techniques is on formulating and testing algorithms and automatic techniques which select and weight search terms for query expansion. the vector space model automatically expands queries by adding all terms in the retrieved relevant documents. the probabilistic model or theory of relevance weights is based on query term distribution in non relevant and relevant documents. many modifications to the probabilistic model have been developed. significant performance improvement has been found by ranking a union of all terms from retrieved relevant documents using the emim measure and then selecting terms from the top of the list. multiple iterations and the addition of only selected terms, between and in number, were found to be more effective than adding all terms from relevant retrieved documents. a recent study evaluated six algorithms for their effectiveness in ranking terms for query expansion, comparing their performance in term ranking for query expansion to rankings by users. other automatic and semi automatic techniques have also been reviewed. secondly, in contrast to the automatic or algorithmic approach, the human approach to query expansion research examines the userrepresentation of their question and whatever tools eg. thesaurus or experiences they use to derive or modify a set of search terms during query expansion. the human approach emphasizes investigating and observing human decision making and human behavior, and the variables in the process of search term selection. the human search term selection process involves the user and information retrieval system. during the mediated situation with a user and human intermediary, the process may involve a user and ir system, intermediary and ir system, or user and intermediary, or all three. there have been few major research studies within the human approach to search term selection. recent research has developed categories of search term types, but not based on the source and effectiveness of search terms. the study reported in this paper developed a classification of search terms according to source and a categorization based on effectiveness in the retrieval of documents judged relevant by the user. the effectiveness of search terms from different sources in retrieving relevant documents during mediated information retrieval could then be compared. no previous study has examined the selection, source of search terms and effectiveness of sources for query expansion during mediated information retrieval, nor the process by which users select search terms for query expansion interactively from retrieved documents. this is a study of how and from where users select search terms for query expansion to improve retrieval performance. to improve ir effectiveness research is needed in both the algorithmic and human streams, and further joint efforts utilizing the results of human studies in interface and system design. different relevance feedback techniques utilize different search term selection strategies. we formulate and study search algorithms that consider a user prior interactions with a wide variety of content to personalize that user current web search. rather than relying on the unrealistic assumption that people will precisely specify their intent when searching, we pursue techniques that leverage implicit information about the user interests. this information is used to re rank web search results within a relevance feedback framework. we explore rich models of user interests, built from both search related information, such as previously issued queries and previously visited web pages, and other information about the user such as documents and email the user has read and created. our research suggests that rich representations of the user and the corpus are important for personalization, but that it is possible to approximate these representations and provide efficient client side algorithms for personalizing search. we show that such personalization algorithms can significantly improve on current web search. domain experts search differently than people with little or no domain knowledge. previous research suggests that domain experts employ different search strategies and are more successful in finding what they are looking for than non experts. in this paper we present a large scale, longitudinal, log based analysis of the effect of domain expertise on web search behavior in four different domains. we characterize the nature of the queries, search sessions, web sites visited, and search success for users identified as experts and non experts within these domains. large scale analysis of real world interactions allows us to understand how expertise relates to vocabulary, resource use, and search task under more realistic search conditions than has been possible in previous small scale studies. building upon our analysis we develop a model to predict expertise based on search behavior, and describe how knowledge about domain expertise can be used to present better results and query suggestions to users and to help non experts gain expertise. web searchers differ from each other in many ways that can greatly influence their ability to carry out successful searches. one way in which they can differ is in their knowledge of a subject or topic area. domain expertise is not the same as search expertise, as it concerns knowledge of the subject or topic of the information need, rather than knowledge of the search process. studies of domain expertise have highlighted several differences between experts and non experts, including: site selection and sequencing, task completion time, vocabulary and search expression, the number and length of queries, and search effectiveness. these studies involved small numbers of subjects with carefully controlled tasks, making it difficult to generalize their findings. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. wsdm, february, year#, barcelona, spain copyright year# acm. in this paper we build on this previous research via a large scale log based study of web search behavior. we contrast the search strategies of domain experts with those of non experts through analysis of naturalistic interaction log data over a three month period of time. this large scale analysis allows us to identify greater diversity in vocabulary, web site visits, and user tasks than is possible with smaller scale laboratory studies. in addition we develop methods for identifying domain experts using online search interaction patterns rather than offline tests of expertise. we focus on four domains medicine, finance, law, and computer science with complex subject matter and a large potential benefit to non experts in identifying effective search strategies. in addition to highlighting differences in the search behavior of experts and non experts, we describe the possible benefits of being able to identify domain experts and leverage their querying strategies and source selection abilities. search tools currently provide the same experience to users regardless of expertise. a cardiologist searching for the latest research studies on heart disease gets the same search results for the query heart disease as a newly diagnosed patient with little background in the area. we believe that by understanding how peopledomain expertise affects their search behavior, we can better support interactions at the appropriate level, and help non experts gain expertise. the remainder of this paper is structured as follows. section # presents related work on domain expertise. section # describes the search logs, and section # the approach used to identify experts within them. in section # we discuss differences and commonalities in the interaction behavior of domain experts and domain non experts. section # presents a classifier that can identify users, actions, and sessions as expert or non expert based on observable behavior, and discusses how such a classifier can be used to improve the web search experience for people of varying domain expertise. domain expertise has been studied extensively in the information science community. a user study with recall oriented search tasks in the genomic domain was conducted. the best model highlights three behavior variables as dk predictors: the number of documents saved, the average query length, and the average ranking position of documents opened. the model is validated using the split sampling method. this study uses regression modeling to predict a user domain knowledge level from implicit evidence provided by certain search behaviors. a number of regression models of a person dk, were generated using different behavior variable selection methods.