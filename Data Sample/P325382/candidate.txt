performance improvement solely through transistor scaling is becoming more and more difficult, thus it is increasingly common to see domain specific accelerators used in conjunction with general purpose processors to achieve future performance goals. there is a serious drawback to accelerators, though: binary compatibility. an application compiled to utilize an accelerator cannot run on a processor without that accelerator, and applications that do not utilize an accelerator will never use it. to overcome this problem, we propose decoupling the instruction set architecture from the underlying accelerators. computation to be accelerated is expressed using a processorbaseline instruction set, and light weight dynamic translation maps the representation to whatever accelerators are available in the system. in this paper, we describe the changes to a compilation framework and processor system needed to support this abstraction for an important set of accelerator designs that support innermost loops. in this analysis, we investigate the dynamic overheads associated with abstraction as well as the static dynamic tradeoffs to improve the dynamic mapping of loop nests. as part of the exploration, we also provide a quantitative analysis of the hardware characteristics of effective loop accelerators. we conclude that using a hybrid static dynamic compilation approach to map computation on to loop level accelerators is an practical way to increase computation efficiency, without the overheads associated with instruction set modification. critical computation subgraphs can be accelerated by collapsing them into new instructions that are executed on specialized function units. the main problem with this approach is that a new processor must be generated for each application domain. in this work, we propose a strategy to transparent customization of the core computation capabilities of the processor without changing its instruction set. a congurable array of function units is added to the baseline processor that enables the acceleration of a wide range of data flow subgraphs. application specific instruction set extensions are an effective way of improving the performance of processors. collapsing the subgraphs simultaneously reduces the length of computation as well as the number of intermediate results stored in the register file. while new instructions can be designed automatically, there is a substantial amount of engineering cost incurred to verify and to implement the final custom processor. to exploit the array, the microarchitecture performs subgraph identification at run time, replacing them with new microcode instructions to configure and utilize the array. we compare the effectiveness of replacing subgraphs in the fill unit of a trace cache versus using a translation table during decode, and evaluate the tradeoffs between static and dynamic identification of subgraphs for instruction set customization. ective at improving performance, typically yielding several orders of magnitude speedup along with reduced energy consumption. or a change in standards, the application will usually no longer be able to take advantage of the asic. rewriting an application can be a large engineering burden. instruction set customization is another method for providing enhanced performance in processors. instruction set extensions also maintain a degree of system programmability, which enables them to be utilized with more exibility. instruction set extensions is that there are signi cant non recurring engineering costs associated with implementing them. for example, a new set of masks must be created to fabricate the chip, the chip must be reveri ed, and the new instructions must. the goal is to extract many of the bene ts of traditional instruction set customization without having to break open the processor design each time. the cca consists of an array of function units that can. several di erent strategies are proposed for accomplishing transparent instruction set customization. one strategy, a fully dynamic scheme, performs subgraph identi cation and instruction replacement in hardware. to reduce hardware complexity, a static strategy performs subgraph identi cation. the contributions of this paper are twofold: we present the design of the cca, which provides the functionality of common application speci. instruction set extensions in a single hardware unit. in embedded computing, a common method for providing performance improvement is to create customized hardware solutions for particular applications. unfortunately, there are also negative aspects to using asics. the primary problem is that asics only provide a hardwired solution, meaning that only a few applications will be able to fully bene. if an application changes, because of a bug. another drawback is that even when an application can utilize an asic, it must be speci cally rewritten to do so. extensions to an instruction set, the critical portions of an applicationdata ow graph can be accelerated by mapping them to specialized hardware. ective as asics, instruction set extensions improve performance and reduce energy consumption of processors. is that automation techniques, such as the ones used by arm optimode, tensilica, and arc, have been developed to allow the use of instruction set extensions without undue burden on hardware and software designers. the addition of instruction set extensions to a baseline processor brings along with it many of the issues associated with designing a brand new processor in the rst place. furthermore, extensions designed for one domain are often not useful in another, due to the diversity of computation causing the extensions to have only limited applicability. to overcome these problems, we focus on a strategy to customize the computation capabilities of a processor within the context of a general purpose instruction set, referred to as transparent instruction set customization. this is achieved by adding a con gurable compute accelerator to the baseline processor design that provides the functionality of a wide range of application speci. instruction set extensions in a single hardware unit. oaded to the cca and then replaced with microarchitectural instructions that con gure and utilize the array. subgraphs that are to be mapped onto the cca are marked in the program binary to facilitate simple cca con guration and replacement at run time by the hardware. a detailed analysis of the cca design shows that it implements the most common subgraphs while keeping control cost, delay, and area overhead to a minimum. we describe the hardware and software algorithms necessary to facilitate dynamic customization of a microarchitectural instruction stream. for example, embedded systems often have one or more applicationspeci. attacking bottlenecks in modern processors is difficultbecause many microarchitectural events overlap witheach other. this parallelism makes it difficult to both assign a cost to an event and assign blame for each cycle. this paper introduces a new model for understandingevent costs to facilitate processor design andoptimization first, we observe that everything in a machine can interact inonly one of two ways. wequantify these interactions by defining interaction cost, which can be zero, positive, or negative. second, we illustrate the value of using interactioncosts in processor design and optimization finally, we propose performance monitoring hardwarefor measuring interaction costs that is suitable formodern processors. the trips system employs a new instruction set architecture called explicit data graph execution that renegotiates the boundary between hardware and software to expose and exploit concurrency. edge isas use a block atomic execution model in which blocks are composed of dataflow instructions. the goal of the trips design is to mine concurrency for high performance while tolerating emerging technology scaling challenges, such as increasing wire delays and power consumption. this paper evaluates how well trips meets this goal through a detailed isa and performance analysis. we compare performance, using cycles counts, to commercial processors. on spec cpu, the intel core outperforms compiled trips code in most cases, although trips matches a pentium. on simple benchmarks, compiled trips code outperforms the core by and hand optimized trips code outperforms it by factor of. compared to conventional isas, the block atomic model provides a larger instruction window, increases concurrency at a cost of more instructions executed, and replaces register and memory accesses with more efficient direct instruction to instruction communication. our analysis suggests isa, microarchitecture, and compiler enhancements for addressing weaknesses in trips and indicates that edge architectures have the potential to exploit greater concurrency in future technologies. growing on chip wire delays, coupled with complexity and power limitations, have placed severe constraints on the permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. because of these trends, major microprocessor vendors have abandoned architectures for single thread performance and turned to the promise of multiple cores per chip. despite these trends, amdahllaw dictates that single thread performance will remain key to the future success of computer systems. in response to semiconductor scaling trends, we designed a new architecture and microarchitecture intended to extend single thread performance scaling beyond the capabilities of superscalar architectures. trips is the rst instantiation of these research efforts. trips uses a new class of instruction set architectures, called explicit data graph execution, which renegotiate the hardware and software boundary. edge isas use a block atomic execution model, in which edge blocks consist of data ow instructions. this model preserves sequential memory semantics and exposes greater instruction level concurrency without requiring explicit software parallelization. we constructed a custom million transistor asic, an instantiation of the isa, trips system circuit boards, a runtime system, performance evaluation tools, and a compiler that optimizes and translatesand fortran programs to the trips isa. the distributed processing cores of a trips processor issue up to instructions per cycle from an instruction window of up to year# instructions contained in blocks. the trips isa and distributed microarchitecture are designed to exploit concurrency and reduce the in uence of long wire delays by exposing the spatial nature of the microarchitecture to the compiler for optimization. this paper presents a performance analysis that explores how well the trips system meets its goals of exploiting concurrency, hiding latency, and distributing control. using the trips hardware and microarchitectural simulators, we use compiled and hand optimized benchmarks to compare the edge isa, microarchitecture, and performance to modern processors. while we measured the power consumed by one trips processor and the memory system to be, a detailed power analysis and an examination of multicore execution is beyond the scope of this paper. our microarchitecture analysis shows that trips can ll much of its instruction window; compiled code shows an average of total instructions in ight and hand optimized code shows an average of. while much higher than conventional processors, the number of instructions in ight is less than the maximum of year# because the compiler does not completely ll blocks and the hardware experiences pipeline stalls and ushes due tocache misses, branch mispredictions, and load dependence mispredictions. the edge isa incurs substantial increases in instructions fetched and executed relative to conventional risc architectures because of predication and instruction overheads required by the data ow model. a strength of the edge isa and distributed control is that trips requires less than half as many register and memory accesses than a risc isa because it converts these into direct producer to consumer communications. furthermore, communicating instructions are usually on the same tile or an adjacent tile, which makes them power ef cient and minimizes latency. we compare the performance of trips to the intel core, pentium iii, and pentium using hardware performance counters on compiled and handoptimized programs. on eembc, the core executes fewer cycles than trips compiled code. on spec, trips compiled code executes more than twice as many cycles than core on integer benchmarks but the same number of cycles on oating point benchmarks. trips executes times fewer cycles than the core on hand optimized benchmarks. these experiments suggest that edge processors have the capability to achieve substantial performance improvements over conventional microprocessors by exploiting concurrency. however, realizing this potential relies on the compiler to better expose concurrency and create large blocks of trips instructions, as well as microarchitectural innovations in control distribution and branch prediction. while many applications can exploit multicore systems, this approach places substantial burdens on programmers to parallelize their codes. a number of simple performance measurements on network, cpu and disk speed were done on a dual arm cortex a machine running linux inside a kvm virtual machine that uses virtio disk and networking. unexpected behaviour was observed in the cpu and memory intensive benchmarks, and in the networking benchmarks. the average overhead of running inside kvm is between zero and percent when the host is lightly loaded, but the relative overhead increases when both host and vm is busy. we conjecture that this is related to the scheduling inside the host linux. arm has special feature called security extensions, which may be used to build a protected execution environment. virtualization has been an important disruptive technology in the server and enterprise space for a number of years now. within the embedded server and platform segment virtualization is gaining interest but still considered a poor match for the speci. requirements of a tightly coupled, high performing software system sharing multiple hardware accelerator and digital signal processing units. however we observe that things are changing and that with the advent of cheap, powerful many core chips the nature of what constitutes such a system is also expected to change dramatically. copyright is held by the authors, vtres year#, taipei, taiwan to date much of the research and engineering. ort around embeddedvirtualizationhasbeenfocusedon micro kernel based hypervisor solutions. indeed with their special attention to high bandwidth, low latency and isolation properties for robustness, micro kernel based hypervisors do indeed seem to be a good match to the speci. however as the linux operating system takes an ever increasing share of embedded server platforms and displaces the more classic real time oswe feel it is necessary to understand how linux and its speci. ecosystem of software can be used to act as an embedded hypervisor supporting guest linux instances with real time requirements. we see this work based on the arma architecture and kvm as. kvm is a part of the linux kernel that makes linux capable of running not just regular application binaries, but also able to run an unmodi ed kernel binary in a special kind of process. the kernel inside the process is called a guest, and the main kernel is called host. the guest is scheduled by the host as a normal process, interleaved with other processes, but it cannot open les or do other syscalls to the host. reasons to run an an application in a kvm process may be to provide isolation of data or fault the guest does not see any of the hostprocesses or resources unless so con gured, and it cannot crash the host system. or the reason could be a need to run legacy software that can not run directly on the host. using kvm or other kinds of virtualization comes at a cost, and di erent hardware platforms have different costs and bottlenecks depending on how suited they are for some speci. a bottleneck may for instance be if a platform does or does not require the execution to pass through the hypervisor when moving between user space to kernel space. likewise, modifying the virtual memory translation may or may not incurr overhead. and one system may support device initiated dma to the virtual memory while another may require the hypervisor to completely virtualize and emulate the devices. earlier papers on kvm performance has been focused on the platform. the earlier results show that kvm has the potential to be an. cient hypervisor on, with an overhead of about percent on average. but have observed that kvm can show low performance on network bound work loads when compared to other hypervisors. in this paper we give the results of measurements that were performed to nd out the execution overhead of running inside a kvm guest, as compared to running directly in the host, on an arm cortex a based platform. we are mainly concerned with networking overhead and to some degree also disk io. kvm and qemu kvm takes advantage of virtualization support in the processor to be able to run code written for typical kernel tasks: direct access to the processorvirtual memory controller and page table, con guring interrupts, dma, and talking to devices on system busses. kvm is a kernel infrastructure that enables connecting a kvm process with a system emulator. the emulator most commonly used with kvm is currently qemu. kvm intercepts system level events that need to be handled by the emulator, such as reading from disk, drawing in a frame bu er, sending a packet on the network, etc. the emulator is invoked, and emulates the functionality of the required hardware, ie, the guestvirtual network card by using the hostnetwork card. qemu may also inject virtual interrupts in the guest when data has arrived. kvm and qemu together constitute a hypervisor, the software that enforces and implements security policy and virtual devices for a virtual machine qemu supports virtio, which is a virtualized mmio, or memory mapped io, for fast communication between the host and the guest. it functions similarly to xenparavirtualization technique with shared memory and circular message bu ers. several pieces of data can be transferred in a batch, which is useful to speed up virtual io. device driver in the guest is needed to take advantage of this way to signal to the hypervisor. in essence, the device driver writes data to a memory area shared with kvm, and signals the hypervisor when a batch of ip operations are ready to be handled by qemu. this reduces the amount of copying and switching between host and guest. hardware support for virtualization hardware support for virtualization in the cpu provides additional ways to con gure the cpu to detect and trap when a guest tries to perform system management tasks. they can be intercepted, to make sure that the guest does not mess up the system. this is usually done by having the host emulating a device in software, and have the guestinteraction operate on that virtual device. this technique works robustly, but introduces additional work to the cpu. the cortex a has support for speeding up some of the tasks by allowing the guest to perform some tasks without interrupting the guest. for instance, the a supports a nested page table. the host sets up the rst page table for the guest, similar to how it is done for a regular process. the guest can then set up a second level page table that it controls completely. all address translations are translated rst via the guesttable, and then via the hosttable, and the nal translation is cached in the translation bu er, tlb. documentation on kvm: http: www linux kvm org page documents there is also support in a for invoking virtual interrupts into the guests via the gic. this complicates things slightly, and the sum of it is that fiqs are used for secure interrupts, and irqs for non secure interrupts, and all interrupts have to be routed to hyp mode, and into the hypervisor. upon receiving an interrupt intended for a guest, the hypervisor will trigger a virtual interrupt in a running guest, or if the guest is suspended, store the interrupt and trigger it when the guest is resumed. the cortex a is intended to be used in a multicore setting. one interesting use case is to have a heterogeneous multicore setting together with the cortex a. they both share the same instruction set, the armv, but they have di erent microarchitecture, ie, the depth of the instruction pipeline, the cpuissue width, etc, which lets the a run faster but to an increased power cost. a kernel can schedule processes on fast or low power cores depending on the particular demands. ciency is to scale down the frequency at which the cpu operates. this is supported by the versatile express platform, but is notusedinthisexperiment. technology scaling has delivered on its promises of increasing device density on a single chip. however, the voltage scaling trend has failed to keep up, introducing tight power constraints on manufactured parts. in such a scenario, there is a need to incorporate energy efficient processing resources that can enable more computation within the same power budget. energy efficiency solutions in the past have typically relied on application specific hardware and accelerators. unfortunately, these approaches do not extend to general purpose applications due to their irregular and diverse code base. towards this end, we propose beret, an energy efficient co processor that can be configured to benefit a wide range of applications. our approach identifies recurring instruction sequences as phases of temporal regularity in a program execution, and maps suitable ones to the beret hardware, a three stage pipeline with a bundled execution model. this judicious off loading of program execution to a reduced complexity hardware demonstrates significant savings on instruction fetch, decode and register file accesses energy. on average, beret reduces energy consumption by a factor of for the program regions selected across a range of general purpose and media applications. the average energy savings for the entire application run was over a single issue in order processor. the traditional microprocessor was designed with an objective of running general purpose programs at a good performance, while treating ef ciency as a second order criteria. however, with a growing demand for on chip resource integration, longer battery life and lower heat dissipation in modern day devices, there is an emerging need to improve computational energy ef ciency. the trend in the silicon integration is also reinforcing this need for energyef cient architectures. over the years, transistor density and performance have continued to increase as per moorelaw, however, the threshold voltage has not kept up with this trend. as a result, the per transistor switching power has not witnessed the bene ts of scaling, causing a steady rise in power density. overall, this limits the number of resources that can be kept active on a die simultaneously. an instance of this trend can be already seen in intelnehalem generation of processors that boost the performance of one core, at the cost of slowing down shutting off the rest of them. while the importance of ef ciency today is being felt across all domains of computing, from datacenters cooling costs to smartphone battery lives, a majority of past works have focussed on the embedded application domain. these solutions have leveraged specialized hardware units, loop accelerators, and wide simd support to save energy. unfortunately, these specialization approaches do not directly extend to general purpose applications such as desktop workloads, spec integer suite, os utilities, library codes etc, for several reasons. first, these applications have a highly irregular program structure, and contain a large amount of control ow. for instance, the large, uncounted, non modulo schedulable loops in these applications cannot be mapped to las. second, these irregular codes exhibit little, if any, data level parallelism. this limits the applicability of simd support for energy savings. and nally, the generalpurpose application space is very diverse and constantly evolving, therefore, designing a custom hardware for each of these programs is not very cost effective. despite its shortcomings, specialized hardware like asics form an important design point in the space of techniques to improve performance and ef ciency. in fact, a recent work makes a case for function level asics in the context of irregular codes, claiming the large availability of dark silicon. the advantage here is that carefully customized data paths and long instruction ranges deliver highest levels of ef ciency. the disadvantage of asics is that each of them can handle only one application function. a second class of performance ef ciency solution that overcomes this challenge is the work on programmable functional units. pfus allow a small chain of operations to execute together using a applications with asic support remaining instruction applications gpp asics and gpp beret loop accelerators generality any application most applications single application range instructions instructions instructions figure #: solution classes to improve computational ef ciency of general purpose processors. range of dynamic instructions offloaded determines frequency of communication with the main core, and correlates well with energy savings. the pfu class covers custom instructions and subgraph accelerators that can target virtually any application, but work on small ranges. asics provide a much larger range, but are typically exclusive to an applicationfunction. beret is our proposed design point, which provides application exibility while also covering large instruction ranges. the advantage is its universal applicability to almost any program. however, the energy ef ciency gains are limited due to a small instruction range, and an emphasis on processor back end. studies have shown that a large fraction of application energy is consumed by the processor front end. the two solution classes discussed above fall into opposing extremes, with one providing large ef ciency gains, and the other. to bridge this gap, this paper proposes beret, a con gurable co processor that achieves signi cant energy savings for the selected program regions mapped onto it, without sacri cing performance. as the approach tries to bridge the gap between pfus and asics, it has to simultaneously achieve two objectives, increase instruction range relative to pfus, and make the design exible across applications unlike asics. for increasing the instruction range, the insight is to leverage recurring instructions sequence in a programexecution. such a sequence consists of instructions that repeatedly execute back toback with a high likelihood, despite the presence of intervening control instructions. these recurring sequences represent phases of temporal regularity in an otherwise irregular code, and make good candidates for mapping to beret. conceptually these are traces or frames, with an added requirement of forming a loop. hereonwards, we refer to them as hot traces or recurring traces. the recurring traces provide several bene ts such as long instruction ranges, predictable code behavior and appearance of structure to irregular codes, all of which help in designing a simple and ef cient co processor hardware. more importantly, as these traces are signi cantly shorter than the original unstructured loops, beret buffers them internally and eliminates redundant fetches and decodes. the second objective of beret is to support multiple applications. the insight here is to use a bundled execution model for running the traces. in this model, instead of executing one instruction at a time, beret uses compiler analysis to break down traces into bundles of instructions, and executes them sequentially. these bundles are essentially subgraphs from the trace wide data ow graph. further, our analysis of application traces demonstrated that many subgraph structures are common within as well as across applications. thus, given a diverse enough collection of subgraph execution blocks, our compilation scheme is able to break down any given recurring trace into constituent subgraphs from this collection. in terms of energy savings, a major advantage of this bundled execution is that it signi cantly cuts down on the redundant register reads and writes for the temporary variables. overall, we consider this bundled execution model a trade off design that lets us achieve ef ciency gains nearer to an application speci. data ow hardware while maintaining application universality of regular von neumann execution model. leveraging these two insights, beret is designed as a subgraphlevel execution pipeline for recurring instruction sequences that enables signi cant energy savings for general purpose programs. primarily, the energy savings come from large reductions in redundant fetches, decodes, and register reads and writes for temporary variables. beret also represents a hybrid accelerator design point in the ef ciency solution space where a large range of instructions are of oaded and most applications bene. this paper identifies a new opportunity for improving the efficiency of a processor core: memory access phases of programs. these are dynamic regions of programs where most of the instructions are devoted to memory access or address computation. these occur naturally in programs because of workload properties, or when employing an in core accelerator, we get induced phases where the code execution on the core is access code. we observe such code requires an ooo core dataflow and dynamism to run fast and does not execute well on an in order processor. however, an ooo core consumes much power, effectively increasing energy consumption and reducing the energy efficiency of in core accelerators. we develop an execution model called memory access dataflow that encodes dataflow computation, event condition action rules, and explicit actions. using it we build a specialized engine that provides an ooo core performance but at a fraction of the power. such an engine can serve as a general way for any accelerator to execute its respective induced phase, thus providing a common interface and implementation for current and future accelerators. we have designed and implemented mad in rtl, and we demonstrate its generality and flexibility by integration with four diverse accelerators. our quantitative results show, relative to in order, wide ooo, and wide ooo, mad provides, and equivalent performance respectively. this paper is a specialization technique targeted at a prevalent and growing category of program behavior: memory access phases. a memory access phase is a dynamic portion of a program where its instruction stream is predominantly for memory accesses and address generation. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than the author must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. isca, june, year#, portland, or, usa copyright is held by the owner author. acm doi: http: dx doi org year# figure #: natural and induced memory access phases in compiler terms, the load and store back slice contribute to of dynamic instructions. observation we observe that memory access phases are naturally prevalent in many applications. pro ling the specint and mediabench suite shows many such phases; table # explains their qualitative roles. we call these natural memory access phases or simply natural phases through the rest of this paper and they are sketched at the top of figure #. a second category are program phases that include code that executes concurrently on an in core accelerator and processor core. here the code running on the main processor feeds values to the accelerator. examples include code running with npu, convolution figure #: performance on natural and induced phases engine, and dyser. on compiler generated or handwritten sse avx code regions, the non compute instructions form such a phase. we call these phases induced memory access phases. their characteristic of memory access behavior is induced by of oading the computation to an accelerator. with more accelerators, there will be more and more induced phases increasing the opportunity for specializing the core itself for such phases. we rst make the overarching observation that many accelerators can be viewed as executing under the decoupled access execute paradigm. our analysis of these phases, leads us to the following observations. the properties of natural and induced phases are similar and present an opportunity. they commonly have abundant instruction level parallelism and memory level parallelism, they have much dynamic behavior in the cache hits misses, they have some control ow, and they commonly have a small static code footprint of to instructions. figure # presents quantitative measurements that support these observations by showing two representative natural phases and three induced phases on three different accelerators, and geometric mean across our entire suite. we rst compare performance on an in order machine to two realistic out of order machines. across natural phases and induced phases, in order cores are to worse than realistic ooo counterparts. while an ooo core with ilp, mlp, and dynamism tolerance is useful, it comes at an exorbitant power cost. compared to the in order core, ooo is almost a factor of three higher in power, and an ooo is another factor of three. because of this, the energy improvement from very low power accelerators like dyser, npu, and even sse is constrained by the watt to watt ooo core. integrating these to ooo cores provides, overall higher energy than integrating them with an in order core. to examine whether there is the opportunity to exceed the performance of an ooo core, we look at two hypothetical con gurations which isolate the bene ts of mlp alone, compare records to keys and update results. media processing mediabench convert samples to the output color space, synthesis lter. irregular codes specint block sort, check the cost of the arcs and update the results. accelerators parboil, rodinia, throughput kernels fast fourier transform, dense linear algebra, convolution. accelerators npu benchmarks fast fourier transform, jpeg encoding. table #: memory access regions ooo with cache ports and mlp ilp. examining the last two bars, we see there is a potential for improvement over an ooo and up to in some cases. problem statement this paper investigates the question of how to build a power ef cient and high performance mechanism for executing memory access phases. such a mechanism can serve as a general way for in core accelerators to integrate with high performance or low performance cores without compromising performance yet running at low power by turning off the core during such phases. lessons from ooo an ooo core does the following for high performance: it dynamically unrolls the program, maintains it in the instruction window, repeatedly extracts the data ow graph, and uses register renaming and a large physical register le to maintain multiple copies of an instructionoutput across its various dynamic instances. abstracting away microarchitecture implementation details, an ooo coreprimitives are: data ow computation of address and control conditions using the extracted graph; the outcome of these few data ow computation patterns create events inside the core pipeline; and based on these events, it performs actions like moving data between the register le and memory. data ow computation extracts ilp and concurrent events and actions exploits mlp and dynamism. overview our execution model and hardware architecture called memory access data ow, expresses the aforementioned ooo coreprimitives actions using the concept of event condition action rules and named event queues for storage. eca rules are borrowed from the databases and algebraic theory literature and comprise two parts: rst it formally state a set of conditions as a boolean function of a set of events, and second a set of actions are triggered when the function evaluates to true. the idea of eca rules elegantly and explicitly speci es the ooo coreprimitives in the mad isa. to explain, named event queues store data values produced or consumed in a access phase. a programmable hardware substrate implements the conditions and triggering logic to figure #: integrating mad with accelerators initiate actions based on the event queues. a data ow substrate computes addresses and values that are deposited into the queues. compared to an ooo, mad extracts ilp by explicit data ow computation on the event queuevalues. it extracts mlp and achieves dynamism tolerance by triggering actions using eca rules avoiding any instruction by instruction serialization. it achieves power bene ts over an ooo by avoiding the overheads of data ow extraction and per instruction overheads of fetch, register renaming, and buffering. microarchitecture and hardware implementation our hardware design comprises of three high level blocks. each block is simple and with well de ned roles as shown in figure #. the mad hardware engine is integrated into a core and interfaces with its load store unit. the computation block includes a data ow computation substrate and produces values and events. the event block includes named event queues using fifos, a programmable boolean logic array which applies boolean functions on events to trigger actions, and the rules on how to trigger actions based on the events. the action block includes an action look up table and a decoder that asserts control signals on communication buses to move values as de ned by the actions. the data ow computation substrate eliminates the need for repeated instruction fetch and rediscovery of data ow. the named event queues explicitly maintain multiple temporally ordered values of variables instead of renaming as accomplished in an ooo using register renaming and a big physical register le. and nally, instead of control signals and dependence control stalls, explicitly speci ed actions are triggered by applying boolean functions on the status of multiple event queues as and when they get populated. there are no large or associative structures like in an ooo. our implementation occupies mm in nm technology, consumes typically mwatts, most of which is address computation power. integration with accelerators and target systems with mad the mad execution model and hardware can serve as a general means for any accelerator to run its induced phase as shown in figure # outlining four diverse accelerators. the core is turned off with mad taking on the role of core in these phases. we emphasize that mad is useful for both acceleratorfree cores and for the emerging class of chips which integrate accelerators into cores. for natural phases, a microprocessor vendor who has interest in workloads that are memory intensive can put mad into the core. mad is a lightweight and simpler alternative to an entire specialized databaseaccelerator for example. for induced phases, the workload is run on an accelerator with mad integrated on the chip. mad serves as the engine to feed memory values to the accelerator turning off the host core. it is instructive to consider whether sophisticated prefetching combined with a simple inorder core is suf cient. our results show the mad engine is lower power than an inorder core. furthermore madooo capability provide it linked data structure prefetching and other irregular prefetching like bene ts. conceptually mad can be viewed as a sophisticated, programmable, yet very power ef cient prefetcher that powers off the core. overview of execution below, we describe an end to end overview of madexecution, revisting figure #, with a generic accelerator that uses mad to execute the induced memory access phase. first, a compiler creates mad regions and encodes them in the program binary. for induced phases, the compiler uses the accelerator compileroutput, while on natural phases it works on the ir. at run time, upon entering a memory access phase, the core rst con gures mad by sending it con guration information created by the compiler. the core also sends initial events to start mad execution. the core is then turned off except for its loadstore unit and mad, or mad accelerator, that execute. the initial events trigger the computations in the computation block; the execution model of the computation block is pure static data ow, which means that whenever a ready data appears at the input queues of the computation block, computation is triggered. we assume any accelerator integrated with mad has a similar interface. based on the outcome of the computation, a new set of events are created in parallel and arrive at the event block. the event block applies boolean algebra on the events rules to generate any actions which are delivered as action indices to the action block the action block takes the indices, selects the ones that could be issued in the current cycle, and controls the data bus to move the data values to the accelerator or between event queues. when executing natural phases, actions are exclusively data movement between event queues and the coreload store unit. this entire cycle repeats itself and stops when the end of program action is triggered. at that point, the mad hardware wakes up the core and through memory passes architectural state changes. results we evaluate mad on natural phases in the specint and mediabench suite, and induced phases of four diverse accelerators. our results show mad is a high performance and energy ef cient access engine: providing almost same performance as a wide ooo while consuming energy, or performance of an in order, consuming of its energy. paper organization section # and section # describes the mad isa and microarchitecture. section # describes how to integrate mad to a conventional core and other accelerators to mad, and section # discusses some complex access scenarios to demonstrate its versatility. section # presents results, section # discusses related work and section # concludes. dataflow architectures offer the ability to trade program level parallelism in order to overcome machine level latency. dataflow further offers a uniform synchronization paradigm, representing one end of a spectrum wherein the unit of scheduling is a single instruction. at the opposite extreme are the von neumann architectures which schedule on a task, or process, basis. this paper examines the spectrum by proposing a new architecture which is a hybrid of dataflow and von neumann organizations. the analysis attempts to discover those features of the dataflow architecture, lacking in a von neumann machine, which are essential for tolerating latency and synchronization costs. these features are captured in the concept of a parallel machine language which can be grafted on top of an otherwise traditional von neumann base. in such an architecture, the units of scheduling, called scheduling quanta, are bound at compile time rather than at instruction set design time. the parallel machine language supports this notion via a large synchronization name space. a prototypical architecture is described, and results of simulation studies are presented. a comparison is made between the mit tagged token dataflow machine and the subject machine which presents a model for understanding the cost of synchronization in a parallel environment. the vast majority of ilp compilation research has been conducted in the context of general purpose computing, and more specifically the spec benchmark suite. most of these processors are targeted at embedded applications such as multimedia and communications, rather than general purpose systems. conventional wisdom, and a history of hand optimization of inner loops, suggests that ilp compilation techniques are well suited to these applications. this paper presents mediabench, a benchmark suite that has been designed to fill this gap. this suite has been constructed through a three step process: intuition and market driven initial selection, experimental measurement to establish uniqueness, and integration with system synthesis algorithms to establish usefulness. significant advances have been made in compilation technology for capitalizing on instruction level parallelism. at the same time, a number of microprocessor architectures have emerged which have vliw and simd structures that are well matched to the needs of the ilp compilers. unfortunately, there currently exists a gap between the compiler community and embedded applications developers. we present a taxonomy and modular implementation approach for data parallel accelerators, including the mimd, vector simd, subword simd, simt, and vector thread architectural design patterns. we have developed a new vt microarchitecture, maven, based on the traditional vector simd microarchitecture that is considerably simpler to implement and easier to program than previous vt designs. using an extensive design space exploration of full vlsi implementations of many accelerator design points, we evaluate the varying tradeoffs between programmability and implementation efficiency among the mimd, vector simd, and vt patterns on a workload of microbenchmarks and compiled application kernels. we find the vector cores provide greater efficiency than the mimd cores, even on fairly irregular kernels. our results suggest that the maven vt microarchitecture is superior to the traditional vector simd architecture, providing both greater efficiency and easier programmability. data parallel kernels dominate the computational workload in a wide variety of demanding application domains, including graphics rendering, computer vision, audio processing, physical simulation, and machine learning. specialized data parallel accelerators have long been known to provide greater energy and area ef ciency than general purpose processors for codes with signi cant amounts of data level parallelism. with continuing improvements in transistor density and an increasing emphasis on energy ef ciency, there has recently been growing interest in dlp accelerators for mainstream computing environments. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. these accelerators are usually attached to a general purpose host processor, either on the same die or a separate die. the host processor executes system code and non dlp application code while distributing dlp kernels to the accelerator. surveying the wide range of data parallel accelerator cores in industry and academia reveals a general tradeoff between programmability and ef ciency. in this paper, we examine multiple alternative data parallel accelerators to quantify the ef ciency impact of microarchitectural features intended to simplify programming or expand the range of code that can be executed. we rst introduce a set of ve architectural design patterns for dlp cores in section #, qualitatively comparing their expected programmability and ef ciency. exibly supports mapping data parallel tasks to a collection of simple scalar or multithreaded cores, but lacks mechanisms for ef cient execution of regular dlp. the vector simd and subword simd patterns can signi cantly reduce the energy on regular dlp, but can require complicated programming for irregular dlp. the single instruction multiple thread and vector thread patterns are hybrids between the mimd and vector simd patterns that attempt to offer alternative tradeoffs between programmability and ef ciency. when reducing these high level patterns to an ef cient vlsi design, there is a large design space to explore. in section #, we present a common set of parameterized synthesizable microarchitectural components and show how these can be combined to form complete rtl designs for the different architectural design patterns, thereby reducing total design effort and allowing a fairer comparison across patterns. in this section, we also introduce maven, a new vt microarchitecture. our modular design strategy revealed a much simpler and more ef cient implementation than the earlier scale vt design. maven is based on a vector simd microarchitecture with only the minimum number of hardware mechanisms added to enable the improved programmability from vt, instead of the decoupled cluster microarchitecture of scale. another innovation in maven is to use the same risc isa for both vector and scalar code, greatly reducing the effort required to develop an ef cient vt compiler. the scale design required a separate clustered isa for vector code, which complicated compiler development. to concretely evaluate and compare the ef ciency of these patterns, we have generated and analyzed hundreds of complete vlsi layouts for the mimd, vector simd, and vt patterns using our parameterized microarchitecture components targeting a modern nm technology. sections describes our methodology for extracting area, energy, and performance numbers for a range of microbenchmarks and compiled application kernels. our results show that vector cores are considerably more ef cient in both energy and area normalized performance than mimd cores, although the mimd cores are usually easier to program. our results also suggest that the maven vt microarchitecture is superior to the traditional vector simd architecture, providing greater ef ciency and a simpler programming model. for both vt and vector simd, multi lane implementations are usually more ef cient than multi core single lane implementations and can be easier to program as they require less partitioning and load balancing. although we do not implement a simt machine, some initial analysis indicates simt will be less ef cient than vt but should be easier to program. heterogeneous multicore systems comprised of multiple cores with varying capabilities, performance, and energy characteristics have emerged as a promising approach to increasing energy efficiency. such systems reduce energy consumption by identifying phase changes in an application and migrating execution to the most efficient core that meets its current performance requirements. however, due to the overhead of switching between cores, migration opportunities are limited to coarse grained phases, reducing the potential to exploit energy efficient cores. we propose composite cores, an architecture that reduces switching overheads by bringing the notion of heterogeneity within a single core. the proposed architecture pairs big and little compute engines that together can achieve high performance and energy efficiency. by sharing much of the architectural state between the engines, the switching overhead can be reduced to near zero, enabling fine grained switching and increasing the opportunities to utilize the little engine without sacrificing performance. an intelligent controller switches between the engines to maximize energy efficiency while constraining performance loss to a configurable bound. we evaluate composite cores using cycle accurate micro architectural simulations and a detailed power model. results show that, on average, the controller is able to map of the execution to the little engine, achieving an energy savings while limiting performance loss to. the microprocessor industry, fueled by moorelaw, has continued to provide an exponential rise in the number of transistors that can. however, transistor threshold voltages have not kept pace with technology scaling, resulting in near constant per transistor switching energy. these trends create a dif cult design dilemma: more transistors can. on a chip, but the energy budget will not allow them to be used simultaneously. this trend has made it possible for todaycomputer architects to trade increased area for improved energy ef ciency of general purpose processors. heterogeneous multicore systems are an effective approach to trade area for improved energy ef ciency. these systems comprise multiple cores with different capabilities, yielding varying performance and energy characteristics. in these systems, an application is mapped to the most ef cient core that can meet its performance needs. as its performance changes, the application is migrated among the heterogeneous cores. traditional designs select the best core by brie. however, every time the application migrates between cores, its current state must be explicitly transferred or rebuilt on the new core. this state transfer incurs large overheads that limits migration between cores to a coarse granularity of tens to hundreds of millions of instructions. to mitigate these effects, the decision to migrate applications is done at the granularity of operating system time slices. this work postulates that the coarse switching granularity in existing heterogeneous processor designs limits their effectiveness and energy savings. what is needed is a tightly coupled heterogeneous multicore system that can support ne grained switching and is unencumbered by the large state migration latency of current designs. to accomplish this goal, we propose composite cores, an architecture that brings the concept of heterogeneity to within a single core. a composite core contains several compute engines that together can achieve both high performance and energy ef ciency. in this work, we consider a dual engine composite core consisting of: a high performance core and an energy ef cient core. as only one engine is active at a time, execution switches dynamically between engines to best match the current applicationcharacteristics to the hardware resources. this switching occurs on a much ner granularity compared to past heterogeneous multicore proposals, allowing the application to spend more time on the energy ef cient engine without sacri cing additional performance. as a composite core switches frequently between engines, it relies on hardware resource sharing and low overhead switching techniques to achieve near zero engine migration overhead. for example, the big and little engines share branch predictors, caches, fetch units and tlbs. this sharing ensures that during a switch only the register state needs to be transfered between the cores. we propose a speculative register transfer mechanism to further reduce the migration overhead. because of the ne switching interval, conventional samplingbased techniques to select the appropriate core are not well suited for a composite core. instead, we propose an online performance estimation technique that predicts the throughput of the unused engine. if the predicted throughput of the unused engine is signi cantly higher or has better energy ef ciency then the active engine, the application is migrated. thus, the decision to switch engines maximizes execution on the more ef cient little engine subject to a performance degradation constraint. the switching decision logic tracks and predicts the accumulated performance loss and ensures that it remains within a user selected bound. with composite cores, we allow the users or system architects to select this bound to trade off performance loss with energy savings. to accomplish this goal, we integrate a simple control loop in our switching decision logic, which tracks the current performance difference based on the performance loss bound, and a reactive model to detect the instantaneous performance difference via online perfor big core little core big core little core instructions instructions inst. interval figure #: ipc measured over a typical scheduling interval for gcc instructions cycle instructions cycle that is available. conversely, the cortex a is a narrow in order pro cessor with a relatively short pipeline. the cortex a has higher performance, but the cortex a is more energy ef cient. always switch share always switch stitch always switch flush randomly switch share randomly switch stitch randomly switch flush quantum length additional switching overhead in big little, all migrations must occur through the coherent interconnect between separate level caches, resulting in a migration cost of about seconds. thus, the cost of migration requires that the system migrate between cores only at coarse granularity, on the order of tens of milliseconds. the large switching interval forfeits potential gains afforded by a more aggressive ne grained switching. switching interval figure #: migration overheads under different switching schemes and probabilities mance estimation techniques. in summary, this paper offers the following contributions: we propose composite cores, an architecture that brings the concept of heterogeneity within a single core. the composite core consists of two tightly coupled engines that enable ne grained matching of application characteristics to the underlying microarchitecture to achieve both high performance and energy ef ciency. we study the bene ts of ne grained switching in the context of heterogeneous core architectures. to achieve near zero engine transfer overhead, we propose low overhead switching techniques and a core microarchitecture which shares necessary hardware resources. we design intelligent switching decision logic that facilitates negrain switching via predictive rather than sampling based performance estimation. our design tightly constrains performance loss within a user selected bound through a simple feedback controller. we evaluate our proposed composite core architecture with cycle accurate full system simulations and integrated power models. overall, a composite core can map an average of of the dynamic execution to the little engine and reduce energy by while bounding performance degradation to at most. spatial computing has been shown to be an energy efficient model for implementing program kernels. in this paper we explore the feasibility of using sc for more than small kernels. to this end, we evaluate the performance and energy efficiency of entire applications on tartan, a general purpose architecture which integrates a reconfigurable fabric with a superscalar core. our compiler automatically partitions and compiles an application into an instruction stream for the core and a configuration for the rf. we use a detailed simulator to capture both timing and energy numbers for all parts of the system our results indicate that a hierarchical rf architecture, designed around a scalable interconnect, is instrumental in harnessing the benefits of spatial computation. the interconnect uses static configuration and routing at the lower levels and a packet switched, dynamically routed network at the top level. tartan is most energyefficient when almost all of the application is mapped to the rf, indicating the need for the rf to support most general purpose programming constructs. our initial investigation reveals that such a system can provide, on average, an order of magnitude improvement in energy delay compared to an aggressive superscalar core on single threaded workloads. market demand and technology push, in the guise of thermal dissipation, energy density, complexity constraints and wire delay, are changing the equation that has, until recently, made complex out of order superscalar designs the workhorse of computing. no longer can architects rely on increasing the clock speed and complexity of the pipeline as the major means of improving perfor permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. microsoft research mountain view, ca mbudiu microsoft com mance. in fact, demand for low power devices may mean that performance may no longer be the crucial metric of success. this changing landscape has prompted many to explore alternative architectures that harness coarse grained parallelism; for example, commercial multi core chips capitalize on parallelism by replicating a number of simple cores, thereby boosting scalability. while these chips are focused on server workloads which are rich in threads, it is not yet clear how single threaded applications can bene. the research community has responded with tile based architectures that harness coarse grained parallelism on an array of light weight processors. in this paper, we explore an alternative array based architecture that uses spatial computation to improve energy ef ciency as measured by the energy delay product. spatial computation is a model of computation optimized for wires: it lays out the computation in space rather than in time. in earlier work, we presented one instantiation of sc: a compi lation framework that can synthesize ansiprograms into gatelevel asynchronous circuit descriptions, which are then laid out as asics using commercial tools. the previous evaluation was restricted to selected media processing kernels, on which we demonstrated that the synthesized asic was better than an aggressive wide superscalar by up to three orders of magnitude in energy ef ciency, while delivering comparable performance. in garp, speedups of up to are achieved when mapping small kernels compiled fromto a recon gurable fabric. these results raise the question: can the sc model be used to form the basis of a scalable general purpose processing system for whole program execution to answer this question we explore a hybrid architecture, tartan, which integrates a large recon gurable fabric with a simple processor core. tartan is designed to be: general purpose: the system must be able to execute a wide variety of programs written in standard high level languages, eg, ansi. applications are compiled into an instruction stream and an rf con guration. the instruction stream is executed on tartanprocessor core, which is more ef cient for control intensive tasks such as the os. the con guration describes an sc circuit and is loaded onto the rf, which is more ef cient for computation intensive tasks. energy ef cient: tartan uses its rf to reduce energy consumption an rf con guration describes an application speci. data ow machine, thereby saving on the power consumed by instruction fetch and decode in grid architectures. further, the rf is implemented as a fully distributed asynchronous circuit which has been shown to be energy ef cient. fabrication friendly: future technologies will place stringent demands on manufacturing. tartanrf reduces these demands by tolerating manufacturing defects and both intra and interdie parametric variation. tartan implements an extreme version of sc on the rf computation structures are never shared, and each program operation is implemented on a dedicated functional unit. this results in circuits which use only local communication, require neither broadcast nor global control, and are self timed. the distributed nature of the sc circuits combined with the asyn chronous rf enables placement and routing around defects. while this strategy affects local timing around the defective area, the absence of a global clock ensures that timing closure is not an issue. furthermore, many styles of self timed circuits are nat urally tolerant of parametric variation. finally, the regular layout of the rf eliminates many of the issues that arise in deep submicron lithography and fabrication. tartanmany advantages arise directly from the extreme form of sc that it implements. however, this comes at a cost: as more of the program is placed on the rf, the distance between two functions on the rf and between a function on the rf and shared memory increases. the heart of this paper is an exploration of the tradeoffs involved in resolving this fundamental tension in sc when implementing large programs on an rf. in particular, we examine the cost of communicating across the fabric, the cost of maintaining memory coherence, the effect of varying page sizes, and the impact of defect tolerance. these trade offs are evaluated in terms of area usage, execution time, power, and most importantly, energy delay. we use a timing and energy accurate simulator of both the cpu core and the rf to carry out our experiments. our simulator allows us to not only determine the overall performance of a design, but to also quantify the time and energy spent in various circuit components. sections and describe the tartan architecture and compi lation methodology. our results show that a hybrid architecture such as tartan is capable of harnessing for whole programs the energy and power bene ts shown for kernels implemented as asics. tartan is up to an order of magnitude more energy ef cient than an aggressive superscalar core. we found, however, that to harness this energy ef ciency, it is important to execute as much of the program on the rf as possible. to achieve this, it is essential for the rf to directly implement most common programming constructs and to act as a peer of the cpu core, invoking services from it. this also allows tartan to use a simple processor core instead of an aggressive, power hungry superscalar core. finally, our results indicate that supporting large programs with good performance requires distributing memory on the fabric. specialized execution using spatial architectures provides energy efficient computation, but requires effective algorithms for spatially scheduling the computation. generally, this has been solved with architecture specific heuristics, an approach which suffers from poor compiler architect productivity, lack of insight on optimality, and inhibits migration of techniques between architectures. our goal is to develop a scheduling framework usable for all spatial architectures. to this end, we expresses spatial scheduling as a constraint satisfaction problem using integer linear programming. we observe that architecture primitives and scheduler responsibilities can be related through five abstractions: placement of computation, routing of data, managing event timing, managing resource utilization, and forming the optimization objectives. we encode these responsibilities as general ilp constraints, which are used to create schedulers for the disparate trips, dyser, and plug architectures. our results show that a general declarative approach using ilp is implementable, practical, and typically matches or outperforms specialized schedulers. hardware specialization has emerged as an important way to sustain microprocessor performance improvements to address transistor energy ef ciency challenges and general purpose processinginef ciencies. the fundamental insight of many specialization techniques is to map large regions of computation to the hardware, breaking away from instruction byinstruction pipelined execution and instead adopting a spatial architecture paradigm. pioneering examples include raw, wavescalar and trips, motivated primarily by performance, and recent energy focused proposals include tartan, cca, plug, flexcore, softhv, mescal, spl, cores, dyser, beret, and npu. a fundamental problem in all spatial architectures is the majority of work completed while author was a phd student at ut austin permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. speci cally, ve intuitive abstractions in terms of graph matching describe the scheduling problem: placement of computation on the hardware substrate, ii routing of data on the substrate to re ect and carry out the computation semantics including interconnection network assignment, network contention, and network path assignment, iii managing the timing of events in the hardware, iv managing utilization to orchestrate concurrent usage of hardware resources, andforming the optimization objectives to meet the architectural performance goals. thus far, these abstractions have not been modeled directly, and the typically np complete spatial architecture scheduling problem is side stepped. schedulers has typically been on developing polynomial time algorithms that approximate the optimal solution using knowledge about the architecture. chronologically, this body of work includes the bug scheduler for vliw proposed in year#, uas scheduler for clustered vliw, synchronous data ow graph scheduling, raw scheduler, cars vliw code generation and scheduler, trips scheduler, wavescalar scheduler, and cca scheduler proposed in year#. while heuristic based approaches are popular and effective, they have three problems: poor compiler developer architect productivity since new algorithms, heuristics, and implementations are required for each architecture, ii lack of insight on optimality of solution, and iii sandboxing of heuristics to speci. architectures understanding and using techniques developed for one spatial architecture in another is very hard. considering these problems, others have looked at exact mathematical and constraint theory based formulations of the scheduling problem. table # classi es these prior efforts, which are based on integer linear programming or satis ability modulo theory. they lack in some prominent ways which perhaps explains why the heuristic based approaches continue to be preferred. in particular, feautrier is the most related but it lacks three of ve abstractions required, and the static placement static issue of vliw restrict its applicability to the general problem. these techniques and their associated problems serve as the goal and inspiration for our work, which is to develop a declarative, constrainttheory based universal spatial architecture scheduler. by unifying multiple facets of the related work above, speci cally the past experience of architecture speci. spatial schedulers, the principal of attaining architectural generality, and the mathematical power of integer linear programming, we seek to create a solution which allows high developer productivity, provides provable properties on results, and enables true architectural generality. achieving architectural generality through the ve scheduling abstractions mentioned above is the key novelty of our work. implementation: in this paper, we use integer linear programming because it allows a high degree of constraint expressability, can provide strong bounds on the solutionoptimality, and has fast commercial solvers like cplex, gurobi, and xpress. related work legend: computation placement ii data routing iii event timing iv utilizationoptimization objective speci cally, we use the gams modeling language. we show that a total of constraints specify the problem. we implement these constraints and report on results for three architectures picked to stress our ilp scheduler in various ways. to stress the performance deliverable by our general ilp approach, we consider trips because it is a mature architecture with sophisticated specialized schedulers resulting from multi year efforts. to represent the emerging class of energy centric specialized spatial architectures, we consider dyser. finally, to demonstrate the generality of our technique, we consider plug, which uses a radically different organization. respectively, only, and additional constraints are required to handle architecture speci. we show that standard ilp solvers allows succinct implementation, provide solutions in tractable run times, and the mappings produced are either competitive with or signi cantly better than those of specialized schedulers. the general and declarative approach allows schedulers to be speci ed, implemented, evaluated rapidly. our implementation is provided as an open source download, allowing the community to build upon our work. paper organization: the next section presents background on the three architectures and an ilp primer. section # presents an overview of our approach, section # presents the detailed ilp formulation, section # discusses architecture speci. section # presents evaluation and section # concludes. heterogeneous multicore systems are composed of multiple cores with varying energy and performance characteristics. a controller dynamically detects phase changes in applications and migrates execution onto the most efficient core that meets the performance requirements. in this paper, we show that existing techniques that react to performance changes break down at fine grain intervals, as performance variations between consecutive intervals are high. we propose a predictive trace based switching controller that predicts an upcoming phase change in a program and preemptively migrates execution onto a more suitable core. this prediction is based on a phase individual history and the current program context. our implementation detects repeatable code sequences to build history, uses these histories to predict an phase change, and preemptively migrates execution to the most appropriate core. we compare our method to phase prediction schemes that track the frequency of code blocks touched during execution as well as traditional reactive controllers, and demonstrate significant increases in prediction accuracy at fine granularities. for a big little heterogeneous system that is comprised of a high performing out of order core and an energy efficient, in order core, at granularities of instructions, the trace based predictor can spend of execution time on the little, while targeting a maximum performance degradation of. this translates to an increased energy savings of on average over running only on big, representing a increase over existing techniques. ne granularity of hundreds of instructions, thus realizing the bene ts. ectively at the granularity of hundreds of instructions. ciency in modern processors is to combine multiple cores with di erent capabilities into a single heterogeneous processor, yielding varying performance and energy characteristics. heterogeneous multi cores trade increased area to provide higher performance and reduced energy consumption by matching an applicationperformance and energy requirements to the most appropriate core type. researchers have demonstrated the potential bene ts of such designs in terms of realizing more energy. commercially available processors include armbig little, which consists of one high performance out of order big core and a lower performance, but much more energy. cient in order little core and nvidiakal el, which combines four high performance and one energy. ciency attained in heterogeneous systems is determined in part by the. ciency of the scheduling or switching mechanism, that dynamically guides program execution to the most energy. existing techniques are reactive in nature they distinguish performance phase changes in applications by sampling for a brief period and assume that the performance will remain stable until the following sampling phase. metrics like performance, load memory intensity, available instruction and memory level parallelism, and branch misprediction rates are some of the factors used to evaluate the microarchitectural requirements of an application and thus determine the best tting core. a reactive controller dynamically monitors performance on the active core at the granularity of an instruction slice or quantum. performance on another core is either monitored by sampling brie. on it or by modeling it using observed performance metrics. this measured or modeled performance on the available core options is compared and contingent upon the desired performance target, the controller maps execution on the most. a reactive controller is well suited for traditional heterogeneous systems that make switching decisions at coarsegrained application phases of tens to hundreds of millions of instructions. ciency can be micro, december, year#, davis, ca, usa achieved by slicing applications into micro phases of a few year# copyright is held by the owner author. more low performance phases exist to acm acm year#. at such ne instruction granularities, increasing opportunities to utilize more energy. finegrained phase changes in general purpose programs can be exploited by coupling heterogeneous general purpose cores in a system that is unencumbered by large transfer overheads between core switches. recently proposed composite cores has implemented a general purpose architecture that enables switching at. it is composed of two compute backends that lie on opposite ends of the energy performance spectrum, a high performing big out of order backend and an energy. together, they aim to achieve both high performance and energy. by sharing the caches, tlb, and pipeline frontends, the switching overhead can be reduced to near zero. we have assumed the composite core architecture for our implementation. the fundamental assumption of reactive controllers however, that the performance observed during sampling is stable over the subsequent intervals, fails at ne granularities because of the high variation between performance in consecutive quanta. we observe that the average performance di erence between consecutive quanta is almost an order of magnitude higher for ne grain quanta over coarse grain. we propose a controller that is able to make accurate phase change predictions at. this work employs a predictive approach to scheduling by nding and exploiting repeatable phase behavior in an application, while maintaining a performance loss target. prior work has shown that programs exhibit repeatable phase based behavior across di erent microarchitecture metrics at a coarse granularity. our goal is to extend that intuition to ner grained phases of a few hundred instructions. however, direct application of those works at ner granularities is infeasible due to the eccentric nature of ne grained quanta and large associated computation overheads. our proposed controller architecture divides the applicationexecution into recurring sequences of instructions at run time, referred to as super traces. a super trace is identi ed by a combination of backward branches, or backedges, that appear together. backedges are useful in identifying loop and function boundaries in a dynamic instruction stream, which represent regular and recurring control independent blocks of code. a super trace captures inter dependencies between blocks that could. ect performance by combining consecutive backedges until a minimum length requirement is met. we propose a predictive trace based switching controller for this architecture that can dynamically learn regular negrained phase behavior and predict an upcoming phase change based on a phaseindividual history and the current program context. this is used to preemptively migrate the execution to a more suitable backend, improving phase tobackend mappings and increasing energy. ers the following contributions: we observe that accuracy of reactive scheduling approaches used in existing heterogeneous architectures decreases at ne switching granularities. we leverage the concept of performance micro phases, figure #: potential increase in time spent on the little core with reduction in quantum size and exploit their regular, repeatable behavior to develop a predictive scheduling technique. we de ne a super trace that can capture program phases. ective hardware controller to predict both which super trace is likely to be executed next in the dynamic instruction stream and which core is more suitable to execute it. we compare the accuracy achieved by our proposed scheme over existing methods of phase detection, such as that implemented by sherwood et. we analyze the proposed system with cycle accurate full system simulations on. overall, with a performance degradation constraint of, our phased based predictive approach to scheduling maps an average of of program execution time onto the little backend, nearly more than what an existing state of art design can achieve. the overall energy consumption is reduced by as compared to full execution on the big core and by over existing techniques. dataflow architectures tolerate long unpredictable communication delays and support generation and coordination of parallel activities directly in hardware, rather than assuming that program mapping will cause these issues to disappear. however, the proposed mechanisms are complex and introduce new mapping complications. this paper presents a greatly simplified approach to dataflow execution, called the explicit token store architecture, and its current realization in monsoon. the essence of dynamic dataflow execution is captured by a simple transition on state bits associated with storage local to a processor. low level storage management is performed by the compiler in assigning nodes to slots in an activation frame, rather than dynamically in hardware. the processor is simple, highly pipelined, and quite general. it may be viewed as a generalization of a fairly primitive von neumann architecture. although the addressing capability is restrictive, there is exactly one instruction executed for each action on the dataflow graph. thus, the machine oriented ets model provides new understanding of the merits and the real cost of direct execution of dataflow graphs. mobile computing as exemplified by the smart phone has become an integral part of our daily lives. the next generation of these devices will be driven by providing an even richer user experience and compelling capabilities: higher definition multimedia, graphics, augmented reality, games, and voice interfaces. to address these goals, the core computing capabilities of the smart phone must be scaled. however, the energy budgets are increasing at a much lower rate, requiring fundamental improvements in computing efficiency. simd accelerators offer the combination of high performance and low energy consumption through low control and interconnect overhead. many applications lack sufficient vector parallelism to effectively utilize a large number of simd lanes. further, the use of symmetric hardware lanes leads to low utilization and high static power dissipation as simd width is scaled. to address these inefficiencies, this paper focuses on breaking two traditional rules of simd processing: homogeneity and static configuration. the libra accelerator increases simd utility by blurring the divide between vector and instruction parallelism to support efficient execution of a wider range of loops, and it increases hardware utilization through the use of heterogeneous hardware across the simd lanes. experimental results show that the lane libra outperforms traditional simd accelerators by an average of performance improvement due to higher loop coverage with less energy consumption through heterogeneous hardware. growing on chip wire delays will cause many future microarchitectures to be distributed, in which hardware resources within a single processor become nodes on one or more switched micronetworks. since large processor cores will require multiple clock cycles to traverse, control must be distributed, not centralized. this paper describes the control protocols in the trips processor, a distributed, tiled microarchitecture that supports dynamic execution. it details each of the five types of reused tiles that compose the processor, the control and data networks that connect them, and the distributed microarchitectural protocols that implement instruction fetch, execution, flush, and commit. we also describe the physical design issues that arose when implementing the microarchitecture in a transistor, nm asic prototype chip composed of two wide issue distributed processor cores and a distributed mb nonuniform on chip memory system. this paper proposes a new approach called explicit loop specialization based on the idea of elegantly encoding inter iteration loop dependence patterns in the instruction set. xloops supports a variety of inter iteration data and control dependence patterns for both single and nested loops. the xloops hardware software abstraction requires only lightweight changes to a general purpose compiler to generate xloops binaries and enables executing these binaries on: traditional microarchitectures with minimal performance impact, specialized microarchitectures to improve performance and or energy efficiency, and adaptive microarchitectures that can seamlessly migrate loops between traditional and specialized execution to dynamically trade off performance vs. we evaluate xloops using a vertically integrated research methodology and show compelling performance and energy efficiency improvements compared to both simple and complex general purpose processors. hardware specialization is an increasingly common technique to enable improved performance and energy efficiency in spite of the diminished benefits of technology scaling. heterogeneous multicore architectures have the potential for high performance and energy efficiency. these architectures may be composed of small power efficient cores, large high performance cores, and or specialized cores that accelerate the performance of a particular class of computation. architects have explored multiple dimensions of heterogeneity, both in terms of micro architecture and specialization. while early work constrained the cores to share a single isa, this work shows that allowing heterogeneous isas further extends the effectiveness of such architectures this work exploits the diversity offered by three modern isas: thumb, and alpha. this architecture has the potential to outperform the best single isa heterogeneous architecture by as much as, with energy savings and a reduction of in energy delay product. architects have proposed heterogeneous chip multiprocessors for both general purpose computing and embedded applications. these architectures exploit heterogeneity in two fundamental dimensions. while some architectures make use of specialized hardware to accelerate the performance of certain workloads, others employ a different set of microarchitectural parameters in order to create energy ef cient processors for mixed workloads. the latter constrain the cores to execute a single instruction set architecture, maximizing ef ciency by allowing a thread to dynamically identify, and migrate to, the core to which it is most suited during a particular phase and under the current environmental constraints. this paper demonstrates that not only is that constraint unnecessary, but limiting an architecture to a single isa restricts the potential heterogeneity, sacri cing performance and ef ciency gains. a critical step in the design of a heterogeneous isa architecture is choosing a diverse set of isas. while isas seem to converge over time, there remains suf cient diversity in existing modern isas to provide useful heterogeneity. we examine some key aspects that characterize isa diversity. these include code density, decode and instruction complexity, register pressure, native oating point arithmetic vs emulation, and simd processing. in this paper, we harness the diversity offered by three isas: armthumb, and alpha. by codesigning the hardware architectures and the isas to provide the best aggregate architecture, we arrive at a more effective and ef cient design than one composed of homogeneous cores, or even heterogeneous cores that share a single isa. the design of a heterogeneous isa chip multiprocessor involves navigating a complex search space, made larger by the additional dimension of freedom. a major contribution of this work is such a design space exploration geared at nding an optimal heterogeneous isa cmp for general purpose mixed workloads. observing the results of the design space exploration, we provide architects with a set of tools to enable isa microarchitecture co design and thereby better streamline their search processes. to reap the full bene ts of the heterogeneity, especially the heterogeneity available in the form of isa diversity, it is important that an application is able to migrate freely between the cores. however, migration in a heterogeneous isa environment is a well known dif cult problem. this is because the runtime state of a program is kept in isa speci. form, and migration to a different isa involves expensive program state transformation. devuyst, et al demonstrate that migration between isas can be achieved at acceptable cost on a cmp; however, that work does not explore the architectural advantages to multiple isas on a single cmp. this research employs several ideas from that work, but also several new optimizations to reduce the overhead of migration. in this paper, we present a detailed compilation methodology and an effective runtime strategy that works for a diverse set of isas. we observe that even a single application can gain up to performance bene. by migrating between heterogeneous isa cores during different phases of its execution. finally, we evaluate the proposed heterogeneous isa cmp against both homogeneous and single isa heterogeneous cmps, under varying power and area budgets. consequently, we make the following major observations: co design of isa and microarchitectural parameters is critical. in the optimal designs, cores employing different isas tend to naturally diverge, and to diverge in consistent directions. isa heterogeneity is not only bene cial across applications, but also within individual applications across phases. we nd that heterogeneous isa cmps can improve singlethread performance by an average of and provide more throughput on multi programmed mixed workloads, as compared to a single isa heterogeneous cmp. additionally, heterogeneous isa cmps can help achieve an average reduction of in energy delay product. the rest of this paper is organized as follows. section # evaluates the diversity offered by the isas chosen for this work. we present our compilation and runtime methodologies in section #. section # evaluates the proposed year# ieee architecture, and draws lessons and guidelines from the design space exploration. growing transistor counts, limited power budgets, and the breakdown of voltage scaling are currently conspiring to create a utilization wall that limits the fraction of a chip that can run at full speed at one time. in this regime, specialized, energy efficient processors can increase parallelism by reducing the per computation power requirements and allowing more computations to execute under the same power budget. to pursue this goal, this paper introduces conservation cores. conservation cores, orcores, are specialized processors that focus on reducing energy and energy delay instead of increasing performance. this focus on energy makescores an excellent match for many applications that would be poor candidates for hardware acceleration. we present a toolchain for automatically synthesizingcores from application source code and demonstrate that they can significantly reduce energy and energy delay for a wide range of applications. thecores support patching, a form of targeted reconfigurability, that allows them to adapt to new versions of the software they target. our results show that conservation cores can reduce energy consumption by up to for functions and by up to for whole applications, while patching can extend the useful lifetime of individualcores to match that of conventional processors. as power concerns continue to shape the landscape of generalpurpose computing, heterogeneous and specialized hardware has emerged as a recurrent theme in approaches that attack the power wall. in the current regime, transistor densities and speeds continue to increase with moore slaw, but limitson thresholdvoltage scaling have stopped the downward scaling of per transistor switching permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. consequently, the rate at which we can switch transistors is far outpacing our ability to dissipate the heat created by those transistors. the result is a technology imposed utilization wall that limits the fraction of the chip we can use at full speed at one time. our experiments with a nm tsmc process show that we can switch less than of a mm die at full frequency within an powerbudget. itrs roadmap projections and cmos scaling theory suggests that this percentage will decrease to less than in nm, and will continue to decrease by almost half with each process generation and even further with integration. the effects of the utilization wall are already indirectly apparent in modern processors: intelnehalem provides a turbo mode that powers off some cores in order to run others at higher speeds. another strong indication is that even though native transistor switching speeds have continued to double every two process generations, processor frequencies have not increased substantially over the last years. in this regime, reducing per operation energy translates directly into increased potential parallelism for the system: if a given computation can be made to consume less power at the same level of performance, other computations can be run in parallel without violating the powerbudget. this paper attacks the utilization wall with conservation cores. hardware circuits created for the purpose of reducing energy consumption on computationally intensive applications. since it is no longer possible to run the entire chip at full frequencyat once, it makes sense to customize the portions of the chip that are running so theywill be as ef cient as possible for the application at hand. in effect, conservation cores allow architects to trade area for energy in a processordesign. the utilization wall has made this trade off favorable, because moorelaw has made increases in transistor counts cheap, while poor cmos scaling has exhausted power budgets, making increases in power consumption very expensive. conservation cores have a different goal than conventional application speci. circuits, and we differentiate betweencores and the more common accelerators along several axes. first, accelerators focus on improving performance, at a potentially worse, equal, or better energy ef ciency. conservation cores, on the other hand, focus primarily on energy reduction. cores that are also accelerators are possible, but this work targets similar levels of performance, and focuses on reducing energy and energy delay, especially at advanced technology nodes wheredvfs is less effective for saving energy. shifting the focus from performance to ef ciencyallowscores to target a broader range of applications than accelerators. for codes with large amounts of parallelism and predictable communication patterns, since these codes map naturally onto hardware. thus, parallelism intensive regions of code that are hot are the best candidates for implementation as accelerators. on the other hand, cores are parallelism agnostic: hot code with a tight critical path, little parallelism, and or very poor memory behavior is an excellent candidate for acore, ascores can reduce the number of transistor toggles required to get through that code. for instance, our results show thatcores can deliver signi cant energy savings for irregular, integer applications that would be dif cult to automatically accelerate with specialized hardware. incorporatingcores into processors, especially at a scale large enough to save power across applications with multiple hot spots, raises a number of challenges: determining whichcores to build in order tobuildcores, we must be able to identify which pieces of code are the best candidates for conversion intocores. the code should account for a signi cant portion of runtime and energy, and stem from a relatively stable code base. transistor density continues to increase exponentially, but power dissipation per transistor is improving only slightly with each generation of moore law. given the constant chip level power budgets, this exponentially decreases the percentage of transistors that can switch at full frequency with each technology generation. hence, while the transistor budget continues to increase exponentially, the power budget has become the dominant limiting factor in processor design. in this regime, utilizing transistors to design specialized cores that optimize energy per computation becomes an effective approach to improve system performance. to trade transistors for energy efficiency in a scalable manner, we propose quasi specific cores, or qscores, specialized processors capable of executing multiple general purpose computations while providing an order of magnitude more energy efficiency than a general purpose processor. our approach exploits these similar code patterns to ensure that a small set of specialized cores support a large number of commonly used computations. our results show that qscores can provide better energy efficiency than general purpose processors while reducing the amount of specialized logic required to support the workload by up to. the qscores design flow is based on the insight that similar code patterns exist within and across applications. we evaluate qscores ability to target both a single application library as well as a diverse workload consisting of applications selected from different domains.