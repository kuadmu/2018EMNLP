this paper reports on an empirical evaluation of the fault detecting ability of two white box software testing techniques: decision coverage and the all uses data flow testing criterion. each subject program was tested using a very large number of randomly generated test sets. for each test set, the extent to which it satisfied the given testing criterion was measured and it was determined whether or not the test set detected a program fault. these data were used to explore the relationship between the coverage achieved by test sets and the likelihood that they will detect a fault previous experiments of this nature have used relatively small subject programs and or have used programs with seeded faults. in contrast, the subjects used here were eight versions of an antenna configuration program written for the european space agency, each consisting of over, lines ofcode for each of the subject programs studied, the likelihood of detecting a fault increased sharply as very high coverage levels were reached. thus, this data supports the belief that these testing techniques can be more effective than random testing. however, the magnitudes of the increases were rather inconsistent and it was difficult to achieve high coverage levels. some of the most dynamic systems being built today consist of physically mobile hosts and logically mobile agents. such systems exhibit frequent configuration changes and a great deal of resource variability. applications executing under these circumstances need to react continuously and rapidly to changes in operating conditions and must adapt their behavior accordingly. the development of such applications demands a reexamination of the notion of context and the mechanisms used to manage the application response to contextual changes. this paper introduces egospaces, a coordination model and middleware for ad hoc mobile environments. egospaces focuses on the needs of application development in ad hoc environments by proposing an agent centered notion of context, called a view, whose scope extends beyond the local host to data and resources associated with hosts and agents within a subnet surrounding the agent of interest. an agent may operate over multiple views whose definitions may change over time. an agent uses declarative specifications to constrain the contents of each view by employing a rich set of constraints that take into consideration properties of the individual data items, the agents that own them, the hosts on which the agents reside, and the physical and logical topology of the ad hoc network. this paper formalizes the concept of view, explores the notion of programming against views, discusses possible implementation strategies for transparent context maintenance, and describes our current prototype of the system. we include examples to illustrate the expressive power of the view abstraction and to relate it to other research on coordination models and middleware. in the recent past, a number of researchers advocated the ability for applications to adapt their behavior in response to changes in their environments. applications in which con text sensors capture changes in the environment and pass them to the application are called context aware. as user mobility has increased, context aware models of computa tion have gained popularity because they offer the ability to include information about the current environment as part of the computation, thereby enhancing the services available to the users. context aware applications and development frameworks existing today generally gain access to context information through context sensors. cyberguide and guide, two tour guide applications, use events gener ated by location sensors to update the user screen accord ing to physical location. the stickdocument frame work allows users to create notes that are triggered when the user encounters the associated context. events indicate context changes, and they trigger the display of a stored note. in both cases, small devices moving about the research complex beacon location events to the architecture via infrared communication links. in parctab, each de vice, called a tab, has an associated agent responsible for mediating the context events. each tab has a single cur rent application to which the tab agent forwards location events generated by the tab. the active badge framework allows applications to query the device current context. additionally, applications can register for notifications of specific context changes by specifying a filter indicating the contextual information that interests it and a callback to invoke when the filter is satisfied. the fieldnotes ap plication extends the types of context used to include time, weather, and user information by allowing researchers in the field to attach varied contextual information to their notes. with the increase in the variety and complexity of context information, frameworks and systems for generalizing its treatment are being developed. the context toolkit generalizes interactions among components through context widgets, while the context fabric provides a service infrastructure. by and large these systems limit the con text to what a component can immediately sense, ignoring what other networked components sense. while discussions of context aware software have hinted at the need for an extended view of context, none of the better known systems allows such access without requiring application developers to code it explicitly. in this paper, we consider systems that entail both phys ical and logical mobility. for presentation purposes, we as sume a system existing of mobile agents that execute over poten tially mobile hosts. as hosts and agents become more mobile and travel to completely new environments, the resources they can access change and the manner of this ac cess evolves. the presence of other agents, the availability of resources associated with them, the specific host provid ing the agent execution environment, and the connectivity of other hosts and their particular location or movement behavior all have the potential to affect the behavior of a single agent. such applications can vary from time and safety critical ones like negotiating the safe passage of au tomobiles through intersections without traffic lights to en tertainment applications including file sharing between cars or distributed video games. in each case, the application constructs its view based on different criteria and requires different guarantees for its operation over the view. for example, in a safe intersection example, a single car will col lect information from other cars in close proximity and from sensors in the intersection. as a second example, imagine a building or construction site with a fixed infrastructure of sensors and information appliances that provide contex tual information for applications running inside the build ing. sensors can provide a multitude of information, includ ing data regarding the structural integrity of the building, the frequency of sounds in the building, or the movement of building occupants. additionally, engineers or inspectors carry pdas or laptops that provide additional context and assimilate context information to accomplish specific tasks. different people have different tasks and will therefore use information from different sets of sensors. our research goal is to develop a formal abstract treat ment of context awareness and offer middleware to the pro grammer to manage an extended notion of context. the middleware should allow the programmer not only to define a specific context but to influence that context definition. it should further provide a flexible and general usage pat tern for the context. our approach centers on the concept of view as an abstraction of a particular agent operating context with respect to a specific agent interest. an agent can specify multiple views, each designed to meet the agent needs for specific contextual data. the agent has full control over the specification of each view and may change it at run time. in this manner, the agent defines exactly which pieces of data belong in the set of data the view encompasses. our middle ware will provide an agent with specification mechanisms through which it can define views based on prop erties of the network topology, properties of the hosts in the network, properties of other agents in the network, and properties of individual data items owned by other agents. as an agent moves through space and time, the content of each view changes to reflect the currently available data. in this paper, we start with the premise that the operating context is all encompassing, provide a precise definition of a view as an agent defined restriction of the operating con text, and develop a formal approach for view specification. we move on to explain the dynamics of view maintenance and usage in the presence of mobility and to demonstrate its implementability and expressive power. the remainder of this paper is organized as follows. in section #, we explain our extended definition of context in a mobile setting. section # explains the view abstraction in detail and discusses what it encompasses, how it is accessed, and how it is maintained. section # discusses how an agent programs to a view. section # reviews the cnrrent status of our implementation and presents an example application. the mobility of hosts and software agents adds a new degree of complexity to this changing environment. the initial impetus for this work came from our attempt to apply other models for ad hoc mobile co ordination to vehicle to vehicle communication applications on the highway. however, the key is to allow the agent to control the scope of the individual views in a manner that facilitates easy program development without excessive performance penalties. both active badge and parctab provide a framework on top of which developers build context aware applications. formally, a view could encompass all the data that the agent can reach in the network. testing of rule based expert systems has become a high priority for many organizations as the use of such systems proliferates. traditional software teting techniques apply to some components of rule based systems, eg, the inference engine. however, to structurally test the rule base component requires new techniques or adaptations of existing ones. this paper describes one such adaptation: an extension of data flow path selection in which a graphical representation of a rule base is defined and evaluated. this graphical form, called a logical path graph, captures logical paths through a rule base. these logical paths create precisely the abstractions needed in the testing process. an algorithm for the construction of logical path graphs are analyzed. when expert systems were strictly in the domain of artificial intelligence research, proof of concept was considered more important than accuracy of operation. the philosophy was simply that if the developing expert system did not handle a particular case, another rule was added. because expert systems are no longer principally research tools, a rule base cannot be modified in an uncontrolled manner, particularly when an expert system is being used in a production application. expert system technology has been used to implement various components of many existing systems and is planned for many others. a system which captures the expertise of a human is often necessary in situations which are too dangerous for human life, or in which human experts are not available. some of this work was performed for the jet propulsion laboratory, california institute of technology, sponsored by the national aeronautics and space administration under jpl contract. authoraddress: department of systems analysis, miami university, oxford, oh. permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the acm copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the association for computing machinery. to copy otherwise, or to republish, requires a fee and or specific permission. year# acm year# acm transactions on software engmeermg and methodology, vol, no, apr, year#, pages. in the case of the space probes of the jet propulsion laboratory which partially funded this research, the hostile environs outside the earthatmosphere limit the direct involvement of humans. when the success of a deep space mission or, more importantly, the life of space travelers depends on the correct operation of expert system components, it is vital to reduce the risk associated with their use. thorough testing is essential to help assure the correct operation of such expert systems. traditional software testing techniques may be useful in testing the inference engine component of a rule based expert system. however, the structure of a rule base requires the developme nt of new testing techniques. although the use of expert systems is becoming a standard development method in software engineering, the investigation and development of techniques and tools for testing expert systems have lagged behind those of traditional information systems. this paper describes an on going investigation into testing techniques whose overall goals are to find a graphical representation of a rule base that is semantically equivalent to control flow graphs of traditional software; use this graphical form to characterize the complexity of the rule base; and use this graphical representation to determine a set of paths through the rule base that adequately test the rules and their interactions. the initial stage of the investigation is to determine the feasibility of extending one specific testing method from the domain of traditional software testing to the testing of rule based expert systems. in particular, the investigation is motivated by the intuition that a rule base analog to cyclomatic complexity and basis path testing, if such could be developed, would provide a useful method of testing rule based expert systems. it was proposed to evaluate one such analog, although this method would not be a complete answer to the need for validation and verification of rule based expert systems. however, it does provide one technique that can reduce the risk associated with their use. the remaining sections of this paper present some background and definitions about expert systems, the thesis of the investigation, an explanation of the testing method developed, an analysis of the algorithm, and an evaluation of the effectiveness of this testing method. although a software application always executes within a particular environment, current testing methods have largely ignored these environmental factors. many applications execute in an environment that contains a database. in this paper, we propose a family of test adequacy criteria that can be used to assess the quality of test suites for database driven applications. our test adequacy criteria use dataflow information that is associated with the entities in a relational database. furthermore, we develop a unique representation of a database driven application that facilitates the enumeration of database interaction associations. these associations can reflect an application definition and use of database entities at multiple levels of granularity. the usage of a tool to calculate intraprocedural database interaction associations for two case study applications indicates that our adequacy criteria can be computed with an acceptable time and space overhead. the process of testing software is di cult because it requires the description of all the interfaces in a software system and the adequate testing of these interfaces. yet, even simple software applications have complicated and everchanging operating environments that increase the number permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. of interfaces and the interface interactions that must be tested. device drivers, operating systems, and databases are all aspects of a software systemenvironment that are often ignored during testing. in recent years, a wide range of traditional software testing techniques have been proposed, implemented, and evaluated. however, relatively little research has speci cally focused on the testing and analysis of applications that interact with databases. ort cannot ensure the production of high quality software if it does not consider the operating environment of the application under test. yet, it is not su cient to simply test the program and each aspect of the programenvironment in isolation. in order to establish a high level of con dence in a software system with a complicated environment, it is necessary to develop test adequacy criteria that correctly capture a programinteraction with its environment. an adequacy criterion can be used to automatically generate test suites that are satisfactory with respect to the selected criterion. these environment aware adequacy metrics can also be used to assess the quality of tests that are manually created by software developers. finally, test adequacy criteria can serve as stopping rules to determine when the testing of an application and its environment can cease. intuitively, a database driven application is a program whose environment always contains one or more databases. given the preponderance of high quality database management systems and the number of organizations that are now collecting an unprecedented amount of data, there is a clear need for software testing techniques that test an application and its interaction with a database. in this paper, we de ne a family of data ow based test adequacy criteria for database driven applications. speci cally, we provide formal de nitions for the all database dus, all relation dus, all attribute dus, all record dus, and all attributevalue dus adequacy metrics as companions to the traditional all dus data ow adequacy criterion. tests that are adequate with respect to this family of criteria exercise all of the database interaction associations for all of the entities within a relational database. moreover, we develop a representation of a database driven application called a database interaction control ow graph in order to calculate our family of test adequacy metrics. we also present algorithms for constructing this representation of a database driven application. next, we provide an empirical analysis of the time and space overhead incurred by a tool that enumerates the database interaction associations required by our family of test adequacy criteria. id acct name user name balance card number primary checking brian zorman year# secondary checking brian zorman year# primary checking robert roos year# primary checking marcus bittman primary checking geoffrey arnold card number pin number user name acct lock brian zorman robert roos marcus bittman geoffrey arnold figure #: an instance of the relational database schema in the atm application. the important contributions of this paper are as follows: the de nition of a database driven application and the presentation of equations that describe the data ow information within this type of application. pervasive context aware software is an emerging kind of application. many of these systems register parts of their context aware logic in the middleware. on the other hand, most conventional testing techniques do not consider such kind of application logic. this paper proposes a novel family of testing criteria to measure the comprehensiveness of their test sets. it stems from context aware data flow information. firstly, it studies the evolution of contexts, which are environmental information relevant to an application program. it then proposes context aware data flow associations and testing criteria. it uses a prototype testing tool to conduct experimentation on an rfid based location sensing software running on top of context aware middleware. the experimental results show that our approach is applicable, effective, and promising. radio frequency identi cation or rfid is widely considered as an enabling technology ranging from internet payment systems to supply chain management. for example, wal mart in usa, this work is supported in part by a grant of the research grants council of hong kong and a grant of the university of hong kong. part of the work was done when chan was with the hong kong university of science and technology, clear water bay, hong kong. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. metro in europe, and hutchison ports in asia are implementing their rfid solutions. these companies were attracted to rfid because it held out the potential of offering perfect supply chain visibility the ability to know the precise location of any product anywhere in the supply chain at any time. because of the emerging importance of rfid technology, the testing of their software applications is essential. since precise location tracking of products is desirable and yet unattainable, it is dif cult to determine whether the imprecision is caused by a fault in the software, hardware, or both. moreover, although researchers are tackling the test oracle problem for the software part, little progress has been reported in the literature on software test adequacy, a criterion that de nes what constitutes an adequate test and one of the most important research problems in software testing. in this paper, we shall focus on the study of the measurement problem in software test adequacy for context aware applications. we propose a novel family of context aware data ow test adequacy measurement criteria for context aware middleware centric applications. we apply and evaluate our proposal in an rfid based location sensing program on a context aware middleware with a published estimation algorithm. the experimentation shows that our approach is very promising. pervasive computing has two core properties, namely context awareness and ad hoc communication. the contextaware property enables an application to acquire and reason about environmental attributes known as contexts. based on the contextual information, the application can react impromptu to changes in the environment. the ad hoc communication property facilitates mobile interactions among components of the application with respect to the changing contexts. for example, a polite phone for car drivers by motorola labs will transfer an ongoing call to the speaker phone when entering a car park, route calls to a voice mailbox in complex driving situations, and call a pre de ned emergence number if an airbag has been deployed. many existing proposals for pervasive computing are middleware centric. one of the major reasons is that the middleware centric multi tier architecture favors the development and con guration of pervasive computing applications in terms of the separation of concern in a highly dynamic and compositional environment, in which the middleware transparently source: what is rfid, rfid journal, available at http: wwwdjournal com article articleview year#. see, for example, the scope of sunrfid test center in dallas, texas available at http: www sun com software solutions. source: gardner, technology solutions toward improved driver focus, panel on technology and distracted driving, international conference on distracted driving, toronto, canada, october, year#. acquires, disseminates, and infers the contexts on behalf of the applications over ad hoc networks. for the purpose of brevity, we shall limit ourselves in this paper to the study of context aware middleware centric systems. we shall refer to a context aware middleware centric program simply as a cm centric program. a component of an application hence interacts with the middleware or other components of the application via a clear, loosely coupled and context aware interface residing in the middleware. although context aware middleware can handle the ad hoc communication property, the other core property the context aware property requires the collaboration of application components; otherwise, an application may not be context aware. in the rest of the paper, an application function directly invoked by a middleware will be referred to as an adaptive action. because of such a common design in context aware systems, the program logic of a cm centric program normally spans over the application tier and the middleware tier. firstly, a context aware interface at the middleware tier may invoke an adaptive action whenever the middleware detects the interesting contexts registered in the interfaces by the cm centric program. secondly, the invoked adaptive action would serve as an entry of the cm centric program at the application tier to react to the interesting contexts. thirdly, other actions of the cm centric program will utilize the results of the adaptive actions to provide tailored services to its users. since black box testing techniques do not consider the structural organization of program units in a context aware system with middleware support, they are intuitively less effective than their white box counterparts in detecting faults speci. to context aware systems designed by the above approach. we shall, therefore, restrict ourselves in this paper to white box testing techniques. the structural organization of program logic poses challenges to white box testing of cm centric programs, since it would be inadequate to consider the program structure of the components in the application tier alone. as to be explained in section #, a fault in the context aware interface may reduce the ef cacy of the contextawareness and is hard to be revealed by traditional approaches, which only consider the adaptive actions. thus, it is necessary to enhance existing test adequacy criteria. the main contributions of the paper are as follows: this paper is among the rst of its kind we are not aware of any work in the literature that takes the context aware middleware into account when formulating test adequacy criteria. new types of data ow associations are formalized to capture the context aware dependencies speci. a novel family of data ow test adequacy criteria is proposed to measure the quality of test sets. the proposed family of adequacy criteria is evaluated on the cabot platform, a pervasive context aware middleware system. our prototype testing tool automatically generates adequacy test sets according to our family of adequacy criteria. evaluation of the fault detection rates of these test sets indicates that our approach is effective and promising. the rest of the paper is organized as follows: sections and outline the technical preliminaries and testing challenges of cmcentric programs, respectively. next, section # will present our novel data ow associations followed by our testing criteria to measure the test sets comprehensiveness in section #. section # evaluates our proposal by an rfid based experimentation. this is followed by discussions, a review of related work, and the conclusion in sections, and. lime is a model and middleware supporting the development of applications that exhibit the physical mobility of hosts, logical mobility of agents, or both. lime adopts a coordination perspective inspired by work on the linda model. the context for computation, represented in linda by a globally accessible persistent tuple space, is refined in lime to transient sharing of the identically named tuple spaces carried by individual mobile units. tuple spaces are also extended with a notion of location and programs are given the ability to react to specified states. the resulting model provides a minimalist set of abstractions that facilitates the rapid and dependable development of mobile applications. in this article we illustrate the model underlying lime, provide a formal semantic characterization for the operations it makes available to the application developer, present its current design and implementation, and discuss lessons learned in developing applications that involve physical mobility. coordination is de ned as a style of computing that emphasizes a high degree of decoupling among the computing components of an application. based on these grounds, section # describes the lime middleware embodying the model concepts. section # reviews our experience with several applications developed using lime. in the arena of modern distributed computing, mobility is emerging as a disruptive new trend that challenges fundamental assumptions across the board, from theoretical foundations to software engineering practices. powerful social forces energized by advances in wireless communication, device miniaturization, and new software design techniques are creating a growing demand for applications that exploit and support the physical mobility of hosts that move through space while maintaining connections with other hosts. at the same time, logical mobility has emerged as a novel architectural style that removes the static binding between software components and hosts, and enables runtime component migration for improved exibility and performance. these two forms of mobility complement each other, meaning that logical mobility can provide the uid software fabric necessary to cope with the high dynamicity imposed by physical mobility. the rapid development of mobile applications demands both a new way of thinking and aggressive experimentation if a new set of best design practices is to emerge soon. the basic premise of this article is that coordination technology can be extended for use in mobile computing and can offer an elegant solution to a set of dif cult engineering problems. as initially proposed in linda, this can be achieved by allowing independently developed agents to share information stored in a globally accessible, persistent, content addressable data structure, typically implemented as a centralized tuple space. a small set of operations enabling the insertion, removal, and copying of tuples provides a simple and uniform interface to the tuple space. temporal decoupling is achieved by dropping the requirement that the communicating parties be present at the time the communication takes place, and spatial decoupling is achieved by eliminating the need for agents to be aware of each otheridentity in order to communicate. a clean computational model, high degree of decoupling, abstract approach to communication, and simple interface are the de ning features of coordination technology. the transition to mobility requires us to revisit the basic model with a new intellectual bias. the process entails accommodating the physical and logical distribution of tuples and the movement of hosts and agents through physical or logical spaces. lime is our response to the software engineering challenge posed by the advent of mobility. it de nes a novel coordination based approach to the development of mobile applications. when it appeared, lime was the rst coordination model and middleware to address the need to integrate concerns having to do with the physical mobility of hosts and logical mobility of agents. the lime computational model assumes a set of hosts that act as the containers in which agents are located. physical connectivity among the hosts is supported by wired or wireless links and may be altered either by mobility or explicit connection and disconnection. agents can move from one host to another reachable host of their own volition. lime preserves the essence of the linda model, its simplicity and decoupled style of computing, by continuing to channel all coordination actions through a simple interface that is perceived by each agent to be merely a local tuple space. access to the tuple space is carried out using an extended set of tuple space operations including several novel constructs designed to facilitate exible and timely responses to changes in the contents of the tuple space. each agent may own multiple tuple spaces that may be shared with other agents within communication range. sharing is made manifest by logically extending the contents of each tuple space to include the tuples present in all participating tuple spaces. the set of tuples being shared changes over time as a result of the agents local control regarding sharing, and in response to the mobility of both agents and hosts. when hosts come into communication range, the set of shared tuple spaces expands and when they move apart, it contracts. the net result is a transparently managed context that expresses itself in terms of changes in the contents of what otherwise appear to be local tuple spaces. the agent behavior is altered both by the availability of new data and by its reactive responses to contextual changes. the development process of lime entailed a close interplay between formal semantic de nition, implementation pragmatics, and application driven evaluation of the resulting model and middleware. the insistence on formalizing the model and semantics of the api was rooted in the conviction that precise semantics are key to dependable development, particularly when working in such a novel and demanding setting as mobility. implementation considerations led to weakening certain constructs, the introduction of features that the formal framework did not identify, and the enhancement of lime middleware so as to ensure its applicability in a wide range of physical and logical mobile settings. overall, the focus on application development and continuous empirical evaluation contributed to practical minded additions to the model. ultimately, the effort culminated in a java based implementation of the lime middleware, currently available as an evolving open source project. several application development exercises with programmers possessing varying skill levels reinforced our conviction that a properly tailored model of coordination, such as lime, can be an effective software engineering tool in a mobile setting. this article constitutes a complete description of lime, from the model to the middleware and applications. the semantics of the lime model are rst described informally in section #, and then formalized in section #. the application programming interface is presented together with an overview of the middleware architecture. finally, section # discusses lessons learned and related work, and section # ends the article with some brief concluding remarks. a novel system for the location of people in an office environment is described. members of staff wear badges that transmit signals providing information about their location to a centralized location service, through a network of sensors. the paper also examines alternative location techniques, system design issues and applications, particularly relating to telephone call routing. location systems raise concerns about the privacy of an individual and these issues are also addressed. efficient location and coordination of staff in any large organization is a difficult and recurring problem, hospitals, for example, may require up todate information about the location of staff and patients, particularly when medical emergencies arise. in an office building, a receptionist is usually responsible for determining the location of staff memlbers; in some organizations, public address systems are provided to help a receptionist locate employees but, more frequently, a telephone is used to contact all the possible locations at which the required person might be found. these solutions can authors addresses: want, xerox parc, coyote hill rd, palo alto, ca. email: want parc xerox tom; hopper, olivetti research ltd, a trumpington st, cambridge cb lqa, england. email: ah cam orl co uk; falc, metaphor computer systems, year# charleston rd, mountain view, ca. email: falcao met, aphor tom; gibbons, sun microsystems laboratories inc, year# garcia ave, mountain view, ca. permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the acm copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the association for computing machinery. to copy otherwise, or to republish, requires a fee andjor specific permission. year# acm year# acm transactions on information systems, vol. cause a great deal of irritation and disruption to other employees; a solution that provides direct location information is more desirable. location information for office staff that is available in a computerreadable format can also be used to improve the operation of the office telephone system. integration of telephone systems with computer systems is also important in the development of the automated office. much work has already been undertaken integrating digital voice and computer data into a single network, but there has been less commercial effort invested in improving the telephone interface. although these interfaces are functionally sophisticated, they are cryptic and their operation is difficult to remember. the features most commonly used by pbx clients are call transfer and call forward. in most cases the execution of these features could be automated by the pbx if it had information about the current location of its clients. context awareness is a key feature of pervasive computing whose environments keep evolving. the support of context awareness requires comprehensive management including detection and resolution of context inconsistency, which occurs naturally in pervasive computing. in this paper we present a framework for realizing dynamic context consistency management. the framework supports inconsistency detection based on a semantic matching and inconsistency triggering model, and inconsistency resolution with proactive actions to context sources. we further present an implementation based on the cabot middleware. the feasibility of the framework and its performance are evaluated through a case study and a simulated experiment, respectively. this gives rise to the problem of context inconsistency, which must be timely detected in order to prevent applications from behaving anomalously. based on this model, we further propose an efficient checking algorithm to detect inconsistent contexts. applications in pervasive computing are typically required to interact seamlessly with their changing environments. to provide users with smart computational services, these applications must be aware of incessant context changes in their environments and adjust their behaviors accordingly. as these environments are highly dynamic and noisy, context changes thus acquired could be obsolete, corrupted or inaccurate. in this paper, we propose a formal model of incremental consistency checking for pervasive contexts. the performance of the algorithm and its advantages over conventional checking techniques are evaluated experimentally using cabot middleware. to enable an application to behave smartly, valid contexts should be available. for example, it would be embarrassing if the smart phone roared during the most important moment of a wedding ceremony if the context mistakenly dictates that the environment is a football match. this poses a natural requirement on consistent contexts. for example, in a highly dynamic environment, contexts are easily obsolete; context reasoning may conclude inaccurate results if its relying contexts have partially become obsolete during the reasoning computation. as a result, we need to deal with inconsistent contexts in pervasive computing. to improve the consistency of the computing environment for its embedding context aware applications, it is desirable to timely identify inconsistent contexts and prevent applications from using them. call forwarding assumes phone receptionists knowing the callees current locations. any violation of such constraints indicates the presence of inconsistent location contexts, which must be detected in time to prevent the application from taking inappropriate action. in this example, the goal of acquiring precise location contexts becomes the basis to reject inconsistent location contexts that violate the pre specified consistency constraints. this research is supported by a grant from the research grants council of hong kong. cheung at the department of computer science, hong kong university of science and technology, clear water bay, hong kong. the checking of consistency constraints, or consistency management, has been recognized as an important issue by software engineering and programming communities. generally, consistency constraints are expressed as rules and checked over interesting content such that any violation of rules, called inconsistency, can be detected. the inconsistency is presented to users or checking systems for further actions such as carrying out repairs. various studies have been conducted on consistency checking. their research attention is paid mainly to interesting content changes and checking is performed eagerly or lazily to identify if any change has violated predefined rules. we take the xlinkit framework from them for discussion. in xlinkit, consistency constraints are expressed as rules in first order logic formulae. for every rule, their approach employs xpath selectors to retrieve interesting content from xml documents. the approach is intuitive in the sense that each time the entire set of xml documents is checked against all rules. we term this type of checking all document checking. however, all document checking is computationally expensive when applied to large datasets that require frequent checking. xlinkitimproved model can identify the rules whose targeted content is potentially affected by a given change. only these affected rules need to be rechecked against the entire dataset upon the change. since it is a rule level checking approach, we term it rule based incremental checking. rule based incremental checking is attractive when there are many rules but each change in the content only affects a few of them. first, the entire formula of an affected rule should be rechecked even though a small sub formula is affected by the changes. in practice, a rule typically contains universal or existential sub formulae in which variable assignments are subject to changes. if the granularity of checking can be confined to sub formulae, it saves many rechecking efforts. second, each affected rule has to be rechecked against the entire dataset or documents even for small changes. this is because the last retrieved content from xpath selectors was not stored, and thus could not be reused in the subsequent checking. the above two limitations present a major challenge when we apply the rule based incremental checking approach to dynamic pervasive computing environments, in which applications require timely responses to context changes and typically context changes rapidly but each change is usually marginal. we envisage using a finer checking granularity and the consistency checking results of previous rounds to achieve efficient consistency checking for pervasive computing. in this paper, we address the two challenges by presenting a novel consistency checking technique, called formula based incremental checking. sections and introduce preliminary concepts on context modeling and consistency checking, respectively. section # discusses three closely related issues: incremental boolean value evaluation, incremental link generation and stateful context patterns. pervasive computing applications are often context aware, using various kinds of contexts such as user posture, location and time to adapt to their environments. for example, a smart phone would vibrate rather than beep seamlessly in a concert hall, but would beep loudly during a football match. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. a context of a computation task in pervasive computing refers to an attribute of the situation in which the task takes place. essentially, a context is a kind of data. it differs from data in conventional databases in the sense that data in a conventional database are integral and consistent during a database transaction. on the other hand, it is difficult for contexts to be precise, integral and non redundant. a detailed analysis of the reasons on inevitably imperfect contexts for pervasive computing can be found in. still, every application may cumber to handle similar consistency issues individually. appropriate common abstraction layers, such as context middleware or frameworks, to provide context querying or subscription services of a high degree of consistency are attractive. one promising approach is to check consistency constraints that meet application requirements at runtime to avoid inconsistent contexts being disseminated to applications. we give the following example: call forwarding is a location aware application, which aims at forwarding incoming calls to the target callees with phones nearby. generally, raw sensory data are collected by the underlying active badge system which is built on the infrared technology, and these raw data are converted into location contexts by certain algorithms. to warrant accurate location contexts, consistency constraints have to be specified and checked. xlinkit is a well recognized tool for consistency checking of distributed xml documents. our proposal addresses the following two challenges: can the checking granularity be reduced from rules to sub formulae that is, for a given rule that needs rechecking, we only check those sub formulae directly affected by context changes and use the last checking results of other sub formulae to get the updated final result of the rule. can stateless xpath selectors be replaced with some stateful context retrieval mechanism we maintain the latest retrieved contexts and update them at the time when any interesting context change occurs. thus, we do not have to re parse the whole context history each time. our experiments report that our technique can achieve more than a five fold performance improvement. although various studies such as have addressed the second challenge, the first has not yet been examined. in particular, our contributions include: the use of context patterns combined with fol formulae in expressing consistency constraints on contexts; the formulation of the boolean value semantics and link gen eration semantics for incremental consistency checking; and the proposal of an efficient and incremental algorithm to detect context inconsistency for pervasive computing. the remainder of the paper is organized as follows. they are followed by the algorithm implementation in section # and a group of comparison experiments in section #. objective measurement of test quality is one of the key issues in software testing. it has been a major research focus for the last two decades. many test criteria have been proposed and studied for this purpose. various kinds of rationales have been presented in support of one criterion or another. we survey the research work in this area. the notion of adequacy criteria is examined together with its role in software dynamic testing. a review of criteria classification is followed by a summary of the methods for comparison and assessment of criteria. goodenough and gerhart made an early breakthrough in research in year#, dijkstra claimed that program on software testing by pointing out that testing can be used to show the presence the central question of software testing of bugs, but never their absence to peris what is a test criterion, that is, the suade us that a testing approach is not criterion that defines what constitutes acceptable. since then, test critelast two decades have seen rapid growth ria have been a major research focus. a of research in software testing as well as great number of such criteria have been intensive practice and experiments. consider has been developed into a validation and verification technique indispensable to able research effort has attempted to software engineering discipline. then, provide support for the use of one crite where are we today what can we claim rion or another. how should we underabout software testing stand these different criteria what are in the mid, in an examination of the future directions for the subject the capability of testing for demonstrat in contrast to the constant attention ing the absence of errors in a program, given to test adequacy criteria by aca authors addresses: zhu, institute of computer software, nanjing university, nanjing, year#,r. may, department of computing, the open university, walton hall, milton keynes, mk aa, uk. permission to make digital hard copy of part or all of this work for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication, and its date appear, and notice is given that copying is by permission of the acm, inc. to copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and or a fee. year# acm year# demics, the software industry has been slow to accept test adequacy measurement. few software development standards require or even recommend the use of test adequacy criteria. are test adequacy criteria worth the cost for practical use addressing these questions, we survey research on software test criteria in the past two decades and attempt to put it into a uniform framework. the notion of test adequacy let us start with some examples. here we seek to illustrate the basic notions underlying adequacy criteria. in software testing practice, testers are often required to generate test cases to execute every statement in the program at least once. a test case is an input on which the program under test is executed during testing. a test set is a set of test cases for testing a program. the requirement of executing all the statements in the program under test is an adequacy criterion. a test set that satisfies this requirement is considered to be adequate according to the statement coverage criterion. sometimes the percentage of executed statements is calculated to indicate how adequately the testing has been performed. the percentage of the statements exercised by testing is a measurement of the adequacy. similarly, the branch coverage criterion requires that all control transfers in the program under test are exercised during testing. the percentage of the control transfers executed during testing is a measurement of test adequacy. the path coverage criterion requires that all the execution paths from the programentry to its exit are executed during testing. software testing is often aimed at detecting faults in software. a way to measure how well this objective has been achieved is to plant some artificial faults into the program and check if they are detected by the test. a program with a planted fault is called a mutant of the original program. if a mutant and the original program produce different outputs on at least one test case, the fault is detected. in this case, we say that the mutant is dead or killed by the test set. the percentage of dead mutants compared to the mutants that are not equivalent to the original program is an adequacy measurement, called the mutation score or mutation adequacy. from goodenough and gerhart point of view, a software test adequacy criterion is a predicate that defines what properties of a program must be exercised to constitute a thorough test, ie, one whose successful execution implies no errors in a tested program. to guarantee the correctness of adequately tested programs, they proposed reliability and validity requirements of test criteria. reliability requires that a test criterion always produce consistent test results; that is, if the program tested successfully on one test set that satisfies the criterion, then the program also tested successfully on all test sets that satisfies the criterion. validity requires that the test always produce a meaningful result; that is, for every error in a program, there exists a test set that satisfies the criterion and is capable of revealing the error. but it was soon recognized that there is no computable criterion that satisfies the two requirements, and hence they are not practically applicable. moreover, these two requirements are not independent since a criterion is either reliable or valid for any given software. since then, the focus of research seems to have shifted from seeking theoretically ideal criteria to the search for practically applicable approximations. currently, the software testing literature contains two different, but closely related, notions associated with the term test data adequacy criteria. first, an adequacy criterion is considered to be a stopping rule that determines whether sufficient testing has been done that it can be stopped. for instance, when using the statement coverage criterion, we can stop testing if all the statements of the program have been executed. generally speaking, since software testing involves the program under test, the set of test cases, and the specification of the software, an adequacy criterion can be formalized as a functionthat takes a program, a specification, and a test setand gives a truth value true or false. formally, letbe a set of programs, be a set of specifications, be the set of inputs of the programs in, be the class of test sets, that is, where denotes the set of subsets of. a test data adequacy criterionis a function: xx. true means thatis adequate for testing programagainst specificationaccording to the criterion, otherwiseis inadequate. second, test data adequacy criteria provide measurements of test quality when a degree of adequacy is associated with each test set so that it is not simply classified as good or bad. in practice, the percentage of code coverage is often used as an adequacy measurement. thus, an adequacy criterioncan be formally defined to be a functionfrom a program, a specification, and a test setto a real numberc, the degree of adequacy. a test data adequacy criterion is a function, xx. means that the adequacy of testing the programby the test setwith respect to the specificationis of degreeaccording to the criterion. the greater the real number, the more adequate the testing. these two notions of test data adequacy criteria are closely related to one another. a stopping rule is a special case of measurement on the continuum since the actual range of measurement results is the set, where means false and means true. on the other hand, given an adequacy measurementand a degreeof adequacy, one can always construct a stopping rule mr such that a test set is adequate if and only if the adequacy degree is greater than or equal to; that is, mr truem. since a stopping rule asserts a test set to be either adequate or inadequate, it is also called a predicate rule in the literature. an adequacy criterion is an essential part of any testing method. first, an adequacy criterion specifies a particular software testing requirement, and hence determines test cases to satisfy the requirement. it can be defined in one of the following forms. it can be an explicit specification for test case selection, such as a set of guidelines for the selection of test cases. following such rules one can produce a set of test cases, although there may be some form of random selections. such a rule is usually referred to as a test case selection criterion. using a test case selection criterion, a testing method may be defined constructively in the form of an algorithm which generates a test set from the software under test and its own specification. it should be noticed that for a given test case selection criterion, there may exist a number of test case generation algorithms. such an algorithm may also involve random sampling among many adequate test sets. it can also be in the form of specifying how to decide whether a given test set is adequate or specifying how to measure the adequacy of a test set. a rule that determines whether a test set is adequate is usually referred to as a test data adequacy criterion. however, the fundamental concept underlying both test case selection criteria and test data adequacy criteria is the same, that is, the notion of test adequacy. in many cases they can be easily transformed from one form to another. mathematically speaking, test case selection criteria are generators, that is, functions that produce a class of test sets from the program under test and the specification. any test set in this class is adequate, so that we can use any of them equally test data adequacy criteria are acceptors that are functions from the program under test, the specification of the software and the test set to a characteristic number as defined in definition. generators and acceptors are mathematically equivalent in the sense of one one correspondence. hence, we use test adequacy criteria to denote both of them. a test data adequacy criterionis a function: x a test sete means thatsatisfieswith respect toand, and it is said thatis adequate for according to. the second role that an adequacy criterion plays is to determine the observations that should be made during the testing process. for example, statement coverage requires that the tester, or the testing system, observe whether each statement is executed during the pro test data selection criteria as generators should not be confused with test case generation software tools, which may only generate one test set. if path coverage is used, then the observation of whether statements have been executed is insufficient; execution paths should be observed and recorded. however, if mutation score is used, it is unnecessary to observe whether a statement is executed during testing. instead, the output of the original program and the output of the mutants need to be recorded and compared. although, given an adequacy criterion, different methods could be developed to generate test sets automatically or to select test cases systematically and efficiently, the main features of a testing method are largely determined by the adequacy criterion. for example, as we show later, the adequacy criterion is related to fault detecting ability, the dependability of the program that passes a successful test and the number of test cases required. unfortunately, the exact relationship between a particular adequacy criterion and the correctness or reliability of the software that passes the test remains unclear. due to the central role that adequacy criteria play in software testing, software testing methods are often compared in terms of the underlying adequacy criteria. therefore, subsequently, we use the name of an adequacy criterion as a synonym of the corresponding testing method when there is no possibility of confusion. the uses of test adequacy criteria an important issue in the management of software testing is to ensure that before any testing the objectives of that testing are known and agreed and that the objectives are set in terms that can be measured. such objectives should be quantified, reasonable and achievable. almost all test adequacy criteria proposed in the literature explicitly specify particular requirements on software testing. they are objective rules applicable by project managers for this purpose. for example, branch coverage is a test requirement that all branches of the program should be exercised. the objective of testing is to satisfy this requirement. the degree to which this objective is achieved can be measured quantitatively by the percentage of branches exercised. the mutation adequacy criterion specifies the testing requirement that a test set should be able to rule out a particular set of software faults, that is, those represented by mutants. mutation score is another kind of quantitative measurement of test quality. test data adequacy criteria are also very helpful tools for software testers. there are two levels of software testing processes. at the lower level, testing is a process where a program is tested by feeding more and more test cases to it. here, a test adequacy criterion can be used as a stopping rule to decide when this process can stop. once the measurement of test adequacy indicates that the test objectives have been achieved, then no further test case is needed. otherwise, when the measurement of test adequacy shows that a test has not achieved the objectives, more tests must be made. in this case, the adequacy criterion also provides a guideline for the selection of the additional test cases. in this way, adequacy criteria help testers to manage the software testing process so that software quality is ensured by performing sufficient tests. at the same time, the cost of testing is controlled by avoiding redundant and unnecessary tests. this role of adequacy criteria has been considered by some computer scientists to be one of the most important. at a higher level, the testing procedure can be considered as repeated cycles of testing, debugging, modifying program code, and then testing again. ideally, this process should stop only when the software has met the required reliability requirements. although test data adequacy criteria do not play the role of stopping rules at this level, they make an important contribution to the assessment of software dependability. generally speaking, there are two basic aspects of software dependability assessment. one is the dependability estimation itself, such as a reliability figure. the other is the confidence in estimation, such as the confidence or the accuracy of the reliability estimate. the role of test adequacy here is a contributory factor in building confidence in the integrity estimate. recent research has shown some positive results with respect to this role. although it is common in current software testing practice that the test processes at both the higher and lower levels stop when money or time runs out, there is a tendency towards the use of systematic testing methods with the application of test adequacy criteria. categories of test data adequacy criteria there are various ways to classify adequacy criteria. one of the most common is by the source of information used to specify testing requirements and in the measurement of test adequacy. hence, an adequacy criterion can be: specification based, which specifies the required testing in terms of identified features of the specification or the requirements of the software, so that a test set is adequate if all the identified features have been fully exercised. in software testing literature it is fairly common that no distinction is made between specification and requirements. this tradition is followed in this article also; program based, which specifies testing requirements in terms of the program under test and decides if a test set is adequate according to whether the program has been thoroughly exercised. it should not be forgotten that for both specification based and program based testing, the correctness of program outputs must be checked against the specification or the requirements. however, in both cases, the measurement of test adequacy does not depend on the results of this checking. also, the definition of specification based criteria given previously does not presume the existence of a formal specification. it has been widely acknowledged that software testing should use information from both specification and program. combining these two approaches, we have: combined specification and programbased criteria, which use the ideas of both program based and specificationbased criteria. there are also test adequacy criteria that specify testing requirements without employing any internal information from the specification or the program. for example, test adequacy can be measured according to the prospective usage of the software by considering whether the test cases cover the data that are most likely to be frequently used as input in the operation of the software. although few criteria are explicitly proposed in such a way, selecting test cases according to the usage of the software is the idea underlying random testing, or statistical testing in random testing, test cases are sampled at random according to a probability distribution over the input space. such a distribution can be the one representing the operation of the software, and the random testing is called representative. it can also be any probability distribution, such as a uniform distribution, and the random testing is called nonrepresentative. generally speaking, if a criterion employs only the interface information the type and valid range for the software input it can be called an interface based criterion: interface based criteria, which specify testing requirements only in terms of the type and range of software input without reference to any internal features of the specification or the program. in the software testing literature, people often talk about white box testing and black box testing. black box testing treats the program under test as a black box. in whitebox testing, the tester has access to the details of the program under test and performs the testing according to such details. therefore, specification based criteria and interface based criteria belong to black box testing. programbased criteria and combined specification and program based criteria belong to white box testing. another classification of test adequacy criteria is by the underlying testing approach. there are three basic approaches to software testing: structural testing: specifies testing requirements in terms of the coverage of a particular set of elements in the structure of the program or the specification; fault based testing: focuses on detecting faults in the software. an adequacy criterion of this approach is some measurement of the fault detecting ability of test sets error based testing: requires test cases to check the program on certain error prone points according to our knowledge about how programs typically depart from their specifications. the source of information used in the adequacy measurement and the underlying approach to testing can be considered as two dimensions of the space of software test adequacy criteria. a software test adequacy criterion can be classified by these two aspects. the review of adequacy criteria is organized according to the structure of this space. we use the word fault to denote defects in software and the word error to denote defects in the outputs produced by a program. an execution that produces an error is called a failure. organization of the article the remainder of the article consists of two main parts. the first part surveys various types of test data adequacy criteria proposed in the literature. it includes three sections devoted to structural testing, fault based testing, and error based testing. each section consists of several subsections covering the principles of the testing method and their application to program based and specification based test criteria. the second part is devoted to the rationale presented in the literature in support of the various criteria. section # discusses the methods of comparing adequacy criteria and surveys the research results in the literature. section # discusses the axiomatic study and assessment of adequacy criteria.