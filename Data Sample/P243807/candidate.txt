we study the problem of semantic concept based query expansion and re ranking for multimedia retrieval. in this paper, we propose several new approaches for query expansion, in which textual keywords, visual examples, or initial retrieval results are analyzed to identify the most relevant visual concepts for the given query. these concepts are then used to generate additional query results and or to re rank an existing set of results. we develop both lexical and statistical approaches for text query expansion, as well as content based approaches for visual query expansion. in addition, we study several other recently proposed methods for concept based query expansion. in particular, we explore the utility of a fixed lexicon of visual semantic concepts for automatic multimedia retrieval and re ranking purposes. in total, we compare different approaches for expanding queries with visual semantic concepts. they are evaluated using a large video corpus and concept detectors from the trecvid year# video retrieval benchmark. we observe consistent improvement over the baselines for all approaches, leading to an overall performance gain of relative to a text retrieval baseline, and a improvement relative to a state of the art multimodal retrieval baseline. while search and retrieval in the text domain are fairly well understood problems and have a wide range of. most large scale multimedia search systems typically rely on text based search over media metadata such as surrounding html text, anchor text, titles and abstracts. onthe other extreme, therehasbeena substantialbodyof work in the research community on content based retrieval methods by leveraging the query by example and relevance feedback paradigms. these methods do not require any additional metadata, but rely on users to express their queries in terms of query examples with low level features such as colors, textures and shapes. finding appropriate examples, however, is not easy, and it is still quite challenging to capture the user intent with just a few examples. once pre trained, these detectors could then be used to tag and index multimedia content semantically in a fully automated fashion. work in this eld related to video concept detection and retrieval has been driven primarily by the trec video retrieval evaluation community, which provides a common testbed for evaluating approachesby standardizing datasets, benchmarked concepts and queries. orts in creating large training corpora include the collaborative annotation. in this paper, we study the problem of semantic conceptbased query expansion and re ranking for multimedia retrieval purposes. xed lexicon of semantic concepts with corresponding visual detectors, and we explore their utility for automatic multimedia retrieval and re ranking purposes. we propose several new approaches for query expansion, in which the query textual terms, query visual examples, or the query baseline results are analyzed to identify relevant visual concepts, along with corresponding weights. the proposed approach uses deep parsing and semantic tagging of queries based on question answering technology. it outperformstwopopular lexical approaches based on synonym expansion and wordnet similarity measures. we also propose a novel statistical corpus analysis approach, which identi es signi cant correlations between words in the english language and concepts from the visual concept vocabulary based on their co occurrence frequency in the video corpus. this approachperforms on par with, orbetter than, lexical query expansion approaches but has the advantage of not requiring any dictionaries, or manually constructed concept descriptions, and is therefore more scalable. finally, we study and evaluate several other previously proposed methods for concept based query expansion and retrieval. in total, we compare and empirically evaluate di erent approaches for expanding ad hoc user queries with relevant visual concepts. to the best of our knowledge, this is one of the most comprehensive reviews and evaluations of concept based retrieval and re ranking methods so far, and it clearly establishes thevalueof semantic concept detectors for answering and expanding ad hoc user queries. search and retrieval are vital parts of multimedia content management, and are increasingly receiving attention with the growing use of multimedia libraries and the explosion of digital media on the web. by its virtue, multimedia spans multiple modalities, including audio, video, and text. ective solutions, other modalities have notbeen explored to the same degree. this approach, however, fails when there is no such metadata, when the rich link structure of the web cannot be exploited, or when the metadata cannot precisely capture the true multimedia content. to address these issues, a new promising direction has emerged in recent years, namely, using machine learning techniques to explicitly model the audio, video, and image semantics. the basic idea is that statistical detectors can be learned to recognize semantic entities of interest such aspeople, scenes, events, and objects by using more training examples than a user would typically provide in an interactive session. a major open problem is how to scale this approach to thousands of reliable concept detectors, each of which may require thousands of training examples in turn. orts undertaken by trecvid participants, donated annotations by the mediamill team from the university of amsterdam, as well as the large scale concept ontology for modeling. ort to de ne and annotate on the order of year# concepts from the broadcast news domain. at present, however, reliable concept detectors are still limited to tens of concepts only, while usable concept detectors exist for a few hundred concepts at best. it is therefore imperative to develop techniques that maximally leverage the limited number of available concept detectors in order to enable or improve video search when the metadata is limited or completely absent. the most salient concepts are then used to generate additional query results or to re rank an existing set of results. we proposea novel lexical query expansion approach leveraging a manually constructed rulebased mapping between a lexicon of semantic text annotations and a lexicon of semantic visual concepts. we also study smart data sampling and content modeling techniques for concept based expansion based on visual query examples. we observe consistent improvement over the baselines for all approaches, leading to an overall performance gain of relative to a text retrieval baseline, and a improvement relative to a multimodal retrieval baseline. because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. this information is traditionally provided through dictionaries, and machine readable dictionaries are now widely available. but dictionary entries evolved for the convenience of human readers, not for machines. wordnet provides a more effective combination of traditional lexicographic information and modern computing. wordnet is an online lexical database designed for use under program control. english nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. current approaches to identifying definitional sentences in the context of question answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets. this is insufficient as they do not address the novelty factor that a definitional nugget must also possess. this paper proposes to address the deficiency by building a human interest model from external knowledge. it is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic. we compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering. one of the major problems in question answering is that the queries are either too brief or often do not contain most relevant terms in the target corpus. in order to overcome this problem, our earlier work integrates external knowledge extracted from the web and wordnet to perform event based qa on the trec task. this paper extends our approach to perform event based qa by uncovering the structure within the external knowledge. the knowledge structure loosely models different facets of qa events, and is used in conjunction with successive constraint relaxation algorithm to achieve effective qa. our results obtained on trec qa corpus indicate that the new approach is more effective and able to attain a confidence weighted score of above. most users are interested in searching for information, while the current search engines are designed to retrieve only documents. there are many simple factoid questions like: who is tom cruise married to or how many chromosomes does a human zygote have while the users expect short precise answers to such questions, typical search engines would return thousands of documents where the actual answer is embedded in some of the documents. this has resulted in a gulf between the expectation of the users and that retrievable by the systems. this gulf will become more severe as we are increasingly being overwhelmed by information overloads. to address this problem, a question answering task was initiated in trec conference series. this has in turn motivated much of the recent research on open domain qa focusing on short, factoid questions. most modern qa systems combine the strengths of traditional information retrieval, permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. natural language processing and information extraction to provide an appropriate way to retrieve concise answers for open domain natural language questions against a large text collection. in the most recent trec conference, the main task consists of short factoid questions posed over more than one million news articles. instead of previous years byte or byte text fragments, exact answers are expected from the qa corpus with supports of documentary evidences. although the traditional bag of words representation has been found to be effective for ir research in retrieving large number of relevant documents, it is ineffective for qa where precise answers are needed. many qa research groups employ a variety of linguistic resources, such as the part of speech tagging, syntactic parsing, pattern matching, semantic relations, named entity extraction, dictionaries, wordnet, etc. in the trec evaluation, moldovan et al successfully illustrated the power of extensive wordnet to perform logic proof relied on knowledge reasoning. in contrast to the linguistic based approaches, the use of the web for qa is an emerging topic of interest among the computational linguistic communities. several studies suggested that using the web as additional knowledge resource could improve the performance of a qa system by. clarke et al and brill et al used the web to introduce data redundancy for more reliable qa. lin gave a detailed comparison of employing the web and wordnet and concluded that combining both approaches could lead to better performance on answering definition questions. in trec, we explored the use of external resources like the web and wordnet to extract terms that are highly correlated with the query, and use them to perform linear query expansion. while the technique has been found to be effective, we found that there is a need to perform structured analysis on the knowledge obtained from the web wordnet in order to further improve the retrieval performance. in this work, we model the trec style qa task as qa entities or events. each qa event comprises of elements describing different facets of the event like time, location, object, action etc. for most qa events, there are inherent associations among their elements. thus if we know a portion of the elements, we can use this information to narrow the search for the rest of unknown event elements, which are likely to contain the answers. in order to incorporate more knowledge of event elements from the external resources and to use event structure systematically for more effective qa, we analyze the external knowledge to discover the event structure. this paper investigates the integration and structured use of both linguistic and web knowledge for the trec style qa, which we call event based qa. in particular, we describe a high performance question answering system called qualifier and analyze its effectiveness using the trec benchmark. our results show that the event based approach, together with the use of successive constraint relaxation, gives rise to an effective question answering system. the rest of the paper is organized as follows. section # presents the idea of event based question answering. section # discusses the design and architecture of the system. section # elaborates on both the linear and structured use of external resources for qa. sections and investigate the use of successive constraint relaxation and answer extraction technique. section # details the experimental results and section # concludes the paper. we explore probabilistic lexico syntactic pattern matching, also known as soft pattern matching, in a definitional question answering system. most current systems use regular expression based hard matching patterns to identify definition sentences. such rigid surface matching often fares poorly when faced with language variations. we propose two soft matching models to address this problem: one based on bigrams and the other on the profile hidden markov model. both models provide a theoretically sound method to model pattern matching as a probabilistic process that generates token sequences. we demonstrate the effectiveness of the models on definition sentence retrieval for definitional question answering. we show that both models significantly outperform the state of the art manually constructed hard matching patterns on recent trec data. a critical difference between the two models is that the phmm has a more complex topology. we experimentally show that the phmm can handle language variations more effectively but requires more training data to converge. while we evaluate soft pattern models only on definitional question answering, we believe that both models are generic and can be extended to other areas where lexico syntactic pattern matching can be applied. de nition questions such as questions like what is tb or who is aaron copland account for a signi cant number of queries submitted to web search engines. the most straightforward way to answer such questions is to look up the appropriate de nition in a dictionary or an encyclopedia. many current search engines, such as google and yahoo, adopt such an approach to de ning terms, relying on existing online de nitional resources. however, the publicinterest is often focused on recent development of events and people. new terms, organizations, and personalities, such as enron, clay aiken, and sars, which are of great interest to the public, are often rst described in news. descriptions for these newly minted terms of interest are not found in authoritative sources such as dictionaries or encyclopedias, but their descriptions can often be found in breaking news web sites. moreover, de nitions of people and organizations often change over time; thus, even with a suitably large pool of human editors, it is impossible for manually edited resources to keep all de nitions up to date. therefore, automatic de nitional question answering systems have been developed. they complement the use of such manually edited de nitions as they accumulate and produce de nitions for these types of terms that are not covered by standard authoritative resources. in response to such questions, a typical de nitional qa system extracts de nition sentences that contain descriptive information about the search term from multiple documents and summarizes these sentences into de nitions. traditional ir systems are only part of the solution: given a topic, search engines can only retrieve relevant documents, but do not distill these documents down to a single, coherent de nition. the synthesis of a complete de nition of such new terms requires the identi cation and collation of de nition sentences across multiple relevant articles. the construction of complete and uent de nitions for terms incorporates many technologies. in this article, we focus on a core component of this task: namely, the identi cation of de nition sentences from relevant documents for a speci ed search target. a de nition sentence contains descriptive information that can be included in an extended de nition of the term. such an extended de nition answers not only what who isbut also what who islike, that is, the extended de nition aggregates all relevant information about the target. de nitional qa systems are based on either sentence retrieval or linguistic construct extraction. in this study, we focus on sentence retrieval for answering de nition questions due to two reasons: rst, the diversity of de nition sentences makes de nition sentence retrieval a dif cult problem. sentences are longer units than linguistic constructs and thus provide better coverage of relevant information and context which aids comprehension. generally, recall is considered more important than precision for a corpus based de nitional qa system. second, we believe that a qa system based on sentence retrieval and summarization provides an adaptable testbed for experimenting with various acm transactions on information systems, vol. soft pattern matching models for de nitional qa techniques. other modules, such as the one to nd ne grained de nition units, can be easily integrated into this system. de nition questions are about event and nonevent targets. de nitions of events such as the return of hong kong to chinese sovereignty require information to be extracted and ordered chronologically. suitable techniques have been developed in the event based summarization community, and thus we deal with de nitions of nonevent targets, such as people, organizations, and other terms. most approaches applicable to de nition sentence retrieval use pattern matching. many systems create patterns manually: harabagiu et al employed manually constructed de nition patterns for plain text news articles; liu et al mined topic speci. de nitions using hand crafted rules from web pages. however, we nd that the manually constructed lexico syntactic rules are too rigid to cover the notion of extended de nitions, as such sentences exhibit diversity in patterns, and rigid rules often fail to match de nition sentences due to inserted or deleted tokens. such mismatches are common in natural language texts because authors often use diverse expressions to convey the same meaning. a similar problem occurs in other applications that make use of lexico syntactic pattern matching, such as information extraction. one promising technique to address this is soft pattern matching. in our previous work, we have shown that soft patterns are able to signi cantly outperform hard patterns in extracting de nition sentences as they model language variations probabilistically. different from regular expression based hard matching patterns employed in existing systems, we treat de nition patterns as sequences of lexical and syntactic tokens. therefore, pattern matching can be considered as probabilistic generation of test sequences based on training sequences. in this article, we develop a de nitional qa system around soft pattern matching and formalize two soft pattern matching models. the rst model is derived from the bigram language model with linear interpolation of unigram and bigram probabilities. the second model is based on the pro le hidden markov model. while the language model and the phmm have been studied in other areas, their use in modeling lexico syntactic pattern matching is a novel contribution of our work. note that we constrain our focus on soft matching models for lexico syntactic patterns on plain text. while the corpus we use for evaluation was constructed from online news, all html tags and hyperlinking information for the documents have been removed. tackling patterns in other structures, such as parsing trees and nonlinear structures of web pages involving hyperlinks, is beyond the scope of this study. in summary, the main contributions of this article are the following: we propose two soft pattern models that signi cantly improve the performance of the baseline de nitional qa system that uses hard patterns in scores on both trec and test sets. such improvement indicates that de nition sentences exhibit diversi ed language patterns that are not captured well by hard matching of manually constructed rules. while we only demonstrate soft pattern models on de nitional qa, both soft pattern models can be extended to other applications that employ lexico syntactic pattern matching. we validate the hypothesis drawn in our previous study that, given more training data, the phmm outperforms the simple bigram model. in this study, we complete the veri cation of the hypothesis experimentally by employing more labeled de nition sentences from previous years trec data for training the soft pattern models. we further analyze the results produced by the phmm and nd that only a small proportion of de nition sentences retrieved by the phmm but missed by the bigram model actually bene. we conjecture that noisy data sets will bene. we rst review the background of de nition extraction in section #. in section #, we present the architecture of our de nitional qa systems and discuss the speci. in section #, we discuss our core work on the two formal soft matching models. in section #, we then show empirical evidence that bear out the effectiveness of the soft pattern approach on trec datasets and discuss the results and limitations of our work. in section #, we present our conclusions and examine future directions for soft pattern matching. in the appendix, we compare and summarize the techniques employed by the current de nitional qa systems in table vii. in addition, we list the commonly used regular expression based rules for de nition extraction in table viii. in this paper, we introduce a multifaceted approach for question answering on lecture videos text extracted from powerpoint slides associated with the lecture videos is used as a source of domain knowledge to boost the answer extraction performance on these domain specific videos. the three steps of this approach are described and the evaluation plan is discussed. nowadays videotaped lectures are commonly provided inlearning systems. however, finding specific information in a library of lecture videos is difficult since a lecture video can easily get longer than an hour. video based question answering technology, which allows users to ask questions in everyday english and retrieves precise answers located in the video stream, is a promising solution for this problem. however, although textbased qa has been studied for a long time, research on videobased qa is just initiated, with a focus on news video collection. the algorithms used in these qa systems, such as video genre analysis, cannot be directly applied to the lecture video collection because the latter is quite different from news video. for example, lecture videos usually have very few scene changes permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. speeches in lecture videos are typically unscripted and spontaneous; and more importantly, speeches in lecture videos normally have many domain specific vocabularies which cannot be found from a general knowledge source such as wordnet. in this paper, we introduce an approach specially designed for video based qa on lecture videos. this approach combines multiple methods including phonetic based transcript error correction, templatebased question processing, and answer extraction with the help of external domain knowledge. while traditional question answering systems tailored to the trec qa task work relatively well for simple questions, they do not suffice to answer real world questions. the community based qa systems offer this service well, as they contain large archives of such questions where manually crafted answers are directly available. however, finding similar questions in the qa archive is not trivial. in this paper, we propose a new retrieval framework based on syntactic tree structure to tackle the similar question matching problem. we build a ground truth set from yahoo answers, and experimental results show that our method outperforms traditional bag of word or tree kernel based methods by in mean average precision. it further achieves up to improvement by incorporating semantic features as well as matching of potential answers. our model does not rely on training, and it is demonstrated to be robust against grammatical errors as well. traditional trec qa task has made significant progress since it was first introduced in year#s. however, research on trec qa has largely targeted on short, factoid based, questions, for which concise answers are expected. for example, trec qa simply expects the year year# for the simple question in what permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. year did sir edmund hillary search for yeti. it was earlier claimed that while qa systems tailored to the trec qa task worked relatively well for factoid type questions, they might not be necessarily effective in question answering applications outside trec. in real world, more complex questions are usually asked, and users are more willing to obtain a longer and more comprehensive answer which contains sufficient context information. traditional qa systems are now facing problems of being deployed into real world. with the blooming of web, social collaborative applications such as wikipedia, youtube, facebook etc. begin to flourish, and there have been an increasing number of web information services that bring together a network of self declared experts to answer questions posted by other people. this is referred to as the community based question answering services. in these communities, anyone can ask and answer questions on any topic, and people seeking information are connected to those who know the answer. as answers are usually explicitly provided by human and are of high quality, they can be helpful in answering real world questions. yahoo answers, launched on december, year#, is now becoming the largest knowledge sharing online community among several popular cqa services. over times, a tremendous number of previous qa pairs have been stored in its database, and in most circumstances, users may directly get the answers from yahoo answers by searching from this qa archive, rather than looking through a list of potentially relevant documents from the web. as such, instead of extracting answers from a certain document corpus, the retrieval task in cqa becomes the task of finding relevant similar questions with new queries. the similar question matching task is, however, not trivial. one of the major reasons is that instead of inputting just keywords or so, users form questions using natural language, where questions are encoded with various lexical, syntactic and semantic features. for example, how canlose weight in a few month and are there any ways of losing pound in a short period are two similar questions asking for methods of losing weight, but they neither share many common words nor follow identical syntactic structure. this gap makes the similar question matching task difficult. similarity measure techniques based purely on the bagof word approach may perform poorly and become ineffective in these circumstances. syntactic or semantic features hence become vital for such task. the tree kernel function is one of the most effective ways to represent the syntactic structure of a sentence. in general, it divides the parsing tree into several sub trees and computes the inner product between two vectors of sub trees. although there have been some successful applications using it, like question the tree kernel was designed based on the idea of counting the classification, the tree kernel like function has not been number of tree fragments that are common to both parsing trees, directly applied to finding similar questions in the qa archive. and it could be defined as: moreover, its matching scheme is too strict to be directly, equals to the number of common fragments rooted in, nodes and. however, to enumerate all possible tree nn of can be efficiently computed as follows: if nn. section # presents the architecture of our syntactical tree matching model. section # describes an improved model with, if, if and they are terminal nodes. and they are pre terminal nodes nc semantic features incorporated. section # reviews some related works and section # concludes our paper with directions for future works. user generated content is re shaping the way people watch video and tv, with millions of video producers and consumers. in particular, ugc sites are creating new viewing patterns and social interactions, empowering users to be more creative, and developing new business opportunities. to better understand the impact of ugc systems, we have analyzed youtube, the world largest ugc vod system. based on a large amount of data collected, we provide an in depth study of youtube and other similar ugc systems. in particular, we study the popularity life cycle of videos, the intrinsic statistical properties of requests and their relationship with video age, and the level of content aliasing or of illegal content in the system. we also provide insights on the potential for more efficient ugc vod systems. finally, we discuss the opportunities to leverage the latent demand for niche videos that are not reached today due to information filtering effects or other system scarcity distortions. overall, we believe that the results presented in this paper are crucial in understanding ugc systems and can provide valuable information to isps, site administrators, and content owners with major commercial and technical implications. the content length is shortened by two orders of magnitude and so is the production time. constant waves of new videos and the convenience of the web are quickly personalizing the viewing experience, leading to a great variability in user behavior and attention span. video content in standard video on demand systems has been historically created and supplied by a limited number of media producers, such as licensed broadcasters and production companies. content popularity was somewhat controllable through professional marketing campaigns. the advent of user generated content has re shaped the online video market enormously. nowadays, hundreds of millions of internet users are self publishing consumers. wired magazine refers to this small sized content pop culture as bite size bits for high speed munching. the scale, dynamics, and decentralization of the ugc videos make traditional content popularity prediction unsuitable. ugc popularity is more ephemeral and has a much more unpredictable behavior. as opposed to the early days of tv where everyone watched the same program at the same time, such strong reinforcement of popularity is diluted in ugc. understanding the popularity characteristics is important because it can bring forward the latent demand created by bottlenecks in the system. ects the strategies for marketing, target advertising, recommendation, and search engines. at the same time, a lack of editorial control in ugc is creating problems of content aliasing or copyright infringement, which seriously threatens the future viability of such systems. to understand the nature and the impact of ugc systems, in this paper we analyze youtube, the worldlargest ugc vod system. the main contribution of this paper is an extensive trace driven analysis of ugc video popularity distributions. to this extent, we have collected a large amount of data from youtube and another ugc system, daum. our analysis reveals very interesting properties regarding the distribution of requests across videos, the evolution of viewerfocus, and the shifts in popularity. such analysis is pivotal in understanding some of the most pressing questions regarding new business opportunities in ugc. our analysis also reveals key results regarding the level of piracy and the level of content duplication in such systems, which could have major implications in the deployment of future ugc services. the highlights of our work could be summarized as follows: we compare some prominent ugc systems with other standard vod systems such as net ix and love lm. we highlight the main di erences between the two systems and point out interesting properties regarding content production, consumption, and user participation patterns. community based question answering systems have become very popular for providing answers to a wide variety of how to questions. however most such systems present only textual answers. in many cases, users would prefer visual answers such as videos which are more direct and intuitive. currently, there is very little research on automatically presenting precise reference videos based on user question. in this paper, we explore how to leverage youtube video collections as a source of reference to fulfilll such task and develop a novel multimedia application named: video reference. there are two steps to generating a video reference. the first is recall driven video search, which is to increase the coverage of question by finding other similar questions. a three level ranking scheme based on visual analysis, opinion analysis and video redundancy is adopted to find the most relevant video reference from youtube. experiments conducted using questions from consumer electronics domain of yahoo answers archive show the feasibility and effectiveness of our approach. community based question answering becomes more and more popular on the web, such as yahoo answers. people ask a variety of how to questions and obtain answers permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. figure #: system flowchart of visual reference either by searching for similar questions on their own or waiting for other users to answer. however, even when the best answer is presented, user may still have di culty grasping the answers. this is because the textual answers are often too complicated for them. for example, a person may ask how doget my camera to put pics. a textual answer may be confusing because he she simply has no idea how to deal with usb cable, from the answer. however, if we can present visual answers such as videos, it will be more direct and intuitive for user to understand. in recent years, community generated video collections on the web have grown rapidly, and video search engines have been utilized to help people to access these resources, such as youtube and yahoo video. millions of video available on the web make them an ideal source for visual reference. however, web video search engine cannot be directly used to nd visual answers. first, most of web video search engines do not work well with verbose query. second, manually identifying some phrases from the original question as query may help to obtain related videos, but it is still inconvenient for people to try di erent query combinations before nding a satisfactory video as reference. third, even if some related videos are retrieved, it is possible that the most related video is still not at the top position. we propose a novel system named video reference as a solution to the above problem. we use community generated video website youtube as the video source. youtube is a growing video sharing website, that contains a large number of how to videos that depict how the video owners solve some speci. each video is also commented by other active viewers. user enters a question, such as how doget my camera to put pics. the rst is a recall driven related video search step that nds similar questions posed in di erent forms form yahoo answers site. similar questions have been found to increase the coverage of the topic. however, source video site like youtube can only take in precise queries, we extract key phrases from these questions as multiple search queries. the second step is the precisiondriven video re ranking, where related videos based on their relevance to the original questions are re ranked. we manually select training images for certain concept using google image search. we then perform salient object recognition based on image matching techniques to recognize the visual relatedness of the video to certain concepts. in addition, community viewers comments play the role of opinion voting for the videopopularity. finally, a rank fusion scheme is adopted to generate a new ranking list based on evidences from visual cues, opinion voting and video redundancy. our initial test shows that our approach is. ers great advantage over other video collections, making youtube a good video source to support video reference. as depicted in figure, video reference works as follows. the proposed framework comprises three stages for multi video summarization. finally, summarization is formulated as an optimization procedure which trades off between relevance of key shots and user defined skimming ratio. the framework provides the summary with the way of dynamic video skimming. the explosive growth of web videos brings out the challenge of how to efficiently browse hundreds or even thousands of videos at a glance. given an event driven query, social media web sites can easily return a ranked list of large but diverse and somewhat noisy videos. users often need to painstakingly explore the retrieved list for an overview of the event. this paper presents a novel solution by mining and threading key shots, which can provide an overview of main contents of videos at a glance, by summarizing a large set of diverse videos. firstly, given an event query, a ranked list of web videos together with their associated tags are retrieved. key shots are then established by near duplicate keyframe detection, ranked according to informativeness and threaded in a chronological order. we conduct user studies on twelve event queries for over hundred hours of videos crawled from youtube. the evaluation demonstrates the feasibility and effectiveness of the proposed solution. summarization may beafeasible way tohelp usere cientlybrowsetheretrieved videos. however, most existing video summarization algorithmsaredesigned only tohandleasinglevideo. thusthe existing summarization methods cannot be applied directly. we de ne the shots displaying these sub events in video as key shots, which are believed to unfold the dominant messages of the event. the modern web activities and contents have pervaded the internet. their increasing popularity originates from their ease of operation and support for interactive services such as tagging, comments and ratings. one exampleis youtube, which is one of the primary video sharing site. ers users a new channel to deliver and share their videos. studieshaveshownthatyoutubeserves milliondistinct videos and, uploads daily, and tra. of this site accounts for over of the web in total and of the whole internet, covering of the videos watched on line. the growing number of videos has motivated a real necessity to provide. however, given aneventtypequery, theretrieved videos are diverse and somewhat noisy. forour scenario, we have to face a large number of videos, most of which arederivativesfromthekey sub events. recent studies on video sharing sites have shown that there exists a signi cant amount of over of duplicate videosdetectedinthesearch results. wecategorizethe content redundancy on web videos into two classes: near duplicate and overlap. the former indicates that most of the frames from the two videos are duplicates and the latter indicates that the video pair shares some near duplicate frames. in this study, we focus on the case of overlap and look atitfrom adi erentperspective. wedemonstratethat such content overlap in web video sharing system may be exploitedfor automatic video summarization. as we know, an event is composed of a connected series of sub eventswith a commonfocus orpurposethathappens in speci. for a given event, the few scenes that convey the main messages, such as the principal sub events or key shots, will be presented more than once in news reports. we take september attacks as an example and divide it into many sub events. it contains several principal sub events, such as the airplane was hijacked, airplane crashed into the world trade tower, the world trade tower caught re and collapsed etc. we observe that thesetypesofkey shotsappearin many web videosretrieved by the event type query. based on this observation, we can identify such shots by rst extracting the keyframes and thenperforming nearduplicatekeyframe detection. this is especially so for those events happened during a limited time span and in a speci. motivatedby the above observation and analysis, wepropose to utilize the key shots to summarize web videos. we rst adopt anearduplicatekeyframe detection method to identify the key shots among the web videos. we then perform key shot ranking and thread them according to the chronological order based on the original videos. finally the resulting summary is obtained by optimally selecting the key shots for video skimming. we conduct experiments on a real world web video dataset, consisting of hours of videos crawled from youtube and with queries. the user evaluation demonstrates the viability of the proposed system. the contributions of this research are twofold: we propose an event driven web video summarization system to help users browse the results of web video search with an overview. to thebestof ourknowledge, this is the rst attempt to summarize multiple web videos relevant to news events. photo based question answering is a useful way of finding information about physical objects. current question answering systems are text based and can be difficult to use when a question involves an object with distinct visual features. a photo based qa system allows direct use of a photo to refer to the object. we develop a three layer system architecture for photo based qa that brings together recent technical achievements in question answering and image matching. the first, template based qa layer matches a query photo to online images and extracts structured data from multimedia databases to answer questions about the photo. to simplify image matching, it exploits the question text to filter images based on categories and keywords. the second, information retrieval qa layer searches an internal repository of resolved photo based questions to retrieve relevant answers. the third, human computation qa layer leverages community experts to handle the most difficult cases. a series of experiments performed on a pilot dataset of, images of books, movie dvd covers, grocery items, and landmarks demonstrate the technical feasibility of this architecture. we present three prototypes to show how photo based qa can be built into an online album, a text based qa, and a mobile application. many research systems and commercial systems have been developed to provide useful question answering services. we propose photo based qa as a solution to the limitation of current text based qa systems. these problems highlight the unique usability bene ts of photo based qa. of matching camera phone images to these database images. compared to conventional keyword based search engines, a qa permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. first, it supports an interaction style more similar to how we naturally interact with other humans stating our questions in plain sentences rather than with carefully chosen, sometimes cryptic, sets of keywords. second, unlike keyword based search engines that often return lengthy webpages through which users need to browse in order to locate the desired information, a qa service allows users to be speci. about what they need to know so that they receive brief and concise answers directed to their particular needs. however, current qa systems are based on text alone and can be di cult to use when questions are centered on physical objects with distinctive visual attributes. for example, a person who has just seen an interesting poster may want to ask the question where canbuy this poster atextbased qa system would require the person to meticulously describe the visual details of the poster in order to identify it. the di culty of this task stems from the fact that such questions are inherently dual modal: it involves a verbal component that states its intent and a visual component that identi es the object. unfortunately, with text as the only available input modality, users of current qa systems are often forced to express in words what would be best expressed visually. by taking advantage of recent advances in qa and image matching technologies, photo based qa supports direct use of photos in phrasing questions and nding answers. in contrast, current text based qa system are hard to use when visual objects are involved. two factors play in our favor in developing useful photo based qa systems. first, many online multimedia data sources can supply a photo based qa system with structured information to handle a variety of common photo based questions automatically. second, many community human users are willing to look at photos and answer questions that the automatic process fails to nd answers for. we describe a three layer system architecture for photobased qa. it draws inspiration from three popular qa approaches: template matching, information retrieval, and human computation. to evaluate the potential of the proposed architecture, we constructed a pilot dataset with more than, multimedia records of books, movies, grocery items, and buildings extracted from various online sources and measure the performance copyright year# acm. figure #: three layer system architecture for photobased qa. for a simple question, the template based layer identi es its category, nds a matched image within the category, and forms a template to extract an answer from the web. for a harder question, the ir based layer searches for relevant photo based questions already resolved. the human based layer handles the rest of the questions too di cult for the rst two layers. we describe three prototype systems designed to demonstrate how photo based qa can be integrated into an online photo album by introducing an extra question modality, into a text based qa service by embracing an additional photo modality, and into a mobile application for ubiquitous access. the approach, context reranking, is formulated as a random walk problem along the context graph, where video stories are nodes and the edges between them are weighted by multimodal contextual similarities. the random walk is biased with the preference towards stories with higher initial text search scores a principled way to consider both initial text search results and their implicit contextual relationships. when evaluated on trecvid year# video benchmark, the proposed approach can improve retrieval on the average up to relative to the baseline text search method in terms of story level mean average precision. in the people related queries, which usually have recurrent coverage across news sources, we can have up to relative improvement. most of all, the proposed method does not require any additional input from users, or complex search models for special queries. multimedia search over distributed sources often result in recurrent images or videos which are manifested beyond the textual modality. to exploit such contextual patterns and keep the simplicity of the keyword based search, we propose novel reranking methods to leverage the recurrent patterns to improve the initial text search results. the phenomenal success in www search has also helped attract increasing interest in investigating new solutions in video search. or commercial advantage and that copies bear this notice and the full citation on the rst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. english broadcast news video with the transcripts figure #: examples of a broadcast news video andthree web news ofdi erentlanguages covering the same topic pope discusses his remarks on islam, collected on september, year#. the images of pope benedict xvi are widely used over all the news sources of the same topic. the use of other modalities such as image content, audio, face detection, and high level concept detection has been shown to improve upon textbased video search systems. such multimodal systems improve the search performance by utilizing multiple query example images, speci. semantic concept detectors, or highly tuned retrieval models for speci. figure # shows an example query of pope, which may be answered by using multimodal cues extracted from the face photos, speech transcripts, aswell assomeprede ned concepts related toactivities or locations. itisquitedi cultforthe users to acquire example images as query inputs. retrieval text relevance score by matching semantic concepts, though promising, strongly text relevance score depends on availability of robust detectors and usually re multimodal video story quires large amounts of data for training the detectors. such patterns existbecause of the common semantic topics sharedbetween multiple videos, and additionally, the common visual content used. apparently, the visual near duplicates of pope benedict xvi are used over all the news sources reporting the same topic. such recurrent images or videos are commonly observed inimagesearch engines andphoto sharing sites. in, wehad analyzed thefrequency of such recurrentpatterns for cross language topic tracking a large percentage ofinternational newsvideosshare re used videoclipsornear duplicates. speci cally explored recurrentpatternsin theinitial search resultsand used aninformationbottleneck principletodevelop a reranking method. it achieved signi cantperformancegainswhenappliedtothe retrieval of video shots. figure # shows another example motivating the story level reranking approach. an initial text query retrieves video stories with the keywords tony blair. the multimodal contextuallinks may be used to link such miss the meanings of context are usually applicationdependent. here, we refertocontextasthoseattributes describing who, where, when, what, etc, shared by documents forming the recurrent patterns. though not relevant to the text query, certain stories can be boosted due to their closeness to some relevant text query stories by the multimodal similarity consisting of text, visual duplicates, etc. duetothelackof explicitlinksbetweenvideostories, context reranking is formulated as a random walk over the context graph whose nodes represent documents in the search set. these nodes are connected by the edges weighted with pair wise multimodalcontextual similarities. the stationary probability of the random walk is used to compute the nal scores of video stories after reranking. the random walk is biased with the preference towards the stories with higher initial text search scores aprincipled way tocombineboth initial text search results and their implicit contextual relationships. our contextual reranking method is in principle inspiredby thepage ranking techniqueswidely usedinweb search. ourexperiments showthattheproposed approach can improve the baseline text retrieval up to in story levelmeanaverageprecision. suchresults are remarkable since no additional advanced methods are used, like query expansion, speci. furthermore, for peoplerelatedqueries, which usuallyhave recurrent coverageacross news sources, our experiments show up to relative improvementin story levelmap throughparameter sensitivity tests, we also discovered that the optimal text vs. visual weight ratiofor rerankinginitial text search resultsis to. it is encouraging and intuitive since, as illustrated in figure #, visual modality plays an important role in de ning the contextual similarity between video stories. we formulate the reranking problem in section #. based onthese methods, the video reranking approach is proposed in section #. visual documents text query tony blair initial text reranked search results documents in documents figure #: the illustration of the proposed visual reranking process which tries to improve the visual documents from initial text search results. we present further discussion and review of related work in section #, and conclusions in section #. imageand video retrievalhasbeenanactive research area thanks to the continuing growth of online video data, personal video recordings, hour broadcast news videos, etc. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. pope benedict xvi expressed regret for the reaction his remarks on islam caused, but did not issue an outright apology for making them. aside from the text transcripts or web text tokens, the visual duplicates provide another similarity link between broadcast news videos or web news and help cross domain topic threading, or boost video search performance. current video search approaches are mostly restricted to text based solutions which process keyword queries against text tokens associated with the video, such as speech transcripts, closed captions, and recognized video text. however, suchtextualinformation may not necessarily come with the image or video sets. however, there are di culties in applying the multimodal search methods mentioned above. additionally, it is still an open issue whether concept models fordi erentclassesofqueries may bedeveloped andproved. additionally, basedonthe observationsinthecurrent retrieval systems, mostusers expect searching images and videos simply through a few keywords. therefore, incorporation of multimodal search methods shouldbe astransparent and non intrusiveaspos sible, inordertokeepthesimplesearch mechanismpreferred text query results text query storiesnot relevant to by typical users today. based on the above observations and principles, we propose to conductsemantic video searchin a reranking manner which automatically reranks the initial text search results based on the recurrent patterns or contextual patterns between videos in the initial search results. for example, figure # shows abroadcast newsvideoand threeweb news articlesindi erentlanguages covering the same topic pope discusses his remarks on islam. such visual duplicates provide strong links betweenbroadcastnews videos or web news articles andhelp cross domain information exploitation. video reranking has been explored in some prior works. however, such a method is restricted because of its dependence on the representation and occurrence frequency estimation at the shot level, rather than the semantic level. typically, a semantic document of videos contains multiple images eg, abroadcastnewsstoryhasmultipleshots and a web page may include multiple photosorvideoclips, and in addition to associated textual data in the same document. therefore, inthispaper, weproposeto use multimodal similaritiesbetweensemanticvideodocuments in exploiting the underlying contextual patterns and developing a new reranking method, context reranking. however, there are still certain relevant stories not retrieved due to the lack of keyword annotations associated withthem. tony blair ordered by relevance scores text query figure #: example of a video search that ben. ts from multimodal story level similarity on a large scale video database, even with unreliable text asr mt transcripts. ing stories to theinitial textqueries andfurtherimprove the search accuracy. ourinnovationslieintheuseof themultimodal similarities andthe novelintegration oftheinitialtext search and reranking resultsviaarigorous randomwalkframework. we also investigate the optimal weights for combining the text search scoresand themultimodal rerankingfordi erent classes of video queries. current web video search results rely exclusively on text keywords or user supplied tags. a search on typical popular video often returns many duplicate and near duplicate videos in the top results. this paper outlines ways to cluster and filter out the near duplicate video using a hierarchical approach. initial triage is performed using fast signatures derived from color histograms. only when a video cannot be clearly classified as novel or near duplicate using global signatures, we apply a more expensive local feature based near duplicate detection which provides very accurate duplicate analysis through more costly computation. the results of queries in a data set of, videos retrieved from google, yahoo and youtube show that this hierarchical approach can dramatically reduce redundant video displayed to the user in the top result set, at relatively small computational cost. as bandwidth accessible to average users is increasing, video is becoming one of the fastest growing types of data on the internet. especially with the popularity of social media in web, there has been exponential growth in videos available on the net. users can obtain web videos easily, and distribute them again with some modifications. for example, users upload, new videos each permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. school of computer science carnegie mellon university forbes avenue, pittsburgh, usa day on video sharing website youtube and the daily video views are over million. among these huge volumes of videos, there exist large numbers of duplicate and near duplicate videos. it becomes important to manage these videos in an automatic and efficient way. to avoid getting swamped by almost identical copies of the same video in any search, efficient near duplicate video detection and elimination is essential for effective search, retrieval, and browsing. current web video search engines tend to provide a list of search results ranked according to their relevance scores given a text query. while some users information needs may be satisfied with the relevant items ranked at the very top, the topmost search results usually contain a vast amount of redundant videos. based on a sample of popular queries from youtube, google video and yahoo video, on average there are redundant videos that duplicate or nearly duplicate to the most popular version of a video in the search results. figure # shows actual search results from three currently popular web video search engines, with redundancy fairly obvious in this case. as a consequence, users need to spend significant amount of time to find the videos they need and are subjected to repeatedly watching similar copies of videos which have been viewed previously. this process is extremely time consuming particularly for web videos, where the users need to watch different versions of duplicate or near duplicate videos streamed over the internet. an ideal solution would be to return a list which not only maximizes precision with respect to the query, but also novelty of the query topic. this problem is generally referred to as novelty ranking in information retrieval. unfortunately, the textbased techniques from ir cannot be directly applied to discover video novelty. for instance, text keywords and user supplied tags attached to web videos are usually abbreviated and imprecise. second, most videos lack the web link structure typical in html documents which can be exploited for finding sub topic relatedness. finding novelty among the relevant web videos must largely rely on the power of content analysis. due to the large variety of near duplicate web videos ranging from simple formatting to complex editing, near duplicate detection remains a challenging problem. accurate detection generally comes at the cost of time complexity particularly in a large video corpus. on the other hand, timely response to user queries is one important factor that fuels the popularity of web. to balance the speed and the accuracy aspects, in this paper, we propose a hierarchical approach combining global signatures and local feature based pairwise comparison to detect nearduplicate web videos. the tool of near duplicate detection can be used in several ways: as a filter to remove redundant videos in the listing of retrieval results, as a tool for finding similar videos in different variations, or as a way to discover the essential version of content appearing in different presentations. we show that the approach is practical for near duplicate retrieval and novelty re ranking of web videos where the majority of duplicates can be detected and removed from the top rankings. the rest of this paper is organized as follows. in section # we give a brief overview of related work. a characterization of different types of near duplicate web videos is provided in section #. the proposed framework for efficient near duplicate detection is introduced in section #. section # presents experiments and results for the two tasks a web result novelty re ranking andfinding similar videos. finally, we conclude the paper with a summary.