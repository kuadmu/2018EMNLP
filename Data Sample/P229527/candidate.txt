we propose a context sensitive query auto completion algorithm, nearestcompletion, which outputs the completions of the user input that are most similar to the context queries. hybridcompletion is shown to dominate both nearestcompletion and mostpopularcompletion, achieving a total improvement of in mrr relative to mostpopularcompletion on average. query auto completion is known to provide poor predictions of the user query when her input prefix is very short. in this paper we show that context, such as the user recent queries, can be used to improve the prediction quality considerably even for such short prefixes. to measure similarity, we represent queries and contexts as high dimensional term weighted vectors and resort to cosine similarity. the mapping from queries to vectors is done through a new query expansion technique that we introduce, which expands a query by traversing the query recommendation tree rooted at the query. in order to evaluate our approach, we performed extensive experimentation over the public aol query log. we demonstrate that when the recent user queries are relevant to the current query she is typing, then after typing a single character, nearestcompletion mrr is higher relative to the mrr of the standard mostpopularcompletion algorithm on average. when the context is irrelevant, however, nearestcompletion mrr is essentially zero. to mitigate this problem, we propose hybridcompletion, which is a hybrid of nearestcompletion with mostpopularcompletion. its main purpose is to predict the userintended query and thereby save her keystrokes. one possible approach to use recent queries to improve query auto completion is to generalize mostpopularcompletion to rely on the popularity of query sequences rather than just the popularity of individual queries. this is in fact the main principle underlying most of the work on context sensitive query recommendations. its average mrr is higher relative to the average mrr of mostpopular completion. query auto completion is one of the most visible features in web search today. ered by all major search engines and in almost all their search boxes. query auto completion helps the user formulate her query, while she is typing it. with the advent of instant as you type search results, the importance of correct query prediction is even more acute, because it determines the speed at which the user sees the suitable results for her intended search and the amount of irrelevant results that are displayed to her along the way. the basic principle that underlies most query auto completion systems is the wisdom of the crowds. the search engine suggests to the user the completions that have been most popular among users in the past. am, bing suggests amazon and american express as the top completions, because these have been the most popular queries starting with am. as the user is typing more characters, the space of possible completions narrows down, and thus the prediction probability increases. for example, if the user is looking for american presidents in bing, after typing the characters american presi the desired query becomes the top completion. clearly, during the rst few keystrokes the user is typing, the search engine has little information about her real intent, and thus the suggested completions are likely to mispredict her query. in our experiments, conducted over the public aol query log, we found that after the rst character, mostpopularcompletionaverage mrr is only. the objective of this study is to tackle the most challenging query auto completion scenario: after the user has entered only one character, try to predict the userquery reliably. being able to predict the userquery on her rst character rather than, say, on her th character would not only save the user a few keystrokes, but would also make the whole search experience more interactive, as the feedback cycle of would be shortened signi cantly. in addition, cutting down the search time for all users implies lower load on the search engine, which translates to savings in machine resources and power. but how can we overcome the inherent lack of information when the user has entered only a few characters of her http: www google com instant intended query our main observation is that the user typically has some context, which can reveal more information about her intent. for example, if just before entering the characters am the user searched for richard nickson, it is more likely that the user is looking for american presidents than for amazon or american airlines. similarly, if the user was browsing a page about president lincoln or reading an article about american history. on the other hand, if the user has just tweeted about a planned trip, american airlines might be the more probable query. recent queries, recently visited web pages, and recent tweets are examples of online activities that may indicate the userintent and, if available, could be used by the search engine to better predict the query even after a few keystrokes. this is called context sensitive query auto completion. while the idea is very intuitive and context has been used in other scenarios to disambiguate user intent, there is almost no published work on its application to query completion. of the many di erent possible user contexts, our focus in this study is on the userrecent queries, as they are readily available to search engines. based on our empirical analysis of the aol log, of the searches are preceded by a di erent search in the same session, and are thus amenable to context sensitive query completion. suppose the userprevious query in the same session isand that the current user input is. then, the search engine will suggest the completions ofthat were most frequently searched for after. for example, if after the query richard nixon the most popular successive query starting with am is american presidents, the search engine will suggest american presidents as its top completion. the main caveat of this approach is that it heavily relies on the existence of reoccurring query sequences in search logs. nevertheless, due to the long tail distribution of query frequencies, many of the query sequences generated by users have never occurred before. some studies tried to mitigate the sparsity of query sequences by clustering similar query sequences together, based on features extracted from queries, like their topical categories or their terms. machine learning techniques, like hmms, are then used to predict the intended query, if the sequence of previous queries can be associated with a cluster in the model. this approach is still challenged by long tail contexts, ie, when the most recent query have rarely occurred in the log. in this case, the sequence of previous queries may not be easily associated with a cluster in the model. moreover, none of these previous studies took the user input into account in the prediction, so their applicability to query auto completion is still unknown. we take a di erent approach to tackle this problem. our algorithm relies on the following similarity assumption: when the context is relevant to the intended user query, the intended query is likely to be similar to the context queries. the similarity may be syntactic or only semantic. by our estimates, of the re nements are non syntactic. based on the similarity assumption, we propose the nearestcompletion algorithm, which works as follows: given a user inputand a context, the algorithm suggests to the user the completions ofthat are most similar to. choosing the suitable similarity measure is non trivial, though, because we would like it to be both correlated with reformulation likelihood and universally applicable. the former requirement guarantees that the completions that are similar to the context are indeed more likely to be the userintended query. the latter requirement makes sure that the algorithm can deal with any user input. the above two requirements make many of the stateof the art query similarity measures less appealing for this problem. for example, syntactic measures, like edit distance, do not take all reformulation types into account. similarity measures that are based on co occurrence in search sessions, on co clicks, or on user search behavioral models, are not universally applicable to all query pairs due to their low coverage of queries, as long tail queries are rare in the query log. similarity measures that are based on search result similarity are not necessarily correlated with reformulation likelihood. given a context, query recommendation algorithms output a list of recommendations that are likely reformulations of the previous query. so a possible similarity measure would be one that associates each query with its recommendations. the main caveat with this approach is that query recommendation algorithms are frequently designed to output only a few high quality recommendations and thus it is plausible that none of them are compatible with the userinput. we propose a new method of measuring query similarity, which expands on the latter recommendations based approach, but is universally applicable and is thus more suitable to query completion. similarly to the result based similarity of broder et al, we expand each query to a richer representation as a high dimensional feature vector and then measure cosine similarity between the expanded representations. the main novelty in our approach is that the rich representation of a query is constructed not from its search results, but rather from its recommendation tree that is, we expand the query by iteratively applying a black box query recommendation algorithm on the query, on its recommendations, on their recommendations, and so on. the nodes of the traversed tree of recommendations are tokenized, stemmed, and split intograms. thesegrams are the features of the expanded representation vector and the weight of eachgram is computed based on its frequency and depth in the tree. first, as the basic building block in the construction is a black box query recommendation algorithm, we can leverage any state of the art algorithm and inherit its power in predicting query reformulations. second, the above scheme provides a continuous spectrum of exceedingly rich representations, depending on the depth of the tree traversed. for example, a depth traversal results in thegrams of the root query itself, while a depth traversal results in thegrams of the query, its recommendations, and their recommendations. the main point is that the feature space remains the same, regardless of the traversal depth. so even if we cannot traverse the recommendation tree of a certain query, the similarity between its representation and the richer representation of other queries is meaningful. this property ensures that our query auto completion algorithm is applicable even for long tail contexts that have never been observed in the log before. our empirical analysis shows that the average mrr of nearestcompletion over queries whose context is relevant is higher relative to the average mrr of mostpopularcompletion over the same queries. however, when the context is irrelevant to the intended query, nearestcompletion becomes destructive, so its average mrr is lower relative to the average mrr of mostpopular completion over all queries. to mitigate this problem, our nal algorithm, hybridcompletion, is a hybrid of mostpopularcompletion and nearestcompletion. each of the two algorithms provides a list of topmatches. we aggregate the two lists by standardizing the contextual score and the popularity score of each candidate completion and then computing. nal score which is a convex combination of the two scores. the completions are ranked by these nal scores. we show that hybridcompletion dominates both nearestcompletion and mostpopularcompletion. as our new algorithm relies on the standard cosine similarity measure between vectors, it can be implemented. this is a crucial property of the algorithm, because query auto completions need to be provided to the user in a splitsecond as she is typing her query. note that the rich representation of the recent queries can be cached and retrieved quickly as the user is typing her current query. the rest of the paper is organized as follows. after reviewing some related work in section #, we provide a brief background about query auto completion algorithms in section #. we describe the nearestcompletion algorithm in section # and the hybridcompletion algorithm in section #. in section # we provide a detailed empirical study of the two algorithms and compare them to mostpopularcompletion. we end with some concluding remarks in section #. we consider the following full text search autocompletion feature. imagine a user of a search engine typing a query. then with every letter being typed, we would like an instant display of completions of the last query word which would lead to good hits. at the same time, the best hits for any of these completions should be displayed. we present a new indexing data structure that uses no more space than a state of the art compressed inverted index, but with times faster query processing times. even on the large trec terabyte collection, which comprises over million documents, we achieve, on a single machine and with the index on disk, average response times of one tenth of a second. we have built a full fledged, interactive search engine that realizes the proposed autocompletion feature combined with support for proximity search, semi structured text, subword and phrase completion, and semantic tags. known indexing data structures that apply to this problem either incur large processing times for a substantial class of queries, or they use a lot of space. one of its early uses was in the unix shell, where pressing the tabulator key gives a list of all le names that start with whatever has been typed on permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. ingmar weber max planck institutur informatik saarbr ucken, germany iweber mpi inf mpg de the command line after the last space. recently, autocompletion has been integrated into a number of search engines like google suggest or applespotlight. for the unix shell, this is the list of all le names in all directories listed in the path variable. for the text editors, this is the list of all words entered into the le so far. in google suggest, completions appear to come from a precompiled list of popular queries. for these kinds of applications we can easily achieve fast response times by two binary ortree searches in the sorted list of candidate strings. the problem we propose and discuss in this paper is of this kind. more informally, imagine a user of a search engine typing a query. all this should preferably happen in less time than it takes to type a single letter. promising completions might then be sigir, sigmod, etc, but not, for example, signature, assuming that, although signature by itself is a pretty frequent word, the query conference signature leads to only few good hits. our results we have developed a new indexing data structure, named hyb, which uses no more space than a state of the art compressed inverted index, and which can respond to autocompletion queries as described above within a small fraction of a second, even for collection sizes in the terabyte range. our main competitor in this paper is the inverted index, referred to as inv in the following. other data structures that could be directly applied to our problem either use a lot of space or have other limitations; we discuss these in section #. we give a rigorous mathematical analysis of hyb and inv with respect to both space usage and query processing times. concerning space usage, we de ne a notion of empirical figure #: a screenshot of our search engine for the query conference sig searching the english wikipedia. entropy, which captures the inherent space complexity of an index independent of a particular compression scheme. we prove that the empirical entropy of hyb is essentially equal to that of inv, and we nd that the actual space usage of our implementation of the two index structures is indeed almost equal, for each of our three test collections. we also take into account the di erent latencies of sequential and random access to data. one of our collections has been publicly searchable over the last year, so that we have autocompletion queries from real users for it. we have built a full edged search engine that supports autocompletion queries of the described kind combined with support for proximity phrase search, xml tags, subword and phrase completion, and category information. typed so far would lead to highly ranked documents. we get a related feature by the subword phrase completion mechanism described in section #. our autocompletion problem is related to but distinctly di erent from multi dimensional range searching problems, where the collection consists of tuples, and queries are asking for all tuples that match a given tuple of ranges. for fast processing times, however, the space consumption of any of these structures is on the order of, where nis the size of an inverted index, and grows with the dimension. for our autocompletion queries, we can achieve fast query processing times and space. finally, there is a large variety of alternatives to the inverted index in the literature. for example, approaches that consider document by document are bound to be slow due to a poor locality of access; in contrast, both inv and hyb are mostly scanning long lists; see section #. signature les were found to be in no way superior to the inverted index in all major respects in. autocompletion is a widely used mechanism to get to a desired piece of information quickly and with as little knowledge and. or commercial advantage and that copies bear this notice and the full citation on the rst page. nowadays, we nd a similar feature in most text editors, and in a large variety of browsing guis, for example, in le browsers, in the microsoft help suite, or when entering data into a web form. in the simpler forms of autocompletion, the list of completions is simply a range from a list of words. more advanced forms of autocompletion take into account the context in which the to be completed word has been typed. the formal problem de nition will be given in section #. then with every letter being typed, we would like an instant display of completions of the last query word which would lead to good hits at the same time, the best hits for any of these completions should be displayed. see figure # for a screenshot of our search engine responding to that query. for a live demo, see http: search mpi inf mpg de wikipedia. our analysis accurately predicts the real behavior on our test collections. the list of completions and hits is updated automatically and instantly after each keystroke, hence the absence of any kind of search button. the number in parentheses after each completion is the number of hits that would be obtained if that completion where typed. query words need not be completed, however, because the search engine does an implicit pre. search: if, for example, the user continued typing conference sig proc, completions and hits for proc, eg, proceedings, would be fromthe hits for conference sig. concerning processing times, we give a precise quanti cation of the number of operations needed, from which we derive bounds for the worst, best, and average case behavior of inv and hyb. we compare inv and hyb on three test collections with di erent characteristics. our largest collection is the trec terabyte benchmark with over million documents. on all three collections and on all the queries we considered, hyb outperforms inv by a factor of in worst case query processing time, and by a factor of in average case query processing time. in absolute terms, hyb achieves average query processing of one tenth of a second or less on all collections, on a single machine and with the index on disk. all of these extensions are described in section #. related work the autocompletion feature as described so far is reminiscent of stemming, in the sense that by stemming, too, pre xes instead of full words are considered. but unlike stemming, our autocompletion feature gives the user feedback on which completions of the pre. the user can then assess the relevance of these completions to his or her search desire, and decide to type more letters for the last query word, eg, in the query from figure #, typeandso that the query is then conference sigir, or to start with the next query word, eg, type a space and then proc, or to stop searching as, eg, the user was actually looking for one of the hits shown in figure #. there is no way to achieve this by a stemming preprocessing step, because there is no way to foresee the userintent. this kind of user interaction is well known to improve retrieval. while our autocompletion feature is for the purpose of nding information, autocompletion has also been employed for the purpose of predicting user input, for example, for typing messages with a mobile phone, for users with disabilities concerning typing, or for the composition of standard letters. in, contextual information has been used to select promising extensions for a query. paynter et al have devised an interface with a zooming in property on the word level, and based on the identi cation of frequent phrases. these data structures could be used for our autocompletion problem, provided that we were willing to limit the number of query words. ciency at the same time because we have the set of documents matching the part of the query before the last word already computed. in a sense, our autocompletion problem is therefore a dimensional range searching problem. we have considered those we are aware of with regard to their applicability to our autocompletion problem, but found them either unsuitable or inferior to the inverted index in that respect. arrays and related data structures address the issue of full substring search, which is not what we want here; a direct application of a data structure like would have the same. ciency problems as inv, whereas multi dimensional variants like require super linear space, as explained above. as with any application of machine learning, web search ranking requires labeled data. the labels usually come in the form of relevance assessments made by editors. click logs can also provide an important source of implicit feedback and can be used as a cheap proxy for editorial labels. the main difficulty however comes from the so called position bias urls appearing in lower positions are less likely to be clicked even if they are relevant. in this paper, we propose a dynamic bayesian network which aims at providing us with unbiased estimation of the relevance from the click logs. experiments show that the proposed click model outperforms other existing click models in predicting both click through rate and relevance. web page ranking has been traditionally based on hand designed ranking functions such as bm. with the inclusion of thousands of features for ranking, hand tuning of ranking function becomes intractable. several machine learning algorithms have been applied to automatically optimize ranking functions. machine learned ranking requires a large number of training examples, with relevance labels indicating the degree of relevance for each querydocument pair. the cost of the editorial labeling is usually quite expensive. moreover, the relevance labels of the training examples could change over time. for example, if the query is time sensitive or recurrent, a search engine is expected to return the copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. most up to date documents sites to the users. however, it would be prohibitive to keep all the relevance labels up to date. click logs embed important information about user satisfaction with a search engine and can provide a highly valuable source of relevance information. compare to editorial labels, clicks are much cheaper to obtain and always re ect current relevance. clicks have been used in multiple ways by a search engine: to tune search parameters, to evaluate di erent ranking functions, or as signals to directly in uence ranking. however, clicks are known to be biased, by the presentation order, the appearance of the documents, and the reputation of individual sites. many studies have attempted to account the position bias of click. carterette and jones proposed to model the relationship between clicks and relevance so that clicks can be used to unbiasedly evaluate search engine when lack of editorial relevance judgment. other research attempted to model user click behavior during search so that future clicks may be accurately predicted based on observations of past clicks. two di erent types of the click models are position models and the cascade model. a position model assumes that a click depends on both relevance and examination. each rank has a certain probability of being examined, which decays by rank and depends only on rank. a click on a url indicates that the url is examined and considered relevant by the user. however this model treats the individual urls in a search result page independently and fails to capture the interaction among urls in the examination probability. take for example two equally relevant urls for a query: a user may only click on the top one, feel satis ed, and then leave the search result page. in this case, the positional bias cannot fully explain the lack of clicks for the second url. the cascade model assumes that users examine the results sequentially and stop as soon as a relevant document is clicked. here, the probability of examination is indirectly determined by two factors: the rank of the url and the relevance of all previous urls. the cascade model makes a strong assumption that there is only one click per search and hence it could not explain the abandoned search or search with more than one clicks. even though the cascade model is quite restrictive, the authors of that paper showed that we refer to url as a shorthand for the entire display block consisting of the title, abstract and url of the corresponding result. it can predict click through rates more accurately than the position models described above. none of the above models distinguish perceived relevance and actual relevance. because users cannot examine the content of a document until they click on the url, the decision to click is made based on perceived relevance. while there is a strong correlation between perceived relevance and actual relevance, there are also many cases where they di er. in this paper, a dynamic bayesian network model is proposed to model the users browsing behavior. as in the position model, we assume that a click occurs if and only if the user has examined the url and deemed it relevant. similar to the cascade model, our model assumes that users make a linear transversal through the results and decide whether to click based on the perceived relevance of the document. the user chooses to examine the next url if he she is unsatis ed with the clicked url. our model di ers from the cascade model in two aspects: because a click does not necessarily mean that the user is satis ed with the clicked document, we attempt to distinguish the perceived relevance and actual relevance. a key source of bias is presentation order: the probability of click is influenced by a document position in the results page. this paper focuses on explaining that bias, modelling how probability of click depends on position. we carry out a large data gathering effort, where we perturb the ranking of a major search engine, to see how clicks are affected. we then explore which of the four hypotheses best explains the real world position effects, and compare these to a simple logistic regression model. a cascade model, where users view results from top to bottom and leave as soon as they see a worthwhile document, is our best explanation for position bias in early ranks. search engine click logs provide an invaluable source of relevance information, but this information is biased. we propose four simple hypotheses about how position bias might arise. the data are not well explained by simple position models, where some users click indiscriminately on rank or there is a simple decay of attention over ranks. as people search the web, certain of their actions can be logged by a search engine. they can also be indicative of success or failure of the engine. or commercial advantage and that copies bear this notice and the full citation on the rst page. these record which results page elements were selected for which query. click log information can be fed back into the engine, to tune search parameters or even used as direct evidence to in uence ranking. a fundamental problem in click data is position bias. the probability of a document being clicked depends not only on its relevance, but on its position in the results page. in top results lists, the probability of observing a click decays with rank. eye tracking experiments show that the user is less likely to examine results near the bottom of the list, although click probability decays faster than examination probability so there are probably additional sources of bias. our approach is to consider several such hypotheses for how position bias arises, formalising each as a simple probabilistic model. we then collect click data from a major web search engine, while deliberately ipping positions of documents in the ranked list. we nally evaluate the position bias models using the ip data, to see which is the best explanation of real world position. although our experiment involves ips, our goal is to model position bias so we can correct for it, without relying on ips. with such a model it should be possible to process a click log and extract estimates of a search resultabsolute click relevance. patterns of behaviour in logs can give an idea of the scope of user activity. when deciding which search results to permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci. bill ramsey microsoft research, redmond usa brams microsoft com present, click logs are of particular interest. such estimates could be used in applications where an estimate of probability of click is useful such as ad ranking or evaluation. in this paper, we study the problem of online spelling correction for query completions. misspelling is a common phenomenon among search engines queries. in order to help users effectively express their information needs, mechanisms for automatically correcting misspelled queries are required. online spelling correction aims to provide spell corrected completion suggestions as a query is incrementally entered. as latency is crucial to the utility of the suggestions, such an algorithm needs to be not only accurate, but also efficient. to tackle this problem, we propose and study a generative model for input queries, based on a noisy channel transformation of the intended queries. utilizing spelling correction pairs, we train a markovgram transformation model that captures user spelling behavior in an unsupervised fashion. to find the top spell corrected completion suggestions in real time, we adapt the a search algorithm with various pruning heuristics to dynamically expand the search space efficiently. evaluation of the proposed methods demonstrates a substantial increase in the effectiveness of online spelling correction over existing techniques. thus, we can decompose the probability of the overall transformation sequence as a product of the transfeme probabilities, each conditioned on the previous transfemes. since different users misspell at different rates, we further propose a mixture model to address the problem of overly aggressive corrections. misspelling is a common phenomenon in search engine queries. according to cucerzan and brill, more than of search engines queries are misspelled. this is even more severe for tail queries, of which more than are misspelled. when typing quickly, users may add or drop letters unintentionally. accidentally hitting an adjacent key on the keyboard, also known as the fat finger syndrome, is also common, especially on mobile devices with small virtual keyboards. in addition to typographical errors, some errors result from the challenge of spelling itself. with inconsistent spelling rules, ambiguous word breaking boundaries, and constant introduction of new words, spelling presents a formidable challenge to foreign and native speakers alike. table # summarizes different types of misspellings and provides examples of each. types of misspellings cause misspelling correction typing quickly exxit mispell exit misspell keyboard adjacency importamt important inconsistent rules concieve conceirge conceive concierge ambiguous word breaking silver light silverlight new words kinnect kinect to assist users in expressing their information needs, it is important for search engines to automatically generate corrections for misspelled queries. the first corrects a query after it is submitted to the search engine. for confident corrections, the search engine can search the corrected query directly. as the entire query string is given, we refer to such an approach as offline spelling correction. the second technique provides corrections to the query completion suggestions as the query is being entered. specifically, the search engine responds to each keystroke with a list of query suggestions that best correct and complete the partial query. compared with offline spelling correction, this task not only needs to address incomplete queries, but also requires lower latency to be effective. we refer to this task as online spelling correction. online correction has many merits that cannot be achieved by offline correction. first, it keeps users informed of potential errors as they type. thus, spelling errors and the resulting ambiguities can be eliminated even before issuing the query. second, it helps users express their information needs. as the quality and quantity of suggestions from current search engines degrade dramatically with misspelled partial queries, the ability to suggest popular completions from corrected partial queries can improve the effectiveness of the suggestions. third, it saves users effort in inputting queries. selecting a corrected suggestion reduces not only the number of keystrokes required to input a query, but often also the additional click on the search result page to confirm the correction. it is worth noting that although many search engines today apply online corrections to query completion suggestions, their abilities work performed while author was an intern at microsoft research. copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use and personal use by others. for instance, neither of the two major us search engines, google and bing, suggests any completion for importamt and miunderstand at the time of this submission, which are only one letter away from important and misunderstand, respectively. in this paper, we model search queries with a generative model, where the intended query is transformed through a noisy channel into a potentially misspelled query. the distribution from which the target query is selected is estimated from the search engine query log based on frequency. for the noisy channel, which describes the distribution of spelling errors, we follow the joint sequence modeling framework to define the probability of transforming the original query into the observed character sequence. specifically, we treat the desired and realized queries as a sequence of substring transformation units, or transfemes for convenience. figure #: example segmentation into transfemes to estimate the conditional probabilities of the transfemes, we train a smoothed transfemegram language model from a set of correction query pairs. as the query pairs are not co segmented in the training data, we apply the expectation maximization algorithm to segment the data with the objective of maximizing the model probability. as suggestion latency is crucial to the usefulness of query suggestions from potentially misspelled partial queries, we further explore efficient data structures and algorithms to dynamically search for the top query completions under the proposed transformation model. we also experiment with different pruning heuristics to reduce search latency. as all query pairs have a non zero transformation probability under this model, we further investigate probability thresholding techniques to reduce irrelevant suggestions. the rest of the paper is organized as follows. after surveying the related work in section #, we introduce the generative model for online correction in section #. next, we present the proposed transformation model and search algorithm in sections and. we follow up with experiment results in section # before concluding in section #. with more comprehensive suggestions that correct potential errors, users are more likely to find the target query in the suggestion list. thus, we are more likely to suggest more popular queries. by applying the markov assumption and experimenting with the length of the transfeme units, we can build transformation models of varying complexities. figure # shows an example segmentation of the input and output queries into a sequence of transfemes. in particular, we apply the a search algorithm against a trie built from the query log, where each node is further annotated with the best score of all descendent queries. search engine click logs provide an invaluable source of relevance information but this information is biased because we ignore which documents from the result list the users have actually seen before and after they clicked. otherwise, we could estimate document relevance by simple counting. in this paper, we propose a set of assumptions on user browsing behavior that allows the estimation of the probability that a document is seen, thereby providing an unbiased estimate of document relevance. our solution outperforms very significantly all previous models. they also explain why documents situated just after a very relevant document are clicked more often. to train, test and compare our model to the best alternatives described in the literature, we gather a large set of real data and proceed to an extensive cross validation experiment. as a side effect, we gain insight into the browsing behavior of users and we can compare it to the conclusions of an eye tracking experiments by joachims et al. in particular, our findings confirm that a user almost always see the document directly after a clicked document. users permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bearthisnoticeand thefull citationonthe rstpage tocopy otherwise, to republish, topost on servers or to redistribute tolists, requiresprior speci. benjamin piwowarski yahoo research latin america bpiwowar yahoo inc com areincreasingly understood tobethedrivingforceof theinternet and many initiatives are aimed at empowering them. social search, as its name implies, supposes participation from users who tag, bookmark, andcomment their search results. in addition to this information explicitly provided by users, there is a much larger source of implicit data which is collectedby search engines. itis apoll of millions of users over an enormous variety of topics. examples of applicationsincludewebpersonalization, web spam detection, query term recommendation. unlike human tags and bookmarks, implicit feedback is also not biased towards socially active web users. that is, the data is collected from all users, notjust usersthat choosetoedit a wikipage, orjoin a social network such asmyspace orfriendster. click data seems the perfect source of information when deciding whichdocuments to showin answerto a query. this information can be fed back into the engine, to tune search parameters or even used asdirect evidencetoin uence ranking. nevertheless, they cannot be used without further processing: a fundamental problem is the position bias. the probability of a document being clicked depends not only on its relevance, but on other factors as its position in the result page. contributions user activity models within web search can be broadly divided in three categories: analysis models where the aim istogaininsightintotypical userbehavior, modelsthat try topredictthenext useraction, and eventually models that estimate the attractiveness or perceived relevance of a document independently of the layout in uence. this work focusses on thelatter, usingas the only source ofinformation the web search logs produced by the search engines. yet users do not browse the whole list and documents situated earlier in the rankinghave ahigherprobability ofbeing examined. as a consequence, they also have a higher probability of being clicked independently of how relevant they are. if we could estimatetheprobability that adocumentis examined by the user, we could estimate its relevance as the ratio of the number of times a user clicked on the document to the expected number of times the document is examined. the main contribution of this work is a model of user browsing behavior when consulting a page of search results. this model estimates the probability of examination of a documentgiven the rankofthedocument andthedistance to the last clicked document. our model sheds light on user behavior, is in agreement with the user experiments ofgranka et al and extends andquanti esthe user model ofjoachims et al. in section # we review the literature for click models and we present our contributions. in section # we compare the predicting abilities on unseen data of the di erent models. westudy in moredetailstheimplications of theuserbrowsing model and we relate the ndings with the eye tracking experiments of insection. social search is quickly gaining acceptance as a promising way of harnessing the common knowledge of millions of users to help each other and search more. arguably, this is a long term trend that started with kleinberg idea of hubs and authorities, which proposed that a hyperlink from one document to another was a vote in favor of the document linked to, an idea in practice exploited in the pagerank algorithm. thisfeedbackprovidesdetailed and valuable information about users interactions with the system as theissuedquery, thepresentedurls, the selected documents and their ranking. it has been used in many ways to mine user interests and preferences. it can be thought as the result of users voting in favor of the documents they nd interesting. in top results lists, the probability of observing a click decays with rank. eye tracking experiments show that a user is less likely to examine results near the bottom of the list, although click probability decays faster than examination probability so there are probably additional sources of bias. experiments also show that a document is not clicked with the same frequency if situated after a highly relevant or a mediocre document. if the users looked with attention all the documents in the ranking list, the relevance of one of them could be estimated simply by counting thenumberof timesitisselected. we investigate how users interact with the results page of a www search engine using eye tracking. the goal is to gain insight into how users browse the presented abstracts and how they select links for further exploration. such understanding is valuable for improved interface design, as well as for more accurate interpretations of implicit feedback for machine learning. the following presents initial results, focusing on the amount of time spent viewing the presented abstracts, the total number of abstract viewed, as well as measures of how thoroughly searchers evaluate their results set. how do users interact with the list of ranked results of www search engines do they read the abstracts sequentially from top to bottom, or do they skip links how many of the results do users evaluate before clicking on a link or reformulating the search the answers to these questions will be beneficial in at least three ways. first, they provide the basis for improved interfaces. second, they suggest more targeted metrics for evaluating the retrieval performance in www search. and third, they help interpreting implicit feedback like clickthrough and reading times for machine learning of improved retrieval functions. in particular, better understanding of user behavior will allow us to draw more accurate inferences about how implicit feedback relates to relative relevance judgments. the following presents the results of an eye tracking study that we conducted. previous studies have analyzed directly observable data like query word frequency. however, unlike eyetracking, these measurements can at best give indirect evidence of how users perceive and respond to the search results. to the best of our knowledge, only one previous study has used eye tracking in the context of information retrieval evaluation. this study attempted to use eye movements to infer the relevancy of documents in the retrieval phase of an information search. the researchers linked relevancy judgments to increases in pupil diameter, as a larger diameter typically signifies high interest in the content matter. however, the sample size and search tasks in this experiment were not robust enough to generate predictable patterns of user search and scanning behavior, which is what our study is able to attain. it is crucial for query auto completion to accurately predict what a user is typing. given a query prefix and its context, conventional context aware approaches often produce relevant queries to the context. the purpose of this paper is to investigate the feasibility of exploiting the context to learn user reformulation behavior for boosting prediction performance. we first conduct an in depth analysis of how the users reformulate their queries. based on the analysis, we propose a supervised approach to query auto completion, where three kinds of reformulation related features are considered, including term level, query level and session level features. these features carefully capture how the users change preceding queries along the query sessions. extensive experiments have been conducted on the large scale query log of a commercial search engine. the experimental results demonstrate a significant improvement over competitive baselines. most of the modern search engines provide the query autocompletion service to help users formulate their queries while they are typing in search boxes. such service is especially helpful for mobile phone users because entering keywords on small touch screens is time consuming. prevalent methods permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. on query auto completion utilize search logs to generate suggested completions. for an input pre, the methods extract those queries with matched leading characters from the logs as candidates. could be short and ambiguous, the number of the candidates might be extremely large. existing methods mostly rank the candidates by their popularity in the logs. consequently, less popular queries are harder to predict. a possible solution to the ranking problem is to exploit query context such as previous queries and click through data in the same session. the idea is reasonable since disambiguating the usersearch intent by context has been studied extensively in query suggestion. query suggestion is closely related to query completion. the major di erence is that it has got no partial input, ie, the pre x. in general, if the outputs coming from query suggestion match the pre, they can also be regarded as the suggested completions. and its context, conventional context aware methods focus mainly on ranking the candidates according to the similarity or the dependency between the candidate and the given context. for example, some works rank the candidates based on their similarity to the recent queries or their occurrence with previous queries. some works model query dependencies along the sessions, by which the candidates whose contexts are more similar to the given context will be ranked higher. the work further clusters relevant queries with common click through data into concepts. query dependency is then transformed to concept dependency. ectiveness in producing relevant queries to the given context, query completion is required to accurately predict what the user is typing. take a query session obtained from a commercial search engine log as an example: stomach sounds. suppose the last query is the intended query. its preceding two queries serve as the context. many relevant queries are generated from conventional context aware metho ds such as colon cancer symptoms, celiac disease, and colon cancer in our experiments. although the suggested queries are conceptually relevant to the context, the problem becomes more challenging when the goal is to predict the actual query cramps stomach. some works take into account other factors for pursuing better approximation in query auto completion is hereinafter referred to as query completion for simplicity. a query session is a sequence of queries with the same information need. di erent from ours, they do not utilize the context information. one possible way to boost the prediction performance by context is carefully investigating how the users re ne their queries to get more satisfactory results. if the system gets to know the how, it would be capable of predicting the potential query the user might submit. in addition to query dependency, context information also demonstrates how the users reformulate their queries repeatedly throughout the search process. the way people reformulate their queries, or user reformulation behavior, has been studied as query reformulation strategy in previous work. there are two types of query reformulation strategies: semantic and syntactic relations. semantic relations such as generalization and speci cation originate from linguistics. syntactic relations refer to the statistical information about consecutive queries along the sessions such as term adding, term reuse and the change of query lengths. as the way to do reformulation expresses how one query is changed to another, to some extent it might be helpful in prediction for query completion. in the previous example, the user reuses stomach to form the last query. understanding user search behavior bene ts various applications, including query suggestion, query classi cation and web search result ranking. the work mines user reformulation activities for query suggestion but it does not consider the context information, which has been shown. though lots of work has studied query reformulation strategies, none of them model user reformulation behavior for query completion. as query reformulation strategies re ect how the users re ne their queries and query completion requires accurate prediction of what a user is typing, these motivate us to investigate the feasibility of exploiting the context to learn user reformulation behavior for boosting prediction performance of query completion. in this paper, we focus primarily on learning user reformulation behavior for query completion. we rst conduct an in depth analysis of how the users reformulate their queries. it is expected that if we know how one query is changed to another, we are able to predict the usernext intended query more accurately. and its context together with a set of candidate queries that start with the pre, the goal of this paper is learning to rank the candidate queries so that the top ranked queries are more likely to be the query the user is typing. in other words, we want to predict the intended query that has been partially entered by the user. the context includes previous queries and their chick through data in the same session. as syntactic relations can be easily identi ed by observing the queries themselves, our ranking model comprehensively considers various factors that are essential in query prediction, including termlevel, query level and session level syntactic features. extensive experiments have been conducted on the large scale query log collected from a commercial search engine. the experimental results reveal that the prediction performance can be signi cantly improved based on our model, compared to com in this paper, we focus on the ranking problem only. such improvement is consistent across di erent datasets with di erent session lengths. in the rest of this paper, we make a brief review on related work in section #. based on a statistical analysis on user reformulation behavior in section #, we describe our problem and approach to query completion in section #. the experimental results are presented in section #. finally, in section #, we give our discussions and conclusions. query suggestion or auto completion mechanisms are widely used by search engines and are increasingly attracting interest from the research community. however, the lack of commonly accepted evaluation methodology and metrics means that it is not possible to compare results and approaches from the literature. moreover, often the metrics used to evaluate query suggestions tend to be an adaptation from other domains without a proper justification. hence, it is not necessarily clear if the improvements reported in the literature would result in an actual improvement in the users experience. inspired by the cascade user models and state of the art evaluation metrics in the web search domain, we address the query suggestion evaluation, by first studying the users behaviour from a search engine query log and thereby deriving a new family of user models describing the users interaction with a query suggestion mechanism. next, assuming a query log based evaluation approach, we propose two new metrics to evaluate query suggestions, psaved and esaved. both metrics are parameterised by a user model. psaved is defined as the probability of using the query suggestions while submitting a query. esaved equates to the expected relative amount of effort a user can avoid due to the deployed query suggestion mechanism. finally, we experiment with both metrics using four user model instantiations as well as metrics previously used in the literature on a dataset of sessions. our results demonstrate that psaved and esaved show the best alignment with the users satisfaction amongst the considered metrics. the query suggestion mechanism within a search engine is a tool aimed to help users type less while submitting a query. in its most basic form, the list of suggested queries is formed to contain queries starting with the userinput as a pre x. how to perform this evaluation is the general topic addressed in this paper. the target of evaluation in information retrieval is to assess how well a retrieval system meets the needs of its users. for most web search evaluation experiments, this assessment is performed by means of measuring document ranking. ectiveness, ie, how highly relevant documents are ranked. a variety of the web search ranking evaluation approaches exist and they can be naturally divided into user based evaluation and system evaluation classes. the system evaluation paradigm originated from the cran eld experiments and is used in evaluation initiatives such as the text retrieval conference. the evaluation is performed using a test collection that consists of three components: a collection of documents, a collection of queries and a set of document relevance assessments, usually obtained through human judgements. as a result of the evaluation, each system is associated with a value of an evaluation metric, such as expected reciprocal rank and discounted cumulative gain. once the test collection is built, it can be repeatedly used to compare di erent retrieval systems or even adjust the system parameters in a learning to rank process. the results of evaluations are reproducible and easily interpretable. however, the system based evaluation approach has several drawbacks, as discussed by voorhees. firstly, due to the manual judgement process, it is hard to perform a large scale evaluation. taking that into account, special care must be taken while selecting a set of queries to be used for evaluation, since they should be representative of the users needs. this becomes even more complicated as these needs and users intentions behind queries tend to change and evolve over time. apart from that, the actual users needs expressed by a query can be signi cantly di erent from those a judge can think of and this results in additional noise in the evaluations. for instance, abtesting and interleaving are examples of the former approach. ine user based evaluation is to use the implicit user feedback observed in historical query logs as a substitute for the human judges. for instance, in the work of bennett et al, the document that received the last satis ed click of the user was labelled as personally relevant to this user. once the labels are generated, the test collection obtained has little di erences from the system based evaluation test collections. ectiveness metrics, eg, mean reciprocal rank or mean average precision used by collins thompson et al. ine user based evaluation is particularly promising for evaluation in the query suggestion domain and we support this with the arguments in section #. indeed, a possible way to ensure alignment is to design the metric on top of the realistic model of the user behaviour. for evaluation in the web search domain, expected reciprocal rank and expected browsing utility are examples of user model inspired. unfortunately, to the best of our knowledge in the query suggestions domain neither a realistic model of the user interaction with the system nor an appropriate. ectiveness metric based on this model were proposed. often, these metrics are selected without justifying their alignment with the user satisfaction, hence it is not necessarily clear if the improvements reported would result in an actual improvement in the users experience. in this paper, we address these gaps and consider our contribution to be four fold: we study the ways users interact with the query suggestion mechanism, as it is observed in the session logs, and propose a simple cascade model of the user behaviour on top of these observations; we introduce an algorithm to learn the parameters of the model; we propose a family of. ectiveness metrics, called saved parametrised by a user model and study several possible instantiations of these metrics; we perform a thorough experimental study of the considered user models and evaluation metrics, as well as the evaluation metrics used in the literature. the remainder of this paper is organised as follows. after reviewing some related work in section #, we study the ways that users interact with a query suggestion mechanism and propose a simple user model in section #. in section #, we introduce an algorithm to learn the parameters of the user model from the session log. discuss the considered evaluation framework in section #. further, we introduce a novel family of evaluation metrics, called saved, and discuss its connection to other metrics used in information retrieval in section #. section # describes a methodology we use to compare the proposed user models and metrics. we report and discuss the experimental results in section #. finally, we close the paper with some conclusions and future work discussion in section #. similar to the evaluation of other information retrieval systems, the evaluation of query suggestions is crucial to ensure progress in this area of research. on the other hand, a user based evaluation can be performed in either online or. however, in order to follow this methodology an appropriate metric must be selected. ectiveness metric should be aligned with the user preferences, ie, favour a system with higher level of user satisfaction. as we will discuss in the next section, a variety of evaluation metrics are used in the query suggestion literature. the dataset used in our experiments is presented in section #. we consider a search task as a set of queries that serve the same user information need. analyzing search tasks from user query streams plays an important role in building a set of modern tools to improve search engine performance. in this paper, we propose a probabilistic method for identifying and labeling search tasks based on the following intuitive observations: queries that are issued temporally close by users in many sequences of queries are likely to belong to the same search task, meanwhile, different users having the same information needs tend to submit topically coherent search queries. to capture the above intuitions, we directly model query temporal patterns using a special class of point processes called hawkes processes, and combine topic models with hawkes processes for simultaneously identifying and labeling search tasks. essentially, hawkes processes utilize their self exciting properties to identify search tasks if influence exists among a sequence of queries for individual users, while the topic model exploits query co occurrence across different users to discover the latent information needed for labeling search tasks. more importantly, there is mutual reinforcement between hawkes processes and the topic model in the unified model that enhances the performance of both. we evaluate our method based on both synthetic data and real world query log data. in addition, we also apply our model to query clustering and search task identification. by comparing with state of the art methods, the results demonstrate that the improvement in our proposed approach is consistent and promising. nowadays, search engines have become the most important and indispensable web portal, whereby people pursue a wide rangeof permission to make digital or hard copies of all or part of thiswork for personal or classroom use is granted without fee provided that copies arenot made or distributed for pro. or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than acm must be honored. tocopy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci. searches in order to satisfy a variety of information needs to better understand users information needs and search behaviors, one important research direction is to detect and split users temporal sequences of queries into disjoint query sessions, which are often de ned as a sequence of queries issued within. xed period of time, ranging from to minutes. however, a usersingle session may contain queries with multiple intents, or consist ofseeking information on single or multiple topics. going beyond a search session, a search task, which is de ned as a set of queries serving for the same information need, has been recognized as a more suitable atomic unit than a single query or session, not only for better modeling user search intent but also for improving other downstream search engines applications, such as query suggestion and personalized search. additionally, analyzing the formation of search tasks also deepens our understanding of the simultaneous temporal diffusion of multiple memes, ie, intents or ideas, in social networks. therefore, how to effectively identify and label search tasks becomes an interesting and challenging problem. recently, there have been attempts to extract in session tasks, and cross session tasks from sequences of queries. they build clustering or classi cation methods to identify tasks based on time splitting, lexicon similarity, and queryreformulation patterns. even though the temporal submission patterns in query sequences carry valuable information for mining search tasks, those existing methods only use them for either simplysplitting sequences of queries into temporally demarcated sessions, or transforming them as features among queries. we believe that by directly modeling temporal information as part of extracting search tasks in a richer way, we can substantially improve search task mining. another key drawback of those existing methods is that most of them focus only on the query sequences of individual users instead of considering the wholequery log. only very recently, there has been an attempt which triesto take advantage of the collective intelligence of many users for discovering tasks. it is obvious that different users may have the same information need, and share the same search task, thus modeling query sequences across different users will be very valuable for capturing semantically similar search tasks in a global context. generally, two consecutive queries issued by a user are more likely to belong to the same search task than two non contiguous queries, but that is not necessary always the case. for example, as shown in figure #, the consecutive queries autotrader and ya hoo autos issued by user ua belong to the same search task, while the consecutive queries yahoo autos and wells fargo belong to two separate search tasks. another complicated case is that two non contiguous queries may belong to the same or different tasks figure #: an illustration of relationship between consecutive queries and search tasks. every circle represents a query issued by a user at time tn the blue arrow line indicates an in uence exists between queries. a set of queries linked by blue lines denotes a search task, and some topically coherent search tasks across three users are labeled by different colors. as well, eg, verizon wireless and autotrader issued byuser ua belong to two different search tasks, but yahoo autos and kbb cars issued by users ub belong to the same search task. these examples show that in reality we cannot simply rely on the time splitting or an individual user behavior for identifying search tasks. it makes more sense to take into account the explicit temporalinformation of query sequences exhibited by many different users in the whole query logs. the basic intuition is that if two consecutive or temporally close queries are issued many times by thesame user or many others users, it is more likely these two queries are semantically related to each other, ie, belong to the same search task. as we can see, the consecutive queries wells fargo and bank of america are issued by both ua and ub, while the consecutive queries yahoo autos and wells fargo are only issued by user ua therefore, according to the above intuition, wells fargo and bank of america are more likely to belong to the same search task compared with yahoo auto and wells fargo. similarly, for non contiguous queries, autotrader and kbb cars are issued temporally very closeby both ub and uc, which indicates they have higher chance of belonging to the same search task. moreover, different users may engage in different search patterns, eg, user ub searches more frequently than user ua, which indicates how likely the search tasks may change within a certain time period for different users, and then they should be treated differently based on their searchactivities. all in all, we choose to identify search tasks by leveraging the temporally weighted query co occurrence this not only guarantees sound performance by making full use of both textual and temporal information of the entire query sequences, but also enables the labeling of the identi ed search tasks since semantically related queries are clustered together through query links determined by co occurrence. to model temporally close query co occurrences, we turn to latent dirichlet allocation, one powerful graphicalmodel that exploits co occurrence patterns of queries in query sequences. existing temporal lda models learned distinguishedtopic distributions from temporal fragments of data, while ignored query co occurrence across different fragments, thus failed to make full use of the temporal information. recently, some spatial ldamodels encouraged queries that are very close in space to share similar topic distributions, ie, weighing the reliability of query co occurrence based on spatial closeness. however, there exists no uniform standards for measuring such closeness across different instances, especially in temporal data. our research, on theother hand, considers making full use of temporal information by weighing the reliability of each co occurrence of a pair of queriesbased on how likely an in uence exists between this pair of queries. here we de ne query in uence as: the occurrence of one query raises the probability that the other query will be issued in the near future. in uence, rather than closeness, enables us to distinguish temporally close query co occurrence from temporally regular query cooccurrence for each user based on his own propensity of query submission. the intensity function of hawkes includes a base intensity, along with a positive in uence of the past events on the current one. such a positive in uence isoriginated from the self exciting property that the occurrence of one event in the past increases the probability of events happening in the future. we nd that hawkesself exciting property coincides with the concept of in uence in our situation, and its base intensity captures the personal propensity. thus we employ hawkes processes to fully utilize temporal information in query sequences for identifying the existence of query in uence. from the perspective of hawkes processes, in uence generally exists between temporally close queries. however, for an observed query sequence, not all temporally close query pairs have the actual in uence in between, since in some cases the occurrence of the later queries may result from the base intensity rather than selfexciting property. nd it intractable to obtain an optimal solution of in uence existence based on temporal information only. last but the most important, it is unable to directly identify search tasks by either generating topics based on query co occurrence using lda, or estimating all in uence candidates by hawkes. to address the above issues, we concentrate on the in uence existence between semantically related queries, whose estimation can be simpli ed by the joint efforts of lda and hawkes and enable a direct identi cation of search tasks. according to the above intuition, a search task can be viewed as asequence of semantically related queries linked by in uence a query that does not satisfy userinformation need will self excite the submission of another semantically related query in the near future. on the other hand, a query rarely excites the submissionof another semantically unrelated query even if their timestamps are very close. thus we believe that those semantic in uence are the in uence that actually take effect, and our paper solves search task identi cation directly by identifying those in uence to limit the solution space of such in uence, we cast both in uence existence and query topic membership into latent variables, and identify the existence probability of pairwise in uence with the similarity of the memberships of associated two queries. this identi cation works as a bridge between lda and hawkes processes, as lda assigns high in uence quali ed co occurred queries to the same topic, while query co occurrence frequency narrows the solution space of in uence in this way, lda and hawkes mutually bene. each other in identifying search tasks using both temporaland textual information. to this end, we propose a probabilisticmodel that incorporates this equalization to combine the lda modelwith hawkes processes, and develop a mean eld variational inference algorithm to estimate the in uence by optimizing the data likelihood. we evaluate our method on synthetic data, and also applyit to mine search tasks in both aol and yahoo query log data. experimental results show that the proposed method can achieve signi cantly better performance than existing state of the art methods. in a nutshell, our major contributions include: we cast search task identi cation into the problem of identify semantic in uence in observed query sequences, and propose a probabilistic model by combining lda model with hawkes processes to address the problem. most importantly, there is mutual reinforcement between hawkes processes and the topic model in the uni ed model that enhances the performance of both. we employ hawkes processes to directly model temporal information as part of search taskidenti cation, which has never been explicitly exploited in the existing works. the rest of the paper is organized as follows. we rst introduce hawkes processes, and the proposed model by combining lda with hawkes processes in section #. in section #, we develop a fast mean eld variational inference algorithm for the resulting optimization problem. we then describe and report the experimental results in section #. finally, we introduce the related work in section #, and present our conclusions and future work in section. query auto completion facilitates faster user query input by predicting users intended queries. most qac algorithms take a learning based approach to incorporate various signals for query relevance prediction. however, such models are trained on simulat ed user inputs from query log data. the lack of real user interaction data in the qac process prevents them from further improving the qac performance. in this work, for the first time we collect a high resolution qac query log that records every keystroke in a qac session. based on this data, we discover two user behaviors, namely the horizontal skipping bias and vertical position bias which are crucial for rele vance prediction in qac. in order to better explain them, we pro pose a novel two dimensional click model for modeling the qac process with emphasis on these behaviors. extensive experiments on our qac data set from both pc and mobile devices demonstrate that our proposed model can accurate ly explain the users behaviors in interacting with a qac system, and the resulting relevance model significant improves the qac performance over existing click models. furthermore, the learned knowledge about the skipping behavior can be effectively incorpo rated into existing learning based models to further improve their performance. query auto completion is one of the most important components of a modern web search engine which facilitates faster user query input by predicting the users intended queries. it is offered by most of the search engines, commerce portals and major browsers. with the prevalence of mobile devices, it becomes more critical because typing takes more effort in mobile devices than in pcs. previous studies addressed the qac problem in different perspectives, ranging from designing more ef cient indexes and algorithms, leveraging context in long term and short term query history, learning to combine more personalized signals such as gender, age and location, suggesting queries from a mis spelled pre. the query auto completion process starts when a user enters the rst character into the search box. after that, she goes through a series of interactions with the qac engine until she clicks on an intended query. such interactions include examining the suggested results, continuing to type, and clicking on a query in the list. although previous approaches model the relevance ranking with many features, these models are usually only trained on the nal submitted queries, ignoring the entire user interactions from the rst character she types to the nal query she has clicked. one dif culty for improving the qac quality is the lack of data about ne grain user interactions in qac. recent work has attempted to leverage all pre xes of a submitted query. however the only available data is the submitted query; while the pre xes are simulated from all possible pre xes of the query. lack of associated information, such as the suggested list, user typing speed and other real user interactions, prevents such methods from further improving their performance. for this purpose, we have collected a high resolution qac dataset from both pc and mobile phones, in which each keystroke of a user and the system response are recorded. as far as we know, this is the rst dataset with this level of resolution speci cally for qac. extensive studies have already demonstrated the importance of query log for web document retrieval. therefore, it is reasonable to believe this new kind of qac log could potentially enable a full spectrum of researches for qac, such as user behavior analysis, relevance ranking, interactive system design for qac, just to name a few. given many possibilities for mining this new data, in this paper we focus on leveraging it for understanding users behavior in qac. speci cally based on our qac log, we have observed a phenomenon that in qac users frequently skip several suggestion lists, even though such lists contain the nal submitted query. the exact reason why this happens and how frequent it happens is largely unknown. besides, we also observed that most of the clicked queries are concentrated at top positions. better understanding of these behaviors has a strong implication to the relevance modeling. for instance, we assume that a user does not click a suggested query due to the lack of relevance; however the skipping behavior complicates this hypothesis. so, if we know the positions where such skipping behavior happens, we could improve the candidate ranking in qac by taking into account the examples in the positions where they are more likely to be examined. despite its importance, little research has been done in explaining such behaviors. the qac process shares similarities with the web document retrieval: in qac people look for intended queries with a pre, while in document retrieval people look for relevant documents with a query. and in document retrieval, click models are widely used to model the users examination and clicking behavior. thus we could potentially adopt an existing click model to shed light on the qac user behavior. however, there are major differences between the qac process and document retrieval. for example, in document retrieval a user usually examines one result page before she lands on a click, while in qac she usually types in a series of pre xes and examines multiple lists of suggestions before landing on a click. due to these differences, most current click models are not applicable to the qac problem without signi cant modi cation. therefore in this paper we propose a novel two dimensional click model for understanding the user behaviors in qac. this click model is consisted of a horizontal component that captures the skipping behavior, a vertical component that depicts the vertical examination bias, and a relevance model that re ects the intrinsic relevance between a pre. we have performed a set of experiments on our qac datasets from pc and mobile phones. results show that our proposed model can effectively model the user behavior in qac. the resulting relevance model signi cant improves the qac performance over existing click models. we also show that the learned knowledge about users behavior, especially the probability of skipping a column of suggestion candidates, could serve as labeling information to improve the performance of existing learning based approaches. furthermore, with the learned model we demonstrated some interesting insights of the user behaviors in qac on both platforms. we summarize our contributions as follows: we have collected the rst set of high resolution query log speci. for the qac process, which could enable many studies on deeper understanding of qac. based on the new qac log, we analyze two important types of user behavior in qac, namely the horizontal skipping bias and vertical position bias. the horizontal skipping bias is unique to qac and is formally introduced here for the rst time. we propose a novel two dimensional click model to model these types of user behavior. our model outperforms the state of the art click models on relevance ranking. we also utilize our model to derive interesting insights about the qac user behavior on pc and mobile devices. a fundamental challenge in utilizing web search click data is to infer user perceived relevance from the search log. not only is the inference a difficult problem involving statistical reasonings but the bulky size, together with the ever increasing nature, of the log data imposes extra requirements on scalability. in this paper, we propose the bayesian browsing model, which performs exact inference of the document relevance, only requires a single pass of the data, and is shown effective. we present two sets of experiments to evaluate the model effectiveness and scalability. on the first set of over million search instances of million distinct queries, bbm outperforms the state of the art competitor by in log likelihood while being times faster. on the second click log set, spanning a quarter of petabyte, we showcase the scalability of bbm: we implemented it on a commercial mapreduce cluster, and it took only hours to compute the relevance for billion distinct query url pairs. such click log constitutes an invaluable source of user feedbacks, which can be leveraged in many search related applications, for example, query recommendation, learning to rank, and personalized search, just to name a few. a central question in click log analysis is to infer the user perceived relevance for each query url pair. as such information is not available in the log, we need to rst understand how users scan and examine search results and make decisions on clicks. thanks to previous exploration in this direction, it is well known that a number of factors should be considered for an accurate interpretation of user clicks, such as the position bias of examination and clicks, and dependency between clicks over different url snippets in the same search result page. quite a few statistical models have been recently developed to leverage user clicks for inferring user perceived relevance, some representatives of which are the cascade model, dependent click model, and user browsing model. we will review them with details in section #. in practice, it is common to have tens or even hundreds of terabytes log data accumulated by search engine servers every day. this data stream nature of click logs imposes two computational challenges for click modeling. therefore, considering the ever growing search volume, we would expect a practical click model to be single pass, which minimizes the storage and io cost for revisiting historical data. existing models, despite of their different user behavior assumptions, all follow the point estimation philosophy: the estimated relevance for each query url pair is a single number normalized to, and a larger value indicates stronger relevance. while this single number estimate could suf ce for many usages, it nevertheless falls short of the capacity for broader applications, for example, it is unspeci ed how to estimate the probability that url ui is preferred to uj for the same query while their relevance estimates are represented by two numbers ri and rj respectively. existing learning to rank algorithms derive pairwise preference relationship either from human ratings or from certain heuristics. and to the best of our knowledge, no principled approach has been proposed to compute the preference probability between any pair of urls, that is, the probability of one url is more relevant to the query than the other one, which could support broader applications in both web search and online advertising. in this article, we propose the bayesian browsing model, which builds upon the state of the art user browsing model and scales to petabytescale data with easy, incremental update algorithms. by virtue of the bayesian approach to modeling the url relevance, preference probabilities could be derived based on inferred url relevance posteriors. moreover, we present an exact inference algorithm for bbm that exploits the particular probabilistic dependency in the model, and reveals the relevance posterior in closed form after a single pass over the log data. in summary, we make the following contributions in this study. we propose bbm, together with an exact inference algorithm which derives the relevance posterior in closed form, and facilitates single pass, incremen tal computation. bbm is both ef fective and ef cient. we implement bbm on a commercial mapreduce cluster, and it takes only hours to compute relevance for more than billion query url pairs in a click log of petabyte. the rest of this article is organized as follows. section # introduces preliminaries and discusses previous click models. we elaborate on bbm and relevant algorithms for inference and parameter estimations in section #, and discuss how to parallelize them on mapreduce clusters in section #. performance comparison results are reported in section #, and the petabyte scale experiment is presented in section #. web search has become indispensable for everyday life: questions ranging from navigating to msn com to how to bleach wine stains are all directed to search engines. the task could be as trivial as computing the click through rate, should the search engine know which url snippets are examined by the user in addition to those clicked. first, the scalability: a click model must scale comfortably to terabyte scale or even petabyte scale data. even better, we would expect the modeling to be effectively parallelizable for better scalability. second, the capability to be incrementally updated: in order to keep sync with the evolving world wide web, a click model, and its relevance estimate thereof, has to be regularly and ef ciently updated. we compare bbm with a state of the art model in a real world dataset of over million search instances for million distinct queries. we discuss related work in section #, and conclude this study in section #. while responding to users information needs, search engines also log down the interaction with users, of which a typical form is what urls are presented in search results and which of them are clicked. search engine advertising has become a significant element of the web browsing experience. choosing the right ads for the query and the order in which they are displayed greatly affects the probability that a user will see and click on each ad. this ranking has a strong impact on the revenue the search engine receives from the ads. further, showing the user an ad that they prefer to click on improves user satisfaction. for these reasons, it is important to be able to accurately estimate the click through rate of ads in the system. for ads that have been displayed repeatedly, this is empirically measurable, but for new ads, other means must be used. we show that we can use features of ads, terms, and advertisers to learn a model that accurately predicts the click though rate for new ads. we also show that using our model improves the convergence and performance of an advertising system. as a result, our model increases both revenue and user satisfaction. most major search engines today are funded through textual advertising placed next to their search results. the market for these search advertisements has exploded in the last decade to billion, and is expected to double again by year#. the most notable example is google, which earned billion in revenue for the third quarter of year# from search advertising alone. though there are many forms of online advertising, in this paper we will restrict ourselves to the most common model: pay per copyright is held by the international world wide web conference committee. distribution of these papers is limited to classroom use, and personal use by others. performance with a cost per click billing, which means the search engine is paid every time the ad is clicked by a user. google, yahoo, and microsoft all primarily use this model. to maximize revenue and user satisfaction, pay per performance systems must predict the expected user behavior for each displayed advertisement and must maximize the expectation that a user will act on it. the search system can make expected user behavior predictions based on historical click through performance of the ad. for example, if an ad has been displayed times in the past, and has received clicks, then the system could estimate its click through rate to be. this estimate, however, has very high variance, and may only reasonably be applied to ads that have been shown many times. this poses a particular problem when a new ad enters the system. a new ad has no historical information, so its expected click through rate is completely unknown. in this paper, we address the problem of estimating the probability that an ad will be clicked on, for newly created ads and advertising accounts. we show that we can use information about the ad itself, the page the ad points to, and statistics of related ads, to build a model that reasonably predicts the future ctr of that ad. query auto completion is one of the most prominent features of modern search engines. the list of query candidates is generated according to the prefix entered by the user in the search box and is updated on each new key stroke. query prefixes tend to be short and ambiguous, and existing models mostly rely on the past popularity of matching candidates for ranking. however, the popularity of certain queries may vary drastically across different demographics and users. for instance, while instagram and imdb have comparable popularities overall and are both legitimate candidates to show for prefix, the former is noticeably more popular among young female users, and the latter is more likely to be issued by men. in this paper, we present a supervised framework for personalizing auto completion ranking. we introduce a novel labelling strategy for generating offline training labels that can be used for learning personalized rankers. we compare the effectiveness of several user specific and demographic based features and show that among them, the user long term search history and location are the most effective for personalizing auto completion rankers. we perform our experiments on the publicly available aol query logs, and also on the larger scale logs of bing. the results suggest that supervised rankers enhanced by personalization features can significantly outperform the existing popularity based base lines, in terms of mean reciprocal rank by up to. the two exceptions are the work by bar yossef and kraus, and weber and castillo. the query frequencies of instagram and imdb according to google trends. all four suggestions are popular queries with comparable historical frequencies. auto completion is among the rst services that the users interact with as they search and form their queries. following each new character entered in the query box, search engines lter suggestions that match the updated pre, and suggest the top ranked candidates to the user. the rst step is often facilitated by using data structures such as pre. in the second step, those suggestions that match the pre. the likelihood values are often approximated with respect to aggregated past frequencies although other approaches that rank suggestions according to their predicted future popularities have been also explored. in majority of previous work, the likelihood of qac suggestions are computed globally and are considered to be the same for all users. hence for a given pre, all users are presented with the same set of suggestions. bar yossef and kraus added a session bias parameter in auto completion ranking by comparing candidates with the queries recently submitted by the user. however, the notion of likelihood for a query does not vary across users, or demographic groups, plus their work is not applicable on single query sessions that account for no less than of the search tra. weber and castillo discussed the di erences in query distributions across various demographics and brie. covered query completion by focusing on predicting the second query term. in essence, they build a conditional probabilistic model for common phrases based on a set of demographic features. their model is based on simple aggregation over di erent demographics and does not address the sparsity issues as more features are added. weber and castillo do not consider any user speci. feature, and do not report the results for more general scenarios where only the rst few characters of queries are entered. in this paper, we propose a novel supervised framework for learning to personalize auto completion rankings. we are motivated by the previous studies that demonstrated that the query likelihoods vary drastically between di erent demographic groups and individual users. inspired by these observations we develop several features based on users age, gender, location, short and long history for personalizing auto completion figure #: the default auto completion candidates for pre. according to the us market version of google com. was typed in private browsing mode and the snapshot was taken on wed, jan, year#. for instance, consider the auto completion candidates returned by google for pre. in particular, the frequency distributions for instagram, and imdb are demonstrated in figure # according to google trends the depicted trends suggest similar likelihoods for both imdb and instagram, although the popularity of the latter is rising. in general, in the absence of any information about the user, the ranking of qac candidates in figure # look reasonable although models based on temporal query frequency trends may boost the position of instagram. the question we are addressing in this work, is how this ranking can be further improved if there are some additional information available about the user. figure # compares the likelihood of instagram and imdb among di erent demographics of users according to yahoo clues at the bottom of figure # the same analysis is repeated using the query logs of bing search engine which we use as one of the testbeds in our experiments. the overall trends are remarkably similar; instagram is mostly popular among young female users below the age of. in contrast, the query imdb is issued more often by male users particularly those between the age of to. hence, going back to figure #, if we knew that the person issuing the query was an under female user, perhaps the original order of qac candidates could be improved by boosting instagram. then again, if the previous query submitted by the user in the session was about ipad covers boosting ipad could be possibly better. we investigate how such additional information can be used in a supervised framework for personalizing autocompletion. to train personalized auto completion rankers, we introduce a new strategy for generating training labels from previous queries in the logs. our experiments on two large scale query logs suggest that integrating demographic and personalized features can signi cantly improve the effectiveness of auto completion. the remainder of this paper is organized as follows; we continue by covering the related work in section #. was submitted to the us market version of google com on january, year#, in private browsing mode. http: www google com trends http: clues yahoo com, discontinued in march year# figure #: the likelihood of instagram and imdb in queries submitted by di erent demographics according to yahoo clues. the likelihood of instagram and imdb in queries submitted by the logged in users of bing. framework for personalized auto completion is described in section #. section # discusses our testbed data, and the features used for personalization. the evaluation results are presented in section #. finally, we conclude in section # and suggest a few directions for future work. query auto completion is a common feature in modern search engines. high quality qac candidates enhance search experience by saving users time that otherwise would be spent on typing each character or word sequentially. current qac methods rank suggestions according to their past popularity. however, query popularity changes over time, and the ranking of candidates must be adjusted accordingly. for instance, while halloween might be the right suggestion after typing ha in october, harry potter might be better any other time. surprisingly, despite the importance of qac as a key feature in most online search engines, its temporal dynamics have been under studied. in this paper, we propose a time sensitive approach for query auto completion. instead of ranking candidates according to their past popularity, we apply time series and rank candidates according their forecasted frequencies. our experiments on queries and their daily frequencies sampled over a period of years show that predicting the popularity of queries solely based on their past frequency can be misleading, and the forecasts obtained by time series modeling are substantially more reliable. our results also suggest that modeling the temporal trends of queries can significantly improve the ranking of qac candidates. query auto completion is a feature incorporated in essentially every search engine, where the goal is to save permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. kira radinsky technion, israel institute of technology haifa, israel kirar cs technion ac il user time by predicting userintent and suggesting possible other queries matching the rst few keystrokes typed. the ltering of related candidates is typically based on string matching, and the ranking is ideally based on the likelihood of the ltered candidate being the query which the user has in mind. the latter challenge is the focus of this paper, for which the goal is to sort queries according to their expected popularity. the common practice is to use past frequencies as proxy for future popularity. however, those approaches assume that user intent is static and does not change over time. simple aggregation can overshadow the temporal trends that could potentially provide valuable signals for better ordering of qac candidates. consider the example in figure # where a user has typed di in google query box on sunday, february th, year#. at the rst glance, knowing that dictionary is generally a more frequent query than disney, it might be di cult to notice how the ranking might be improved. however, looking at the daily trends for these queries in figure # reveals that disney is more popular on weekends. hence, given that the rst snapshot was taken on a sunday, swapping disney and dictionary couldleadtoabetterrankingatthetimeofthis query. similar observations can be made for di erent granularity of time spans. for instance, previous work has shown that users are more likely to search for entertainmentrelated queries at night, while queries related to personal nance are more common in the morning. therefore, the ranking of qac candidates can vary dynamically even according to the time of the day. in this paper, we add time sensitivity to ranking autocompletion candidates. we consider the temporal variations of query popularity in ranking qac suggestions, that are normally shadowed by aggregation. rather than summarizing the entire query history in one aggregated number as expected popularity, we rely on shorter but more frequent aggregation of data, and model the overall query trends by time series. we show that the expected popularity values produced by our time sensitive approaches are closer approximations to what will be observed later in the logs, and are more. the contributions of this work are three fold: first, we study the shadowing problem that is caused by ignoring the temporal variations of query frequency in data aggregation. we investigate several aggregation options and show that query popularity predictions based on shorter but more recent data are more accurate than those produced based on aggregation over longer periods. this simple and counter figure #: google auto completion candidates after typing di on sunday, february th, year#. figure #: daily frequencies for queries dictionary and disney during january year# according to google trends. among the two queries, disney is more popular on weekends, while dictionary is issued more commonly by users on weekdays. intuitive nding, can be useful in scenarios where applying more sophisticated time series models is feasible. second, we introduce a time sensitive query auto completion model in which the expected popularity of each query suggestion is forecasted by time series, and dynamically varies depending on the time of query. finally, we perform a large scale analysis over years of sampled log data, and evaluate our predictions on daily and monthly intervals. our experimental results indicate that accurate query popularity forecasts are essential in generating high quality qac rankings. query auto completion is a common interactive feature that assists users in formulating queries by providing completion suggestions as they type. in order for qac to minimise the user cognitive and physical effort, it must: suggest the user intended query after minimal input keystrokes, and rank the user intended query highly in completion suggestions. typically, qac approaches rank completion suggestions by their past popularity. accordingly, qac is usually very effective for previously seen and consistently popular queries. users are increasingly turning to search engines to find out about unpredictable emerging and ongoing events and phenomena, often using previously unseen or unpopular queries. consequently, qac must be both robust and time sensitive that is, able to sufficiently rank both consistently and recently popular queries in completion suggestions. to address this trade off, we propose several practical completion suggestion ranking approaches, including: a sliding window of query popularity evidence from the past days, the query popularity distribution in the lastqueries observed with a given prefix, and short range query popularity prediction based on recently observed trends. using real time simulation experiments, we extensively investigated the parameters necessary to maximise qac effectiveness for three openly available query log datasets with prefixes of characters: msn and aol, and sogou year#. optimal parameters vary for each query log, capturing the differing temporal dynamics and querying distributions. results demonstrate consistent and language independent improvements of up to over a non temporal qac baseline for all query logs with prefix lengths of characters. this work is an important step towards more effective qac approaches. web searching is a ubiquitous activity performed by millions of users daily. however, cognitively formulating and physically typing search queries is a time consuming and error prone process. spelling mistakes, forgetfulness and information need uncertainty often make textual query input laborious. in response, search engines have widely adopted query auto completion as a means of reducing the effort required to submit a query. as the user types their query into the search box, qac suggests possible queries the user may have in mind, beginning with the currently input character sequence. the primary objective for effective qac is to present the userintended query after the fewest possible keystrokes, and at the highest rank in the list of completion suggestions. the most common approach to qac is to extract past queries with each pre. from a query log, and rank them by their past popularity; this assumes current query popularity is the same as past query popularity. although this approach provides satisfactory qac on average, it is far from optimal since it fails to take into account clues such as time or user context which often in uence the queries most likely to be typed. as a result, this paper is most concerned with qac approaches which are sensitive to changing query popularity where the popularity is not predictable from long term past query popularity observations. as the web increasingly becomes a platform for real time news and media, time plays a central role in information interaction. a substantial proportion of the daily query volume is the result of users turning to search engines for recent information about new or ongoing events. indeed, of daily google queries have not been seen in the past days. while the long tail will inevitably account for a large proportion of these queries, many will be the result of short term temporal events. figure # shows the four completion suggestions offered by google for the single character query pre. the list of completion suggestions indicates the historically most likely queries to be submitted with the pre, possibly in the context of some undisclosed ranking features such as geo location. despite the recency and prominence of the kenya westgate mall terrorist attacks, the query kenya ranks very low in the completion suggestions. in figure # we show the dramatic change in query popularity caused by the events kenya becomes by far the most popular query. yet, despite the fact that kenya is trending because of the ongoing events, googleqac fails to support users searching for information about the event as it ranks completion suggestions based on the past query distribution which is www, april, year#, seoul, korea. http: googleblog blogspot co uk year# http: dx doi org year#. this week in search year# html figure #: google auto completion suggestions for the query pre. screenshot taken september rd year#, during the ongoing westgate shopping mall terrorist attack in kenya. further compounding this issue, qac for short pre xes is often unsuccessful as there are such a large number of possible completion suggestions. it is typically consistently popular head queries that are provided as completion suggestions for such short pre xes. therefore, there is a need to take the temporal aspect into account for effective qac. this work is an attempt towards this objective. furthermore, queries that need to be included in completion suggestions fall into two main categories. the rst category corresponds to predictably popular queries which are: consistently popular, temporally recurring or known foreseeable events and phenomena. the second category corresponds to unpredictably popular queries related to entirely unforeseeable current events and phenomena. indeed, queries are likely to switch between these categories over time, making longer term predictions problematic. therefore, achieving optimal qac effectiveness for all users, on average, is a trade off between opposing objectives: time sensitivity, or recency, and robustness. recency requires that completion suggestions include emerging and increasingly popular queries. conversely, robustness requires that completion suggestions also reliably include long term and consistently popular queries. these two goals are at odds completion suggestions comprised only of short term popular queries might lead to lower ranking of many consistently popular queries. alternatively, completion suggestions comprised of long term popular queries will likely exclude the most recently popular queries. we propose an approach to address this trade off by developing models which take recent evidence into account when possible. lengths, insight into the qac approach parameters necessary to obtain optimal qac effectiveness in different settings, and reproducible experimental ndings due to the open datasets we study. the source code used to produce the datasets and results in this paper is openly available at http: www stewh com recent robust qac figure #: google search trends indicating temporal popularity of the completion suggestions in figure # during september year#. motivation large scale news, events and world phenomena undoubtedly play a central role in collective searching behaviour, and in turn dramatically affect query popularity over time. while relying on a long period of past query log evidence to rank completion suggestions will ensure qac is robust for consistently popular queries, it will also have the effect of smoothing over sensitivity to recently popular queries. for example, imagine a scenario in which query is consistently popular, occurring, times every day in the query log. aggregating query popularity over the past day period would mean that query, which became popular today, would need to occur, times before it outweighed the long term popular query despite the fact it is far more popular today than. at the same time, reducing the aggregation period risks not fully representing the long term query popularity, allowing arbitrary temporal uctuations to reduce its ranking. in past work we studied the effectiveness of using a shorter query log aggregation period for qac. in this paper, we propose novel qac approaches which address the recency robustness trade off to combine both recent and robust queries in completion suggestions. rather than relying passively on previously observed query popularity distributions for qac, there is an opportunity to improve qac over time by anticipating the current query popularity distribution based on previously observed trends and better rank completion suggestions at each moment in time. as a result of this, in the past, long term time series modelling for query trends has been used to improve qac for predictably recurring temporal query trends, such as seasonal or weekday weekend related queries. however, long term time series modelling can break down for many temporal events that are not always constant. for example, holidays and natural phenomena such as the weather seasons occur on an indeterminate schedule, where rigid temporal modelling of such queries might result in increased popularity prediction at incorrect times. furthermore, for previously unseen and therefore unpredictable recent queries, time series modelling often proved problematic due to lag and over tting. this is due to the fact that time series models often struggled to model increasing trends quickly, and likewise, continued to predict increased popularity for some time after brief periods of popularity. aside from the prediction issues of long term time series modelling, we are motivated to explore alternative approaches since many qac systems will not have long term and in depth past query logs available for robust modelling. therefore, many organisations may only have more recent smaller query logs available, yet still need to bootstrap effective qac systems with far less data and resources. as such, in this work we take a conceptually different approach to anticipating current query popularity for effective completion suggestion ranking: relying only on recent query popularity evidence, and short range query popularity prediction based on recently observed trends. in section # we present work related to qac effectiveness. section # outlines existing approaches to qac, and proposes novel approaches for qac. in section # we describe our experimental setup and procedure, including query log datasets and real time simulation methodology. in section # we present experimental qac approach results, which we discuss with regard to the research questions in section #. finally, in section # we make conclusions and recommend further work. recent advances in search users click modeling consider both users search queries and click skip behavior on documents to infer the user perceived relevance. tcm characterizes user behavior related to a task as a collective whole. using these biases, tcm is more accurately able to capture user search behavior. most of these models, including dynamic bayesian networks and user browsing models, use probabilistic models to understand user click behavior based on individual queries. the user behavior is more complex when her actions to satisfy her information needs form a search session, which may include multiple queries and subsequent click behaviors on various items on search result pages. previous research is limited to treating each query within a search session in isolation, without paying attention to their dynamic interactions with other queries in a search session. investigating this problem, we consider the sequence of queries and their clicks in a search session as a task and propose a task centric click model. specifically, we identify and consider two new biases in tcm as the basis for user modeling. the first indicates that users tend to express their information needs incrementally in a task, and thus perform more clicks as their needs become clearer. the other illustrates that users tend to click fresh documents that are not included in the results of previous queries. extensive experimental results demonstrate that by considering all the task information collectively, tcm can better interpret user click behavior and achieve significant improvements in terms of ranking metrics of ndcg and perplexity. we conduct experiments with a large scale realworld dataset which shows that the tcm can be scaled up. while previous research seeks to model a userclick behavior based on browsing and click actions after she enters a single query, often several queries are entered sequentially and multiple search results obtained from di erent queries are clicked to accomplish a single search task. clearly, a typical search can include complex user behavior, including multiple queries and multiple clicks for each query, etc. in this paper, we consider the latter, and refer to a search session as a task. as mentioned above, the previous research considers query sessions only, but ignores other sources of information and their relations to the same task. the dbn model, for example, assumes that users are always satis ed with the last click of each query, without considering subsequent queries and clicks. the rst bias indicates that users tend to express their information needs incrementally and then perform more clicks as their needs become clearer. our experiments used more than million search tasks as the research dataset. search engine click through logs are an invaluable resource that can provide a rich source of data on user preferences in their search results. the analysis of click through logs can be used in many search related applications, such as web search ranking, predicting click through rate, or predicting user satisfaction. in analyzing click through logs, a central question is how to construct a click model to infer a userperceived relevance for each query document pair based on a massive amount of search click data. using a click model, a commercial search engine can develop a better understanding of search users behavior and provide improved user services. previous investigations of click models include dynamic bayesian networks, the user browsing model, the click chain model and the pure relevance model. a user may rst issue a query, examine the returned results and then click on some of them. if the existing results do not satisfy her information needs, she may narrow her search and reformulate her query to construct a new query. this process can be repeated until she nds the desired results or gives up. collectively, all the useractions provide an overall picture of the userintention as she interacts with the search engine. the multiple queries, clicked results, and underlying documents are all sources of information that can help reveal the usersearch intent. traditionally, user sessions are obtained from a consecutive sequence of user search and browsing actions within. these sessions can be partitioned into two categories: a session and a search session, where the former refers to the browsing actions for an individual query while the latter encompasses all queries and browsing actions that a user performs to satisfy her information need. thus, most previous research su ers from a lack of accuracy in many cases. the above line of thinking has led us to consider the advantage of a task centric click model in this paper for understanding and predicting click behavior. in this paper, we rst point out the necessity of modeling task level user behavior by letting the real data speak for itself. we then de ne and describe two new user biases that in uence a search task but have been ignored in previous investigations. the second bias illustrates that users tend to click on fresh documents that they have not seen before under the same task. we design our tcm using a probabilistic bayesian method to address these two biases. tcm is general enough to integrate most other existing click models. ectiveness of the tcm by comparing its performance to the dbn and ubm models. the experimental results show that, by considering all of the task information, the tcm can better model and interpret user click behavior and achieve signi cant improvements in terms of ndcg and perplexity. recent advances in click model have positioned it as an attractive method for representing user preferences in web search and online advertising. yet, most of the existing works focus on training the click model for individual queries, and cannot accurately model the tail queries due to the lack of training data. simultaneously, most of the existing works consider the query, url and position, neglecting some other important attributes in click log data, such as the local time. obviously, the click through rate is different between daytime and midnight. in this paper, we propose a novel click model based on bayesian network, which is capable of modeling the tail queries because it builds the click model on attribute values, with those values being shared across queries. we called our work general click model as we found that most of the existing works can be special cases of gcm by assigning different parameters. experimental results on a large scale commercial advertisement dataset show that gcm can significantly and consistently lead to better results as compared to the state of the art works. utilizing implicit feedback is one of the most essential techniques for a search engine to better entertain its millions or billions of users. implicit feedback can be regarded as a vector of attribute values, including the query text, timestamps, localities, the clickor not flag, etc. given a query, whether user clicks a url is permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. wsdm, february, year#, new york city, new york, usa. formalized as the examination hypothesis or the position model. in year#, craswell et al compared this hypothesis to their new cascade model, which describes a userbehavior by assuming she scans from top to bottom. as the cascade model takes into account the relevance of urls above the clicked one, it outperforms the examination hypothesis. in year#, dupret et al extended the examination hypothesis by considering the dependency on the positional distance to the previous click in the same query session. chapelle et al proposed two similar bayesian network click models, that generalized the cascade model by this work was done when the first author was visiting microsoft research asia. linux user non linux user with firefox non linux user with opera other users, including ie users the local hour click through rate in the query session, each display of a url is called a url click through rate figure #. the empirical ctr with respect to the local hour. analyzing user behavior in a chain style network, within which the probability of examining the next result depends on the position and the identity of the current document. nevertheless, despite their successes, previous works have some limitations. first, they focus on training the click model for each individual query, and cannot accurately predict tail queries due to the inadequate training data. second, the aforementioned models only considered the positionbias, neglecting other bias such as the local time and the user agent, which are parts of the session specific attributes in the log. we remark that the clicks through rate in these two graphs are averaged over all the advertisements from jul. this type of phenomenon has also been observed by. agarwal et al in predicting clicks for front page news of yahoo. based on these observations, it is fairly straightforward to build a click model on multiple attribute values shared across queries, instead of building models for individual queries. this may bring us the generalization to predict tail queries despite the lack of training data for a single query. furthermore, we believe some other attributes are very important for click prediction and can be figure #. the empirical ctr with respect to the user agent. the rest of the paper is organized as follows: we first introduce some definitions and comment on prior works in section #, in which the discoveries motivate us to establish our general click model proposed in section #. next in section # we conduct experiments upon advertisement data and web search data, and compare the results in a number of metrics. we then discuss the extensions in section # and conclude in section #.