in a multi programmed computing environment, threads of execution exhibit different runtime characteristics and hardware resource requirements. not only do the behaviors of distinct threads differ, but each thread may also present diversity in its performance and resource usage over time. a heterogeneous chip multiprocessor architecture consists of processor cores and caches of varying size and complexity. prior work has shown that heterogeneous cmps can meet the needs of a multi programmed computing environment better than a homogeneous cmp system. in fact, the use of a combination of cores with different caches and instruction issue widths better accommodates threads with different computational requirements a central issue in the design and use of heterogeneous systems is to determine an assignment of tasks to processors which better exploits the hardware resources in order to improve performance. in this paper we argue that the benefits of heterogeneous cmps are bolstered by the usage of a dynamic assignment policy, ie, a runtime mechanism which observes the behavior of the running threads and exploits thread migration between the cores. we validate our analysis by means of simulation. specifically, our model assumes a combination of alpha ev and alpha ev processors and of integer and floating point programs from the spec benchmark suite. we show that a dynamic assignment can outperform a static one by to on average and by as much as in extreme cases, depending on the degree of multithreading simulated. chip multiprocessors will dominate commercial processor designs for at least the next decade, during which we will likely see an annual doubling of the number of processor cores integrated onto a single chip. it is possible to efficiently scale a cmp design by increasing the number of cores while maintaining or reducing overall power consumption by reducing clock frequency. as long as the increase in cores offsets the reduction in clock frequency, peak system performance will improve. while replicating cores is an efficient strategy, architects are confronted with a basic question: what type of core should be replicated a given die area can accommodate: many small, simple cores; fewer cores of a larger more sophisticated variety; or some combination of the two. thus, in cmps it is common to see either many simple processors or a moderate quantity of high performance cores. the first solution meets the needs of computing environments characterized by higher thread parallelism, while the second better accommodates scenarios with lower thread parallelism and higher individual thread complexity. heterogeneous cmp systems, consisting of a combination of processor cores of varying type on the same chip, have been proposed recently as a compromise between the two alternatives above. the idea comes from the observation that a multiprogrammed computing environment may present threads of execution with different hardware resource requirements, and that such needs may vary over time. hence, an appropriate mapping of different threads to heterogeneous processor cores can maximize resource utilization and, at the same time, achieve a high degree of interthread parallelism. in order to take advantage of a heterogeneous architecture, an appropriate policy to map running tasks to processor cores must be determined. the overall goal of such a strategy must be to maximize the performance of the whole system by accurately exploiting its resources. the control mechanism must take into account the heterogeneity of the system and of the workload, and the varying behavior of the threads over time. moreover, it must be easily implementable and introduce as little overhead as possible. in this paper we argue that, in a heterogeneous cmp system with a multiprogrammed workload, a dynamic policy, ie, a mechanism which observes the runtime behaviors of the running static static dynamic; init: dynamic; init: threadp: thread a: threadb: threada: thread a: threadp: thread a: threadthread assignment figure #: execution times with different mapping of two threads onto heterogeneous dual core system. threads and exploits thread migration between the cores, is preferable to a static assignment. our evaluation takes into consideration various combinations, both homogeneous and heterogeneous, of alpha ev and alpha ev processors all occupying the same die area. the workloads consist of several combinations of programs from the spec benchmark suite. we compare two homogeneous and three heterogeneous cmp configurations when a static assignment policy is used. specifically, we distinguish an average case from an ideal case, which assumes a priori knowledge of the performance of each thread on the two kinds of processors in use. we finally define two dynamic assignment policies, round robin and ipc driven, and compare their performance with the static case. we show that a heterogeneous system adopting a dynamic assignment policy is able to accommodate a variety of degrees of thread parallelism more efficiently than both a homogeneous and a heterogeneous system adopting a static assignment policy, and we quantify the performance improvements over both of them. the rest of the paper is organized as follows. in section # we introduce the problems addressed in this paper through a simple example. in section # we describe our simulation methodology, processor configurations, and workload configuration. in section # we present an analysis of the behavior of the adopted benchmarks on the two considered alpha cores. in section # we describe our simulation model. in section # we detail describe the static and dynamic thread assignment policies. in section # we present the results of simulations performed on homogeneous and heterogeneous cmp architectures. in section # we briefly relate our work to the literature. finally, in section # we summarize the goals and the results of our analysis. power dissipation and thermal issues are increasingly significant in modern processors. as a result, it is crucial that power performance tradeoffs be made more visible to chip architects and even compiler writers, in addition to circuit designers. most existing power analysis tools achieve high accuracy by calculating power estimates for designs only after layout or floorplanning are complete. in addition to being available only late in the design process, such tools are often quite slow, which compounds the difficulty of running them for a large space of design possibilities. this paper presents wattch, a framework for analyzing and optimizing microprocessor power dissipation at the architecture level. wattch is year#x or more faster than existing layout level power tools, and yet maintains accuracy within of their estimates as verified using industry tools on leading edge designs. this paper presents several validations of wattch accuracy. in addition, we present three examples that demonstrate how architects or compiler writers might use wattch to evaluate power consumption in their design process. we see wattch as a complement to existing lower level tools; it allows architects to explore and cull the design space early on, using faster, higher level tools. it also opens up the field of power efficient computing to a wider range of researchers by providing a power evaluation methodology within the portable and familiar simplescalar framework. in canonical parallel processing, the operating system assigns a processing core to a single thread from a multithreaded server application. since different threads from the same application often carry out similar computation, albeit at different times, we observe extensive code reuse among different processors, causing redundancy. moreover, largely independent fragments of computation compete for the same private resources causing destructive interference. together, this redundancy and interference lead to poor utilization of private microarchitecture resources such as caches and branch predictors we present computation spreading, which employs hardware migration to distribute a thread dissimilar fragments of computation across the multiple processing cores of a chip multiprocessor, while grouping similar computation fragments from different threads together. this paper focuses on a specific example of csp for os intensive server applications: separating application level computation from the os calls it makes when performing csp, each core becomes temporally specialized to execute certain computation fragments, and the same core is repeatedly used for such fragments. we examine two specific thread assignment policies for csp, and show that these policies, across four server workloads, are able to reduce instruction misses in private caches by, private load misses by, and branch mispredictions by. this paper proposes a cycle accounting architecture for simultaneous multithreading processors that estimates the execution times for each of the threads had they been executed alone, while they are running simultaneously on the smt processor. this is done by accounting each cycle to either a base, miss event or waiting cycle component during multi threaded execution. single threaded alone execution time is then estimated as the sum of the base and miss event components; the waiting cycle component represents the lost cycle count due to smt execution. the cycle accounting architecture incurs reasonable hardware cost and estimates single threaded performance with average prediction errors around for two program workloads and for four program workloads. the cycle accounting architecture has several important applications to system software and its interaction with smt hardware. for one, the estimated single thread alone execution time provides an accurate picture to system software of the actually consumed processor cycles per thread. the alone execution time instead of the total execution time may make system software scheduling policies more effective. second, a new class of thread progress aware smt fetch policies based on per thread progress indicators enable system software level priorities to be enforced at the hardware level. simultaneous multithreading processors seek at improving single core processor utilization by sharing resources across multiple active threads. whereas resource sharing increases overall system throughput, it may affect single thread performance in unpredictable ways, and may even starve threads. however, system software, eg, the operating system or virtual machine monitor, assumes that all threads make equal progress when assigning timeslices. the reason for this simplistic assumption is that there is no mechanism for tracking per thread progress during smt execution. this paper proposes a cycle accounting architecture for smt processors for at run time per thread performance accounting. for threads executing on the smt processor, the counter architecture accounts each cycle into three cycle components: base cycles: the processor consumes cycles doing computation work for the given thread, miss event cycles: the processor consumes cycles handling miss events for the given thread, and waiting cycles: the processor is consuming cycles for another thread and can therefore not make progress for the given thread. computing these three cycle components at run time for each of the co executing threads provides insight into how single thread performance is affected by smt execution, eg, a thread starving because of another thread clogging resources will experience a large number of waiting cycles. the cycle accounting architecture proposed in this paper predicts per thread cycle components within at most compared to the cycle components computed under single threaded execution. a threadalone execution time, ie, the execution time had the thread been executed alone on the processor, is estimated by adding the base cycle component to the miss event cycle components. the cycle accounting architecture is shown to be accurate across fetch policies with average errors around for twoprogram workloads and for four program workloads for predicting a threadalone execution time. the counter architecture incurs a reasonable hardware cost around kb of storage. there are several applications for the cycle accounting architecture in support of system software running on smt hardware. first, communicating the per thread alone execution time to system software instead of the total execution time provides a more accurate measurement of the per thread consumed processor cycles, and may make system software scheduling policies more effective; both classical os scheduling policies as well as smt hardware aware policies such as symbiotic job scheduling and non work conserving policies may bene. also, the alone execution time is a useful metric when selling compute cycles in a data center consisting of smt processors, ie, customers get an accurate bill for the consumed compute cycles. second, per thread cycle accounting eliminates the single thread sampling phase in several smt optimizations, such as symbiotic job scheduling with priorities, hill climbing smt resource partitioning, and qualityof service on smt processors. third, per thread progress which is de ned as the ratio of the per thread alone execution time to the total execution time, enables a new class of smt fetch policies based on per thread progress indicators. these thread progress aware smt fetch policies enforce system software priorities in hardware, ie, they enforce one thread to make faster single thread progress than another thread proportionally to the difference in systemlevel priorities. existing smt fetch policies on the other hand do not control per thread progress. the abstraction provided to system software, consumed processor cycles or alone execution time, is consistent with the notion of a timeslice used by system software for managing qos, priorities and performance isolation through time multiplexing. we propose a new processing paradigm, called the expandable split window paradigm, for exploiting fine grain parallelism. this paradigm considers a window of instructions as a single unit, and exploits fine grain parallelism by overlapping the execution of multiple windows. the basic idea is to connect multiple sequential processors, in a decoupled and decentralized manner, to achieve overall multiple issue. we also present an implementation of the expandable split window execution model, and preliminary performance results. this processing paradigm shares a number of properties of the restricted dataflow machines, but was derived from the sequential von neumann architecture. the execution of a program, in an abstract form, can be considered to be a dynamic dataflow graph that encapsulates the data dependencies in the program. the nodes of the graph represent computation operations, and the arcs of the graph represent communication of values between the nodes. the execution time of a program is the time taken to perform the computations and communication in its dynamic dataflow graph. if there are altogethercommunication arcs in the graph, and at a time a maximum ofc mununication arcs can be cmied out, then the execution time involves a miniiurn ofm communication steps, no matter how many parallel resources are used for computations. with the continued advartee in technology, switching components become smaller and more efficient. the effect is that computation becomes faster and faster. communication speed on the other hand, seems to be restricted by the speed of ligh and eventually becomes the major bottleneck. the abstract dataflow graph hides information about the dktances involved in the inter node or inter operation communication. fkst, depcndmg on the matching between the dataflow graph and the processing structure, adjacent nodes in the graph may get mapped to either the same processing elemen physically adjacent processing elements, or distant processing elements. based on the mapping used, the communication arcs of the graph get stretehed or shrunk, causing changes in the communication cost because it takes more time to send values permission to copy without fee all or part of this material is granted prowded that the copies are not made or distributed for dmect commercial advantage. since there can onlv be a finite amount. of fast storage for temporarily storing the intermediate computation vahtes, the values have to be either consumed irnmedlatcly or stored away into some form of backup storage, creating more nodes and communication arcs. an irtspeetion of the dynamic dataflow graph of many sequential programs reveals that there exists a large amount of theoretically exploitable instruction level parallelism, is, a large number of computation nodes that can be executed in parallel, provided a suitable processor model with a suitable communication mechanism backed up by a suitable temporary storage mechanism exists. where is this pamllelism most likely to be found in a dynamic execution of the sequential progrrun because most programs are written in an imperative language for a sequential machine with a limited number of architectural registers for storing temporary values, it is quite likely that instructions of close proximity are data dependent. this means that most of the parallelism can be found only further down in the dynamic instruction stream. the obvious way to get to that parallelism is to use a large window of dynamic instructions. a good scheme should optimize not only the number of operations exeeuted in parallel, but also the communication eosfs by reducing the communication distances and the temporary storing away of values, thereby allowing expandability. we consider a processor model to be expandable if its abilities can be expanded easily, as hrudware and software technology advances. this requires the processor model to have no centralized resources that can become potential bottlenecks. several processing paradigms have been proposed over the years for exploiting fine grain parallelism. the most general one, the dataflow model, considers the entire dataflow graph to be a single window and uses an unconventional programming paradigm to expose the maximum amount of parallelism present in the application such a generaf model allows the maximum number of computation nodes to be active simultaneously, but incurs a large performance penalty due to the stretching of the communication arcs. it also suffers from the inability to express critical sections and imperative operations that are essential for the efficient execution of operating system functions, such as resource management. this is where the power of sequentiality comes in. by ordering the computation nodes in a suitable manner, sequentiality can be used to introduce different kinds of temporal locality to minimize the costs of communication and intermediate result storage. this has the effect of combtig several adjacent nodes into one, because the communication arcs between them become very short when temporal locality is exploited by hardware mearts. a good example of thii phenomenon are the vector machines, which exploit the regular type of parallelism found in many numeric applications effectively by hardware means suchchaining. sequential execution can be augmented to exploit the irregsh type of parallelism found in most non numeric appli cations. superscalar processors and vliw processors do exactly this; they stay within the reahn of sequential executiou but attempt to execute multiple operations every cycle. for achieving this, superscalars scan through a window of operations every cycle and dynamically detect independent operations to be issued in a cycle. these von neuman based machines use the conventional programming paradigm, but require a sufficiently large centralized window, if even moderate sustained issue rates are to be desired. extract independent instructions from a large centralized window and to enforce data dependencies typically involves wide associa tive searches, and is non trivial. furthermore, superscalars require multi ported register files and wide paths from the instruction cache to the issue unit. although dynamic scheduling with a large centralized window has the potential for high issue rates, a realistic implementation is not likely to be possible because of its complexity, unless novel schemes are developed to reduce the complexity. vliw processors partirdly circumvent this problem by detecting parallelism at compile time, and by using a large instruction word to express the multiple operations to be issued in a cycle. a major limitation of static scheduling is that it has to play safe, by always making worst case assumptions about information that is not available at compile time. vliw processors also require some of the centralized resources required by super scalars, such as the multi ported register files, crossbars for connecting the computation units, and wide paths to the issue unit. these centralized resources can easily become bottlenecks and be a severe limitation on the performance of these machines. looking at these existing models, we see that the concept of sequential execution with a large window is good, but definitely not sufficient by itself. that is, the window should consist mostly of instructions that are guaranteed to execute, and not instructions that might be executed. factors such as the ability to feed instructions into this window, should not be a limitation. there should be provision to issue loads before proper address dkunbiguation, ie, in a speculative manner. we also need a powerful way of decentralizing the critical resources in the system. our objective is to use the conventional sequential programming paradigm with dynamic scheduling and large windows, but augmented with novel techniques to decentral ize the critical resources. tms is achieved by splitting a large operation window into small windows, and executing many such small windows in parallel. the principle of datafiow is used in a restricted manner to pass vahses efficiently across the multiple windows in execution; the execution mowithin each window can be a simple, sequential processor. as we will see later in thk paper, such an approach has the synergistic effect of combkirtg the advantages of the sequential and the dataflow execution models, and the advantages of static and dynamic scheduling. organization of the paper we have outlined the important issues pertaining to designing a klgh performance fine grain parallel processor. the rest of this paper is organized as follows. section # describes the basic philosophy behind the expandable split window processing paradigm. section # describes a possible implementation of this paradigm. the description includes details of the instruction issue mechanism, the dktributed register file, the distributed memoryatnbiguation uni rmd the distributed data cache. section # presents preliminary performance results for the new paradigm, obtained horn a simulation study using the spec benchmarks. section # provides a susntmuy of the research done so far, and the future course of work. when a dataflow graph is mapped onto a processing structure, interesting dynamics are introduced into the picture. that is, it should be possible to get farther down in the instruction stream, without fetching and decoding the instructions in between. the acm copyright notice and the title of the pubhcation and its date appear, and notice is gwen that copying is by perrmssion of the association for computmg machinery. to copy otherwise, or to repubhsh, reqrnres a fee and or speclflc perrmsslon over atanee. the creation of the large window should be accurate. this paper presents a set of new run time tests for speculative parallelization of loops that defy parallelization based on static analysis alone. it presents a novel method for speculative array privatization that is not only more efficient than previous methods when the speculation is correct, but also does not require rolling back the computation in case the variable is found not to be privatizable. we present another method for speculative parallelization which can overcome all loop carried anti and output dependences, with even lower overhead than previous techniques which could not break such dependences. again, in order to ameliorate the problem of paying a heavy penalty for speculatively parallelizing loops that turn out to be serial, we present a technique that enables early detection of loop carried dependences. our experimental results from a preliminary implementation of these tests on an ibm smp machine show a significant reduction in the penalty paid for mis speculation, from roughly to between and of the serial execution time. for parallel loops, we obtain about the same, and often, even better performance relative to the previous methods, making our techniques extremely attractive. the support consists of a number of software speculation control handlers and modifications to the shared secondary cache memory system of the cmp this support is evaluated using five representative integer applications. thread level speculation is a technique that enables parallel execution of sequential applications on a multiprocessor. this paper describes the complete implementation of the support for threadlevel speculation on the hydra chip multiprocessor. our results show that the speculative support is only able to improve performance when there is a substantial amount of medium grained loop level parallelism in the application. when the granularity of parallelism is too small or there is little inherent parallelism in the application, the overhead of the software handlers overwhelms any potential performance benefits from speculative thread parallelism. overall, thread level speculation still appears to be a promising approach for expanding the class of applications that can be automatically parallelized, but more hardware intensive implementations for managing speculation control are required to achieve performance improvements on a wide class of integer applications. with power dissipation becoming an increasingly vexingproblem across many classes of computer systems, measuringpower dissipation of real, running systems has becomecrucial for hardware and software system research and design live power measurements are imperative for studiesrequiring execution times too long for simulation, such asthermal analysis. furthermore, as processors become morecomplex and include a host of aggressive dynamic powermanagement techniques, per component estimates of powerdissipation have become both more challenging as well asmore important in this paper we describe our technique for a coordinatedmeasurement approach that combines real totalpower measurement with performance counter based, per unitpower estimation. the resulting tool offers live totalpower measurements for intel pentium processors, andalso provides power breakdowns for of the major cpusubunits over minutes of spec and desktop workloadexecution. as an example application, we use the generatedcomponent power breakdowns to identify program powerphase behavior. overall, this paper demonstrates a processorpower measurement and estimation methodology andalso gives experiences and empirical application resultsthat can provide a basis for future power aware research. energy and power density concerns in modern processors have led to signi cant computer architecture research efforts in power aware and temperature aware computing. as with any applied, quantitative endeavors in architecture, it is crucial to be able to characterize existing systems as well as to evaluate tradeoffs in potential designs. unfortunately, cycle level processor simulations are time consuming, and are often vulnerable to concerns about accuracy. in certain cases, very long simulations can be required. this is particularly true for thermal studies, since it takes a long time for processors to reach equilibrium thermal operating points. furthermore, researchers often need the ability to measure live, running systems and to correlate measured results with overall system hardware and software behavior. live measurements allow a complete view of operating system effects, io, and many other aspects of real world behavior, often omitted from simulation. while live measurements gain value from their completeness, it can often be dif cult to zoom in and discern how different subcomponents contribute to the observed total. for this reason, many processors provide hardware performance counters that help give unit by unit views of processor events. while good for understanding processor performance, the translation from performance counters to power behavior is more indirect. nonetheless, some prior research efforts have produced tools in which perunit energy estimates are derived from performance counters. prior counter based energy tools have been geared towards previous generation processors such as the pentium pro. since these processors used little clock gating, their power consumption varied only minimally with workload. as a result, back calculating unit by unit power divisions is fairly straightforward. in todayprocessors, however, power dissipation varies considerably watts or more on an application by application and cycle by cycle basis. as such, counter based power estimation warrants further examination on aggressively clock gated superscalar processors like the intel pentium. the primary contributions of this paper are as follows: we describe a detailed methodology for gathering live, per unit power estimates based on hardware performance counters in complicated and aggressively clock gated microprocessors. we present live total power measurements for spec benchmarks as well as some common desktop applications. as an application of the component power estimates, we describe a power oriented phase analysis using bayesian similarity matrices. the remainder of this paper is structured as follows. section # gives an overview of our performance counter and power measurement methodology. sections and then go into details about our mechanisms for live monitoring of performance counters and power. following this, section # develops a technique for attributing power to individual hardware units like caches, functional units, and so forth by monitoring appropriate performance counters. section # gives results on total power and per unit power measurements for collections of spec and desktop applications, and section # gives an example use of component power estimates to track program power phases. finally, section # discusses related work and section # gives conclusions and offers ideas for future work. with billion transistor chips on the horizon, single chip multiprocessors are likely to become commodity components. speculative cmps use hardware to enforce dependence, allowing the compiler to improve performance by speculating on ambiguous dependences without absolute guarantees of independence. although the decomposition problem lends itself to a min cut based approach, the overheads depend on the thread size, requiring the edge weights to be changed as the algorithm progresses. one recent work uses a set of heuristics, each targeting a specific overhead in isolation, and gives precedence to thread prediction, without comparing the performance of the threads resulting from each heuristic. this method achieves an average speedup of for floating point programs and for integer programs on a four processor chip, improving on the and achieved by the previous heuristics. the compiler is responsible for decomposing a sequential program into speculatively parallel threads, while considering multiple performance overheads related to data dependence, load imbalance, and thread prediction. the changing weights make our approach different from graph theoretic solutions to the general problem of task scheduling. by contrast, our method uses a sequence of balanced min cuts that give equal consideration to all the overheads, and adjusts the edge weights after every cut. single chip multiprocessors are likely to become commodity components within a few years, as the number of transistors per chip crosses the one billion mark. cmps may be operated as conventional multiprocessors, where explicit parallelism is exploited from concurrently running applications or parallel sections within a single application. it is di cult, however, for programmers to parallelize applications manually. speculative cmps provide the compiler with the same interface as a standard, sequential processor while supporting the safe, simultaneous execution of potentially dependent threads. the compiler may viewa speculative cmp as a multiprocessor in which the simultaneous execution of dependent threads results in performance degradation rather than incorrect execution, and can select threads to optimize run time. these algorithms share a subset of the goals minimize dependences and load imbalance of this decomposition problem. decomposition using xed weights results in poor speculative threaded performance because the weights lack a relationship to overhead. some past approaches for decomposition have focused exclusively on loops. unfortunately, non loop code sections are crucial for non numerical programs, which are the primary target for speculative cmps. the heuristics target the overheads of data dependence, load imbalance, and thread prediction separately, and give precedence to thread prediction, without comparing the performance of the threads resulting from each isolated heuristic. in contrast to, this paper chooses the best performing threads by giving equal consideration to all the overheads using a min cut based approach. both techniques are back end compiler algorithms that do not modify the applicationsource code. for balancing, modi es a min cut to reduce the di erence between the vertex sizes of the cuttwo vertex sets. our balancing is signi cantly more sophisticated in that it reduces the overall run time of the threads resulting from the cut. combined with the previously mentioned requirement that our edge weights change as newer cuts are made, our approach performs a sequence of balanced min cuts where the edge weights are adjusted after each balanced cut. our main contributions are: we are the rst to map the speculative thread decomposition problem onto a graph theoretic framework. our method achieves an average speedup of for oating point programs and for integer programs, improving on the and achieved by the approach in. in section # we explain the execution model of a speculative cmp, followed by a discussion of our algorithm in section # and results in section #. additional related work is discussed in section #. compilers have been successful parallelizing numerical applications; non numerical codes confound compilers with dependences that are not statically analyzable. to alleviate this problem, speculative cmps have been proposed to exploit the parallelism implicit in an applicationsequential instruction stream. because speculative cmps use hardware to enforce dependence, the compiler can improve performance by speculating on ambiguous dependences without absolute guarantees of independence. the compiler is responsible for decomposing the control owgraph, and hence the sequential instruction stream, into these speculatively parallel threads. the compiler creates speculative threads by inserting boundary marks into the sequential instruction stream to tell the cmp where to speculate; that is, which code segments to try to execute in parallel with each other. the cmp uses prediction to select and execute a set of threads while enforcing correctness, such that the programoutput is consistent with that of its sequential execution. to enforce correctness, the cmp employs data dependence tracking mechanisms, keeps uncertain data in speculative storage, rolls back incorrect executions, and commits data to the memory system only when speculative threads succeed. because thread decomposition is a critical factor in determining the performance achieved by a speculativelythreaded program, the decomposition scheme used by the compiler is key to the success of the speculative approach. to perform thread decomposition, the compiler faces multiple performance overheads related to data dependence, load imbalance, thread size, and thread prediction. ideally, no data dependence should cross a thread boundary to avoid dependence synchronization delays and dependenceviolation rollbacks; thread sizes should be chosen to avoid load imbalance; a thread should be large enough to amortize its dispatch overhead, but small enough such that all of its speculative data can be bu ered; and thread sequences should be predictable to avoid misprediction rollbacks. finding optimum program decompositions in general is np complete. because this problem requires partitioning a program, it naturally lends itself to a min cut based approach. certainly, others have used graph theoretic approaches to solve compiler problems. most relevant are static scheduling algorithms that map threads from an explicitly parallel program onto processors while minimizing interthreadcommunication overhead and load imbalance. nevertheless, there is a fundamental di erence between scheduling and our decomposition. in our case, all the overheads due to data dependence, thread sequence misprediction, and load imbalance depend on the size of the threads. because an edgeweight represents the run time overhead incurred by cutting the edge, our weights depend on thread sizes and change as the algorithm progresses and newer threads are made. in contrast, the weights in scheduling are xed at the beginning and do not change. apart from the changing weights, most scheduling algorithms make assumptions that are not applicable to our problem, such as unlimited processors, uniform thread run time, or constant communication cost. to that end, applies several di erent heuristics to build threads from loops and non loops. the heuristics, however, do not consider load imbalance and use limited dependence and prediction information. the heuristics conservatively attempt to make threads out of all loop bodies, and terminate threads at all but the smallest function calls, ignoring opportunities for coarser parallelism. we apply min cut on the cfg of each procedure of an application. in any min cut based approach including ours, where overhead due to data dependence and thread prediction can be represented as edge weights, load imbalance does not lend itself to being represented as edge weight. we employ an abstract execution based scheme to estimate run times of candidate thread sets. by using a sequence of balanced min cuts and adjusting the edge weights after every cut, we give equal consideration to the overheads of data dependence, thread prediction, and load imbalance. we introduce a method for assigning edge weights such that the cost of cutting a control owedge models the data dependence and thread misprediction overhead cycles incurred by placing a thread boundary on the edge. we present an abstract execution based scheme for comparing execution times of candidate threads. we have implemented the algorithm as part of a fullyautomated, pro le based compilation process that measures and fortran spec cpu programs. chip multiprocessors, or multi core processors, have become a common way of reducing chip complexity and power consumption while maintaining high performance. in these systems, a sequential program is decomposed into threads to be executed in parallel; dependent threads cause performance degradation, but do not affect correctness. because these overheads depend on the runtimes of the threads that are being created by the decomposition, reducing the overheads while creating the threads is a circular problem. static compile time decomposition handles this problem by estimating the run times of the candidate threads, but is limited by the estimates inaccuracy. dynamic execution time decomposition in hardware has better run time information, but is limited by the decomposition hardware complexity and run time overhead. we propose a third approach where a compiler instruments a profile run of the application to search through candidate threads and pick the best threads as the profile run executes. we avoid static decomposition estimation accuracy problem by using actual profile run execution times to pick threads, and we avoid dynamic decomposition overhead by performing the decomposition at profile time. to address this issue, we make the key observation that the run time overhead of a thread depends, to the first order, only on threads that overlap with the thread inexecution. this observation implies that a given thread affects only a few other threads, allowing pruning of the space. using a cmp simulator, we achieve an average speedup of on four cores for five of the spec cfp benchmarks, which compares favorably to recent static techniques. speculative cmps use hardware to enforce dependence, allowing a parallelizing compiler to generate multithreaded code without needing to prove independence. thread decomposition attempts to reduce the run time overheads of data dependence, thread misprediction, and load imbalance. the resultant decomposition is compiled into the application so that a production run of the application has no instrumentation and does not incurany decomposition overhead. because we allow candidate threads to span arbitrary sections of the application call graph and loop nests, an exhaustive search of the decomposition space is prohibitive, even in profile runs. a cmp may be used as a conventional multiprocessor to run multiple applications concurrently; however, an individual application may need to take advantage of multiple cores for high performance because each core on a cmp may be less powerful than a traditional uniprocessor. although compilers are relatively successful at parallelizing numerical applications, dependences that are not statically analyzable hinder compilers. speculative cmps use hardware to enforce dependence, allowing a compiler to focus on improving performance without needing to prove independence. a sequential program is decomposed into threads for a speculative cmp to execute in parallel; dependent threads cause performance degradation, but do not affect correctness. the cmp uses prediction to select and execute a sequence of threads while enforcing correctness, such that the programoutput is consistent with that of its sequential execution. the cmp employs data dependencetracking mechanisms, keeps uncertain data in speculative storage, rolls back incorrect executions, and commits data to main memory only when speculative threads succeed. thus, a speculative cmp provides the same programming interface as a uniprocessor while supporting the safe, simultaneous execution of potentially dependent threads referred to as thread level speculation. thread decomposition is the critical factor in determining the performance of a program executed by tls. the key factors contributing to runtime overhead in tls are inter thread data dependence, inter thread control ow misprediction, and inter thread load imbalance. data dependence and misprediction cause roll backs and load imbalance causes idling. the penalty due to a data dependence violation or misprediction depends on how late in a threadexecution the violation or misprediction is detected the longer the thread, the later the potential detection, and the larger the penalty. static compile time decomposition handles this problem by estimating the run times of the candidate threads, but is limited by the inherent inaccuracy of the estimates and of predicting the run time interaction among threads. dynamic execution time decomposition in hardware has better run time information in that hardware can measure actual run times instead of relying on estimates. nevertheless, dynamic decomposition is limited by the decomposition hardwarecomplexity and run time overhead of performing decomposition during execution. it is limited also by lacking knowledge of the overall program structure, which is useful in nding good decompositions, and by the dif culty of performing in hardware the complex tradeoffs among the various overheads of tls. to address these limitations of the current decomposition schemes, we propose a different approach where a compiler instruments a pro le run of the application to perform an empirical search through candidate threads and pick the best threads as the pro le run executes. as the pro le run proceeds, it uses the naturally occurring invocations of procedures and loops to try out various candidate threads. the resultant decomposition is compiled into the application so that a production run of the decomposed application has no instrumentation and does not incur any decomposition overhead. we avoid dynamic decompositionrun time overhead by performing the decomposition at pro le time, even though we evaluate tradeoffs among tlsvarious overheads during the decomposition. also, our search uses knowledge of the overall program structure. our approach has some similarities to loop versioning and procedure cloning. whereas our approach chooses among various candidate threads, those optimizations choose among various versions of loops and procedures. while those optimizations are only locally applicable to loops and procedures, our approach is more general and tries numerous levels of parallelism spanning arbitrary sections of a programcall graph and loop nests. those optimizations compile multiple static versions into the program and choose among them at run time, but the number of versions is limited by code expansion. by contrast, our scheme examines candidate threads only during a pro le run and uses only the best threads in compilation for production runs, avoiding code expansion issues. one may think that code expansion problems remain in the pro le run. fortunately, speculative cmps provide hardware support to dynamically coalesce all the static threads of a procedure call or loop into a single dynamic thread during execution without requiring an explicit static version of that single thread. another paper creates two ver sions of loops serial and parallel in cases where the correctness of the parallel version cannot be proved at compile time. the parallel version is executed and then checked for correctness via a run time dependence test; if the test fails, then the serial version is executed. by contrast, our scheme has many parallel versions and has to choose among them, for which we use empirical search. because our hardware can always guarantee correctness with any parallel version, we do not need to perform any dependence tests. finally, because we allow candidate threads to span arbitrary sections of the applicationcall graph and loop nests, an exhaustive search through the decomposition space is prohibitive, even in pro le runs. in addition, because our search uses the naturally occurring invocations of procedures and loops to try out candidate threads, the number of invocations in a single run may not be enough to try out all candidate threads. to address this issue, we make the key observation that the run time overhead of a thread depends, to the rst order, only on those threads that overlap with that thread in execution. we call this property speculation locality and say that two threads are independent by separation if there are enough other threads between them to prevent them from executing at the same time. consequently, the search does not need to revisit other threads that are independent by separation of the thread it just replaced, enabling pruning of the search space. our main contributions are: we are the rst to propose that decomposition for thread level speculation be performed by a pro le time empirical search that is embedded into the program we show how to embed search code within an application to nd the most parallelism, with low pro le run time overhead and while avoiding measurement induced error. we identify properties called speculation locality and independence by separation to prune the search. we give all tls overheads equal priority, even those that static approaches ignore. empirical optimization produces results better than static approaches with less analysis effort and without the drawbacks of dynamic decomposition. we discuss our optimization system in section # and our results in section #. architectures called single chip multiprocessors, or multicore processors, help reduce chip complexity and power consumption while maintaining high performance. parallelism within a single application can come from explicitly parallel sections, but it is dif cult for programmers to parallelize applications manually. to alleviate this problem, speculative cmps exploit the parallelism implicit in an applicationsequential instruction stream. the decomposition problem is to partition a program into speculatively parallel threads while optimizing run time. the amount of run time overhead caused by an instance of these factors depends on the thread size. similarly, the longer the threads, the larger the potential load imbalance, and the larger penalty. because these overheads depend on the run times of the threads that are being created by the decomposition, reducing the overheads while creating the threads is a circular problem. because we use actual pro le run execution times to pick the threads, we avoid static decompositionestimation accuracy problem. consequently, the compiler needs to generate only one version of each procedure or loop. our pro le run compares that version to alternative executions, in which some calls or loops are treated as single threads, via the hardware support. this dynamic ability obviates creation of multiple static versions, avoiding code expansion issues even in our pro le run. two key differences to our approach are: as sumes the parallel version is always faster than the serial version and, therefore, there is no comparison between them. speculation locality implies that, when our search replaces a candidate thread with a better thread, the tls overheads of only those threads that overlap with the candidate thread will change. our results show an average speedup of on four cores for ve spec cfp benchmarks, compared to an average speedup of obtained with a recent static technique. in section #, we discuss related work, followed by an explana tion of the speculative execution model in section #. chip multiprocessor architectures are a promising design alternative to exploit the ever increasing number of transistors that can be put on a die. in the past, it has been unclear whether the higher hardware cost of register level communication is cost effective in this paper, we show that the wide issue dynamic processors that will soon populate cmps, make fast communication a requirement for high performance. finally, the scheme allows the system to achieve near ideal performance. to deliver high performance on applications that cannot be easily parallelized, cmps can use additional support for speculatively executing the possibly data dependent threads of an application while some of the cross thread dependences in applications must be handled dynamically, others can be fully determined by the compiler. for the latter dependences, the threads can be made to synchronize and communicate either at the register level or at the memory level. consequently, we propose an effective hardware mechanism to support communication and synchronization of registers between on chip processors. our scheme adds enough support to enable register level communication without specializing the architecture so much toward speculation that it leads to much unutilized hardware under workloads that do not need speculative parallelization. this paper proposes and evaluates single isa heterogeneousmulti core architectures as a mechanism to reduceprocessor power dissipation. energy savings aresubstantially more than chip wide voltage frequency scaling. our design incorporatesheterogeneous cores representing different points inthe power performance design space; during an applicationexecution, system software dynamically chooses themost appropriate core to meet specific performance andpower requirements our evaluation of this architecture shows significant energybenefits. for an objective function that optimizes forenergy efficiency with a tight performance threshold, for spec benchmarks, our results indicate a average energyreduction while only sacrificing in performance an objective function that optimizes for energy delay withlooser performance bounds achieves, on average, nearly afactor of three improvement in energy delay product whilesacrificing only in performance. previous studies have demonstrated the advantages of single isa heterogeneous multi core architectures for power and performance. the performance ordering of cores on such processors is different for different applications; there is only a partial ordering among cores in terms of resources and complexity. this methodology produces performance gains as high as. the performance improvements come with the added cost of customization. a single isa heterogeneous multi core architecture is achip multiprocessor composed of cores of varying size, performance, and complexity. this paper demonstrates that thisarchitecture can provide significantly higher performance inthe same area than a conventional chip multiprocessor. it doesso by matching the various jobs of a diverse workload to thevarious cores. this type of architecture covers a spectrum ofworkloads particularly well, providing high single thread performancewhen thread parallelism is low, and high throughputwhen thread parallelism is high this paper examines two such architectures in detail, demonstrating dynamic core assignment policies that providesignificant performance gains over naive assignment, andeven outperform the best static assignment. it examines policiesfor heterogeneous architectures both with and withoutmultithreading cores. one heterogeneous architecture we examineoutperforms the comparable area homogeneous architectureby up to, and our best core assignment strategyachieves up to speedup over a naive policy. as multi core architectures with thread level speculation are becoming better understood, it is important to focus on tls compilation. tls compilers are interesting in that, while they do not need to fully prove the independence of concurrent tasks, they make choices of where and when to generate speculative tasks that are crucial to overall tls performance this paper presents posh, a new, fully automated tls compiler built on top of gcc. first, to partition the code into tasks, it leverages the code structures created by the programmer, namely subroutines and loops. second, it uses a simple profiling pass to discard ineffective tasks. with the code generated by posh, a simulated tls chip multiprocessor with superscalar cores delivers an average speedup of for the specint year# applications. moreover, an estimated of this speedup is a result of the implicit data prefetching provided by squashed tasks. to achieve high performance, contemporary computer systems rely on two forms of parallelism: instruction level parallelism and thread level parallelism. with insufficient tlp, processors in an mp will be idle; with insufficient ilp, multiple issue hardware on a superscalar is wasted. by permitting multiple threads to share the processor functional units simultaneously, the processor can use both ilp and tlp to accommodate variations in parallelism. we examine two alternative on chip parallel architectures for the next generation of processors. first, we identify the hardware bottlenecks that prevent multiprocessors from effectively exploiting ilp. the use of tlp is especially advantageous when per thread ilp is limited. this study also addresses an often cited concern regarding the use of thread level parallelism or multithreading: interference in the memory system and branch prediction hardware. we find the multiple threads cause interthread interference in the caches and place greater demands on the memory system, thus increasing average memory latencies. wide issue super scalar processors exploit ilp by executing multiple instructions from a single program in a single cycle. multiprocessors exploit tlp by executing different threads in parallel on different processors. unfortunately, both parallel processing styles statically partition processor resources, thus preventing them from adapting to dynamically changing levels of ilp and tlp in a program. this article explores parallel processing on an alternative architecture, simultaneous multithreading, which allows multiple threads to complete for and share all of the processor resources every cycle. the most compelling reason for running parallel applications on an smt processor is its ability to use thread level parallelism and instruction level parallelism interchangeably. when a program has only a single thread, all of the smt processor resources can be dedicated to that thread; when more tlp exists, this parallelism can compensate for a lack of per thread ilp. we compare smt and small scale, on chip multiprocessors in their ability to exploit both ilp and tlp. then, we show that because of its dynamic resource sharing, smt avoids these inefficiencies and benefits from being able to run more threads on a single processor. the ease of adding additional thread contexts on an smt allows simultaneous multithreading to expose more parallelism, further increasing functional unit utilization and attaining a average speedup. by exploiting threading level parallelism, however, smt hides these additional latencies, so that they only have a small impact on total program performance. we also find that for parallel applications, the additional threads have minimal effects on branch prediction. the remainder of this article is organized as follows. to achieve high performance, contemporary computer systems rely on two forms of parallelism: instruction level parallelism and thread level parallelism. wide issue superscalar processors exploit ilp by executing multiple instructions from a single program in a single cycle. a multiprocessor must statically partition its resources among the multiple cpus; if insufficient tlp is available, some of the processors will be idle. a comparison of issue slot utilization in various architectures. each square corresponds to an issue slot, with white squares signifying unutilized slots. hardware utilization suffers when a program exhibits insufficient parallelism or when available parallelism is not used effectively. multiprocessors physically partition hardware to exploit tlp, and therefore performance suffers when tlp is low. by allowing multiple threads to share the processorfunctional units simultaneously, thread level parallelism is essentially converted into instruction level parallelism. an smt processor can therefore accommodate variations in ilp and tlp. an smt processor can uniquely exploit whichever type of parallelism is available, thereby utilizing the functional units more effectively to achieve the goals of greater throughput and significant program speedups. this article explores parallel processing on a simultaneous multithreading architecture. in this work, we use parallel applications to explore the utilization of execution resources in the future, when greatly increased chip densities will permit several alternative on chip parallel architectures. this study makes several contributions in this respect. then, we show that smt avoids these inefficiencies, because its resources are not statically partitioned, and benefits from being able to run more threads on a single processor. first, we investigate the amount of interthread interference in the shared cache. second, we assess the impact resulting from smtincreased memory bandwidth requirements. section # describes the out of order superscalar processor that serves as the base for the smt and mp architectures. section # also discusses the extensions that are needed to build the smt and mps. section # discusses the methodology used for our experimentation. section # presents an analysis of smteffect on the memory system and branch prediction hardware. section # discusses some implications that smt has for architects, compiler writers, and operating systems developers, and suggests areas of further research. although they correspond to different granularities of parallelism, ilp and tlp are fundamentally identical: they both identify independent instructions that can execute in parallel and therefore can utilize parallel hardware. multiprocessors exploit tlp by executing different threads in parallel on different processors. unfortunately, neither parallel processing style is capable of adapting to dynamically changing levels of ilp and tlp, because the hardware enforces the distinction between the two types of parallelism. a superscalar executes only a single thread; if insufficient ilp exists, much of that processormultiple issue hardware will be wasted. simultaneous multithreading allows multiple threads to compete for and share available processor resources every cycle. a superscalar processor achieves low utilization because of low ilp in its single thread. because it allows multiple threads to compete for all resources in the same cycle, smt can cope with varying levels of ilp and tlp; consequently utilization is higher, and performance is better. when executing parallel applications is its ability to use thread level parallelism and instruction level parallelism interchangeably. when a program has only a single thread all of the smt processorresources can be dedicated to that thread; when more tlp exists, this parallelism can compensate for a lack of per thread ilp. our investigation of parallel program performance on an smt is motivated by the results of our previous work. in tullsen et al, we used a multiprogrammed workload to assess the potential of smt on a high level architectural model and favorably compared total instruction throughput on an smt to several alternatives: a superscalar processor, a fine grained multithreaded processor, and a single chip, shared memory multiprocessor. in tullsen et al, we presented a microarchitectural design that demonstrated that this potential can be realized in an implementable smt processor. the microarchitecture requires few small extensions to modern out of order superscalars; yet, these modest changes enable substantial performance improvements over wide issue superscalars. in those two studies, the multiprogrammed workload provided plenty of tlp, because each thread corresponded to an entirely different application. in parallel programs, however, threads execute code from the same application, synchronize, and share data and instructions. these programs place different demands on an smt than a multiprogrammed workload; for example, because parallel threads often execute the same code at the same time, they may exacerbate resource bottlenecks. in particular, we compare smt and small scale on chip multiprocessors in their ability to exploit both ilp and tlp. first, we identify the hardware bottlenecks that prevent multiprocessors from effectively exploiting ilp in parallel applications. this is especially advantageous when per thread ilp is limited. the ease of designing in more thread contexts on an smt allows simultaneous multithreading to expose more thread level parallelism, further increasing functional unit utilization and attaining a average speedup. finally, we analyze how tlp stresses other hardware structures on an smt. we find that, although smt increases the average memory latency, it is able to hide the increase by executing instructions from multiple threads. consequently, interthread conflicts in the memory system have only a small impact on total program performance and do not inhibit significant program speedups. third, we find that in parallel applications, the additional threads only minimally degrade branch and jump prediction accuracy. in section #, we examine the shortcomings of small scale multiprocessors and demonstrate how smt addresses these flaws. in section #, we discuss related work, including a comparison with our previous results. robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation, and bug detection. instrumentation tools are written inc using pin rich api. pin follows the model of atom, allowing the tool writer to analyze an application at the instruction level without the need for detailed knowledge of the underlying instruction set. the api is designed to be architecture independent whenever possible, making pintools source compatible across different architectures. however, a pintool can access architecture specific details when necessary. for example, pin is faster than valgrind and faster than dynamorio for basic block counting. to illustrate pin versatility, we describe two pintools in daily use to analyze production software. pin is publicly available for linux platforms on four architectures: ia, em, itanium, and arm. to meet this need, we have developed a new instrumentation system called pin. our goals are to provide easy to use, portable, transparent, and efficient instrumentation. instrumentation with pin is mostly transparent as the application and pintool observe the application original, uninstrumented behavior. pin uses dynamic compilation to instrument executables while they are running. for efficiency, pin uses several techniques, including inlining, register re allocation, liveness analysis, and instruction scheduling to optimize instrumentation. in the ten months since pin was released in july year#, there have been over downloads from its website. this fully automated approach delivers significantly better instrumentation performance than similar tools. as software complexity increases, instrumentation a technique for inserting extra code into an application to observe its behavior is becoming more important. pin is a software system that performs run time binary instrumentation of linux applications. the goal of pin is to provide an instrumentation platform for building a wide variety of program analysis tools for multiple architectures. as a result, the design emphasizes ease of use, portability, transparency, ef ciency, and robustness. its user model is similar to the popular atom api, which allows a tool to insert calls to instrumentation at arbitrary locations in the executable. users do not need to manually inline instructions or save and restore state. the pin distribution includes many sample architecture independent pintools including pro lers, cache simulators, trace analyzers, and memory bug checkers. the api also allows access to architecture speci. pin provides ef cient instrumentation by using a just in time compiler to insert and optimize code. this fully automated approach distinguishes pin from most other instrumentation tools which require the userassistance to boost performance. for example, valgrind relies on the tool writer to insert special operations in their intermediate representation in order to perform inlining; similarly dynamorio requires the tool writer to manually inline and save restore application registers. another feature that makes pin ef cient is process attaching and detaching. like a debugger, pin can attach to a process, instrument it, collect pro les, and eventually detach. the application only incurs instrumentation overhead during the period that pin is attached. the ability to attach and detach is a necessity for the instrumentation of large, long running applications. pinjit based instrumentation defers code discovery until run time, allowing pin to be more robust than systems that use static instrumentation or code patching. the application observes the same addresses and same values as it would in an uninstrumented execution. for example, some applications unintentionally access data beyond the top of stack, so pin and the instrumentation do not modify the application stack. the recentlyreleased second generation, pin, extends the support to four architectures: ia, em, itanium, and arm. we rst give an overview of pininstrumentation capability in section #. in section #, we discuss two sample pintools used in practice. instrumentation can be performed at various stages: in the source code, at compile time, post link time, or at run time. this paper describes the design of pin and shows how it provides these features. pin can seamlessly handle mixed code and data, variable length instructions, statically unknown indirect jump targets, dynamically loaded libraries, and dynamically generated code. transparency makes the information collected by instrumentation more relevant and is also necessary for correctness. this paper presents an in depth description of pin, and is organized as follows. finally, we relate pin to other work in section # and conclude in section #. pin preserves the original application behavior by providing instrumentation transparency. we follow by discussing design and implementation issues in section #. pin provides a rich api that abstracts away the underlying instruction set idiosyncrasies, making it possible to write portable instrumentation tools. in addition to some standard techniques for dynamic instrumentation systems including code caching and trace linking, pin implements register reallocation, inlining, liveness analysis, and instruction scheduling to optimize jitted code. pin has been gaining popularity both inside and outside of intel, with more than downloads since pin was rst released in july year#. we then evaluate in section # the performance of pininstrumentation and compare it against other tools. thus we propose dynamic performance tuning mechanisms that determine where and how to create speculative threads at runtime. the creation of speculative threads is adjusted based on the estimation. in response to the emergence of multicore processors, various novel and sophisticated execution models have been introduced to fully utilize these processors. one such execution model is thread level speculation, which allows potentially dependent threads to execute speculatively in parallel. while tls offers significant performance potential for applications that are otherwise non parallel, extracting efficient speculative threads in the presence of complex control flow and ambiguous data dependences is a real challenge. this task is further complicated by the fact that the performance of speculative threads is often architecture dependent, input sensitive, and exhibits phase behaviors. this paper describes the design, implementation, and evaluation of hardware and software support that takes advantage of runtime performance profiles to extract efficient speculative threads. in our proposed framework, speculative threads are monitored by hardware based performance counters and their performance impact is estimated. this paper proposes speculative threads performance estimation techniques, that are capable of correctly determining whether speculation can improve performance for loops that corresponds to of total loop execution time across all benchmarks. this paper also examines several dynamic performance tuning policies and finds that the best tuning policy achieves an overall speedup of on a set of benchmarks from spec suite, which outperforms static thread management by. if no dependence is violated, the speculative thread commits; otherwise, it is squashed and re executed. speculative threads empower the compiler to parallelize programs that were previously non parallelizable. however, it is dif cult and sometimes impossible for compilers to accurately estimate the performance impact of speculative threads even when extensive pro le information is available. for loops with complex control ow, such as the loop shown in figure #, it is dif cult to determine whether the load store instructions in consecutive iterations will be dependent, and if so, which stores in the later thread will occur before the load in the earlier thread. in particular, interaction between the speculative threads and the cache components has a profound impact on performance. when the program executes with the three ref input sets source, graph, and program, we found that the percentage of total execution cycles that are wasted due to speculation failure was, and, respectively. speculative threads experience phase behavior: for some applications, it has been reported that the same codes may exhibit different performance characteristics as the program enters different phases of execution. we refer to this behavior as phase behavior. in the context of tls, phase behavior can be manifested as changing the effectiveness of speculative threads speculative threads that improve performance during certain phases of execution, can potentially degrade performance during other phases of execution. probabilistic pro les cannot capture this behavior, as speculative decisions that are compiled statically into the binary cannot adapt to this behavior. if the execution of speculative threads can be monitored, it is possible for the runtime system to determine the impact of the above factors. to dynamically tune performance for speculative threads, our proposed framework addresses the following issues: thread monitoring: the performance of speculative threads must be collected dynamically. the pro le can be obtained through either software instrumentation or hardware performance monitor sampling. in this paper, hardware based performance counters are used. thread evaluation: once the pro le is collected through hardwarebased performance counters, the pro le is analyzed to determine whether the speculative threads are effective. by evaluating these policies, we identify important runtime information and compiler annotations that could substantially improve the ef ciency of dynamic performance tuning. the rest of this paper is organized as follows: we rst describe the compiler and simulation infrastructure in section #, then describe how the ef ciency of speculative threads can be estimated from execution cycle breakdowns collected using hardware based performance counters in section #. section # describes necessary runtime support for pro le collection and decision making. multicore processors have been on the roadmap of most major processor manufacturers for some time. numerous execution models have been proposed to facilitate the utilization of these cores. many models, such as thread level speculation and transactional memory, require support for coarse grain speculation and speculation failure recovery. here we focus on one such execution model thread level speculation. under tls, parallel threads that are potentially dependent on each other are speculatively executed in parallel and the correctness of the execution is veri ed at runtime. the most straightforward way to parallelize a program is to execute multiple iterations of a loop in parallel. with tls, the loop in figure # can be parallelized by the compiler without proving whetherpoints to the same memory location asfrom a different iteration. figure # demonstrates the speculative parallel execution of the loop on a four processor cmp that supports tls, where each thread corresponds to a single iteration of the loop. speculation will succeed as long as no loads execute out of order with respect to a store to the same address by a logically earlier thread. existing work on tls mostly relies on compilers to statically analyze programs, and then extract speculative threads, which we refer to as static thread management. these compilers often analyze extensive pro le information to estimate the performance impact of speculative threads and then determine where and how to create speculative threads. being able to perform global and even inter procedure analyses, compilers can extract coarse grain parallelism in which speculative threads may contain several thousand instructions. if improperly deployed, speculative threads not only waste resources, but can also lead to signi cant performance degradation. we have identi ed four key issues that make it dif cult for the compiler to determine the ef ciency of speculative threads: pro ling information cannot accurately represent complex control and data dependences: the performance impact of speculation is determined by the number of useful execution cycles in the speculative threads that can overlap with the execution of the nonspeculative thread. to determine this overlap, the compiler must determine the size of consecutive speculative threads, the cost of speculation failures and the time of their occurrence, the cost of synchronization, and the cost of managing speculation. however, these factors are often dif cult to estimate even with accurate pro ling information. for example: the rate of speculation failures not only depends on the number of inter thread data dependences, but also on the timing of its occurrence. probability based data and control dependence pro ling, which is used in many tls compilers, is insuf cient to come up with such an estimation. performance impact of speculative threads depends on the underlying hardware con guration: because speculative threads must share the underlying hardware resources, the con guration of the underlying hardware can change the behaviors of these threads. on the one hand, speculative threads, even when they fail, can potentially bring data items into the cache and improve the performance of the non speculative threads. on the other hand, speculative threads can modify data items that are shared with the non speculative threads and introduce coherence misses that otherwise do not exist. the impact of such cache behavior is dif cult for the compiler to determine even with accurate pro le information. speculative threads behaviors vary as input sets change: the performance of speculative threads is often dependent on the characteristics of the input data. tls takes advantage of probabilistic data dependences by speculatively assuming that these dependences do not exist. this mechanism is bene cial only if these data dependences are infrequent. choosing the threshold that separates frequent and infrequent dependences is a delicate matter, since a high threshold leads to excessive speculation failures, and a low threshold leads to serialization of the threads. however, once this threshold is chosen and the set of frequently occurring dependences are synchronized, this decision is compiled into the binary, even if the decision is not proper for some input sets. when extracting speculative threads for bzip in spec, we collected pro les using the train input set and decided which dependences to speculate on. to summarize, speculative threads that are created to improve performance under one workload can potentially degrade performance when the input set changes. for example, in algorithms that search for the maximum or minimum in a large data set, the frequency of updating the global variables decreases as the algorithm progresses. for speculative execution earlier in the program can become a good candidate during later phases of the execution. the impact of speculative threads on the application performance is multi fold: they can commit speculative work, move data between various memory and cache components, and compete for shared resources with the non speculative thread. a crucial task in creating speculative threads is to allocate work to each thread, taking into consideration inter thread dependences and resource requirements. since such decisions are not unique and it is dif cult for the compiler to make decisions that are optimal for all programs under all workloads on a large variety of machines, we propose to build a runtime system to make such decisions. in particular, for speculative threads that are parallelizing iterations of loops in a loop nest, the runtime system decides which loop level to parallelize. such pro les can be application dependent, such as loop iteration count; architecture dependent, such as memory access latency; or both, such as branch misprediction rate and cache miss rate. a small piece of code that initializes these counters is executed at the beginning of execution by modifying a libc entry point routine named libc start main. this mechanism has been proposed by lu et al. the performance impact of speculative threads is often multi faceted. the interaction between the main and the speculative threads also complicates the estimation of performance. for instance, data brought in by the speculative threads can be used by the main thread but can also displace useful data needed by the main thread. the results of this analysis can be stored in a performance table that can be maintained by either hardware or software. thread management: once the effectiveness of speculative threads is determined, the dynamic performance tuning system can decide whether to create speculative threads and save this decision in a hardware software decision table. at the beginning of each candidate loop, the runtime system queries the decision table and decides whether a speculative thread should be created. contributions and future extensions this paper explores the feasibility and effectiveness of performance tuning systems that dynamically adjust the behavior of speculative threads based on runtime performance counters. under the context of dynamic performance tuning for tls, this paper makes the following contributions: we propose an execution framework that allows the runtime system to collect speculative thread performance pro les and make decisions on exploiting loop level parallelism for tls on the y. we propose and evaluate dynamic performance evaluation methodologies that analyze accurate execution cycle breakdown of speculative threads to determine the ef ciency of those threads. we also discuss how hardware counters could be programmed and utilized to collect the required execution cycle breakdown. we propose, implement and evaluate various dynamic performance tuning policies to adjust the exploitation of speculative threads based on the performance pro le. section # evaluate the proposed system with a spectrum of performance tuning policies. finally we present our conclusions and future work in section #. phase change is a natural phenomenon in real world applications, and can occur as a result of ordinary programming constructs. therefore, dynamically managing speculative threads can be an attractive alternative. this paper focuses on the problem of how to find and effectively exploit speculative thread level parallelism. our studies show that speculating only on loops does not yield sufficient parallelism. we propose the use of speculative procedure execution as a means to increase the available parallelism. an additional technique, data value prediction, has the potential to greatly improve the performance of speculative execution. in particular, return value prediction improves the success of procedural speculation, and stride value prediction improves the success of loop speculation. chip multiprocessors with thread level speculation have become the subject of intense research. however, tls is suspected of being too energy inefficient to compete against conventional processors. to do so, we first identify the main sources of dynamic energy consumption in tls. then, we present simple energy saving optimizations that cut the energy cost of tls by over on average with minimal performance impact. the resulting tls cmp, populated with four issue cores, speeds up full specint year# codes by on average, while keeping the fraction of the chip energy consumption due to tls to only. compared to a issue superscalar at the same frequency, the tls cmp is on average faster, while consuming only of its total on chip power. while architects understand how to build cost effective parallel machines across a wide spectrum of machine sizes, the real challenge is how to easily create parallel software to effectively exploit all of this raw performance potential. one promising technique for overcoming this problem is thread level speculation, which enables the compiler to optimistically create parallel threads despite uncertainty as to whether those threads are actually independent. in this paper, we propose and evaluate a design for supporting tls that seamlessly scales to any machine size because it is a straightforward extension of writeback invalidation based cache coherence. our experimental results demonstrate that our scheme performs well on both single chip multiprocessors and on larger scale machines where communication latencies are twenty times larger. finally, high end machines have long exploited parallel processing. machines which can simultaneously execute multiple parallel threads are becoming increasingly commonplace on a wide variety of scales. for example, techniques such as simultaneous multithreading and single chip multiprocessing suggest that thread level parallelism may become increasingly important even within a single chip. beyond chip boundaries, even personal computers are often sold these days in two or fourprocessor con gurations. perhaps the greatest stumbling block to exploiting all of this raw performance potential is our ability to automatically convert single threaded programs into parallel programs. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. isca vancouver, british columbia canada copyright year# acm ow and memory access patterns. in particular, it is the fact that memory addresses are dif cult to statically predict in part because they often depend on run time inputs and behavior that makes it extremely dif cult for the compiler to statically prove whether or not potential threads are independent. one architectural technique which may help us overcome this problem is thread level speculation. thread level speculation thread level speculation allows the compiler to automatically parallelize portions of code in the presence of statically ambiguous data dependences, thus extracting parallelism between whatever dynamic dependences actually exist at run time. to illustrate how tls works, consider the simple while loop in figure # which accesses elements in a hash table. this loop cannot be statically parallelized due to possible data dependences through the array hash. while it is possible that a given iteration will depend on data produced by an immediately preceding iteration, these dependences may in fact be infrequent if the hashing function is effective. hence a mechanism that could speculatively execute the loop iterations in parallel while squashing and reexecuting any iterations which do suffer dependence violations could potentially speed up this loop signi cantly, as illustrated in figure #. here a read after write data dependenceviolation is detected between epoch and epoch; hence epoch is squashed and restarted to produce the correct result. this example demonstrates the basic principles of tls it can also be applied to regions of code other than loops. in this example we assume that the program is running on a shared memory multiprocessor, and that some number of processors have been allocated to the program by the operating system. each of these processors is assigned a unit of work, or epoch, which in this case is a single loop iteration. we timestamp each epoch with an epoch number to indicate its ordering within the original sequential execution of the program. we say that epochis logically earlier than epochif their epoch numbers indicate that epochshould have preceded epochin the original sequential execution. any violation of the data dependences imposed by this original program order is detected at runtime through our tls mechanism. finally, when an epoch is guaranteed not to have violated any data dependences with logicallyearlier epochs and can therefore commit all of its speculative modi cations, we say that the epoch is homefree. we provide this guarantee by passing a homefree token at the end of each epoch. further examples of the use of thread level speculation, and an ex ploration of the interface between tls hardware and software, can be found in an earlier publication. hash; hash; execution using thread level speculation processor processor processor processor epoch epoch epoch. violation hash etc hash etc hash hash etc hash. attempt commit attempt commit attempt commit attempt commit redo epoch time. related work knight was the rst to propose hardware support for a form of thread level speculation; his work was within the context of functional languages. the multiscalar architecture was the rst complete design and evaluation of an architecture for tls. there have since been many other proposals which extend the basic idea of thread level speculation. in nearly all of these cases, the target architecture has been a very tightly coupled machine eg, one where all of the threads are executed within the same chip. these proposals have often exploited this tight coupling to help them track and preserve dependences between threads. for example, the stanford hydra architecture uses special write buffers to hold speculative modi cations, combined with a write through coherence scheme that involves snooping these write buffers upon every store. while such an approach may be perfectly reasonable within a single chip, it was not designed to scale to larger systems. one exception is a proposal by zhang et al for a form of tls within large scale numa multiprocessors. while this approach can potentially scale up to large machine sizes, it has only been evaluated with matrix based programs, and its success in handling pointer based codes has yet to be demonstrated. in addition, it does not appear to be a good choice for small scale machines. concurrent with our study, cintra et al have proposed using a hierarchy of mdts to support tls across a numa multiprocessor comprised of speculative chip multiprocessors. while there are many subtle differences between our respective approaches, perhaps the most striking difference is that their hardware enforces a hierarchical ordering of the threads, with one level inside each speculative multiprocessor chip and another level across chips. in contrast, since we separate ordering from physical location through explicit software managed epoch numbers and integrate the tracking of dependence violations directly into cache coherence, our speculation occurs along a single at speculation level, and does not impose any ordering or scheduling constraints on the threads. objectives of this study the goal of this study is to design and evaluate a uni ed mechanism for supporting thread level speculation which can handle arbitrary memory access patterns and which is appropriate for any scale of architecture with parallel threads, including: simultaneous multithreaded processors, single chip multiprocessors, more traditional shared memory multiprocessors of any size, and even multiprocessors built using software distributed shared memory. our approach scales to all of these architectures because it is built upon writeback invalidation based cache coherence, which itself scales to any of these machines. our uni ed approach to supporting thread level speculation offers the following advantages. first, we could build a large scale parallel machine using either single chip multiprocessors or simultaneouslymultithreaded processors as the building blocks, and seamlessly perform thread level speculation across the entire machine. second, once we compile a program to exploit thread level speculation, it can run directly on any of these machines without being recompiled. we demonstrate this in our our experimental results: the same executables exploit our uni ed thread level speculation mechanism to achieve good speedup not only on a single chip multiprocessor, but also on multi chip multiprocessors. the remainder of this paper is organized as follows. section # describeshow invalidation basedcachecoherencecan beextended to detect data dependence violations, and section # gives a possible hardware implementation of this scheme. we describe our experimental framework in section #, evaluate the performance of our scheme in section #, and conclude in section #. to improve the performance of a single application on chip multiprocessors, the application must be split into threads which execute concurrently on multiple cores. in multi threaded applications, critical sections are used to ensure that only one thread accesses shared data at any given time. in acs, selected critical sections are executed by a high performance core, which can execute the critical section faster than the other, smaller cores. as a result, acs reduces serialization: it lowers the likelihood of threads waiting for a critical section to finish. our evaluation on a set of critical section intensive workloads shows that acs reduces the average execution time by compared to an equal area core symmetric cmp and by compared to an equal area acmp. moreover, for out of the workloads, acs improves scalability by increasing the number of threads at which performance saturates. critical sections can serialize the execution of threads, which significantly reduces performance and scalability. this paper proposes accelerated critical sections, a technique that leverages the high performance core of an asymmetric chip multiprocessor to accelerate the execution of critical sections. modern microprocessors achieve high performance by exploiting instruction level parallelism in sequential programs. they establish a large dynamic window of instructions and employ wide issue organizations to extract ilp and execute multiple instructions simultaneously. larger windows enable more dynamic instructions to be examined, which leads to the identification of more inde pendent instructions that can be executed by wider proces sors. however, large centralized hardware structures for larger windows and wider processors may be harder to engineer at high clock speeds due to quadratic wire delays, limiting overall performance. the multiscalar architecture, stanford hydra, cmu stampede, and minnesota super threaded architecture. in section #, we describe our compiler heuristics to select tasks with favorable character year# ieee istics. in section #, we analyze the effects of the various heuristics on overall performance and present measure ments of the key task characteristics. we draw some con clusions in section #. with the prevalence of server blades and systems on a chip, interconnection networks are becoming an important part of the microprocessor landscape. however, there is limited tool support available for their design. while performance simulators have been built that enable performance estimation while varying network parameters, these cover only one metric of interest in modern designs. system power consumption is increasingly becoming equally, if not more important than performance. it is now critical to get detailed power performance tradeoff information early in the microarchitectural design cycle. this is especially so as interconnection networks consume a significant fraction of total system power. it is exactly this gap that the work presented in this paper aims to fill we present orion, a power performance interconnection network simulator that is capable of providing detailed power characteristics, in addition to performance characteristics, to enable rapid power performance trade offs at the architectural level. this capability is provided within a general framework that builds a simulator starting from a microarchitectural specification of the interconnection network. using component power models and a synthesized efficient power simulator, a microarchitect can rapidly explore the design space. as case studies, we demonstrate the use of orion in determining optimal system parameters, in examining the effect of diverse traffic conditions, as well as evaluating new network microarchitectures. in each of the above, the ability to simultaneously monitor power and performance is key in determining suitable microarchitectures. a key component of this construction is the architectural level parameterized power models that we have derived as part of this effort. multiprocessor operating systems pose several unique and conflicting challenges to system virtual machines. for example, most existing system vms resort to gang scheduling a guest os virtual processors to avoid os synchronization overhead. however, gang scheduling is infeasible for some application domains, and is inflexible in other domains in an overcommitted environment, an individual guest os has more vcpus than available physical processors, precluding the use of gang scheduling. in such an environment, we demonstrate a more than two fold increase in runtime when transparently virtualizing a chip multiprocessor cores. to combat this problem, we propose a hardware technique to detect several cases when a vcpu is not performing useful work, and suggest preempting that vcpu to run a different, more productive vcpu. our technique can dramatically reduce cycles wasted on os synchronization, without requiring any semantic information from the software we then present a case study, typical of server consolidation, to demonstrate the potential of more flexible scheduling policies enabled by our technique. we propose one such policy that logically partitions the cmp cores between guest vms. this policy increases throughput by for consolidated server workloads due to improved cache locality and core utilization, and substantially improves performance isolation in private caches. multiple clock domain processors are a promising future alternative to today fully synchronous designs. dynamic voltage and frequency scaling in an mcd processor has the extra flexibility to adjust the voltage and frequency in each domain independently. most existing dvfs approaches are profile based offline schemes which are mainly suitable for applications whose execution char acteristics are constrained and repeatable. while some work has been published about online dvfs schemes, the prior approaches are typically heuristic based. in this paper, we present an effective online dvfs scheme for an mcd processor which takes a formal analytic approach, is driven by dynamic workloads, and is suitable for all applications. in our approach, we model an mcd processor as a queue domain network and the online dvfs as a feedback control problem with issue queue occupancies as feedback signals. a dynamic stochastic queuing model is first proposed and linearized through an accu rate linearization technique. a controller is then designed and verified by stability analysis. finally we evaluate our dvfs scheme through a cycle accurate simulation with a broad set of applications selected from mediabench and spec benchmark suites. compared to the best known prior approach, which is heuristic based, the proposed online dvfs scheme is substantially more effective due to its automatic regulation ability. for example, we have achieved a fold increase in efficiency in terms of energy delay product improvement. in addition, our control theoretic technique is more resilient, requires less tuning effort, and has better scalability as compared to prior online dvfs schemes we believe that the techniques and methodology described in this paper can be generalized for energy control in processors other than mcd, such as tiled stream processors. due to the trends of increasing clock speed and increasing scale of integration, two fundamental problems are present in todaysynchronous microprocessor designs. first, global clocking distribution with low clock skew is becoming increasingly dif cult. second, power consumption is becoming a limiting factor to performance. one possible solution to these problems is a multiple clock domain processor, which uses the globally asynchronous and locally synchronous clocking style. an mcd processor has less clock distribution and skew burden. it is also more power ef cient due to the absence of a global clock distribution tree. from an mcd processor is the extra exibility in dynamic voltage and frequency scaling, as the frequency and voltage in each function block or domain can be adjusted independently. therefore, dvfs in an mcd processor can achieve better energy savings than dvfs in a traditional synchronous processor. the goal of this work is to design an analytic online dvfs scheme for an mcd processor. we categorize existing dvfs work along two dimensions. the rst dimension is the level of dynamism. the level of dynamism ranges from low: pro le based of ine schemes, to high: online schemes driven by dynamic workloads. most existing dvfs schemes are of ine, for example. they typically use pro ling to do of ine analysis to obtain optimal dvfs settings for some set of assumptions about program data sets and execution model. then either a compiler or a binary editor is used to write the dvfs con guration into the application source. naturally, the effectiveness of a pro le based dvfs scheme depends on how similar the run time characteristics are to the of ine analysis at pro le time. therefore, dvfs schemes in this group are best suited to special purpose applications, such as multimedia applications, whose run time characteristics are constrained and repeatable. in contrast, in our work we propose an online dvfs scheme which takes runtime information directly from the processor to infer characteristics of the application dynamically, and is thus suited to all kinds of applications. the other dimension for categorizing existing dvfs schemes is the level of formalism. the level of formalism ranges from purely heuristic based to formal analytic schemes. although many of ine schemes take mathematical optimization based formal approaches, nearly all existing online dvfs schemes are heuristic based. they typically include a set of manually selected rules and threshold values. at run time, certain processor metrics, such as cache miss rate or queue occupancy, are monitored. these metrics are then compared to the threshold values and one of the rules is applied depending on the result of the comparison. the best known online dvfs scheme for an mcd processor is the attackdecay algorithm by semeraro et al. however, there are signi cant limitations in heuristicbased schemes. first, for a result obtained from a given set of rules and parameters, it is not analytically clear how to further improve it and thus make the dvfs more effective. second, the trial anderror tuning process for parameters is very time consuming. third, it is generally hard to scale the heuristics for a large system, as the number of rules and the tuning effort required can grow exponentially. to overcome the above limitations, the online dvfs scheme in this paper takes a rigorous analytical approach. we model an mcd processor as a queue domain network. the online dvfs problem is formulated as a feedback control problem with issue queue occupancies as feedback signals. speci cally, a stochastic model is proposed for the queue domain dynamics. since the queue domain system is inherently nonlinear, we rst linearize the system through an accurate feedback linearization. the controller is then designed and veri ed by stability analysis. next, a possible hardware implementation of the controller is described. finally the proposed online dvfs scheme is evaluated by a cycle accurate architecture simulator with a broad set of applications selected from the mediabench and spec benchmark suites. overall, we achieve a power saving to performance degradation ratio of, as compared to a ratio of for the conventional synchronous voltage scaling. compared to the best known prior online dvfs approach for an mcd processor which is heuristic based, the proposed dvfs scheme has several signi cant advantages. first, the analytic online dvfs scheme is more effective in terms of energy saving for the same level of performance degradation. experimental results show we achieve an energy delay product improvement higher than that of the best known heuristic based online scheme in, and higher than the semi oracle based dvfs scheme in. we attribute this promising result to the automatic frequency voltage regulation ability in the proposed dvfs controller, which leads to a more effective and precise decision on when, where, and how much to scale. in addition, the proposed analytic online dvfs scheme requires less tuning effort than heuristic based dvfs as the analytic dvfs chooses its control parameters through stability analysis. furthermore, the analytic dvfs scheme is more scalable we do not have to re tune parameters or re set rules in order to include new resources. also, we can extend it to a global and centralized dvfs scheme, which will handle interactions among multiple clock domains. overall, we feel that the primary contribution of this work is threefold. first, our focus is on voltage frequency control for mcd architectures, which are relatively new and have great potential in terms of energy saving and performance improvement. second, the proposed control theoretic techniques applied to dvfs in mcd processors have led to a fold increase in ef ciency compared to the best known previous heuristic based dvfs scheme in mcd processors. in addition, our control theoretic technique is more resilient, complete, and boundable as well. for example, we can guarantee stability, achieve signi cant energy savings, and offer more graceful degradation even under extreme cases. third, this is one of the rst rigorous analytic approaches to dvfs control. previous control theoretic techniques exist, but only for multimedia processors with predictable workloads, while our work is for general workloads. the rest of the paper is structured as follows. in section #, we give a brief review of the mcd processor design and implementation. section # describes the modeling, design, and analysis of our figure #: the clock domain partitions in an mcd processor by semeraro et al online dvfs controller. this is followed in section # by experimental results for the purpose of evaluation. in section #, we give a general outline of centralized dvfs design. dynamic voltage and frequency scaling is an effective technique for controlling microprocessor energy and performance. existing dvfs techniques are primarily based on hardware, os timeinterrupts, or static compiler techniques. however, substantially greater gains can be realized when control opportunities are also explored in a dynamic compilation environment. there are several advantages to deploying dvfs and managing energy performance tradeoffs through the use of a dynamic compiler. most importantly, dynamic compiler driven dvfs is fine grained, code aware, and adaptive to the current microarchitecture environment. this paper presents a design framework of the run time dvfs optimizer in a general dynamic compilation system. a prototype of the dvfs optimizer is implemented and integrated into an industrialstrength dynamic compilation system. the obtained optimization system is deployed in a real hardware platform that directly measures cpu voltage and current for accurate power and energy readings. experimental results, based on physical measurements for over spec or olden benchmarks, show that significant energy savings are achieved with little performance degradation. specfp benchmarks benefit with energy savings of up to. in addition, specint show up to energy savings, spec fp save up to, and olden save up to. on average, the technique leads to an energy delay product improvement that is better than static voltage scaling, and is more than better than the reported dvfs results of prior static compiler work. while the proposed technique is an effective method for microprocessor voltage and frequency control, the design framework and methodology described in this paper have broader potential to address other energy and power issues such as di dt and thermal control. dynamic voltage and frequency scaling is an effective technique for microprocessor energy and power control and has been implemented in many modern processors. current practice for dvfs control is os based power scheduling and management. in this work, we focus instead on more ne grained intra task dvfs, in which the voltage and frequency can be scaled during program execution to take advantage of the application phase changes. while signi cant research efforts have been devoted to the intratask dvfs control, most of them are based on hardware, os time interrupt, or static compiler techniques. very little has been done to explore dvfs control opportunities in a dynamic compilation or optimization environment. in this paper, we consider dynamic compiler dvfs techniques, which optimize the application binary code and insert dvfs control instructions at program execution time. a dynamic compiler is a run time software system that compiles, modi es, and optimizes a programinstruction sequence as it runs. in recent years, dynamic compilation is becoming increasingly important as a foundation for run time optimization, binary translation, and information security. since most dvfs implementations allow direct software control via mode set instructions, a dynamic compiler can be used to insert dvfs mode set instructions into application binary code at run time. if there exists cpu execution slack, these instructions will scale down the cpu voltage and frequency to save energy with no or little impact on performance. using dynamic compiler driven dvfs offers some unique features and advantages not present in other approaches. most importantly, it is more ne grained and more code aware than hardware or os interrupt based schemes. also it is more adaptive to the runtime environment than static compiler dvfs. in section #, we will give statistical results to further motivate dynamic compiler driven dvfs, and discuss its advantages and disadvantages. this paper presents a design framework of the run time dvfs optimizer in a dynamic compilation environment. key design issues that have been considered include code region selection, dvfs decision, and code insertion transformation. in particular, we propose a new dvfs decision algorithm based on an analytic dvfs decision model. a prototype of the rdo is implemented and integrated into an industrial strength dynamic optimization system. the obtained optimization system is deployed into a real hardware platform, that allows us to directly measure cpu current and voltage for accurate power and energy readings. the evaluation is based on experiments with phys average cache misses average memory bus transactions memory transactions year# year# qsort input size time figure #: average number of memory bus transactions for the function qsort with differ figure #: number of cache misses for every million instruc ent input sizes and different input patterns is accomplished for spec benchmarks. on average, the technique leads to energy delay product improvements of for spec fp, for specfp, for specint, and for olden benchmarks. these average results are x better than those from static voltage scaling, and are more than better than those reported by a static compiler dvfs scheme. overall, the main contributions of this paper are twofold. first, we have designed and implemented a run time dvfs optimizer, and deployed it on real hardware with physical power measurements. the optimization system is more effective in terms of energy and performance ef ciency, as compared to existing approaches. second, to our knowledge, this is one of the rst efforts to develop dynamic compiler techniques for microprocessor voltage and frequency control. a previous work provides a partial solution to the java method dvfs in a java virtual machine, while we provide a complete design framework and apply dvfs in a more general dynamic compilation environment with general applications. the structure for the rest of the paper is as follows. section # presents the design framework of the rdo. section # describes the implementation and deployment of the rdo system. this is followed by the experimental results in section #. finally, our conclusions are offered in section #. examples of dynamic compiler based infrastructures include hp dynamo, ibm daisy, intel ia el, and intel pin. while there have been many recent proposals for hardware that supports thread level speculation, there has been relatively little work on compiler optimizations to fully exploit this potential for parallelizing programs optimistically. in this paper, we focus on one important limitation of program performance under tls, which is stalls due to forwarding scalar values between threads that would otherwise cause frequent data dependences. we present and evaluate dataflow algorithms for three increasingly aggressive instruction scheduling techniques that reduce the critical forwarding path introduced by the synchronization associated with this data forwarding. in addition, we contrast our compiler techniques with related hardware only approaches. with our most aggressive compiler and hardware techniques, we improve performance under tls by for of applications, and by at least for half of the other applications. multithreading within a chip is becoming increasingly common place: examples include the ibm power, sun majc, alpha year#, hp pa, and sibyte bcm year#. while using this multithreaded hardware to improve the throughput of a workload is straightforward, using it to improve the performance of a single application requires parallelization. the ideal solution would be to convert sequential programs into parallel programs au tomatically, but unfortunately this is difficult for many general purpose programs due to their use of pointers, com plex data and control struetures, and run time inputs. thread level speculation is a potential solution to this problem since it allows the compiler to create parallel threads without having to prove that they are independent. the underlying hardware ensures that inter thread dependences through memory are satisfied, and re executes any thread for which they are not. the key to extracting parallelism from these programs and hence improving performance is in the efficiency of speculative execu tion. while recent research has investigated hardware optimization for tls, there has been relatively little work on compiler optimization in this area. one potential opportunity permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the firrst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. asplos san jose, ca, usa copyright year# acm. a wait wait do else a a; a signal; work; while; before instruction scheduling. figure #: impact of scheduling on the critical forwarding path. for optimization focuses on data dependences between speculative threads that occur frequently: if the compiler is able to identify the source and the destination of a frequent inter thread data depen dence, then it is beneficial to insert synchronization and forward that value explicitly to avoid failed speculation. figure shows an example loop that the compiler has speculatively parallelized by partitioning the loop into speculative threads. since the variable a is read and written in every iteration, the compiler de cides to synchronize and forward a by inserting a wat operation before the first use of, and a signal operation after the last def inition of a we describe, implement, and evaluate this algorithm in section #. the synchronization results in the partially parallel execution shown in figure, where each epoch stalls until the value of a is produced by the previous epoch. the flow of the value of a between epochs serializes the parallel execution, and so we re fer to it as a critical forwarding path. in the next section, we show that the overall performance of speculation is limited by the size of this critical forwarding path. ihh im to further reduce the critical forwarding path. for example, if a cer tain path is executed more frequently than alternative paths, then it is advantageous to speculatively schedule the critical forwarding path to exploit this fact. to illustrate, if the else clause is more frequently executed than the then clause in figure, we could schedule a a; signal; from the else clause above the fstructure to further shrink the critical forwarding path in the com mon case. thus our new schedule involves control speculation, and requires the ability to recover whenever our speculation is incorrect. similarly, we can schedule code from the critical forwarding path past ambiguous data dependences, given the additional hardware support to detect when such speculation has failed. we describe and evaluate schemes for scheduling the critical forwarding path using intra epoch control speculation and data dependence specu lation in section #. related work parallelization of a loop where the compiler synchronizes a loop carried data dependence is known as a doacross paral lelization and has been exploited in previous work. all schemes for tls support include some form of doacross syn chronization, although few use the compiler to optimize this aspect of speculative execution. the most relevant related work is the wisconsin multiscalar compiler, which performs synchronization and scheduling for register values. the multiscalar scheduler was designed with multiscalar tasks in mind, and these usually consist of a few basic blocks that do not contain procedure calls or loops. in contrast, our speculative threads are much larger on average than multiscalar tasks and contain complex control flow. this inspired the dataflow based scheduler presented in this paper, which can move instructions past inner loops and procedure calls. the mul tiscalar compiler does not schedule code beyond the point within a task where it is no longer critical, as determined by a simplified machine model; in contrast, because we believe that accurate de termination of this point at compile time is extremely difficult, we schedule producer instructions as early as possible. another dif ference is that our more general approach to scheduling handles loop induction variables automatically, rather than having to treat them as a special case. a final difference is that we evaluate the benefits of speculatively scheduling code past control and data dependences. we modified our scheduler to mimic the mul tiscalar scheduler, and we contrast the performance impact of both approaches later in section #. other schemes for tls hardware support provide the means to synchronize and forward values between speculatiw: threads but do not use the compiler to optimize loop inductionables or syn chronize frequent dependences, while others pro vide such support but do not schedule instructions to reduce the critical forwarding path. research on tls hardware sup port has shown the importance of the critical forwarding path and how the prediction of forwarded values may be used to increase parallelism; it also showed that hardware is ineffective at improving performance by scheduling the critical forwarding path. other hardware techniques for improving the efficiency of speculation include prediction of loads to memory, dynamic syn chronization, and squashing of silent stores. concurrent with our work, zilles and sohi recently pro posed decomposing a program into speculative threads by having a master thread execute a distilled version of the program that or chestrates and predicts values for slave threads. in this scheme, values are precomputed by the master thread and distributed to the slave threads. a potential advantage of this master slave approach is that it effectively removes interproces sor communication from the critical forwarding path. we note that the scheduling techniques that we present later in this paper could potentially be applied to the distilled code in the master thread. our algorithm for reducing the critical forwarding path builds upon previous dataflow approaches to code motion, namely partial redundancy elimination, path sensitive dataflow analysis, and hotpaths. previous work on speculative code motion to ex ploit a frequently executed path includes trace scheduling and superblock scheduling. there has also been work on aggres sive load store reordering where the runtime check and recovery are performed entirely in software or through a hybrid hard ware software approach. contributions in the context of thread level speculation, this work makes the following contributions. first, we show that the critical forward ing path is a significant performance bottleneck for many applica tions. second, we present novel dataflow scheduling algorithms for reducing the critical forwarding path, and show that schedul ing loop induction variables and other scalars results in significant performance gains for most applications. finally, we compare and contrast our compiler scheduling techniques with hardware tech niques for optimizing the critical forwarding path; comparison with hardware techniques for automatically synchronizing depen dences is beyond the scope of this paper. efficient inter thread value communication is essential for improving performance in thread level speculation. in contrast, value communication for frequently occurring data dependencesmust be very efficient in this paper, we propose using the compiler to first identifyfrequently occurring memory resident data dependences, then insert synchronization for communicating values to preserve thesedependences. we find that by synchronizing frequently occurringdata dependences we can significantly improve the efficiency ofparallel execution. a comparison between compiler inserted andhardware inserted memory synchronization reveals that the two techniques are complementary, with each technique benefitting different benchmarks.